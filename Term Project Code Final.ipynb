{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v52S4rZMHsK3"
      },
      "source": [
        "# Term Project Group 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceU4aZcRIdh8"
      },
      "source": [
        "The Dataset: https://www.kaggle.com/datasets/waqi786/global-black-money-transactions-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyyUVrF_IioY"
      },
      "source": [
        "### Explanation:\n",
        "\n",
        "This dataset gives a solid overview of black money transactions in different countries, focusing on financial activities tied to illegal dealings. It includes details like transaction amounts and risk scores, making it super useful for anyone looking to study financial crime trends or work on anti-money laundering tools.\n",
        "\n",
        "\n",
        "### Dataset:\n",
        "\n",
        "Transaction ID: Unique identifier for each transaction. (e.g., TX0000001)\n",
        "\n",
        "Country: Country where the transaction occurred. (e.g., USA, China)\n",
        "\n",
        "Amount (USD): Transaction amount in US Dollars. (e.g., 150000.00)\n",
        "\n",
        "Transaction Type: Type of transaction. (e.g., Offshore Transfer, Property Purchase)\n",
        "\n",
        "Date of Transaction: The date and time of the transaction. (e.g., 2022-03-15 14:32:00)\n",
        "\n",
        "Person Involved: Name or identifier of the person/entity involved. (e.g., Person_1234)\n",
        "\n",
        "Industry: Industry associated with the transaction. (e.g., Real Estate, Finance)\n",
        "\n",
        "Destination Country: Country where the money was sent. (e.g., Switzerland)\n",
        "\n",
        "Reported by Authority: Whether the transaction was reported to authorities. (e.g., True/False)\n",
        "\n",
        "Source of Money: Origin of the money. (e.g., Legal, Illegal)\n",
        "\n",
        "Money Laundering Risk Score: Risk score indicating the likelihood of money\n",
        "laundering (1-10). (e.g., 8)\n",
        "\n",
        "Shell Companies Involved: Number of shell companies used in the transaction. (e.g., 3)\n",
        "\n",
        "Financial Institution: Bank or financial institution involved in the transaction. (e.g., Bank_567)\n",
        "\n",
        "Tax Haven Country: Country where the money was transferred to a tax haven. (e.g., Cayman Islands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFAjkKdlH4LW"
      },
      "source": [
        "# Pre-process and clean the dataset as appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piBKVR5r6wGk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NfzVSt-I6wGk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "# from lightgbm import LGBMClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE, SelectFromModel, chi2, SelectKBest\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bNKeluqIu7D"
      },
      "source": [
        "## Exploring and visualizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgPt-hfO6wGk"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GTUfUTkk6wGl"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ericrlessa/global-illicit-money-transactions/refs/heads/main/Big_Black_Money_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "RM-m9W_C797X",
        "outputId": "c1c4ccbc-4b97-4c7c-c339-ac1078c9962c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualise some features\n",
        "plt.pie(df['Reported by Authority'].value_counts(), labels=df['Source of Money'].unique(), autopct='%1.1f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "XvQRDQYP83Aa",
        "outputId": "e81671e7-474f-4d80-dc47-6fc5e424c28e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='Shell Companies Involved', ylabel='Amount (USD)'>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHACAYAAABeV0mSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAACZl0lEQVR4nOzdd3xc5ZU//s/t02fURt1WcwF3sAFjUwIJIaQsm7Ipm0rY8M0aQkLa8tvNN5vdfEM2JAHSyCabJSEEkmUJgYQkdEyzKS7YYGPLktX7jKbP7ff3x0jCNi6SfEdzZ+a8Xy/9YXvm6pEszZz7nPOcw1iWZYEQQgghpESwhV4AIYQQQoidKLghhBBCSEmh4IYQQgghJYWCG0IIIYSUFApuCCGEEFJSKLghhBBCSEmh4IYQQgghJYWCG0IIIYSUFApuCCGEEFJSKLghhBBCSEkp6+Dm6aefxrvf/W40NDSAYRj84Q9/mPM1LMvCd7/7XSxduhSSJKG5uRnf+ta37F8sIYQQQmaFL/QCCimdTmPNmjX41Kc+hfe9733zusb111+PRx55BN/97nexatUqxONxTExM2LxSQgghhMwWQ4MzcxiGwf33348rr7xy5u9UVcW//Mu/4De/+Q1isRhWrlyJ//iP/8DFF18MANi/fz9Wr16NV199FcuWLSvMwgkhhBBylLJOS53Kpz71KTz33HP47W9/iz179uADH/gALr/8cnR2dgIA/vjHP6KtrQ1/+tOf0NraipaWFlx99dWIRqMFXjkhhBBSvii4OYGuri7cc889uPfee3HBBRegvb0dX/rSl7B582bccccdAIDu7m709vbi3nvvxZ133olf/vKX2LFjB97//vcXePWEEEJI+SrrmpuT2blzJyzLwtKlS4/6e0VRUFVVBQAwTROKouDOO++cedwvfvELnH322Thw4AClqgghhJACoODmBEzTBMdx2LFjBziOO+rffD4fAKC+vh48zx8VAJ1xxhkAgL6+PgpuCCGEkAKg4OYE1q1bB8MwMDY2hgsuuOC4j9m0aRN0XUdXVxfa29sBAAcPHgQALF68eMHWSgghhJA3lPVpqVQqhUOHDgHIBTPf//738Za3vAWVlZVYtGgRPvrRj+K5557D9773Paxbtw4TExN44oknsGrVKlxxxRUwTRMbNmyAz+fDrbfeCtM0sWXLFgQCATzyyCMF/uoIIYSQ8lTWwc1TTz2Ft7zlLW/6+0984hP45S9/CU3T8M1vfhN33nknBgcHUVVVhY0bN+Ib3/gGVq1aBQAYGhrCddddh0ceeQRerxfveMc78L3vfQ+VlZUL/eUQQgghBGUe3BBCCCGk9NBRcEIIIYSUFApuCCGEEFJSyu60lGmaGBoagt/vB8MwhV4OIYQQQmbBsiwkk0k0NDSAZU++N1N2wc3Q0BCam5sLvQxCCCGEzEN/fz+amppO+piyC278fj+A3DcnEAgUeDWEEEIImY1EIoHm5uaZ9/GTKbvgZjoVFQgEKLghhBBCisxsSkqooJgQQgghJYWCG0IIIYSUFApuCCGEEFJSKLghhBBCSEmh4IYQQgghJYWCG0IIIYSUFApuCCGEEFJSKLghhBBCSEmh4IYQQgghJYWCG0IIIYSUFApuCCGEEFJSKLghhBBCSEmh4IYQQgghJYWCG1LSZM2AZVmFXgYhhJAFRMENKVkZVcerg3FMZrRCL4UQQsgCouCGlCTLstAXyWA4nkU8oxZ6OYQQQhYQBTekJEXSKgYms3AJPMaSCkyTUlOEEFIuKLghJUczTPRMpMEwQJVXRErVkVT0Qi+LEELIAilocHPTTTdhw4YN8Pv9CIfDuPLKK3HgwIGTPueTn/wkGIZ508eKFSsWaNXE6YZjWUykFFR5JQgcC80wkchS3Q0hhJSLggY3W7duxZYtW7B9+3Y8+uij0HUdl112GdLp9Amfc9ttt2F4eHjmo7+/H5WVlfjABz6wgCsnTpVSdPREMvBLAjiWAQC4OA4TKYVOTRFCSJngC/nJ//rXvx715zvuuAPhcBg7duzAhRdeeNznBINBBIPBmT//4Q9/wOTkJD71qU/lda3E+XJFxGlkNR0NQc/M33tEHomshqxmwCMW9EeeEELIAnBUzU08HgcAVFZWzvo5v/jFL/DWt74VixcvzteySJEYTykYjMmo8kpH/b1LYCHrBhJZqrshhJBy4JjbWMuycMMNN2Dz5s1YuXLlrJ4zPDyMv/zlL7j77rtP+BhFUaAoysyfE4nEaa+VOI+q54qIeZaBxHNH/RvDMOAYFtG0grqgq0ArJIQQslAcs3Nz7bXXYs+ePbjnnntm/Zxf/vKXCIVCuPLKK0/4mJtuumkmlRUMBtHc3GzDaonTDMUyiKRUVHjE4/67V+IRTatQdGOBV0YIIWShOSK4ue666/Dggw/iySefRFNT06yeY1kW/vu//xsf+9jHIIrHf0MDgBtvvBHxeHzmo7+/365lE4dIyBp6IxmE3OJMEfGxPCKHjEqpKUIIKQcFTUtZloXrrrsO999/P5566im0trbO+rlbt27FoUOH8OlPf/qkj5MkCZIknfQxpHiZpoWeiTRk3USl98T/zyyTC3riWRU1fvp5IISQUlbQnZstW7bgrrvuwt133w2/34+RkRGMjIwgm83OPObGG2/Exz/+8Tc99xe/+AXOPffcWdfnkNI0nlIwEpdR4zt1wOIReUykVBjUrZgQQkpaQYOb22+/HfF4HBdffDHq6+tnPn73u9/NPGZ4eBh9fX1HPS8ej+O+++475a4NKW2yZuDwRBoix0LgTv2j7BE5pGQdSZka+hFCSCkreFrqVH75y1++6e+CwSAymUweVkSKyeBkFrGMivqge1aPFzgWumkhkdUROkHhMSGEkOLniIJiQuYqntHQH80VEU/X08yGW+AwlpSpWzEhhJQwCm5I0TFMCz2RNDTThFea2+bjdGoqrdKRcEIIKVUU3JCiM5qQMRLPvqkT8Wy4BA4KDdIkhJCSRsENKSqyZqA3koZb4GdVRHw8PMtgIqWc+oGEEEKKEgU3pKj0RzOIZzWEPMK8r+GTeMQyGmSNUlOEEFKKKLghRSOaVtE/mUGFRwQzhyLiY7kFDllVp9QUIYSUKApuSFHQDRO9kTQsK9eM73QwDAOOZRHLUHBDCCGliIIbUhRGEjJGEwoqbepP4xY4TKQUaIZpy/UIIYQ4BwU3xPEyqo6eSAY+iQc/zyLiY3klHmnFQFKmQZqEEFJqKLghjmZZFvoiGaRkHQGXfQ21OZaBCRPxjGrbNQkhhDgDBTfE0SJpFQOxLKq8p1dEfDxugcd4SoFJgzQJIaSkUHBDHEszTPRMpMEg13zPbl6RR0rRkVIpNUUIIaWEghviWMOxLCZSyrw6Ec+GyLNQqVsxIYSUHApuiCOllFwRsV8SwLH2pqOOJHEcIimquyGEkFJCwQ1xnFwRcRpZTUfAPf9OxLPhFXnEMioylJoihJCSQcENcZzxlILBmJy3dNSRXAILWTeRyFJwQwghpYKCG+Ioqp4rIuZZBhJvfxHxsRiGAccwiGZokCYhhJQKCm6IowzFMoimVVTY1Il4Nrwij2hag6LTIE1CCCkFFNwQx0jIGnojGQRdYl6LiI/lFjlkFJ26FRNCSImg4IY4gmla6J3IQNZN+GzsRDwbHMvAAhCjbsWEEFISKLghjjCeUjAcz6LGl/8i4uPxijwmUioM6lZMCCFFj4IbUnCyZuDwRBoix0KwaTDmXHlELtetmFJThBBS9Ci4IQU3OJlFLKOiwrtwRcTHEjgWhmkhIVO3YkIIKXYU3JCCimc09EczCLlFsDYPxpwriWMxlpRhWZSaIoSQYkbBDSkYw7TQE0lDM014pYUtIj4er8QjmdWRVulIOCGEFDMKbkjBjCVljMSzC9KJeDZcAgdFN2iQJiGEFDkKbkhByJqBnok03AJfsCLi4+E5FpE0dSsmhJBi5px3FVJW+qMZxLMaQp78Dsacq9wgTQ2yRqkpQggpVhTckAUXTavon8ygwiOCKXAR8bGmuxXTqSlCCCleFNyQBaUbJnojaZgm4BELX0R8LJZhwLIMYmkKbgghpFhRcEMW1EhCxmhCQVUBe9qcikfgMZFSoBtmoZdCCCFkHii4IQsmo+roiWTgk3jwDioiPpZX4pFRDRqkSQghRcq57zCkpFiWhb5IBilZR2CBB2POFccyMCwT8SwN0iSEkGJEwQ1ZEJG0ioFYFlVe5xURH4+b5zGeUmHSIE1CCCk6FNyQvNOmiogZ5BrlFQOvxCMpa0iplJoihJBiQ8ENybvhWBbjScUxnYhnQ+RZaLpF3YoJIaQIUXBD8iql6OiNZuCXBHCs89NRRxJ5FpEU1d0QQkixoeCG5E2uiDiNjKoj4HZWJ+LZ8IgcYlkVWRqkSQghRYWCG5I34ykFQzG5qNJRR3ILHLKqSd2KCSFkDnTDRCRV2Bl9FNyQvFB1E72RDDiWgcQXRxHxsRiGAccyNEiTEELmIJJW0RNJF/S0KQU3JC+GYhlEUgoqPM7tRDwbPpFHNK1B1albMSGEzMZ4UoGimShkIw0KbojtErKG3kgGQZdYdEXEx3KLHLKqjiSlpggh5JQyqu6I3W4KboitTNNC70QGsm7C5/BOxLPBsQxMC4hlKLghhJBTiWc1pBwwuoaCG2Kr8ZSC4XgWNb7iLCI+Ho/AYTylwKBuxYQQckKWZWEsocAJM4cpuCG2kTUDhyfSEDkWgoMHY86VV+KRUnRH3I0QQohTpVUDk2kVfgfs2pfOOxApuMHJLGIZFRXe4i4iPpbAsdANOhJOCCEnE8uokHUDLgeckKXghtgintHQH80g5BbBFsFgzLly8RzGUzIsi1JThBByLMuyMBKXHRHYABTcEBsYpoWeSBqaacIrFX47Mh+8Eo9EVkeGuhUTQsibJGQdCVmD3+WMbvQU3JDTNpaUMRLPFm0n4tlwCRwUzaDUFCGEHEcso0LVTIi8M8IKZ6yCFC1ZM9AzkYZb4EuqiPh4eJYGaRJCyLEM08JoQoZHdM7OfWm/Gy0wy7IQTauQtfJJXfRHM4hnNYQ8ztiKzCevxGMyU17/v4QQcipJWUMiqzuqt5lzVlICMqqBfcMJmKaFKq+Iar+EgEuAW3RGgZXdJtMq+iczqPCIYEqwiPhYbpFDIqEhKetwCaX5f0oIIXMVTaswLNNRu/cU3NjIAqDpBgSOw3BcRv9kFh6RQ6VXRLVPQtBdOoGObpjojaRhmnDUVmQ+sQwDhsnllmv8pVtfRAghs6UbJsYSCryis3bvy+NdaYF5RA5BtwDLspDVDIzGZQzGMnAJuUCnxudCwM0XdVAwkpAxklBQW2Zv8m6Bx0RSRWu1Cd5Bdylk4UynnzmWQajIB8MScrriWQ1JRUONz1XopRyleN9diwDDMPCIuSBmOtAZiysYimXhEjhUeETU+HM7OsUU6GRUHT2RDHwSX3Zv8F6RQyStIinrJdeskJyarBnoi2TQN5kBxwCLq7xorvQ4ajuekIUUTauwLDhuSHLxvKMWueMFOuPJXKDjFjiEPCLCgVyNjpN7xViWhb5IBilZR0PQWZH6QuA5FrppIp4tvU7M5MQsy8JESkXXeAqxjIpKjwTTsnBwNInJjIq2ah/9PJCyo+omxpIKfA58z3LeisrA8QKdiZSCkXhuRyd0xI6O0wKdSFrFQCyLKq/zi4iHYllMpBSsbgrZel2PwGM8pWJRpQXWYXcrxH4zuzXRDDiWQX3QPdOF2yVwmEgp2J2dpF0cUnZiWRVpRUfY77wbXWe9c5ahYwMdWTMRSakYjr+RuqqeDnRErqABhTZVRMwAjj8tFEkp+OK9ryCl6Lj1g2vRXuOz7doeiUNS1pBWdcd04yT2sywL4ykF3ePpmd2aYw8EcCyD2oALaUWnXRxSdiZSCliGcVxKCqDgxlEYhoFb5GZeQLOqgUhKxUhChsSzBQ90hqd2Qmr97gX9vHNlWRZ+8MQhpJTcFO/nDk3YGtxIPIeIriIhU3BTqk62W3M8XomnXRxSVmQt9/7ktOzCNGeuigDAUYGOrBmIpqcCHY5F0CPMpK58Ep/3QCel6OiNZuATBUdG6Ud6+LVR7OybnPnz810RfHxji62fQ+BYTCQVNIacHeiRuTl2t6bKK816l5J2cUg5iWc1pBUDDUFn3uBRcFMkXAI38yIrawZiGQ1jCQUSn/9AJ1dEnEZG1dEQ9Nh6bbsNx7P4xXPdAICPnLMI//NyPwZjWfRHM2iutG/tXolDLKsiqxol07uo3E2PEhmYzIKfxW7NidAuDikHY0kZAss4tvaSgpsidGygE58KdESeQcAtIOx3IegR4Lcp0BlPKRiKyY4fjGmYFm55rBOyZmJVYxAf3NCMA6NJ7OidxPPdEXzQxuDGLXCIZTUkZY2CmyJnWRbGkwq6J+a+W3MitItDSllG1RFNqY4at3AsupUoci6BQ5VPQkPIDZ8kIJnV8dpQHDt6otjZN4n+aAYJWYNlWfO6vqqb6I3k6g4k3tlv4g/sHsT+4QTcAofrL10ClmGwsa0KALCta8LWz8UwDFiGQTRNgzSLmawZODCSxJ6BOGTVQH3QbWuxvFfiURdwYzKtYXf/JLrHU9AM07brE1IIsYyGrGbA7eCDJc4Nu8icHbmjo+gGklkd46k4RI5FwCWgNuBCwJ3b0ZntEeahWAaRlIK6gLNrS3om0vj19l4AwD9c0IraQO5o4rmtlfjJU0DXeBpjCRnhgH1HFqcb+qm6CZGn+4Riko/dmhOhXRxSSqZ/dyS+sKd3T4VekUuUxOd2dBqDHgRcAtKKkdvR6X1jRyee1WCaJ97RScgaeqMZBF2io4uINcPELY8dhG5aOKelEm89o3bm30IeEWfUBwAA27ojtn5ej8gjo+pIypqt1yX5lVVzuzWvTO3WNNi8W3MitItDSkFK0TGZVh3ZuO9IFNyUAYnPzbRqDHkQdIkzgc7LvVHs6JtEXySDeOboQMc0LfROZCBrpqPzqgDw25f60T2Rht/F49q3dLzpbuL89qnUlM3BDccyMM3cqQHifJZlYTQh45X+GHom0gi5BVT5pAW9+5zexXELPA6OJrFnIIZJSm2SIhLLaJB1w/G9zpz9rkVsJ/IsKnkRgAhVN5FWdOxPJ8BzDAIuAWG/hJBbREbTMZLIotrhRcSvjyTwvzv6AQBbLu447lb/ea1V+Pkzh7FvKIFYRrV12KFH5DCWVNBS5aVuxQ6WVQ30RFIYmMyd8GgIuQu6pU4nqkgxMs3cDYLL4fWXQIF3bm666SZs2LABfr8f4XAYV155JQ4cOHDK5ymKgn/+53/G4sWLIUkS2tvb8d///d8LsOLSIvIsKrwiGoJuVLhFZBUD+4eTeKk3ioOjSQgs6+haElkzcMujB2FawMVLa7Cpo/q4jwsHXOio8cEC8MLhqK1r8Eo80qqO5FTDQOIsR+7W9E5kCrJbcyK0i0OKTVLREZe1omheWtCdm61bt2LLli3YsGEDdF3HP//zP+Oyyy7Dvn374PV6T/i8v/u7v8Po6Ch+8YtfoKOjA2NjY9B1enM5HQKXC3QqkKthyaoG/F5nb+z96vkeDMVlVHlFXHNh+0kfe157FQ6Np7CtO4K3r6izbQ0Cx0LTTSSyGoJu5//Cl5OsauBwJIXBySxEjiv4bs2J0C4OKRaxjApNtxx90zutoO9ef/3rX4/68x133IFwOIwdO3bgwgsvPOFztm7diu7ublRWVgIAWlpa8r3UsiJwLAS3s394d/fH8Ke9wwCA6y9dcsq6oPPbqnDX9l680h9DWtFtbRnu4nNvTE0VznzzLDe53RoFhydSiGd1VPtEx7cxoBNVxOkM08JwXIanSPp6OeodLB6PA8BM0HI8Dz74INavX4/vfOc7aGxsxNKlS/GlL30J2Wz2uI9XFAWJROKoD1LcUoqO2x4/CAC4YlU91i2qOOVzmis9aAy5oZsWXu6dPOXj58Ir8YhnNWRUw9brkrnLqDr2DyewdzAGVbfQEHQ5PrA50vSJqliGTlQRZ0lkNaRk3fGnpKY5JrixLAs33HADNm/ejJUrV57wcd3d3Xj22Wfx6quv4v7778ett96K//3f/8WWLVuO+/ibbroJwWBw5qO5uTlfXwJZID9/uhsTKRX1QRc+dX7LrJ83c2rK5oZ+Es/m+grJlBotFNO0MBKXsbsvhr5oFhUeEZVesSh30jiWQdhPtTjEWaJpFYZlFk261DGrvPbaa7Fnzx7cc889J32caZpgGAa/+c1vcM455+CKK67A97//ffzyl7887u7NjTfeiHg8PvPR39+fry+BLIDnuybwxIExsAxww1uXzuk44nS34h19k1B0+3ZZGIYBz7CIpBTbrklmL6PqeH0kt1ujG8W3W3MitItDnEI3TIwnFXjF4qkrdERwc9111+HBBx/Ek08+iaamppM+tr6+Ho2NjQgGgzN/d8YZZ8CyLAwMDLzp8ZIkIRAIHPVBitNkRsWPnzwEAHjfWU1YXj+3/8uOsA/VPgmyZmJ3f8zWtXkkHpMZFbJGqamFYpoWhuNZ7OqLoTeaQYVHREWR7tacCO3iECeIZzUkFa1oUlJAgYMby7Jw7bXX4ve//z2eeOIJtLa2nvI5mzZtwtDQEFKp1MzfHTx4ECzLnjIwIsXLsiz8+MlDSMg6Wqo8+PA5i+Z8DYZhsLEtV8/1fJfd3Yo5ZFRKTS2Umd2agTgMw0Jj0F0SuzUnQrs4pJCiaRWWBUd3qj9WQYObLVu24K677sLdd98Nv9+PkZERjIyMHJVeuvHGG/Hxj3985s8f+chHUFVVhU996lPYt28fnn76aXz5y1/GVVddBbfb2fOPyPw9/voYXjgcBc8yuOFty+ad951OTb10OArdxjcHdmq3IJahu+p8OnK3pi+aQZVXKrndmhOhXRxSCIpuYDSpFNWuDVDg4Ob2229HPB7HxRdfjPr6+pmP3/3udzOPGR4eRl9f38yffT4fHn30UcRiMaxfvx5///d/j3e/+934wQ9+UIgvgSyAsYSMnz/TDQD4+3MXo7X6xD2QTuXMhiACLh5JRcdrQ/aenPOIPCIp1dagibwhrUydhBqIwzAtNATdRdFvw260i0MWUjyrIS1rtrbPWAgFXa1lnXho47Rf/vKXb/q75cuX49FHH83DiojTmJaF2x7vREY1cEadH3+7rvG0rsexDM5tq8Kj+0axrTuCNc0hexaKN6aEpxTd1hEP5c40LYwkZHRPpJGSNdT4XGUZ1BxpeheH+uKQfJtIKWBZdmZ3uliU9ysEcbw/7RnGnsE4JJ7F59+61Jac73Rqalt3BOYsAuzZ4jkWhmnRIE0bpRUd+4YTeHUwDrOMd2tO5MhdnFf6YzhMuzjERrJmIJJS4S+yXRuAghviYP2TGfzq+R4AwFWbWtEQsqemak1TCG6BQzStonM0deonzIFL4DCeVGa1K0lOzDQtDMWy2NUfw2BsqrbGUx61NXM1vYvjEjgcoFocYqNYRkNaMYqmK/GRKLghjmSYFm559CBUw8RZi0J4x0r75kGJPIv1Lbmuxtu67W3o55U4JGUNKRqkOW+pI3ZrLNNCfYB2a2aDdnGI3caSMgSWKcqbCnrFII50745+dI6l4JU4fO6SJbb/ck2npp7viti6yyLxHFTDoiPh8zC9W7O7bxKDsQyqfRJCtFszJ7SLQ+ySUXVMptVTzu1zKgpuiOMcGkvhty/lOkn/nwvbUeWTbP8cZy+ugMAxGI7L6ItmbL22wLIYT1K34rmY3q3ZOxCDZQENQZqKfTpoF4ecrlhGQ1Yz4J5DF3gnoVcP4iiqbuL7jx2EYVrY1F6Fi5bW5OXzeEQea6dOStnd0M8rcYhnNepWPAuGaWFwardmYDKDGr+LTprZhHZxyHxZloXRhAyJ54p255SCG+Iov97ei/5oBiGPgM9e3JHXX6zz26oBANu77Q1u3AKHrGYgQaemTiql6HhtKI5Xp3ZrGkO0W5MPtItD5iql6Ihl1KJr3HckeiUhjrF3MI4Hdg8CAK57yxIE3fkd0rahtRIsA3RPpDESl227LsMw4BgGUbpLPq7p3ZpdfZMYjsm0W7MAaBeHzEUso0ExzDkNJnYaCm6II2RUHbc+dhAWgLedWYtzWivz/jmDbgErG3IDWO0+NeWZauhHd8hHO3K3hgGDhpCbdmsWEO3ikFMxzVxKys0X764NQMENcYhfPHsYY0kFYb+EqzefeoCqXc6baegXtfW6HpGnQZrHyKoGXhuMz+zW5Htnjhzfsbs4u/tjGEvIME3qzUSApKwjLhfXBPDjoeCGFNxLPVE8sm8UDIDPv3UpPOLC/VJNBzevDyds3abnWAaWZSFOgzQBALphonMsNyagLuii3RoHmN7FSck6XhmI4dWhOA1+JZjMKNB0q+h7SxX36knRS2Q1/PCJTgDA36xtwKrG4IJ+/hq/hKW1PlgAth+2t7DYJXAYSyplf0dsWRZ6ImkMxbII+11FN6OmlHEsg2qfhCqvhNGEjF19kzg4kkSamlCWJcO0MJJQirIj8bEouCEFY1kWfrK1C5MZDc0VbnzsvJaCrGMmNWXzkXCfxCOl6kiW+RvFcFzG4YkMKj0S7dg4lMCxqAu44RF5dE+ksbNvEr2RNFSd6nHKSSKrISXrRTlL6lj0SkMK5unOCTx3aAIcy+CGty0r2Dbo9JHwPYNxW8cmCBwLzTCRlMv3SHgso6JzLAkXz8JdAneDpc4j8mgMucGCwf7hBHb2TWIkLsMo893HchFNqzBMC3wJ3IQU/1dAilIkpeD2rYcAAB9c34yOsK9ga2mscKO50gPDtPBSj72FxS6ufAdpZlUDB0eT0HSLjnoXmYBbQH3QDVk1sGcghr0DMURS5flzXC40w8RoQoa3BHZtAApuSAFYloUfPNGJtGKgI+zDB85uKvSScH6eUlMekUcim2tjXk6mC4ijaQ01fvvHZ5D8YxkGVT4J1T4JkbSKXf0x7B9JlPVOZCmLZ3MDf4v9lNQ0Cm7IgvvrayPY2ReDyLG44a1LHbEFurE9F9zs6Ju0dWyCS2Ah6wYS2fKpuzm6gFiiAuIiJ3Aswn4Xgi4B/dEMdvZNons8ReNFSkw0pYJhckXmpaDw7yqkrAzFsvjFs4cBAB/fuBjNlZ4CryinrdqLsF+CqpvY1Tdp23Vz3YpZRNPlM0hzJCGjZyKDCo9IBcQlxCVwaAh6ILIcDo4msatvEoOxLHRqAlj0FN3AWEqBdwHbcOQbvfKQBWOYFm59vBOKbmJVYxDvXtNQ6CXNYBgGG2ca+tk9SJNHNK1C0Uv/Tjee0dA5moLIswvar4gsHJ+LR0PQDc2wsHcgjlcGYmVbV1Yq4hkNaVkrmXobgIIbsoD+sHsQ+4cTcAscPn/pEselK6ZTUy/2RG29G/WIXFl0K5Y1AwfHklB1ExVUQFzSGIZBhUdEXcA1M8rhtaEE4jQstihNpBSwLOu41+TTQcENWRCHJ9K4a3svAOAzF7QhHHAVeEVvtrwugJBbQFoxsHcwbtt1p18wSrn7q26YODSWQiSpUAFxGZke5RDyCBiKZbGrdxKdo0lk1dLfpSwVsmZgIqWWRG+bI1FwQ/JOM0zc8thB6KaFc1oqcekZ4UIv6bg4lsG5UwM77U5NeUQeEym1JPuFWJaF3kgGg7EMwgHqQFyOJJ5DfdANt8ihazyFHX2T6I9maChnEYhlNGQ1vSS6Eh+JghuSd/e82IfDE2kEXDyuvaQDjIPf/Da25xr6be+OwLSxhsAjckjJekkeox1NKDg8kUbITQXE5c4j5upxYAGvDcVpKGcRGE3I4FnW0a/L80GvRCSvXh9O4L6dAwCAf7y4w/G1GKubgvCIHCYzGg6MJG27rsCx0E2r5I6Ex7MaOseSEDkqICY5DMMg6BZQF3AjLevY3R/D3sG4rYNpiT3Sio5YRoVfEgq9FNtRcEPyRtYM3PLYQZgWcPGyGmzqqC70kk5J4FhsaMmlpp63uaGfW+AwlpRL5lSJrBnoHE1C0UxUeJ0dtJKFx7FvNAEcTyrY1T+J14cTNJTTQWJZDbJuluRoFApuSN788vkeDMVlVHlFXHNhe6GXM2vTR8K3d0dsDUSmU1PpEii2NEwLh8ZSmKACYnIKAseiNuCCTxRwOJLGjt5J9EykyqI1gpNZloWxhAwxD6lkJ9zAUXBD8mJX3yQe2jsMALj+0iVF1dL77MUVEDk214wukrbtui6Bg2KYSJTAcdm+SBoDk1kqICaz5hY5NIU84FkGr4+ksKsvhuF4tiSL7ItBciolZfdrs6qbuOHeV3DH8z22DiKeq3l/Vf39/ejp6UEmk0FNTQ1WrFgBSaI7OAKkZB23Pd4JAHjnqnqsW1RR4BXNjUvgsG5RCC8cjuL5rghaq+0b6smzDCZSChpCbtuuudBGEzK6JtIIuQUqICZz5ncJ8Eo8YhkNe/pjCAdcWFTpQaVXLLmiVieLZzQouolqn70pqR29UYwkZKi9JjxC4dJdc3pl6u3txY033oiWlha0tLTgoosuwjve8Q6sX78ewWAQb3vb23DvvffCNOn4Xzn7z2e6EEmrqA+68MnzWwq9nHk5v/2N1JSdfFMv6sU6lyee1dA5moTAsiXVzZQsLJZhUOkVEfa7EEmp2NUXw/7hBBIleJrQiUzTwkhChluw/3d468FxALlhxGwB51TNOri5/vrrsWrVKnR2duLf/u3f8NprryEej0NVVYyMjODPf/4zNm/ejK997WtYvXo1XnrppXyumzjUc4cm8NSBcbAMcMNbl8JVwMj9dGxoqQTLAD2RDIZiWduu6xY4ZFW9KF/EpwuIs5qBSiogJjbgp+pxgm4B/dFcE8CuMRrKmW9JWUc8q8Hvsje4yag6XuyJAgDO76iy9dpzNeuvTBRFdHV1oaam5k3/Fg6Hcckll+CSSy7B17/+dfz5z39Gb28vNmzYYOtiibNNZlT8+KlDAID3ndWE5fWBAq9o/vwuAaubQtjdH8P27gjee1aTLddlGAYcy2IyrSLsd16X5hMxTAvd4ylMpBTUBYo3pUacySVwaAi5kVJ0dI4lMZqQsbjai1q/BJ5Sn7abzCgwDMv2tPL27gg0w0JD0IXFBR6KPOvg5uabb571Ra+44op5LYYUL8uy8KMnDiEp62it9uLD5ywq9JJO23ltVdjdH8PzXfYFN0Bu9yaSUqEZZtHUrPRH0+ifzKLG5wJXwK1mUtp8Eg+vyCGe1bB3II5hn4hFVR5Ue6WCpjhKiWFaGEkoeelIPJ2S2tRRXfD6qXm/sk5MTODll1/Gjh07EInYW5dAis/j+8fwYk8UPMvgC29dWjRv2idz3tQohgOjSURSim3X9Uo80krxDNIcmyogDrgEiHzx/78SZ2MYBqGpoZyJrI5X+uN4dSiOeKb4UrlOFM9qSMm67aekYhkVu/tjAIDz2wrf02zOr1SvvfYaLrzwQtTW1uLcc8/FOeecM5OWOnDgQD7WSBxuNCHjZ890AwD+/tzFaK32FnhF9qjySVhW6wcAbD8cte26HMvAhIl4EQzSTMgaDo6lwIEpquP8pPhxLIMav4RKj4iRuIxdfbmhnBm1OG4KnCqaVmCYlu3pvucOTcC0gCVhH+qChU+5z+mrGxkZwUUXXYTx8XF8//vfx5///Gc89NBDuPnmmzE8PIwLLrgAY2Nj+VorcSDTsnDb453IagbOqPPjb9c1FnpJtpo+NbWta8LW67oFHuMpxdEzdxQ9V0CcUXRU+ajNAykMkWffGMo5lsLO3kn0RdJQdTqVO1eaYWIsoeTlpON0SuqipW+uyy2EOQU3t9xyCxYvXoxdu3bh+uuvx9vf/nZcfvnluOGGG7Bz5040NzfjlltuyddaHe/Fw1FkS6D77Fz88ZUh7B2MQ+JZfP6tS0uuHuO8qW7Fewfjtg699Io8UoqOlEPvQk3TQtd4CuNJpagKn0np8oj8VH8oBvuHE9jdP4lRGso5J/GshpRif0pqNCFj/0gSDIALlhRhcPPoo4/iq1/9KlyuN7/Yud1ufPnLX8bDDz9s2+KKSVLWcM2vX8Z1v92Nf//TPvz11RFMFkHa4XT0RzO4c1svAODTm1uLujHdiTSE3Gip8sC0csGrXUSehergbsX9kxn0RTJUQEwcZXooZ23AjYxizAzljKZVR7T8d7pISgHDwPbf6ac7c7s2q5qCjmkTMafwrbu7G2edddYJ/339+vXo7u4+7UUVo8FYFo0hN3oiGezqj2FXfww/eQpYXufHeW1V2Nhehfpg6bz564aJ7z92EKph4qxFIVy+oq7QS8qbjW1V6IlksK07gkvPqLXtuhKXOzXVVFHYI5PHGkvK6BpPIegWqYCYONL0UE7NMDGeVDCRVrCowoOOsK/gp3ScStENjCcV+ET7J4A/7bCUFDDH4CaZTCIQOHHvEr/fj1QqddqLKkbL6wL40+cuwAO7BvDaUBIv9UTROZbC/pEk9o8kccfzPVhc6cF57VXY2FaFtmpvUf8S3rtjAIfGUvBKHD53yZKi/lpOZWN7Fe55qR+7+mLIqoZtE3S9Io9YRkVG1eERnVGsm5Q1dI6mwFIBMSkC00M5M6qOnkgaXokvyR1kO8QzuZSU3TfZvZE0eiIZ8CzjiFNS0+b86pVMJo+blgKARCJR9luDDSE3zqgP4sPnLMJ4UsGLhyPY1h3B3sE4eqMZ9EYz+N1L/Qj7JZzXVoXz2qpwZn2gqLb+O0eT+N3L/QCAz17UUfLFpi1VXtQFXBhJyNjZN4lNHfb8ArsEFpNZE0nZGcGNohs4NJbKvQAGqM6GFA+PyEPVzdwNl8gj6LF/d6LYjScV8Cxr+6Db6ULisxdXwGdzx+PTMaeVWJaFpUuXnvTfS/kOfq5q/BLeuboB71zdgKSs4aWeSWzvjmBH3yTGkgoefGUID74yBL+Lx7mtldjYVoU1zSFIvHNHFii6gVseOwjDtLCpoxoXLnFOpJ4vDMNgY3sV7t81iG3dEduCG4ZhwDEMImkFtQUOJsypDsSjCRl1ATf9HpOiE/KIGE3IODSexMrGoKNfRxdaVjUQSau2j1uwLGum3sZJKSlgjsHNk08+ma91lDy/S8Aly8O4ZHkYspYrhNvWHcFLh6NIyjoe2z+Gx/aPwSWwOGtRBTa2VWF9S6XjUgN3be9F/2QWFR4Bn72ovWzeBDe25YKbl3qitnYW9oo8omkNim4U9MV4YKqAuNonFdUuYiEkZQ17BuI4a1GFbSlKYo9qn4SRRBaHx9NYWuunrsZTYlkVWU1HyG1vSurAaBKjCQVugcOGlkpbr3265vTOedFFF+VrHWXFJXAzKSnDtPDaUBzbuiPY3h3FRErB810RPN8VAccyWNUYxMa2KpzbWlnw9M/ewTge2D0EALjukiUIustn63dZnR8VHgGTmdwb29mLK2y5rlvkMJaUkZR1SL7CvFGOJxV0jacQcIl0t3sKL/VE8cMnOjGZ0RD2S7jukiVY2xwq9LLIFI5lUOWV0BfNwO8W0Ej1NwCAsUQuJWX3zeh0SurctkrHDUmeU3BjmiZM0wTPv/G00dFR/PSnP0U6ncZ73vMebN682fZFljKOZbC6KYTVTSF85oI2dI2nsa07V6fTH81gd38Mu/tjuH1rF5bV+qeCosoFP2GTUXXc+thBWAAuO7PWcVF6vrEMg/PaqvCXV0ewrWvCtuCGYxlYyLUury5A8JpSdBwaSwJgHJUvd5qMquO/njmMR/ePAgBYBhhLKvjaA6/isjNrcdWm1rw0RiNz5xI4eEQeh8aS8IocQh5nHE0ulLSiYzKjwi/ZezNqmBae7cw1N3VaSgqYY3Dz6U9/GoIg4Gc/+xmAXHHxhg0bIMsy6uvrccstt+CBBx6gwZnzxDAMOsI+dIR9+Nh5izE4mcX2wxFs64rgwGhy5uNX23rQXOGe2f1ZsgDHH//r2cMYSyoI+yV8enNrXj+XU22cCm5eOBzFZ03LtvSNV+QxkVLRWm3fNWdD1U0cGk0ikdVR74B26U61ZyCGWx/vxHhSAQPgb9Y24v1nN+G3L/XhT3uG8ci+UezoncSWt3SUXdDvVEG3kKu/GUthZWPQcbsKCymW1SBrJqq89n4P9gzEEMtq8Lt4rG0K2XptO8wpuHnuuefwox/9aObPd955J3RdR2dnJ4LBIL761a/i5ptvpuDGJo0VbryvognvO6sJ0bSKF6YCnT2DcfRPZtG/YwD37hhAtU/Eea25QGdFQ8D2mSEvHo7i0X2jYAB8/q1LHXGyZ7ZMy7LtdMCqxiC8EodYVsPrIwmsaAjacl2PyCGaUZGS9QU75TFdQDySUFAXcJVN7dRcyJqBO7f14I97hgEAtQEJn790KVY25v7fr7mwHZs7qvGDxzsxFJfxb3/ah7csq8E/XNAGv6t8UrZONVN/M5HCstpAWdbfWJaF0UQWrjz0q5pOSW3uqLb9PccOc3qXGhwcxJIlS2b+/Pjjj+N973sfgsHcL/snPvEJ3HHHHfaukAAAKr0i3rGyHu9YWY+UouPlnujMyauJlIo/7R3Gn/YOwyfxOKelEue1VWLdoorTvmOJZzX88MlOAMDfrG3AqkZ73tAXgqwZGEnIqPZJthRm8xyLc1oq8eSBcTzfFbEtuBE4FoZpISFrCxbcDMYy6ItmUOUVqYD4OA6MJHHLYwcxGMsCAC5fUYerNrW+qYB4RUMQt31oHe5+sQ8P7B7EkwfGsas/hs9e1I7z20v/JKGTcSyDap+E/mgWfpfguGaZCyGp6IhlNARsDrZV3cS27ggAZ6akgDkGNy6XC9lsdubP27dvx80333zUv5drE7+F5JN4XLwsjIuXhaHqJnb3x7C9O4IXDkeQkHU8cWAMTxwYg8izWNccwsa2KpzTWjnnu0nLsnD7U4cQy2horvTgY+e15OcLygPTshBJK6gPujCZUW07dbaxvRpPHhjH9u4Irt7catuOh8SxGEvKaKrI/zHsiZSCrrE0fBJf1tv1x6MZJu55sQ/37RyAaeVuKj53yZKT1li5BA5XbWrFpvZq3PZEJ/qjGdz0l9exuaMa11zYVvY1H4Uk8bn6m66p/jcVDhkNsFDiGQ2qbtp+UODl3igyqoFqn4Qz6k/c2LeQ5vSKv2bNGvz617/GTTfdhGeeeQajo6O45JJLZv69q6sLDQ0Nti+SnJjIszintRLntFbCMDuwfzgxdfIqgrGkghcOR/HC4ShYBljZGJxJX9X4T128uvXgOJ6bOrV1w1uXFlUr/mhaRcgjojbgQjSt2paeWtccgsizGEsq6BpPoyPss2G1gFfikczqSKtGXo//pxUdh0ZTMC1Q6uQYhyfS+P6jB9ATyQAALl5ag2subJ91ofWyOj9u++Ba/O6lfty7ox/PHprAKwMxXHNhOy5cUk2pvwKZqb8ZT2GVWD71N6ZpYSQhwyPkcwJ4te1NAe0yp6/6a1/7Gq644gr8z//8D4aHh/HJT34S9fX1M/9+//33Y9OmTbYvkswOxzJY2RjEysYgrt7cisMT6ZlApyeSwZ6BOPYMxPGzZ7rRUeObGQXRfJzdgkhKwU+f7gIAfHB9s21v4gshqxowLAttNV4E3QJ6ozwyimHLaSCXwOHsRRUz31e7vi8ugUM0rSApa3kLblTdROdoEvGsRgXERzBMC7/fOYC7X+yDbloIuHj848Ud82rWKHAsPnreYmxsr8Jtj3fi8EQa333kAJ7pHMdnL2oveDuHclXjlzAcz6J7PIXldeVRf5OQNcSzGipt3jlMKzpe6skNEXZqSgqYY3Dzlre8BS+//DIee+wx1NXV4QMf+MBR/7527Vqcc845ti6QzA/DMGir8aGtxoe/P3cxhuNZbJ/qpbN/OIFD4ykcGk/hru29aAy5cV5bJc5rq8LSWj8YALc93om0YmBJ2IcPnN1U6C9n1gzTQjSjor3GixqfBIZhUO0V0R/N2nbUeWN7FbZ1R/B8dwQfPW+xLdcEcjU9EyklLwNWLctCz0SaCoiPMTiZxS2PHcSB0SQA4NzWSmx5SwcqTvMNob3Gh+9/YA3u2zmA377UjxcOR/HqUBxXb27DpcvD9P1fYCzDoMbnQn80A7+LR3Olt9BLyrvJtArdxoaj07Z3R6AZFpor3Gipcu73cc6v9itWrMCKFSuO+2+f+cxnTntBJD/qg2787bom/O26JkxmVLx4OFeQvLs/hsFYFvftHMR9OwdR6RHRWuPFrv4YRI7FF9621JGV8CcSSSmo9olYVOWZeQOp9IroiaRtS01taKkExzLoj2YwMJmxrVAxN0hTg6wZtm+dD8ay6ImkqYB4imlZeGjPMH65rQeqbsIjcrjmwja8ZZl9gQfPsfjghkU4ry23i9M5lsJtj3fimc5xbHlLB8J+2j1bSCLPwu8S0DWehlcSUFnC9Te6YWI0qcCbh5OtW4+YAO7kIH1OX/mdd9553L8PBoNYtmwZli9fbsuiSH5VeES8fUUd3r6iDhlVx47e3Myrl3omEc2oiPaqAIBPnL8YzUV0wiCt6GBZoK3Gd1QBXcAtwCvxyNhUz+KTeKxpCmJnX26ExgfOtud75BY5xOMqErJma3ATSSkzAwXLpd7gZMYSMm57ohN7BuIAgLXNIXzukiWzqkObj8VVXtz8/jV4YPcg7nqhFzv7Yrj27l341KYWvH1FnWNrFkqR3yVgLCHj0FgSq5tCJfv7kJB1JLM6qn32BnCTGRWvDMQAABc6OCUFzDG4uf7664/796lUCqZp4oorrsDdd98Nv99vy+JI/nlEHhcsqcEFS2qgGSb2DMTxwuEIfBKPd60unuJww7QQy6pYVut/0x2ZS+BQ6RExHJdtq2c5r60qF9x0RfCBs5ttuSbLMGBZBrG0ZttdfUbV0TmWgmFaqPKWdwGxZVl4fP8YfvZMN7KaAYln8alNrXjHyvwHGBzL4L1nNeGc1kr84IlD2D+cwE+e6sKznRO49pKOvKQiyfFV+yUMx3L1N8vqAiW5kxlNKzBh2b7r/tyhCZgWsLTW5/if2Tl95ZOTk8f9UBQF27dvR19fH77xjW/ka60kzwSOxdmLK/CPF3fg4xtbiuqOcjwlozbgQlPl8XdRqv0SdNOCZVm2fL7zWqvAAOgcS2E8qdhyTQDwCDwmUgp0wzzta2mGic7RFOJZFTVlXsg6mVbxzYf247YnOpHVDJxR58cPPrQO71xVv6A/500VHnz7vavwDxe0QeJZ7BmM47p7duHBVwZhmPb8bJKTYxkGNX4X+qIZDE5mCr0c22mGibGEAl+eU1JOZ0tYx7IsNmzYgO9973v44x//aMclCZm1RFaDyLNoq/adsHgu4BLgETlkNcOWz1nhFbF8qr/D9qlmVnaYTp8lZf20rjNdQDwczyLsK+8C4mcPTWDLPTvxYk8UPMvgk+e34Kb3rkZDgYYqsgyD96xpwI8+fBZWNwah6CZ+/sxh3Pj7PegvwTdbJxJ5FkGXiO7xNCIp+25OnCCW0ZBSdNtnnY0kZLw+kgTLAJs7yiS4mdbR0YGBgQE7L0nISWmGiaSio63ae9Luvm6RQ4VXQOo0g4Yjnd9WBcDe4IZjGRiWiXhWPa3rDMXlqQJiqagKwu2UlDXc/PDr+I+/vo6knPsZueXv1uJ9ZzU5IhVRF3Thm1euxJaLO+AWOOwfSeL63+7C/+4YoF2cBeBz8TAsC51jKWRVe256nCCSUsAwsP1n/JmpXZtVjcGiKMa29VWvq6sLTU3Fc2yYFL/xpIKGkAsNoVMX9Vb7JGimaV9qqj0X3Lw6FEc8q9lyTQBw8zzGUyrMeb7BRdMqDo0m4RbKt4D45Z4orr17F57unADLAB/c0IzvfmANWqqddXSVYRhcvrIOP/rIOpy1qAKaYeFX23rwpf99BT0T6UIvr+TV+CTEMhq6xlMlEVDKmoGJlAKfaH99XTGlpACbghvLsrBz50588YtfxLvf/W47LknIKcUyKrwSh7Ya36zuUoJuAW7BvtRUXcCFtmovTAt48bC9qamkrCGlzn2XKaPqODiahG5YCLrLr4A4o+r40ROd+Maf9iGaUdFU4cbN71+Dj5672PZ+H3YK+13413efic9fugReicOhsRS+8D+7cc+LfdBsqL8ix8cwDMJ+CQOTWfRHiz+YTGQ1pFUDHsnem5qeiTR6oxnwLIONRTIzbU5JuYqKiuPm7lOpFAzDwOWXX45//dd/tWtthJyQqpvIagZWNgZnfQLKI/IIeUREUqptk803tlehe6oT9NvOrLPlmiLPQtMtJGV9TgPvNMNE51gKsYyKBoefZMiHvYNx3PrYQYwlFTAA3rOmAR/buNj2uTr5wjAMLj2jFusWVeAnTx3CC4ejuPvFPjzfNYHrL11aVF3Ci4nAsQi6BRyeSMPnElBdxMX340kFHMPYXiT/dGdu12Z9S0Vex8PYaU6rvPXWW4/794FAAMuXL8cZZ5xhx5oIOSnLsjCeUrCo0o26wNyOTNf4JYwkZNvWsrGtCr95oQ+7+mLIqLptQZPIs5hIKmicZdGrZVnonUhjOJZFrb+8CogV3cCd23rx4CtDAICwX8Ln37q0qCbYH6nSK+KfrzgDzx6awE+3dqEnksEX792N953VhA9tWFRUM96KhU/ikVUNHBpLwSNytv0eL6SsamAircBvUyf2aZZlHZGSCtt67Xya03fhE5/4RL7WQcisRdMqQm4BLdXeOc+ICbgFSBxrWxfgRZUeNARdGIrL2NE7iQuW2JOP9ogcYlkVWdWAWzz1OofjMg5H0qj0lFcB8cHRJG557CAGJrMAgLefWYurNrcW5ZvTkRiGwQVLarC6KYT/fLoLz3RO4N4dA9jeHcHnLl2C5XXOnMRczKp9IobiWRwaS+HM+kDR/R5Nv15UuO0t9n19JImxpAK3wGFDS4Wt186nWf/vpdNzy0fO9fGEzIasGdBNC6013nm9gXlFDkGPgJRiz6kphmGwcaqweJuNp6bcAoesaiIhn7pQeTKtonMsV0A8m0CoFGiGibu29+LL//sKBiazqPSI+Pq7z8S1lywp+sDmSEG3gK+8fTn+v3csR8gjoH8yi6/etwe/ePYwZJtqx0hOrv7GhaFYFv3R4jqSb1kWxhIKRI6zfdf26aldm/PaKosmxQvMIbjp6OjAt771LQwNDZ3wMZZl4dFHH8U73vEO/OAHP7BlgYRMMy0LkbSC5koPwvNslc8wDGr8ElTdviLNjW25AruXeyZtuy7DMOBYBpH0yXtwZFUDnWNJaHr5FBD3TKTxpf99Bb97uR+mBVy4pAY/+sg6rF9cWeil5c3G9mr85CNn4ZJlYZgW8Ifdg/jcb3fh1cF4oZdWUgSORcgtojuStrU5Z76lVQOTGdX2lJRhWnjm0ASA4kpJAXNISz311FP4l3/5F3zjG9/A2rVrsX79ejQ0NMDlcmFychL79u3Dtm3bIAgCbrzxRhqiSWwXSamo9IpYfMRQzPkIugWIPANFN2y5E1lS60OVV0QknZu7sqHFnjdZn8gjmtag6uZx6yx0w0TnWBKRdHkUEBumhft3DeI3L/RCNy34XTz+8eIObO4ojtMbp8vvEvCFty3FBUuq8eOnDmE4LuPG+/finavq8YmNLWWza5dvXolHVjPQNVV/Y3czvHyIZVTImokqr70/A6/0xxDPagi6BaxpKq4atln/ry1btgz33nsvBgYGcO+99+Lpp5/G888/j2w2i+rqaqxbtw4///nPccUVV4BliytXSZwvo+qwGAttNb7TrpXxSTz8rlxDP8l3+i8GLMPgvLYqPLR3GNu6IrYFN26Rw3hKRlLWUHXMCQ7LstATSWNoqoC4mEZlzMdQLItbHzuI/SNJAMA5LZW49i0dqCiCZmJ2W99SiR99+Czc8dxhPLxvFA/tHcZLPVFcd8kSrG0OFXp5JaHKK2I4kUXXuPPrbyzLwmhChisPheZbp05JbeqodvT34HjmHJI2NTXhC1/4Ar7whS/kYz2EvIlhWpjMaOgIe205pskwDGoDLkyk7NvS39ieC25eOByBYXbY0h2UYxmYVq6d+rHBzUhCxuGJDCo8oqP7t5wu07Lwl73DuOP5Hii6CY/I4R8uaMOly8NldSLsWF6Jx7WXLMHmJTX44ROdGEsq+NoDr+KyM2tx1abWothtcDKGYVDjy9XfeEUe7Q4+hp9UdMSz2pzaRsyGohvY1pWrIyyWxn1HKuir4k033YQNGzbA7/cjHA7jyiuvxIEDB076nKeeegoMw7zp4/XXX1+gVZOFNpFSUOMXsajSvu6yAbcAgWdtq5FZ2RCEX+KRkHXsG7IvaPIIHMZTylHdU2MZFZ2jKbh4tqSKZ481nlTw9Qdfw0+f7oaim1jdFMQPP7wObz2jtqwDmyOtbQ7hRx8+C+9aXQ8AeGTfKLbcvRMv9UQLvLLiJ3AsKjwiDkfSGEva1z7CbrG0ClU3bS/2fblnElnNQNgvYXmd39ZrL4SCBjdbt27Fli1bsH37djz66KPQdR2XXXbZrE5aHThwAMPDwzMfS5YsWYAVk4WWknXwHIO2ap+t/T38Eo+AS0DaplNTHMvgnNZcOsrOU1NeiUdK0WdmYsmagc6xFFTdRMhTmikZy7Lw+P5RXHvPTuzuj0HkWVxzYRv+/W9WIuyfW1+jcuAWOVxzYTu+/d5VaAi6EEmr+Lc/7cP3Hj2AhI1jQcqRR+TBMwwOjaZsO2FpJ9O0MJKQ4RHyNwH8giU1RZn2Luht31//+tej/nzHHXcgHA5jx44duPDCC0/63HA4jFAolMfVkULTDRMJRcXyuoDttRUsm2u7vj+VhF2dG85vr8Ljr49hW3cE/3BBmy27CwLH5r4PsjbTlj+SUlEfLM03+cmMih8/mevOCwDLav34wluXorGi9AumT9eKhiBu+9A63P1iHx7YPYinDoxjd38Mn72oHecXSct8J6r0ihiOy+gaS+HMhoCj0sAJWUMiq9s+yDKl6Hi5N/c7WIwpKaDAOzfHisdz2/mVlacuyFy3bh3q6+tx6aWX4sknnzzh4xRFQSKROOqDFIfxlIK6gHvWXXrnKuQWwXOMbbN71jZXwCWwmEip6BxL2XJNAHDxucLi3kgGg7EMwn6pKO+kTuW5QxO49u6deOFwFDzL4OMbF+M/3reaAps5cAkcrtrUiu+8bw2aKz2IZTTc9JfX8e2/vo5Y5vQmzZer6flTw/Es+iIZ2wbv2mEyrcKwTNsDru1dEWiGheZKD1qqTj2U2Inm9R3p6+s77n+wZVno6+ub10Isy8INN9yAzZs3Y+XKlSd8XH19PX72s5/hvvvuw+9//3ssW7YMl156KZ5++unjPv6mm25CMBic+Whubp7X+sjCimc1uAQOrTXevFXp+1w8fFNpHzuIPIuzp3qtbLc5NZXI6jg8kUbIXXoFxClZx/ceOYBv//V1JGQdrdVefP/v1uIDZzfbUphdjpbV+XHbB9fig+ubwTK5wPEf796Jpw6MOerNuVjw0/U3E87pf6MbJkaTCrz5mADe+cYE8GKtb2OsefykcxyH4eFhhMNHN/WJRCIIh8MwjLl3ztyyZQseeughPPvss2hqaprTc9/97neDYRg8+OCDb/o3RVGgKG/8MCYSCTQ3NyMejyMQsLeFeUrR8dLhCIIl+Aa0kDTDxHhSxorGIJoq8nvXcHg8hYNjSTQE7fk8Ww+O47uPHEBjyI2ffvRsW64J5I5CeyW+5Br17eidxA+e6EQ0rYJlgPef3YwPbWim3x8bdY2ncNvjnTg8katlPKelEv94cfubTuCRU4umVfAcgzXNoYIPkIykFOzsi6HGJ9l6EzCZVvHJX74I0wJ+/rH1qJtHCjwl67Bg4dy2KlvXlkgkEAwGZ/X+Pa9XEMuyTjgd3OWa+zfiuuuuw4MPPognn3xyzoENAJx33nno7Ow87r9JkoRAIHDUB3Gu3FBMGY0VHtQvQGO6kEcEx+TqWuywoaUCPMtg0OYW7g0hd0kFNlnVwI+fPIR//eNriKZVNIbc+M771uBj5y2mwMZm7TU+fP8Da/DRcxeBZxm82BPFlrt34rF9o7SLM0cVU6NbDo0mbUtnz1c0rcK0LNt3N585NAHTytW7zSewcYo5hZ433HADgFwO8mtf+xo8njfudg3DwAsvvIC1a9fO+nqWZeG6667D/fffj6eeegqtra1zWc6MXbt2ob6+fl7PJc4Sy2jwSgJaq70LkpLwu3j4XDzSqoGg+/TfVD0ijzXNIezoncTz3RF8sLI489X59NpQHLc8dhCjidyO6nvWNOBj5y22ZZApOT6eY/HBDYtwXlsVbnu8E51jKdz2RCee7hzH/7moHQ15qmsrNQzDoNbvwkgiC5+LR3uNryBpG1U3MZZU4MtDK4jpWVIXFmkh8bQ5fWd27doFIBeU7N27F6L4RoW2KIpYs2YNvvSlL836elu2bMHdd9+NBx54AH6/HyMjIwCAYDAItzv3y3bjjTdicHAQd955JwDg1ltvRUtLC1asWAFVVXHXXXfhvvvuw3333TeXL4U4kKIbUAwDK+uCC9aEjOdY1PgkHBpL2bYzsrGtCjt6J7G9K4IPrqcar2mqbuLX23vxwO5BWADCfgnXX7oEq5tChV5a2Vhc5cXN71+DB3YP4q4XerGrP4Z/vHsnLjuzFh/asMj2UzeliGMZVHok9Exk4HcJqA0s/O5GPKshrei2t0YYics4MJoEywAXFPlYkzm9g0yfSvrUpz6F22677bRTPLfffjsA4OKLLz7q7++44w588pOfBAAMDw8fVaSsqiq+9KUvYXBwEG63GytWrMBDDz2EK6644rTWQgrLsixMJBUsrvagboFfLEIeASzLwDDt2eI9t7USP3kKODSewlhCRrgAL35Ooxsm/v2hfdjdHwMAvO3MWly9ubWkmxA6FccyeO9ZTTintRL/9exh7OidxF9eHcETr4/hPWsa8N6zmgpeT+J0bpGb6TnlETn4be4OfCqRlAJ2ariunaYLiVc3hYp+tMm8CoqL2VwKkuaKCornL5JS4BI5rG0OLXh6QjNMvHQ4CtOCbbs3//T7PXhtKIGrN7fib9Y22nLNYvbTrV14aO8wXAKLL1+2fKbhISm8vYNx3LmtB69Pze3ySTw+cHYT3rm63vaut6VmOJ5FjV/CioagrU1GT0bWDLzcEwXHsrYGoZZlYcs9u9AfzeD6S5bgrWfWzvtaRVtQnE6n8bWvfQ3nn38+Ojo60NbWdtQHIXORVQ3opoW2Gm9B6i4EjkWNX0JGta8D6fntVQDs7VZcrB7aM4SH9g6DAXDD25ZRYOMwqxqD+M77VuNf3nkGFlV6kFJ03PF8D6759Q48/NrIUaM/yNHCfhdGEwp6I+kFK86OZzWkVQMem6fA90Qy6I9mIHAMNk69fhWzeYV9V199NbZu3YqPfexjqK+vL9pz8KTwDNNCNKOgvcaHmgIeTa3w5npY2JWaOq+1Cj9/5jD2DSUQy6glOyrhVHb1TeJnz3QDAD6+sQUb24r/RbMUMQyDc1ursH5xJZ46MIbfvNiH8aSCHz15CPfvGsTHzluM89ur6LX+GLn6GxE9kQx8Ln5BTniOJxXwDGN7I8/pcQvrF1eWxODVeX0Ff/nLX/DQQw9h06ZNdq+HlJlISkGVT8KiKk9BXzgDLgFeiUdG1W3Jn4cDLnTU+HBoPIUXDkfx9hV1NqyyuAxMZvAff30dpgVcsiyM951F6Tmn41gGl55RiwuX1uAvrw7jdy/1YzCWxbf/+jo6wj58YmML1jaHCr1MR3GLHGTdwKGxFLxTM+vyJaPqiKQV+Fz2Bh+mZeGZIxr3lYJ5paUqKipmNSKBkJNJKzpYFmir9hY8ty/yLKp9ItI2pqY2lnFqKilr+Pc/7UNaNXBGnR/XXtJBd/1FROBYvGdNI37+8fX48IZmuIXcXLOvPfAqvvbAq+gcTRZ6iY5S4RGRVQ0cGs0Ntc2XWEaDrBpw25y+f30kibGkArfAYX2LXdP2Cmtewc2///u/4//+3/+LTMa+JmWkvBimhVhWxeIqr2M6pVZ4RVhW7i7GDtMpmFf6Y7ZNHy8GumHi2399HUNxGTV+Cf/fFWdQgX2R8og8PnLuYvzsY2fjPWsawLMMdvfHcMO9r+Dbf9mPgUl6D5gW9rswllTQM5Gf+hvLsjCeVCBwnO03CtMpqY1tVQW/0bTLvPa2vve976Grqwu1tbVoaWmBIBy9Dbdz505bFkdK10RKQW3AhWYHNbkLugV4RA4ZxbBl27e50oOmCjcGJrN4uXeyZLZ7T8ayLPzsmW7sGYjDLXD42jvPLNt6o1IS8oj4hwva8J41Dbj7hT48eWAMz3VFsK07gredUYsPnbMI1Q65SSkUjmVQ5RXRG03D5+Jtb4yYVg1MplX4bU5J6YaJ5w5NACidlBQwz+DmyiuvtHkZpJwkZQ0Cx6Ct2ueoO3qJ51DpEzE4KduW097YVoV7dwxgW9dESb1wnMhDe4fxl1dHwAD40mVL0VrtLfSSiI1qAy584W1L8d6zGvHr7b144XAUD+8bxZMHxvGu1fV4/9lNC97zxUlcAgeXxs3U39g5MiWWUaHoJqp89u6svDIQRzyrIegWsKaE6qnm9Qr+9a9/3e51kDKhGSYSso4z6/0Iepz3IljtldAfzZ5wftpcTQc3O/omoehGyWz5Hs/Ovkn8fOpk1CfPb8E5rXQyqlQtrvLiX955JvYNJ/Cr53uwbziB3+8axMOvjeB9ZzXh3WsaynacRsgjYjQh49BYEisbg7b8zluWhdGEDCkPvXSmxy1s7qhekJE3C8U5t82kLIynFNQHXWjM87Tv+QpMp6bUuU+2P56OsA/VPgmyZs505y1F/ZMZfGfqZNSly8P423V0MqocnFkfwLffuwpff9eZaKnyIK0auHN7Lz7z65fx573Dtg2kLTbVPgnjSQWHx9MwbegTlJB1xLOa7btiim7MHHgotZ3leQU3LMuC47gTfhByPLGMCo/Ioa1mYYZizodL4FDpEZGyqQCYYRhsbMudLNzWVZqnphLZN05GnVkfwJa30MmocsIwDNa3VOK2D63DF9+2FHUBFyYzGm7f2oV/vHsnnj44bluRfrHI1d9I6ItmMJyQT/t6sYwKVbds74L8Us8kspqBsF/C8jq/rdcutHmlpe6///6j/qxpGnbt2oVf/epX+MY3vmHLwkhpUXUTWc3Aisag43PyVT4JAzEbU1Pt1fjjnmG8eDgK3TDBO6jO6HRpUyejhuMywnQyqqyxDIOLl4WxqaMaj7w2gt++3I/huIybHzmA+3YO4OMbW3DWolDZBL4ugYNH5HFoLAmfyM87DW+YuZSUJw9pvpkJ4EtqSu7/ZV7Bzd/8zd+86e/e//73Y8WKFfjd736HT3/606e9MFI6LMvCeErGokoP6otgiGTQLcAlsMhqhi2DHc+sDyDg4pGQdbw2lCiZoj3LsvCfW7uwdzB3Mur/vutMWwsoSXESOBbvXN2AS5bX4sE9Q/j9zgF0T6Txr398DSsbAvjExhYsr7d3rp9TBd0CRhMyOqfqb+ZTh5SUNSSyuu0T21OKjpd6ogBKLyUF2Fxzc+655+Kxxx6z85KkBExmcpX4LdVesA5NRx3JLXKosDE1xbEMzm0rvYZ+f9wzhIf3jYIB8OW3L8PiKjoZRd7gFjl8cH0zfv6x9bhybSMEjsGrQwl8+b49+OZD+9AbSRd6iQui2idhIqXg8ERqXvU30bQKwzJt3xHd1jUB3bSwuNKDlhI81WjbdyubzeKHP/whmpqa7LokKQGyZkA1TLTV+GzZBVkoNX4JumHa1oxr4xHBTSnUH+zoncQvnj0MAPjUphZsaKGO5eT4Am4Bn97civ/86Hq87cxasAzwwuEoPvfbXbj1sYMYs6Emxck4lkG1L3cKcyiendNzdcPEWEKBV7R/R/TpzlxvmwtLcNcGmGdaqqKi4qj8nGVZSCaT8Hg8uOuuu2xbHClupmUhklbQUuVF2F9cDb4CLgEugYOsmXDbMH13TVMIboFDNK2iczSFZUVcvNcXzeA7D+dORr3tjFpcuZZORpFTq/FL+NwlS/C3a3M9crZ1R/D462PYenAcV6yqx9+tby7ZtKbE5+pvusZS8En8rBtbxrMakoqGGp+96fxoWsWegRgACm6Ocuuttx71Z5ZlUVNTg3PPPRcVFaUxl4KcvkgqNw27pdpbdMVq3qkXoEhKtSW4EXkWG1oq8HTnBLZ1TxRtcBOfOhmVUQ2saAjgsxe3F93/LSms5koP/r8rzsDB0SR+9XwP9gzG8eArQ3h03yj+dl0j/mZtQ1Ht8s5W0C1gLCmjcyyFVbOsv4mmVVgWbD9d+uyhcZgWsLzOj7oiqIOcj3n9BH3iE5+wex2kxGRUHSYstNf4iraZV7VfwoiNW+bntVXh6c4JPN8VwSc2thRdUKAZJr79l/0YScioDUi48R10MorM39JaP7555Urs7o/hV9t60DWext0v9uGhvcP4u/XNeMfKupL7+ar2SRiOZ9E9nsLyusBJaxBV3cRYUoFPsj/Qe/rgVEpqSWnu2gDzDG4AIBaL4Re/+AX2798PhmFw5pln4qqrrkIwGLRzfaQIGaaFyYyGjrAXNUWWjjpS0C1A4ljImmFLgHb24goIHIPhuIy+aKaoCnAty8LtW7vw6lBiZmZUqaYQyMJhGAbrFlVgTXMIzx2awF3bezEUl/HzZ7rxwO5B/P25i3HR0hrH9sWaK5ZhUONzoT+agd8lnHS2XiyrIq3oqLV5Z2U4nsWB0SRYBti8pNrWazvJvMLil19+Ge3t7bjlllsQjUYxMTGB73//+2hvb6ehmQQTKQXVPhGLKovnzft4vCKHoEewbaK3R+SxduoY+PNF1tDvgam0AcsAX7mcTkYRe7EMgwuW1ODHHzkLWy7uQKVXxFhSwS2PHcT1v92FFw9H8jJpuxBEnoVPEtA1nsJkWj3h4yZSCliGAWvzDu90b5s1TSFUlPBQ23kFN1/4whfwnve8Bz09Pfj973+P+++/H4cPH8a73vUufP7zn7d5icVH1cuz5TiQ653Aswzaa3y2d9NcaAzDoMYvQdbtGcUAAOe35e6UthfRkfCXe6K447ncyairNrVi/WI6GUXyg+dYXL6yDv/50bPxyfNb4JU49EYz+PeH9uOrv9+L14bihV6iLQJuAYZhoXMsCVl78+uLrBmIpFR4bU5JWZaFrdON+0q0kHjavHduvvrVr4Ln3/jG8zyPr3zlK3j55ZdtW1yxkXgWNX4XspqOwVgGiaxWMncbs6EbJuJZFS3VHlTY3HCqUAJuASLPQrEpwNnQWgmWAbon0rbW8+RLbySN7zx8AKYFXHZmLd6zpqHQSyJlwCVweN9ZTfivj23A+89qgsiz2D+cwD/9fi++8cfXcHii+HvkVPslRFMqusdTMI7pfxPPakgrBrw2HGY4Uk8kjf7JLASOmWlPkQ+aYULgWRQymziv4CYQCKCvr+9Nf9/f3w+/vzhPgdhB4FisaAjg7JZKLAn7YTHAcCKLSEqBVgYD5MZTCuoCLjQ5dCjmfPglHgGXgJRsT2oq6BawsiFXl7bd4ampeFbDvz+0D1nNwMqGAP7PRXQyiiwsn4vHJ85vwc8+ejbesbIOLAO83DuJ63+7C9975ABG4s6/QTgRlmFQ43ehL5rB4GTmqH8bS8oQWMb237fpXZsNLZW27wodKaPpqPW7Cvp6Ma/g5oMf/CA+/elP43e/+x36+/sxMDCA3/72t7j66qvx4Q9/2O41FhWGYRBwCWgP+7B+cQVWNYbgc/GIpFSMJLLIqPa8STpNPKtBEli01vhKanYSwzCoDbhsTU1tbM/dMT3v4NSUZpi46S/7MZrITXGnk1GkkKp8Ev7x4g7c/vdn44Il1bAAPHVwHJ/9zQ78dGsXJjMnrl1xMpFnEXAJ6B5PI5JSAOROmkZTKnwue4MP07LeaNyXx1NSqm5C5Nl5z9Kyy7y+e9/97nfBMAw+/vGPQ9dzb9aCIOCzn/0svv3tb9u6wGLmEjg0hNyoDbgQy6gYTcgYSyqYzKjwSQL8Lt72YrFC0AwTKUXDioZgSZ6gCbgFCDyb22q14Q3+vLYq/OfT3Xh9OIHJtOq4FJ5lWfjJU4fw2lACHjF3MipQgv+vpPg0hNz4ytuX473rUvj19h7s7Ivhob3DePz1UfzjxR14y7JwoZc4Z37XG/1vPCKPWEaDrBm2F/vuH05gPKnAI3JY35K/fnRJWUPAJSBgc3A2V/N6pRZFEbfddhsmJyexe/du7Nq1C9FoFLfccgskqXiP/uYLxzKo8kk4syGI9S2VWF7nB8sAIwkZ40mlqAuQLcvCWFJGQ8iNhpC70MvJC7/Ewy/xtqWmqn0Sltb6YAHYfth5uzd/2D2Ix/aP5U5GvX35SY+rElIIHWEfvvGelfh/V67E0lofZM3EDx7vRPd4qtBLm5dqn4RYRkPXeArjSQUiz+UtJXVeWxUkPn+9x2TdQG2gsCkp4DRnS3k8HqxatQqrV6+Gx0MvgLPhk3i0VPuwvqUydxTPK2Ayq2IonkVa0YuuADmW1eBzCWir8ZVML4pjsWwuNZU5zqmG+TpvetaUw+puXjwcxR3P9QAAPr25DWcvpo7jxLlWN4Xw3fevwTktldBNC995+ACyqn2/pwuFZRiE/RIGJrOIpu1v3KcbJp47lEtJ5XMCuKwZcPEcQgVOSQHzTEvJsowf/vCHePLJJzE2NgbTPHrngXrdnJrIs6gLuhD2S4hnNYwnFYwkZMTiGjwih4BLcHywoOgGZM3AqqZgXrpoOknQLYBnGdtSU+e3VePObb3YMxhHStEd8f3rjaTx3UcOwAJw+Yo6vHt1faGXRMgpMQyD6y9dgs/9dhcGY1n89OkufOGtSwu9rDkTOBYVHgEpRbe9q/vugRgSso6QW8CappCt1z5SStER8giOeD2b1wquuuoqPProo3j/+9+Pc845p+DbT8WMZRlUeEVUeEU0VboRSakYjGUxlpTBMQwCbsGR4wssy8JEUsGiag9q/aU5m+RIfpcAn4tHWtFnPfTuZBor3FhU6UFfNIOXeqIFrxWIZVT8259yJ6NWNwZxzYVt9HtNikbALeBLly3DP/9hL554fQxrmkK4ZHnx1d94RD4vc7WmU1KbO6rzetOsGibCDkhJAfMMbh566CH8+c9/xqZNm+xeT1nziDw8lTzqgy5EMypG4jImUgqiGQV+KRcNO+GHBsgNdAt5RLRW+U46H6VUcCyDWr+Eg2NJhGBPod/Gtir0RTPY1hUpaHCjGSa+9ZfXMZbMnYz66uXLS+rEGykPKxuD+NCGRbj7xT7cvvUQltX60VhRmnWAcyFrBl7ojgLIb0oqqxqQeBYhhxw+mNcrWGNjY1n3s8k3nmMR9ruwqjGIsxdXor3GB9OyMBzP9czRC9wzJ6sa0E0LbTVeWyZmF4uQRwTHsLZ9/6ePhO/omzxul9KFYFkWfvzkIewfTsArcvjau+hkFClef7e+GSsbApA1E995+PWy6C92Ki/1RJHVDIT9EpbV5e99O6loqPCIee2fMxfzCm6+973v4atf/Sp6e3vtXg85AsMwCLoFdIT9WN9SiZVNIXhdPCbSuWPlhSicMy0L0YyCRZWeoh6KOR9+Fw+fxCNt0/e9rdqLsF+CqpvY1TdpyzXn6v5dg3j89amTUZcvR3MJNWAk5YdjGXzpsmUIuHh0T6RnxoaUs+mU1EVLa/K2829ZFnTTRDjgnPeEeQU369evhyzLaGtrg9/vR2Vl5VEfxH4ugUNjyI2zFlVg3aIQGkIuZKbGPMSzGswFOmU1kVJQ5ZOwuNrjmBTZQuE5FtV+EWmbGjEyzBst0LcVoKHfC4cj+OXzPQCAf7igDWctopNRs5XIara1BiD2qvJJ+PxUQfEf9wzjBQe2W1goKVnHjt7cjVNeU1KaAZfAIeR2Ts+uee0fffjDH8bg4CC+9a1voba2tuze5AqJYxlU+yRU+yQ0yxoiKQVDMRnDcRkSzyLoFvLWSTat6GCY3I5DPvskOFmFRwTLpGGYli2FeRvbq/DAK0N4sScK3TAXrNbl8MQbJ6PesbIO71xFJ6NmwzAtTKQUiDwL3TQxktBQ7ZWoRslhNrRU4sq1DfjD7iHc9lgnbvuQr+x2mgHg+e4J6KaFlioPFld58/Z5UrKO2qDLUWUK8wpunn/+eWzbtg1r1qyxez1kDvwuAX6XgIaQB9G0OlWTo8K0LARcgq25T8O0EMuqWBL2o8pXfi8S0wJuAV4xd2rKjtqU5XUBhNwCYlkNewfjWLcAuyeTGRX//tA+yJqJNU1BfOYCOhk1G7JmIJJWUOOX0FGTq104PJHCaFJGQBJtb5dPTs/HN7bg1cEEDo2n8N1HDuBbf7vK8e017DYzATyP4xYsy4JmWo4LHud1u7F8+XJks1m710LmabpnzpqmEM5eXIFFlR4ohoHBWAaTGfVNE2fnYyKVe1FfVFXeNRkCx6LGL9mWmuJYBucuYGpK1U3c9Of9GE8qaAi68E+Xn0G7DrMQy6iIZVW0VvuwsjGIoEdA0CNgZWMQy+sCUA0DownZlt81Yg+BY/GVy5fBLXDYN5zAPS+9edhzKYumVewdiAMALshjSiqjGvBKnONG78zrVe3b3/42vvjFL+Kpp55CJBJBIpE46oMUxnTPnOX1AWxoqcSKhiBEnsVYUsZYQoYyz+GPSVkDzzFor/HR8ERgps+NXW9k03U327sjea2dsiwLP3qyE/tHkvBKuZNRtNtwcoZpYSSRBRhgRUMQS2t9R6VkeY7F4iov1jZXoMonYiSRpVocB6kPunHtWzoAAP/zUj/2DMQKu6AF9EznOCwAZ9T5URfIXy+ypKKjyic6rh/bvF7ZLr/8cgDApZdeetTfW5YFhmFgGMXX/rrUTPfMqQu6MJlWMRyXEUkr0AwFAUmEV5rd7BLdMJGQNZxRH7CleV0pCLoFeCQeGVWH33X6dyurm4LwiBwmMxoOjCRxRn3AhlW+2X07B/HkgXGwDPBPl5+BJjoZdVKyZmAirSDsl9AR9p/0znR6F6fCI6AnkkE6oaPaJ5VdGsSJLlxag90DMTy6bxTfe+QgfvDhdY7bZciHmZRUHndtTMuCaZqodmCpwryCmyeffNLudZA8ETgW4YALNX4JiayO8ZSMkbiMobgKt8Aj4OJPmpYYSymoD7rRWKJDMedD5FlUe0UMTGZtCW4EjsWGlkpsPTiO57sieQlutnVHcOe2HgDAZy5sx9rmkO2fo5RMZlQouoH2ah9aqr0Q+VPvWAoci5ZqH0IeEd3jaYwksgi6RUe0oi93n7mgDa8PJ9A/mcUtjx3E/33XmWBLuM5sKJZF51gKLANs6qjO2+fJKAa8LsGRweK8fusuuuiiE/7b7t2757sWkkcMw8zUCTRVeBBJqxiKZTGRVsCAQcAlvKnSPZ7Nzblqq/FSXcYxKn0ieiNpmJZly4vkxrYqbD04ju3dEVy1qcXWAt/DEyl8/9Hcyah3rqqnk1EnYZgWxlMyXAKHlY1B1M2jlXzII2JVE4+BqIDeSAZphXZxCs0lcPjK25fji/e+gh29k3hg9yD+dl1ToZeVN0935nZt1jaHUJHHHfeUqmFRpceRp2dteceKx+P4yU9+grPOOgtnn322HZckeXR0z5wK1AVdyKg6BmPZmZ45mmEirWhoq/HZsjtRaqZPo2Vsauh39uIKiByLkYSMnkjalmsCwGRaxb/9aT9kzcTa5hD+4YI2265damTNwEhCRpVXwprmEOqD7nkHmQLHorXGhzXNIVR4BYwkskgrVItTSC3VXlx9QSsA4FfbenFwNFngFeWHZVkLckrKMC1YFlDpdV5KCjjN4OaJJ57ARz/6UdTX1+OHP/whrrjiCrz88st2rY3k2XTPnJWNQZzdUoFltT4wDDCSkDGcyKKxwoP6PBaiFTOXwKHSJyJl0xuWS+CwblEIAPB8lz2nplTdxP/7835MpBQ0htz46tuX0+7BCUxmVMSyGtqqvVjZGETApoC+witidVMIy2r9yGo6nagqsMtX1GFTexUM08LNDx8oyYDz8EQaA5NZCBwzM+IlH9KKDp/EI+Rx5s3vnIObgYEBfPOb30RbWxs+/OEPo6KiApqm4b777sM3v/lNrFu3Lh/rJHnmdwlorfFhfUsFVjcF0VrlRVuNtyyGYs5XlVeCaVmwbDrhdH77G6emTpdlWfjhE504MJqET+Lxf+lk1HEZZm5mG8sAKxsDWFLrm1V9zVxM7+Ksba6gXZwCYxgG116yBGG/hJGEjB8/dci231+nmN61OaelMi8TxqelVB01fsmxJ2jntKorrrgCZ555Jvbt24cf/vCHGBoawg9/+MN8rY0UgMRzqA+6cWZDMK+/GKUg6BbgFjjbUlMbWirBMkBPJIOh2On1kbp3xwCeOjgOjmXwT+9YjgYqCH+TrJpLQ9X4Jaw+zTTUbFR4RaxqDGFJOLeLM0a7OAXhk3h8+e3LwDLAM50TeGTfaKGXZBvTsmbqbfJ5Ssowc7WGlV7nnqCdU3DzyCOP4Oqrr8Y3vvENvPOd7wTHOa+IiJCF4hI4VHpF2+7C/S4Bq5tCAE5v9+b5rgn8entuqO01F7ZhzdQ1yRsm0yoSsob2Gi9WNNiXhjoVkWfRHvZhTXMFgh4BIwkZGZsaQpLZW14XwMfOawEA/OyZbvTaWOdWSPuHE5hIqfCIHNYvzt+cx5Siwy/xjjwlNW1Owc0zzzyDZDKJ9evX49xzz8WPfvQjjI+P52tthDhetU+Cbpm2bW1PN/Sbb91N13gK33/0IADgXavr8Y6VdDLqSIZpYSieBcsCKxuD6Ajbn4aajcqpWpyOsBdplXZxCuG9ZzViXXMIqm7iOw8fmHeTUyeZTkmd316V15/rtKqhxu/smWpzWtnGjRvx85//HMPDw7jmmmvw29/+Fo2NjTBNE48++iiSydKsPifkRAJuHi6BQ1az54Xx3Nbc3daB0SQiKWVOz42mVXzzoX1QdBPrmkO4ejOdjDpSLg2VRa0/dxqqLjj3Y952EnkWHWE/1jSFEKBdnAXHMgy+8LalCHkE9EUz+Pkzhwu9pNOiGyaePTQBIL+npDTDBMewjk5JAfM8LeXxeHDVVVfh2Wefxd69e/HFL34R3/72txEOh/Ge97zH7jUS4lgekUfIbd+pqSqfhOV1uaGM2w9HZ/08RTfwrT/vx0RKRVOFG1+5nE5GHSk6lYbqqPFhRWPQUe0NqnwSVjcF0RH2IqXqGEvKeR3DQd5Q4RHxxbctAwPg4ddG8Exn8WYidvfHkJR1hDxvpLfzIaXo8Ll4WwYH59Np7yktW7YM3/nOdzAwMIB77rnHjjURUlRq/BJ0w7TtetOpqW1dE7N6vGVZ+MHjh3BgNAm/xONr7zyTuuJOmU5DcRyDVU1BtIedOR9N4jl0hP1Y2xRCwC1gOJ6lXZwFsrY5hPefnWvo96MnD2EkIRd4RfMznZK6oKM6rzc2WdVAXcD5TSlt+y3nOA5XXnklHnzwQbsuSUhRCLoFSAKHrE2nps6bCm72DsaRlLVTPv5/Xu7H0525k1E30smoGRlVfyMN1RRE7Ty6DS+06V2c9hofUqqO8aRCuzgL4CPnLMIZdX5kVAM3P/y6rTcrC0HWDGw/nKvTy+cpKc0wwXEMKjzObNx3JOfdwhBSZDwihwqPiLRNd9oNITdaqjwwLeDFU6Smnjs0gbte6AMAfPaidqyik1GwLAvRtIqkrKMj7Lw01KlIPIcltblaHJ+Lw3A8a1vgTI6P51h86bJl8EocDo6mcNcLvYVe0py81BOFrJmoDUhYVuvP2+dJyTqCbgH+IuiZRcENIaeJYRhU+yUoeh5SUyc5En5oLIXvP5Y7GfWeNQ14+4o62z5/sdINE8NxGfx0GqrGmWmo2aj25Qqf22t8SCgaJlK0i5NP4YALn7tkCQDgvp2D2NE7WeAVzd50SuqipeG87k5mNB11AVdRNHctzt96Qhwm4OLh4lnINp2a2tiem+S7qy923GtGUgq++dA+qLqJsxZV4KpNrbZ83mKWmSrGrQu6sKY5VBRpqFPJ1eL4sKYpBK9Euzj5dn57Na6YGix7y2MHEU2rBV7RqaVkfSYQu3BJ/iaAK7oBkWcRdOi4hWNRcEOIDXwSD7+bt62hX0uVB3UBF1TDfNMdpKIb+H9/3o9IWkVzhRtfefsyxxf35ZNlWYikFCQVHe1hH85sCJRUQTXDMLkuyk0htFX7kJBpFyefPr2pFS1VHsSzGr736AHH9x96rmsCummhpcqDxVXevH2emZRUkfxuUXBDiA0YhkHY74Js2HNXzTBvDL07MjVlWRZue7wTnWMp+F08vvauM+EtkhebfJhOQwk8i9WNxZ2GOhWXwGFJrQ+rm4Nwi7ldHLt2CskbRJ7FVy5fDolnsWcgjvt2DhR6SSf19BEpqXySdaOodkNL81WAkAIIegSIHGtbp9PpupuXeqLQpk5v/PalfjzTOQGeZfD/veMM1AfL92RURtUxekQaKlxEL7zzNR1Er23O7eLEsxoiKaXkhj8WWnOFB//nonYAwG9e6MW+4USBV3R8kZSCvYNxAPlNScmaARfPOXrcwrEouCHEJj6RR8AlIK3YE9wsq/OjwiMgoxrYMxDHs4cmcPeLUyejLm7HysagLZ+n2ByZhloS9mNFiaWhZuPIXRyXyGGIdnFsd+nyMC5eWgPTAm5++MCs2jIstGcOTcACcEZ9AOGAK2+fJ6XkmgMW0+8ZBTeE2IRlGdQGXMhq9tTdsAwz0/Pm3h39uGXqZNSVaxtw2ZnleTJKN0wMJ7IQeTbXDybsc/R8m3w6chenpcpLuzg2YxgGn724HfVBFyZSCn7wRKfjvrczp6TyuGsD5Or8aopsZ7Q8XxUIyZOAS4DAszNppNM1nZp6bSgBVTexfnEFPnl+eZ6Myqg6xlIy6oLuXBrKn7871WLiEjgsq/NjddPULk6MdnHs4hF5fOXty8GzDLZ3R/HnvcOFXtKMoVgWh8ZSYBlgU0f+gpusasAtcAgVUUoKoOCGEFv5XTz8Eo+UbM/uzarGILwSBwBYVOnBl8vwZNR0Giql6uio8WNFfaCsi6iPh2EYhAO5XZzF1R7EaBfHNh1hHz61qQUA8F/PHkb3eKqwC5oyvWuztrkCIU/+hlgmFQ0hj1h0v3MU3BBiI5ZlEPZLyNiUmuI5Fh87dzFWNwbxtXedCY9YXC8wp0ubSkO5BA6rGss7DTUbLoHD8rpAbhdH4DAUl20rcC9n717dgA0tFdBNC995+EDBew1ZlnVE47787dpYlgXdNBEOOH/cwrHoVYIQm4U8InjWvtTUO1c34P/97SrU5bFg0InSio7xlIz6oBurmoKUhpolhsnVfq1pDmFxlRuTGRXRtEq7OKeBYRhcf+lSVHlFDMay+OnTXQVdT/dEGoOxLESOnanLy4esZsAt8Ai587czlC8U3BBiM79LgM9lX0O/cmNZFiZSCtKajqVhP86kNNS8uMXpXZwQRJ6hXZzTFHQL+NJly8AywBOvj+GJ18cKtpbpXZsNrZV53c1NyjoqvSLcIpe3z5EvFNwQYjOOZVDrl2wbpFlONMPEUDwLt8BhdWMIrTWUhjodR+7iLKrM7eJM0i7OvK1sDOJDGxYBAG7fegiDk9kFX4NpWXimM/+npCzLgmFZqPEXX0oKoOCGkLwIekRwDAvdptRUOUgpudlQDSE3VjcHi/ZF1Yk8Io/ldQGsagyB4xgMxbNQbRz0Wk7+bn0zVjYEIGsmvvPI67aln2dr31ACEykVXpHD2Ysr8/Z50qoBj1hcjfuORMENIXkQcPHwSTzSNOTwlKbTUFlNx/K6XBqq3AqnFwLLMqgLurBuUQiLKj2IpBVMZmgXZ644lsGXLluGgItH93gadzx3eEE//3RK6vz2aoh8/t7C04qOKp8Il1B8KSmAghtC8oLnWNQEREpNncJMGkrksLophJZqSkPl28wuTlMQHJurxcnQz+mcVPkkfP6tSwEAf9wzjBcOR07xDHtohonnDk0AAC5cWpO3z2NaFgzTRLWveHdPC/oqctNNN2HDhg3w+/0Ih8O48sorceDAgVk//7nnngPP81i7dm3+FknIPFV4RLAMHD9VuFByaSgFjSEPVjcFi/qFtNiwLIP6oBvrFoXQVu1FVjVohMMcbWipxJVrGwAAtz3WifGkkvfPubs/huTUKIRVeRy/klZ0eF1C0aakgAIHN1u3bsWWLVuwfft2PProo9B1HZdddhnS6fQpnxuPx/Hxj38cl1566QKslJC5C7gFeEQ6NXUsy7IwnlQgazqW1/lwZgOloQrFI/JYWufHusUVaK5wIy5rGE3IC15HUqw+vrEFHTU+JBUd333kQN5vZKZTUhcuqclrM8+0qiPskyDxxZmSAgoc3Pz1r3/FJz/5SaxYsQJr1qzBHXfcgb6+PuzYseOUz73mmmvwkY98BBs3blyAlRIydwLHosZnX0O/UqAZJgbjWXgkDqum0lDl1nHZiYJuAWfUB3DWogrU+CVE0grGkwrtOp6CwLH48tuXwS1w2DecwD0v9eXtc8mage3dufTXhUvyl5IyTAuWBVT6iq+3zZEcldyOx3Oj2ysrT14Bfscdd6Crqwtf//rXF2JZhMxbhVeEaVFqCgBU3cRYUkZzhQdrmkKUhnIYhmFQ6RWxqjGINc0hBD08RhNZRFIU5JxMQ8iNLW/pAAD8z0v92DMQy8vnefFwFIpuoi7gwtJaX14+B5BLSfkkvqhTUoCDghvLsnDDDTdg8+bNWLly5Qkf19nZiX/6p3/Cb37zG/D8qbeyFUVBIpE46oOQhRJw8/BKfMHbtReaZpgYTylYVOnB8jp/UTYFKxe5ESIurGkKYXVzCC6Rw0giixidrDqhi5bW4G1n1sIC8L1HDiKe1Wz/HG+MW6jJ63TulKojHJAgFHlhv2NWf+2112LPnj245557TvgYwzDwkY98BN/4xjewdOnSWV33pptuQjAYnPlobm62a8mEnJLEc6j2lvepKcO0MJaU0RhyY0mtn05DFQmeY1EfdOOsRRVY0RAEy+b64yRl+9+4S8FnLmhDc4Ub0YyKWx47CNPGQDApa9jZNwkgv6ekDNMCCwYVeRzEuVAYywGh+HXXXYc//OEPePrpp9Ha2nrCx8ViMVRUVIDj3rjrM00TlmWB4zg88sgjuOSSS456jqIoUJQ3qtgTiQSam5sRj8cRCATs/2IIOcZYUsau3knUBd1g83jH5USGaWEkIaMuIOGMhkBRFyiWO1kzMBLPon8yi7RioMIjUCH4MXom0rjh3t3QDAtXbWrB365rsuW6D782gh89eQit1V784EPrbLnm8cSzGniOwfrFFY68CUkkEggGg7N6/y7oT6ZlWbjuuutw//3346mnnjppYAMAgUAAe/fuPervfvKTn+CJJ57A//7v/x73+ZIkQZIot08KJ+AS4JF4ZFQDvjKakWRaFkYTMmr8IpbVUWBT7FwCh5ZqH2r8LgzFshiMZRHLaKj0Fm+jN7u1VHvxDxe04SdPdeFX23qxoiGIpbX+077ukSmpfEqrGpaGS2N3taCvtFu2bMHdd9+NBx54AH6/HyMjIwCAYDAIt9sNALjxxhsxODiIO++8EyzLvqkeJxwOw+VynbROh5BCcgkcqrwihmJy2QQ3lpVLRVV4BSyvC1CNTQnxSjyW1PoRDrgwOJnBcEJGPJsLcoq9TsMOl6+owyv9MTzXFcHNDx/ArR9ce1qDXyMpBa8O5g7bXJDHWVKaYYJjWIRKICUFFLjm5vbbb0c8HsfFF1+M+vr6mY/f/e53M48ZHh5GX1/+jtcRshCqfBJ00yqbgszxlAK/JGA5TfQuWTPHx5srEA5IiKRVjCXlsp+nxjAMrr1kCcJ+CSMJGT9+6tBp/d4/0zkBC8CZ9QGE/S77FnqMlKLD7+IRKPJTUtMcUXOzkOaSsyPELrJm4MXDUUg8W/J1CuNJBaLAYmVDoGTuAsnJmaaFSFpFXzSNiaQCiecQ8ohl3cPo9ZEEvnrfHpgWcO1bOvD2FXXzus4Xfrcbh8ZT+OxF7bhiVb3Nq3zDYCyL5XU+tFTn75j56ZrL+zftIRKyAFwChwqvgJRc2qemomkVPMfgjDo/BTZlhGUZ1PglrG2uwJrmCngkDqPJ8j4+vrwugI+d1wIA+Nkz3eiNnLrz/rEGJ7M4NJ4CywCbOvKbkhJ4BhWe0qlPpeCGkAVS7ZOgW2bJvthPv5Etr/Ojihr0lSVuavL42ubc8fHpwZyJrFayP/cn896zGrGuOQRVN/Gdhw9A0efW7+rpzlwh8bpFFXltqpeUdQRcAvyu0tlVpuCGkAUSdAtw8RyyJTicMClrUAwTy+pzhaakvIk8i6YKD85aXIHldT6YsDAUy5bdnDWWYfCFty1FyCOgL5rBfz1zeNbPtSxrwU5JZTUddQEX2BJKI1JwQ8gC8Yg8Qh4RaaW0gpu0oiOtGlhW60d90F3o5RAHmT4+fvbiCrSHfZB1A0Ox8po+XuER8cW3LQMD4K+vjeDZQxOzel7XeBqDsSxEjsW5rScfSXQ6FN2AyLMIekqjkHgaBTeELKAavwTVKJ0X9qxqICFrWBL2oqmCAhtyfB4xd3z8rMUVWFTlRkLJTR9X9fI4WbW2OYT3n51r6PfDJzoxkpBP+ZzpXZtzWivzegghJesIugX4S+xUIwU3hCyggFuAS+BK4s5V1gxEMyraarxYVOnN67wbUhoCrlzfo+nj45MZFWOJ8jg+/pFzFmF5nR8Z1cDND79+0q/ZMK2Zept8p6QU3URtwFVyv78U3BCygLwih6BbQKrIaw9U3UQkraCt2ovWal9J5epJfjEMgwqviJUNuenjlT4R4yml5KeP8xyLL1+2DF6Jw8HRFO56ofeEj903FEc0rcIrcTh7cUXe1iRrBiS+dBr3HYmCG0IWEMPkjswW83b8kRO+22q8Zd3LhMzf9PHx1U0hrG4KzRwfn8yotg6ddJJwwIXPXbIEAHDfzkHs7J087uO2dubqcs5vr85r1+ekrKPCK8Jbgh3EKbghZIEF3QJEninK1NT0hO+GkAsdJTKDhhTW9PHxdYsqsLIxBJFjMVzCx8fPb6+eacb3/ccOIppWj/p3zTDx3FTRcT5TUpZlQTMM1PilkktJARTcELLgfFKuxXmxHYs1TAvDiSzqAi4srfVD5Onlg9hH4Fg0htxYuyiEM+r9sGBhKF6ax8c/vakVLVUexLMavvfogaPScbv6JpFSdFR6cqm7fMlqBlwCl9f+OYVEr06ELDCGYRD2uyDPsaFXIU1P+A77JSyrC9AUaJI3LoHD4iovzlpcgY6wD4puYCieQVYtnt+XUxF5Fl+5fDkknsWegTju2zkw829bD+Z2bTYvqc5ryjelTKWkSuyU1DQKbggpgIBbgMizRVF7QxO+SSF4RB4d4anj45UepFQNI4lsUfzOzEZzhQf/56J2AMBvXujFvuEEsqqBFw5HAOQ/JaWbZl4HcRYaBTeEFIBf4uF3FcepqbEkTfgmheN3CTijPoizFlWgNuCaOT6ulcDx8UuXh3Hx0hqYFnDzwwfwxOujUHQT9UEXloTzN8AyqxlwC3zJpqQACm4IKQiWZRD2S44fxTCRUuASOSyv9yPgKt0XQuJ8IY+IVY1BrF2UOz4+UQLHxxmGwWcvbkd90IWJlIKfPdMNALhwaU1ei3yTso4qr1jSu7AU3BBSICG3CJ5jHHsHGk2rYFnQhG/iGAzDoNqXOz6+pjkEr4vHSELGZLp4j497RB5fefty8CyD6TjtoiX5S0mZlgXDslDtL+3hthTcEFIgfhePgIt3ZGoqPnUM94y6AE34Jo7DsQxqAy6sbQ5hVVMQIs9iOJ4t2gCnI+zDpza1AACWhH1orvTk7XNlVGOmmWgpowQ6IQUynZp6fSSJCgftjCRlDbJu4Mz6AE34Jo42fXy80iNiz0AMkZSKmiLdkXj36gY0V3iwKI+BDZAbdNtU4S75E4+0c0NIAQXdIniWdcxsnSMnfDeEaBAmKQ5ukUN72AeLsZBRnbcTOhsMw2Ddooq87pQapgXTslDpc87NVL5QcENIAQXcAnwOSU3RhG9SzKp9EhZXejCZ0Yq6yDifMqoOj8Qj5KbghhCSRxzLoMYnIVPgBmU04ZuUgkWVXlT7RETSSqGX4kgpRUfYJ5VFd/HS/woJcbgKrwiWYQp2t6kZuQnfLVUemvBNiprIs2iv8YFjGEfshjqJYVpgGJRFSgqg4IaQggu4ePikwqSmdMPEWDI34bsj7KMJ36ToVXhFtFR7EM+qjqllc4KUosMrlnbjviNRcENIgfEci5qAiPQCF0IapoVRmvBNSlBThQd1ARfGU5SempZWdIQDEoQy+T0vj6+SEIcLeUQwwIKlpgzTwghN+CYliudYtNb4IAksElmt0MspON0wwTIMKr3FeUx+PugVjRAHCLgEeCV+QY6xWlMTvqt9NOGblK6gW0BbtQ8pVXdsF/CFklJ0+N25pqHlgoIbQhxA5FnU+KS8p6YsK5eKqvDmBmGW8mwZQhpCbtQHXRhLyoVeSkFlNB21fqmsUs/l85US4nAVXhGWhby2kD9ywrePJnyTEsexDNpqfPC5BExm1EIvpyA0wwTHsKjwlscpqWkU3BDiEAE3D4/IIaPkp+fNREqBS2BpwjcpKz6JR3uNF7JmQNEL20+qEFKKjoCbh7/MfucpuCHEISSeQ5VPQioPqanJqQnfy+sDNOGblJ1avwtNFW5MpFRYRTpcc74yqoHagKvs2jxQcEOIg1R5RRimaWtqKp7VYFgmzqgLoJomfJMyxLIMWqt9CLkFRNPlk55SdRMCz5TFuIVjUXBDiIME3LlTU1mbxjGkZB2ybmBZHU34JuXNLXJoq/FCNy3IWnmkp1KKjqBLgL+MTklNo+CGEAdxCRwqPaIt3Yozqo6UqmFp2EcTvgkBUOOXsKjSg0hayWvhvlNkdR21AVdZjlSh4IYQh6n2S9BN67RqA7KqgXhWQ0eND82VHhtXR0jxYhgGi6o8qPSKmCjx7sWKbkDiWAQ95VVIPI2CG0IcJuAS4BZZZOe5da7oUxO+q71YXEUTvgk5kkvg0F7jA8NgQZpmFkpK1hH0CPCXacsHCm4IcRi3yKFinqkpzTARSU1N+K6hCd+EHE+VT8LiSi8mM+qCjTxZaLJuIux3le3NDQU3hDhQjV+CbphzSk3phonRpIxmmvBNyCk1V3pQ45dKMj0lawZcAlvWbR8ouCHEgYJuAS6Bg6zNbibO9ITvxpCbJnwTMgsiz6K9xgeeY5CSSys9lZR1VHhEeMt4vAq9AhLiQB6RR2iWqSnTognfhMxHyCOitdqLhKJCL5HhmpZlQTMM1Pilsk1JARTcEOJY1X4JmnnyF1zLsjBCE74JmbfGkBt1ATfGSyQ9ldUMuAQOoTI9JTWNghtCHCroFiBx7AkbjlmWhbGkgqCbJnwTMl88x6K1xguXwCGe1Qq9nNOWknVUeEV4xPI8JTWNghtCHMorcgh6BKRPkJoaTyrwShzOoAnfhJyWgEtAW40XaUWDVsTpKcuyoFsWwn7qRk7BDSEOxTAMavwS5ONMMp5IKZAEFsvrAgi6y3v7mRA7NATdaKzwYDwlF+1wzYxqwE0pKQAU3BDiaAG3AIlnoRwR4EymVbBMbsJ3hbd8j3oSYqfccE0vfJKAWKY401NJRUe1T6TaO1BwQ4ij+SUefpcwc1R1esL38nqa8E2I3bwSj/YaHxTDOOqGohiYlgXDNOl1YQoFN4Q4GMMwqA24IOtGbsK3lpvwXUsTvgnJi9qAhKYKNyaSSlGlpzKKAZ/EI0BpagAU3BDieAG3AIFncxO+a2nCNyH5xDAMWqt9CHlFRNNqoZcza2lVR7VPopTUFApuCHE4v8SjyivShG9CFohL4NBW44VuWsiqzk9PGaYF07JQRSmpGRTcEOJwLMtgZUMQLdU04ZuQhVLjk7C4yoNoRnH8cM2MqsMr8XRy8ggU3BBSBHiOpcCGkAXEMAwWVXlQ5ZMQcXj34pSiI+yXaPTKEeg7QQghhByHxHNor/GBZXHCZpqFZpgWGAbUFuIYFNwQQgghJ1DpFdFS5UU8qzoyPZVSdPhEgVJSx6DghhBCCDmJpkoPwgEXJhyYnkorOsIBCQJHb+dHou8GIYQQchICx6Kt2geBY5CUndO9WDdMcCxDKanjoOCGEEIIOYWgR0BrtRcJWXfMcM2UosPnolNSx0PBDSGEEDILjRUe1AddGHdIeiqt6qj1S+BYOkl5LApuCCGEkFngWAZtNV54RQ6xTGG7F2uGCZ5jKSV1AhTcEEIIIbPkdwlorfEhqxlQ9cKlp1KyjoCLR8BFKanjoeCGEEIImYP6gAtNFW6Mp+SCDdfMqDpqAy6wlJI6LgpuCCGEkDlgWQYt1V4EXAImMwt/ekrVTQgCi5CHUlInQsENIYQQMkcekUd72AfVMCFrCztcMylrCLoE+CV+QT9vMaHghhBCCJmHsF9Cc4UbkbQCcwHTU7JhUErqFCi4IYQQQuaBYXLpqZBHRDS9MKenZM2AxLEIeaiQ+GQouCGEEELmySXkhmsaloWMmv/hmilFR8gjwkcpqZOi4IYQQgg5DTV+CS1VHkxmtLwP11R0E+GACwxDKamToeCGEEIIOU2LKr2o9ol5Ha6ZVQ24BJbGLcwCBTeEEELIaRJ5Fu01PvAsg5SSn/RUStFR4RHhFbm8XL+UUHBDCCGE2KDCK6Kl2oN4VrU9PWVZFlTDQDggUUpqFgoa3Nx0003YsGED/H4/wuEwrrzyShw4cOCkz3n22WexadMmVFVVwe12Y/ny5bjlllsWaMWEEELIiTVVeFAXcGE8Jdt63axmwC1ylJKapYKWW2/duhVbtmzBhg0boOs6/vmf/xmXXXYZ9u3bB6/Xe9zneL1eXHvttVi9ejW8Xi+effZZXHPNNfB6vfjMZz6zwF8BIYQQ8gaeY9Fa40Nc1pDIagjYFIykZB3hoASPSKekZoOxCjUY4zjGx8cRDoexdetWXHjhhbN+3nvf+154vV78+te/PuVjE4kEgsEg4vE4AoHA6SyXEEIIOa7+aAb7hhOo8UkQuNNLkliWhaG4jLXNIdQFXTatsPjM5f3bUTU38XgcAFBZWTnr5+zatQvPP/88LrroouP+u6IoSCQSR30QQggh+dQQcqM+6MJ48vRPT2VUA16Jo8Z9c+CY4MayLNxwww3YvHkzVq5cecrHNzU1QZIkrF+/Hlu2bMHVV1993MfddNNNCAaDMx/Nzc12L50QQgg5CscyaKvxweviMZk5ve7FSUVHlVeES6BTUrPlmODm2muvxZ49e3DPPffM6vHPPPMMXn75Zfz0pz/FrbfeesLn3XjjjYjH4zMf/f39di6bEEIIOS6fxKOt2gtZM6Do8xuuaVoWTNNEtU+yeXWlzRGVSddddx0efPBBPP3002hqaprVc1pbWwEAq1atwujoKP71X/8VH/7wh9/0OEmSIEn0Q0EIIWTh1QVciGVU9EWzaAjOvbNwRjHglXgEKSU1JwXdubEsC9deey1+//vf44knnpgJWOZzHUXJX1dIQgghZD5YlkFrtQ8htzCv4ZopVUO1T4LEU0pqLgq6c7NlyxbcfffdeOCBB+D3+zEyMgIACAaDcLvdAHJppcHBQdx5550AgB//+MdYtGgRli9fDiDX9+a73/0urrvuusJ8EYQQQshJuEUObTVe7BmIQ9aMWdfOGKYF0wKqKCU1ZwUNbm6//XYAwMUXX3zU399xxx345Cc/CQAYHh5GX1/fzL+Zpokbb7wRhw8fBs/zaG9vx7e//W1cc801C7VsQgghZE5q/BIWVXrQPZFCfdANdhbpqbSiwyfx1LhvHhzV52YhUJ8bQgghhSBrBvYMxJCSDdT4T70bMxTPoq3aiyW1/gVYnfMVbZ8bQgghpFS5BA5tNT5YjIWMevLhmoZpgWWASq+4QKsrLRTcEEIIIQuk2iehpdKLyczJh2umFB1+SaCU1DxRcEMIIYQsoOZKD2r8EiZSJz7lm1Z01Pgl8Kc5uqFc0XeNEEIIWUAiz6K9xgeeY5CS35ye0g0THMtQSuo0UHBDCCGELLCQR0RrtRcJRYVumEf9W0rR4XPxtk0UL0cU3BBCCCEF0Bhyoy7gxvgx6am0qqMuIIFj59bNmLyBghtCCCGkAHiORWuNFy6BQzyrAQA0wwTPsQh5KCV1Oii4IYQQQgok4BLQVuNFWtGgGSZSso6gW0DARSmp0+GIwZmEEEJIuaoPuhHLaBiMZWBZQGuNFyylpE4L7dwQQgghBcSxDFqrvfBJAkSepUJiG9DODSGEEFJgXolHR9iH8aSCgIvemk8XfQcJIYQQB6gNuBD2S2BmMVTz/2/v3oOiKv8wgD/LdYFdEUEQDEHTBERDQE1FyUoIzMFCVFxU0hxTTNCUMi/9yguKaZYaDo0RIoSVmOQFJUUQzBveCky85KU0yRsiJHJ5f3+YZ1wRikQP7j6fmZ3xXPZ9n3PedfY77znsofrxshQREVETwcKmcbC4ISIiIp3C4oaIiIh0CosbIiIi0iksboiIiEinsLghIiIincLihoiIiHQKixsiIiLSKSxuiIiISKewuCEiIiKdwuKGiIiIdAqLGyIiItIpLG6IiIhIp7C4ISIiIp1iJHeAx00IAQC4ceOGzEmIiIjo37r7vX33e7w+elfclJaWAgAcHR1lTkJEREQNVVpaCktLy3r3UYh/UwLpkJqaGly4cAFqtRoKhaJR275x4wYcHR1x/vx5NGvWrFHbpobjeDQtHI+mh2PStHA86ieEQGlpKRwcHGBgUP9dNXo3c2NgYICnnnrqkfbRrFkzfjCbEI5H08LxaHo4Jk0Lx6Nu/zRjcxdvKCYiIiKdwuKGiIiIdAqLm0ZkamqK999/H6ampnJHIXA8mhqOR9PDMWlaOB6NR+9uKCYiIiLdxpkbIiIi0iksboiIiEinsLghIiIincLippF89tlnaNu2LZRKJby8vLBr1y65I+mtmJgYdOvWDWq1Gra2thg0aBCOHz8udyz6W0xMDBQKBaKiouSOord+//13hIWFwdraGubm5vDw8EB+fr7csfRSVVUVZs6cibZt28LMzAzt2rXDhx9+iJqaGrmjPdFY3DSCtWvXIioqCjNmzMChQ4fQp08fBAQE4Ny5c3JH00vZ2dmIiIjAnj17kJmZiaqqKvj5+aGsrEzuaHpv//79iI+PR5cuXeSOoreuXbuG3r17w9jYGFu2bEFhYSEWL16M5s2byx1NLy1cuBArV67E8uXLcezYMcTGxmLRokVYtmyZ3NGeaPxrqUbQo0cPeHp6Ii4uTlrn6uqKQYMGISYmRsZkBAB//vknbG1tkZ2djb59+8odR2/dvHkTnp6e+OyzzzB37lx4eHhg6dKlcsfSO++++y7y8vI4u9xEvPLKK7Czs8OqVaukdcHBwTA3N0dSUpKMyZ5snLl5SLdv30Z+fj78/Py01vv5+WH37t0ypaJ7lZSUAABatGghcxL9FhERgQEDBuCll16SO4peS09Ph7e3N0JCQmBra4uuXbvi888/lzuW3vLx8cH27dtRVFQEADhy5Ahyc3MRGBgoc7Inm949W6qxXb58GdXV1bCzs9Nab2dnhz/++EOmVHSXEAJTpkyBj48P3N3d5Y6jt1JTU3Hw4EHs379f7ih67/Tp04iLi8OUKVPw3nvvYd++fZg0aRJMTU0xcuRIuePpnXfeeQclJSVwcXGBoaEhqqurMW/ePISGhsod7YnG4qaR3P+EcSFEoz91nBpu4sSJOHr0KHJzc+WOorfOnz+PyMhIbNu2DUqlUu44eq+mpgbe3t6YP38+AKBr164oKChAXFwcixsZrF27FmvWrEFKSgo6deqEw4cPIyoqCg4ODhg1apTc8Z5YLG4eko2NDQwNDWvN0hQXF9eazaHH66233kJ6ejpycnIe+ZPgqW75+fkoLi6Gl5eXtK66uho5OTlYvnw5KioqYGhoKGNC/WJvbw83Nzetda6urli3bp1MifTbtGnT8O6772LYsGEAgM6dO+Ps2bOIiYlhcfMQeM/NQzIxMYGXlxcyMzO11mdmZqJXr14ypdJvQghMnDgRaWlp2LFjB9q2bSt3JL324osv4qeffsLhw4ell7e3NzQaDQ4fPszC5jHr3bt3rZ9GKCoqgpOTk0yJ9Ft5eTkMDLS/ig0NDfmn4A+JMzeNYMqUKRgxYgS8vb3Rs2dPxMfH49y5c3jzzTfljqaXIiIikJKSgg0bNkCtVkuzapaWljAzM5M5nf5Rq9W17neysLCAtbU174OSweTJk9GrVy/Mnz8fQ4YMwb59+xAfH4/4+Hi5o+mlgQMHYt68eWjTpg06deqEQ4cOYcmSJRg9erTc0Z5sghrFihUrhJOTkzAxMRGenp4iOztb7kh6C8ADXwkJCXJHo7/5+vqKyMhIuWPore+//164u7sLU1NT4eLiIuLj4+WOpLdu3LghIiMjRZs2bYRSqRTt2rUTM2bMEBUVFXJHe6Lxd26IiIhIp/CeGyIiItIpLG6IiIhIp7C4ISIiIp3C4oaIiIh0CosbIiIi0iksboiIiEinsLghIiIincLihoiIiHQKixuiJkihUOC77757qDaef/55REVFScvOzs5YunTpQ7WpC8LDwzFo0CC5YzSqnTt3QqFQ4Pr164+0H108d6SbWNwQPWbFxcUYN24c2rRpA1NTU7Rq1Qr+/v748ccf5Y6GGzduYMaMGXBxcYFSqUSrVq3w0ksvIS0tDbryY+affPIJvvzyy0fWPgsAIvnxwZlEj1lwcDAqKyuRmJiIdu3a4dKlS9i+fTuuXr0qa67r16/Dx8cHJSUlmDt3Lrp16wYjIyNkZ2cjOjoaL7zwApo3by5rxsZgaWkpdwQiesQ4c0P0GF2/fh25ublYuHAh+vXrBycnJ3Tv3h3Tp0/HgAEDtPa9fPkyXn31VZibm6NDhw5IT0/X2l5YWIjAwECoVCrY2dlhxIgRuHz58n/O9t577+HMmTPYu3cvRo0aBTc3NzzzzDMYO3YsDh8+DJVKBQC4du0aRo4cCSsrK5ibmyMgIAAnTpyQ2vnyyy/RvHlzbNy4ER07doS5uTkGDx6MsrIyJCYmwtnZGVZWVnjrrbdQXV0tvc/Z2Rlz5szB8OHDoVKp4ODggGXLlmllXLJkCTp37gwLCws4OjpiwoQJuHnzZq2+t27dCldXV6hUKrz88su4ePGitM/9MytCCMTGxqJdu3YwMzPDs88+i2+//Vbafu3aNWg0GrRs2RJmZmbo0KEDEhIS/vV5ff755zFp0iRER0ejRYsWaNWqFf73v/9J20NDQzFs2DCt91RWVsLGxkbqp6KiApMmTYKtrS2USiV8fHywf//+B/ZXUlICMzMzZGRkaK1PS0uDhYWFdL5+//13DB06FFZWVrC2tkZQUBDOnDkj7V9dXY0pU6agefPmsLa2RnR0tM7M3pHuY3FD9BipVCqoVCp89913qKioqHffDz74AEOGDMHRo0cRGBgIjUYjze5cvHgRvr6+8PDwwIEDB5CRkYFLly5hyJAh/ylXTU0NUlNTodFo4ODg8MDcRkZ3JnrDw8Nx4MABpKen48cff4QQAoGBgaisrJT2Ly8vx6efforU1FRkZGRg586deO2117B582Zs3rwZSUlJiI+P1yoiAGDRokXo0qULDh48iOnTp2Py5MnIzMyUthsYGODTTz/Fzz//jMTEROzYsQPR0dFabZSXl+Ojjz5CUlIScnJycO7cOUydOrXOY585cyYSEhIQFxeHgoICTJ48GWFhYcjOzgYAzJo1C4WFhdiyZQuOHTuGuLg42NjYNOj8JiYmwsLCAnv37kVsbCw+/PBD6bg0Gg3S09O1irStW7eirKwMwcHBAIDo6GisW7cOiYmJOHjwINq3bw9/f/8HzvZZWlpiwIABSE5O1lqfkpKCoKAgqFQqlJeXo1+/flCpVMjJyUFubq5UCN6+fRsAsHjxYnzxxRdYtWoVcnNzcfXqVaxfv75Bx00kGzkfSU6kj7799lthZWUllEql6NWrl5g+fbo4cuSI1j4AxMyZM6XlmzdvCoVCIbZs2SKEEGLWrFnCz89P6z3nz58XAMTx48eFEEL4+vqKyMhIabuTk5P4+OOPH5jp0qVLAoBYsmRJvdmLiooEAJGXlyetu3z5sjAzMxNff/21EEKIhIQEAUCcPHlS2mfcuHHC3NxclJaWSuv8/f3FuHHjtPK9/PLLWv0NHTpUBAQE1Jnn66+/FtbW1tLyg/pesWKFsLOzk5ZHjRolgoKChBB3zqtSqRS7d+/WanfMmDEiNDRUCCHEwIEDxeuvv173SbnPve0LcWccfHx8tPbp1q2beOedd4QQQty+fVvY2NiI1atXS9tDQ0NFSEiIlNHY2FgkJydL22/fvi0cHBxEbGysEEKIrKwsAUBcu3ZNCCFEWlqaUKlUoqysTAghRElJiVAqlWLTpk1CCCFWrVolOnbsKGpqaqQ2KyoqhJmZmdi6dasQQgh7e3uxYMECaXtlZaV46qmntI6NqKnizA3RYxYcHIwLFy4gPT0d/v7+2LlzJzw9PWvd5NqlSxfp3xYWFlCr1SguLgYA5OfnIysrS5oJUqlUcHFxAQCcOnWqwZnE35cbFApFvfsdO3YMRkZG6NGjh7TO2toaHTt2xLFjx6R15ubmePrpp6VlOzs7ODs7S5e27q67ezx39ezZs9byve1mZWWhf//+aN26NdRqNUaOHIkrV66grKyszr7t7e1r9XNXYWEhbt26hf79+2udy9WrV0vncfz48UhNTYWHhweio6Oxe/fues/Rg9w7lvdnMjY2RkhIiDTTUlZWhg0bNkCj0QC4M56VlZXo3bu39H5jY2N0795d69zca8CAATAyMpIuZa5btw5qtRp+fn4A7nx+Tp48CbVaLR1zixYtcOvWLZw6dQolJSW4ePGi1ngYGRnB29u7wcdOJAfeUEwkA6VSif79+6N///6YPXs23njjDbz//vsIDw+X9jE2NtZ6j0KhQE1NDYA7l5EGDhyIhQsX1mrb3t6+wXlatmwJKyurOr8s7xJ13HMhhNAqjB6Uvb7jqc/dds+ePYvAwEC8+eabmDNnDlq0aIHc3FyMGTNG65LYg/qpK/fd/jdt2oTWrVtrbTM1NQUABAQE4OzZs9i0aRN++OEHvPjii4iIiMBHH330j9nry3TvsWs0Gvj6+qK4uBiZmZlQKpUICAgAUHfhef85v5eJiQkGDx6MlJQUDBs2DCkpKRg6dKh0abGmpgZeXl61Ll0Bdz4LRE86ztwQNQFubm5asw//xNPTEwUFBXB2dkb79u21XhYWFg3u38DAAEOHDkVycjIuXLhQa3tZWRmqqqrg5uaGqqoq7N27V9p25coVFBUVwdXVtcH93m/Pnj21lu/OSB04cABVVVVYvHgxnnvuOTzzzDMPzNoQbm5uMDU1xblz52qdR0dHR2m/li1bIjw8HGvWrMHSpUsRHx//UP3er1evXnB0dMTatWuRnJyMkJAQmJiYAADat28PExMT5ObmSvtXVlbiwIED9Z5zjUaDjIwMFBQUICsrS5oJAu58fk6cOAFbW9tax21paQlLS0vY29trjUdVVRXy8/Mb9biJHhUWN0SP0ZUrV/DCCy9gzZo1OHr0KH799Vd88803iI2NRVBQ0L9uJyIiAlevXkVoaCj27duH06dPY9u2bRg9erTWXyA1xPz58+Ho6IgePXpg9erVKCwsxIkTJ/DFF1/Aw8MDN2/eRIcOHRAUFISxY8ciNzcXR44cQVhYGFq3bt2g/HXJy8tDbGwsioqKsGLFCnzzzTeIjIwEADz99NOoqqrCsmXLcPr0aSQlJWHlypUP1Z9arcbUqVMxefJkJCYm4tSpUzh06BBWrFiBxMREAMDs2bOxYcMGnDx5EgUFBdi4cWOjFHL3UigUGD58OFauXInMzEyEhYVJ2ywsLDB+/HhMmzYNGRkZKCwsxNixY1FeXo4xY8bU2aavry/s7Oyg0Wjg7OyM5557Ttqm0WhgY2ODoKAg7Nq1C7/++iuys7MRGRmJ3377DQAQGRmJBQsWYP369fjll18wYcKER/4jgUSNhcUN0WOkUqnQo0cPfPzxx+jbty/c3d0xa9YsjB07FsuXL//X7Tg4OCAvLw/V1dXw9/eHu7s7IiMjYWlpCQOD//bf2srKCnv27EFYWBjmzp2Lrl27ok+fPvjqq6+waNEi6fdhEhIS4OXlhVdeeQU9e/aEEAKbN2+udenlv3j77beRn5+Prl27Ys6cOVi8eDH8/f0BAB4eHliyZAkWLlwId3d3JCcnIyYm5qH7nDNnDmbPno2YmBi4urrC398f33//Pdq2bQvgziWe6dOno0uXLujbty8MDQ2Rmpr60P3eT6PRoLCwEK1bt9a6vwYAFixYgODgYIwYMQKenp44efIktm7dCisrqzrbUygUCA0NxZEjR7RmbYA79yXl5OSgTZs2eO211+Dq6orRo0fjr7/+QrNmzQDcGYuRI0ciPDwcPXv2hFqtxquvvtrox030KChEXRejiYgeI2dnZ0RFRWk9MoKI6L/gzA0RERHpFBY3REREpFN4WYqIiIh0CmduiIiISKewuCEiIiKdwuKGiIiIdAqLGyIiItIpLG6IiIhIp7C4ISIiIp3C4oaIiIh0CosbIiIi0iksboiIiEin/B+z+vJzS/UauQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.lineplot(x=df['Shell Companies Involved'], y=df['Amount (USD)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OOIer9R86wGl",
        "outputId": "29b4429b-5f0a-4105-8359-f7866bb6ddbd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Transaction ID</th>\n",
              "      <th>Country</th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Transaction Type</th>\n",
              "      <th>Date of Transaction</th>\n",
              "      <th>Person Involved</th>\n",
              "      <th>Industry</th>\n",
              "      <th>Destination Country</th>\n",
              "      <th>Reported by Authority</th>\n",
              "      <th>Source of Money</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <th>Financial Institution</th>\n",
              "      <th>Tax Haven Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TX0000000001</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>3.267530e+06</td>\n",
              "      <td>Offshore Transfer</td>\n",
              "      <td>2013-01-01 00:00:00</td>\n",
              "      <td>Person_1101</td>\n",
              "      <td>Construction</td>\n",
              "      <td>USA</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Bank_40</td>\n",
              "      <td>Singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TX0000000002</td>\n",
              "      <td>China</td>\n",
              "      <td>4.965767e+06</td>\n",
              "      <td>Stocks Transfer</td>\n",
              "      <td>2013-01-01 01:00:00</td>\n",
              "      <td>Person_7484</td>\n",
              "      <td>Luxury Goods</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>Bank_461</td>\n",
              "      <td>Bahamas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TX0000000003</td>\n",
              "      <td>UK</td>\n",
              "      <td>9.416750e+04</td>\n",
              "      <td>Stocks Transfer</td>\n",
              "      <td>2013-01-01 02:00:00</td>\n",
              "      <td>Person_3655</td>\n",
              "      <td>Construction</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Bank_387</td>\n",
              "      <td>Switzerland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TX0000000004</td>\n",
              "      <td>UAE</td>\n",
              "      <td>3.864201e+05</td>\n",
              "      <td>Cash Withdrawal</td>\n",
              "      <td>2013-01-01 03:00:00</td>\n",
              "      <td>Person_3226</td>\n",
              "      <td>Oil &amp; Gas</td>\n",
              "      <td>Russia</td>\n",
              "      <td>False</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>Bank_353</td>\n",
              "      <td>Panama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TX0000000005</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>6.433784e+05</td>\n",
              "      <td>Cryptocurrency</td>\n",
              "      <td>2013-01-01 04:00:00</td>\n",
              "      <td>Person_7975</td>\n",
              "      <td>Real Estate</td>\n",
              "      <td>USA</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>Bank_57</td>\n",
              "      <td>Luxembourg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Transaction ID       Country  Amount (USD)   Transaction Type  \\\n",
              "0   TX0000000001        Brazil  3.267530e+06  Offshore Transfer   \n",
              "1   TX0000000002         China  4.965767e+06    Stocks Transfer   \n",
              "2   TX0000000003            UK  9.416750e+04    Stocks Transfer   \n",
              "3   TX0000000004           UAE  3.864201e+05    Cash Withdrawal   \n",
              "4   TX0000000005  South Africa  6.433784e+05     Cryptocurrency   \n",
              "\n",
              "   Date of Transaction Person Involved      Industry Destination Country  \\\n",
              "0  2013-01-01 00:00:00     Person_1101  Construction                 USA   \n",
              "1  2013-01-01 01:00:00     Person_7484  Luxury Goods        South Africa   \n",
              "2  2013-01-01 02:00:00     Person_3655  Construction         Switzerland   \n",
              "3  2013-01-01 03:00:00     Person_3226     Oil & Gas              Russia   \n",
              "4  2013-01-01 04:00:00     Person_7975   Real Estate                 USA   \n",
              "\n",
              "   Reported by Authority Source of Money  Money Laundering Risk Score  \\\n",
              "0                   True         Illegal                            6   \n",
              "1                  False         Illegal                            9   \n",
              "2                   True         Illegal                            1   \n",
              "3                  False         Illegal                            7   \n",
              "4                   True         Illegal                            1   \n",
              "\n",
              "   Shell Companies Involved Financial Institution Tax Haven Country  \n",
              "0                         1               Bank_40         Singapore  \n",
              "1                         0              Bank_461           Bahamas  \n",
              "2                         3              Bank_387       Switzerland  \n",
              "3                         2              Bank_353            Panama  \n",
              "4                         9               Bank_57        Luxembourg  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "gYV9SDr56wGl",
        "outputId": "9f15bd63-6e3d-40d5-e23c-748f0c8d15f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transaction ID                 0\n",
              "Country                        0\n",
              "Amount (USD)                   0\n",
              "Transaction Type               0\n",
              "Date of Transaction            0\n",
              "Person Involved                0\n",
              "Industry                       0\n",
              "Destination Country            0\n",
              "Reported by Authority          0\n",
              "Source of Money                0\n",
              "Money Laundering Risk Score    0\n",
              "Shell Companies Involved       0\n",
              "Financial Institution          0\n",
              "Tax Haven Country              0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "LYEVd61j6wGm",
        "outputId": "260725a0-45d3-4172-b73b-20e5229554fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transaction ID                  object\n",
              "Country                         object\n",
              "Amount (USD)                   float64\n",
              "Transaction Type                object\n",
              "Date of Transaction             object\n",
              "Person Involved                 object\n",
              "Industry                        object\n",
              "Destination Country             object\n",
              "Reported by Authority             bool\n",
              "Source of Money                 object\n",
              "Money Laundering Risk Score      int64\n",
              "Shell Companies Involved         int64\n",
              "Financial Institution           object\n",
              "Tax Haven Country               object\n",
              "dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "kANCm_zF6wGm",
        "outputId": "33e75b1b-38bc-4a78-9dc6-59fe2e9b2fed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.000000e+04</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.501818e+06</td>\n",
              "      <td>5.526400</td>\n",
              "      <td>4.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.424364e+06</td>\n",
              "      <td>2.893603</td>\n",
              "      <td>2.879773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.003180e+04</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.279005e+06</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.501310e+06</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.722416e+06</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.999812e+06</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Amount (USD)  Money Laundering Risk Score  Shell Companies Involved\n",
              "count  1.000000e+04                 10000.000000              10000.000000\n",
              "mean   2.501818e+06                     5.526400                  4.469400\n",
              "std    1.424364e+06                     2.893603                  2.879773\n",
              "min    1.003180e+04                     1.000000                  0.000000\n",
              "25%    1.279005e+06                     3.000000                  2.000000\n",
              "50%    2.501310e+06                     6.000000                  4.000000\n",
              "75%    3.722416e+06                     8.000000                  7.000000\n",
              "max    4.999812e+06                    10.000000                  9.000000"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znkDwy846wGm"
      },
      "source": [
        "## Processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7YWgMej6wGm"
      },
      "source": [
        "### Handle missing values if applicable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R1AAOvjD6wGm"
      },
      "outputs": [],
      "source": [
        "# For numerical features\n",
        "numerical_features = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[numerical_features] = imputer.fit_transform(df[numerical_features])\n",
        "\n",
        "# For categorical features\n",
        "categorical_features = ['Country', 'Transaction Type', 'Person Involved', 'Industry',\n",
        "                        'Destination Country', 'Financial Institution', 'Tax Haven Country']\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "df[categorical_features] = imputer_cat.fit_transform(df[categorical_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5cXCduR6wGm"
      },
      "source": [
        "### Dropping Features and OHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4tgHlLPf6wGm"
      },
      "outputs": [],
      "source": [
        "# Drop Irrelevant Features\n",
        "df.drop('Transaction ID', axis=1, inplace=True) # Dropped because it is unique for each transaction\n",
        "# df.drop('Person Involved', axis=1, inplace=True) # Frequency Encoding will be implemented\n",
        "# df.drop('Financial Institution', axis=1, inplace=True) # Implement Frequency Encoding\n",
        "df.drop('Date of Transaction', axis=1, inplace=True) # Date of transaction is not relevant\n",
        "\n",
        "# Convert 'Reported by Authority' to integer\n",
        "df['Reported by Authority'] = df['Reported by Authority'].astype(int)\n",
        "\n",
        "# Frequency encoding for 'Financial Institution'\n",
        "df['Financial Institution'] = df.groupby('Financial Institution')['Financial Institution'].transform('count')\n",
        "\n",
        "# Frequency encoding for 'Person Involved'\n",
        "df['Person Involved'] = df.groupby('Person Involved')['Person Involved'].transform('count')\n",
        "\n",
        "# Encode target variable\n",
        "le = LabelEncoder()\n",
        "df['Source of Money'] = le.fit_transform(df['Source of Money'])\n",
        "\n",
        "# One-Hot Encode nominal categorical features\n",
        "nominal_features = ['Country', 'Transaction Type', 'Industry',\n",
        "                    'Destination Country', 'Tax Haven Country']\n",
        "df = pd.get_dummies(df, columns=nominal_features, drop_first=True)\n",
        "\n",
        "dummy_columns = df.filter(like='_').columns\n",
        "df[dummy_columns] = df[dummy_columns].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRGGPdSZ6wGm",
        "outputId": "b50b172e-b0f2-40e8-86e3-31928874c553"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n",
              "       'Source of Money', 'Money Laundering Risk Score',\n",
              "       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n",
              "       'Country_India', 'Country_Russia', 'Country_Singapore',\n",
              "       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n",
              "       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n",
              "       'Transaction Type_Offshore Transfer',\n",
              "       'Transaction Type_Property Purchase',\n",
              "       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n",
              "       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n",
              "       'Industry_Oil & Gas', 'Industry_Real Estate',\n",
              "       'Destination Country_China', 'Destination Country_India',\n",
              "       'Destination Country_Russia', 'Destination Country_Singapore',\n",
              "       'Destination Country_South Africa', 'Destination Country_Switzerland',\n",
              "       'Destination Country_UAE', 'Destination Country_UK',\n",
              "       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n",
              "       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n",
              "       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "W6CsiyNz6wGm",
        "outputId": "17179b88-34fe-42f4-f1f2-ad06c9bb390b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Person Involved</th>\n",
              "      <th>Reported by Authority</th>\n",
              "      <th>Source of Money</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <th>Financial Institution</th>\n",
              "      <th>Country_China</th>\n",
              "      <th>Country_India</th>\n",
              "      <th>Country_Russia</th>\n",
              "      <th>...</th>\n",
              "      <th>Destination Country_South Africa</th>\n",
              "      <th>Destination Country_Switzerland</th>\n",
              "      <th>Destination Country_UAE</th>\n",
              "      <th>Destination Country_UK</th>\n",
              "      <th>Destination Country_USA</th>\n",
              "      <th>Tax Haven Country_Cayman Islands</th>\n",
              "      <th>Tax Haven Country_Luxembourg</th>\n",
              "      <th>Tax Haven Country_Panama</th>\n",
              "      <th>Tax Haven Country_Singapore</th>\n",
              "      <th>Tax Haven Country_Switzerland</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.267530e+06</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.965767e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.416750e+04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.864201e+05</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.433784e+05</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Amount (USD)  Person Involved  Reported by Authority  Source of Money  \\\n",
              "0  3.267530e+06                2                      1                0   \n",
              "1  4.965767e+06                1                      0                0   \n",
              "2  9.416750e+04                1                      1                0   \n",
              "3  3.864201e+05                5                      0                0   \n",
              "4  6.433784e+05                4                      1                0   \n",
              "\n",
              "   Money Laundering Risk Score  Shell Companies Involved  \\\n",
              "0                          6.0                       1.0   \n",
              "1                          9.0                       0.0   \n",
              "2                          1.0                       3.0   \n",
              "3                          7.0                       2.0   \n",
              "4                          1.0                       9.0   \n",
              "\n",
              "   Financial Institution  Country_China  Country_India  Country_Russia  ...  \\\n",
              "0                     17              0              0               0  ...   \n",
              "1                     24              1              0               0  ...   \n",
              "2                     12              0              0               0  ...   \n",
              "3                     18              0              0               0  ...   \n",
              "4                     19              0              0               0  ...   \n",
              "\n",
              "   Destination Country_South Africa  Destination Country_Switzerland  \\\n",
              "0                                 0                                0   \n",
              "1                                 1                                0   \n",
              "2                                 0                                1   \n",
              "3                                 0                                0   \n",
              "4                                 0                                0   \n",
              "\n",
              "   Destination Country_UAE  Destination Country_UK  Destination Country_USA  \\\n",
              "0                        0                       0                        1   \n",
              "1                        0                       0                        0   \n",
              "2                        0                       0                        0   \n",
              "3                        0                       0                        0   \n",
              "4                        0                       0                        1   \n",
              "\n",
              "   Tax Haven Country_Cayman Islands  Tax Haven Country_Luxembourg  \\\n",
              "0                                 0                             0   \n",
              "1                                 0                             0   \n",
              "2                                 0                             0   \n",
              "3                                 0                             0   \n",
              "4                                 0                             1   \n",
              "\n",
              "   Tax Haven Country_Panama  Tax Haven Country_Singapore  \\\n",
              "0                         0                            1   \n",
              "1                         0                            0   \n",
              "2                         0                            0   \n",
              "3                         1                            0   \n",
              "4                         0                            0   \n",
              "\n",
              "   Tax Haven Country_Switzerland  \n",
              "0                              0  \n",
              "1                              0  \n",
              "2                              1  \n",
              "3                              0  \n",
              "4                              0  \n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "y3M1fT0C6wGn",
        "outputId": "48534882-bb57-41ed-ff12-bd34c0d4dc48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Source of Money\n",
              "0    7017\n",
              "1    2983\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Source of Money'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FVJFdDDr6wGn"
      },
      "outputs": [],
      "source": [
        "features_to_modify = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n",
        "\n",
        "def scale_features(df, features):\n",
        "    df_S = df.copy()\n",
        "    scaler = StandardScaler()\n",
        "    df_S[features] = scaler.fit_transform(df[features])\n",
        "    return df_S\n",
        "\n",
        "def normalize_features(df, features):\n",
        "    df_N = df.copy()\n",
        "    scaler = MinMaxScaler()\n",
        "    df_N[features] = scaler.fit_transform(df[features])\n",
        "    return df_N\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvwpmmQE6wGn"
      },
      "source": [
        "### Biased data correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dKMUlEYs6wGn"
      },
      "outputs": [],
      "source": [
        "def Undersampling(X,Y, test_size):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = test_size, random_state=0)\n",
        "    rus = RandomUnderSampler(random_state=0)\n",
        "    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "    return X_resampled, X_test, y_resampled, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLpCpmLS6wGn"
      },
      "source": [
        "## Feature Selectors (Optional):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR3MQUR16wGn"
      },
      "source": [
        "### Feature selector functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "WYkFROWN6wGn"
      },
      "outputs": [],
      "source": [
        "def cor_selector(X, y,num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    cor_list = []\n",
        "    feature_name = X.columns.tolist()\n",
        "    for i in feature_name:\n",
        "        cor = np.corrcoef(X[i], y)[0, 1]\n",
        "        cor_list.append(cor)\n",
        "    #print(np.argsort(np.abs(cor_list)))\n",
        "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
        "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
        "    #print(cor_feature)\n",
        "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
        "    # Your code ends here\n",
        "    return cor_support, cor_feature\n",
        "\n",
        "def chi_squared_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    X_norm = MinMaxScaler().fit_transform(X)\n",
        "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
        "    chi_selector.fit(X_norm, y)\n",
        "    chi_support = chi_selector.get_support()\n",
        "    #print(chi_support)\n",
        "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return chi_support, chi_feature\n",
        "\n",
        "def rfe_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    rfe_selector = RFE(estimator=LogisticRegression(random_state=42), n_features_to_select=num_feats, step=10, verbose=5)\n",
        "    rfe_selector.fit(X, y)\n",
        "    rfe_support = rfe_selector.support_\n",
        "    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return rfe_support, rfe_feature\n",
        "\n",
        "def embedded_log_reg_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    embedded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\", random_state = 42), max_features=num_feats)\n",
        "    embedded_lr_selector.fit(X, y)\n",
        "    embedded_lr_support = embedded_lr_selector.get_support()\n",
        "    embedded_lr_feature = X.loc[:,embedded_lr_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_lr_support, embedded_lr_feature\n",
        "\n",
        "def embedded_rf_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), max_features=num_feats)\n",
        "    embeded_rf_selector.fit(X, y)\n",
        "    embedded_rf_support = embeded_rf_selector.get_support()\n",
        "    embedded_rf_feature = X.loc[:,embedded_rf_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_rf_support, embedded_rf_feature\n",
        "\n",
        "def embedded_lgbm_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    lgbc = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
        "    embeded_lgbm_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
        "    embeded_lgbm_selector.fit(X, y)\n",
        "    embedded_lgbm_support = embeded_lgbm_selector.get_support()\n",
        "    embedded_lgbm_feature = X.loc[:,embedded_lgbm_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_lgbm_support, embedded_lgbm_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdq2lh2f6wGn"
      },
      "source": [
        "### Feature Selectors Combined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uwkj0SqO6wGn"
      },
      "outputs": [],
      "source": [
        "def autoFeatureSelector(X, y, num_feats, methods=[]):\n",
        "\n",
        "    support_dict = {}\n",
        "\n",
        "    feature_name = list(X.columns)\n",
        "    support_dict['Feature'] = feature_name\n",
        "\n",
        "    if 'pearson' in methods:\n",
        "        cor_support, cor_feature = cor_selector(X, y, num_feats)\n",
        "        support_dict['Pearson'] = cor_support\n",
        "    if 'chi-square' in methods:\n",
        "        chi_support, chi_feature = chi_squared_selector(X, y, num_feats)\n",
        "        support_dict['Chi-2'] = chi_support\n",
        "    if 'rfe' in methods:\n",
        "        rfe_support, rfe_feature = rfe_selector(X, y, num_feats)\n",
        "        support_dict['RFE'] = rfe_support\n",
        "    if 'log-reg' in methods:\n",
        "        embedded_lr_support, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)\n",
        "        support_dict['Logistics'] = embedded_lr_support\n",
        "    if 'rf' in methods:\n",
        "        embedded_rf_support, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)\n",
        "        support_dict['Random Forest'] = embedded_rf_support\n",
        "    if 'lgbm' in methods:\n",
        "        embedded_lgbm_support, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)\n",
        "        support_dict['LightGBM'] = embedded_lgbm_support\n",
        "\n",
        "    # Combine all the above feature list and count the maximum set of features that got selected by all methods\n",
        "\n",
        "    print(\"Combining all methods\")\n",
        "    feature_selection_df = pd.DataFrame(support_dict)\n",
        "    feature_selection_df['Total'] = feature_selection_df.apply(lambda row: np.sum(row[1:].astype(int)), axis=1)\n",
        "    print(\"Sorting features\")\n",
        "    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
        "    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
        "    print(\"Selecting best features\")\n",
        "    best_features = feature_selection_df['Feature'].tolist()[:num_feats]\n",
        "    return best_features, feature_selection_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S14DFsWA6wGn"
      },
      "source": [
        "# Models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB2eHh_Q6wGn"
      },
      "source": [
        "- Utilize GridSearchCV to tune the parameters of each of the models.\n",
        "- Check if better results can be obtained for any of the models.\n",
        "- Discuss your observations regarding model performance.\n",
        "- Randomly remove some features (or based on a certain hypothesis) and re-evaluate the models.\n",
        "- Document your observations concerning model performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPCRye0hI5va"
      },
      "source": [
        "## Logistic Regression: Saif, Dwip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUwZsu36wGn"
      },
      "source": [
        "### Data for LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXrNCYAEuQ1i",
        "outputId": "b317d58c-0b75-43a5-b073-bcca0db39f2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n",
              "       'Source of Money', 'Money Laundering Risk Score',\n",
              "       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n",
              "       'Country_India', 'Country_Russia', 'Country_Singapore',\n",
              "       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n",
              "       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n",
              "       'Transaction Type_Offshore Transfer',\n",
              "       'Transaction Type_Property Purchase',\n",
              "       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n",
              "       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n",
              "       'Industry_Oil & Gas', 'Industry_Real Estate',\n",
              "       'Destination Country_China', 'Destination Country_India',\n",
              "       'Destination Country_Russia', 'Destination Country_Singapore',\n",
              "       'Destination Country_South Africa', 'Destination Country_Switzerland',\n",
              "       'Destination Country_UAE', 'Destination Country_UK',\n",
              "       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n",
              "       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n",
              "       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the data for Logistic Regression\n",
        "df_LR = df.copy()\n",
        "\n",
        "# Normalize the variables that are greater than 1 as this will affect the models proformnce\n",
        "df_LR = normalize_features(df_LR, ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved', 'Financial Institution', 'Person Involved'])\n",
        "\n",
        "df_LR.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xoujTX6wGn"
      },
      "source": [
        "### Simple LR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQVmIY1D6wGn",
        "outputId": "b29c2ba5-dc0c-4c4d-c1df-ade1dfee889c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression\n",
            "[[1279  110]\n",
            " [ 558   53]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.92      0.79      1389\n",
            "           1       0.33      0.09      0.14       611\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.51      0.50      0.46      2000\n",
            "weighted avg       0.58      0.67      0.59      2000\n",
            "\n",
            "Accuracy:  0.666\n"
          ]
        }
      ],
      "source": [
        "# Implement a logistic regression model\n",
        "X = df_LR.drop('Source of Money', axis=1)\n",
        "Y = df_LR['Source of Money']\n",
        "\n",
        "# Create a poly feature data\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698W25HN6wGo"
      },
      "source": [
        "#### Using Undersampling to train the LR Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpHJFNju6wGo",
        "outputId": "b748e910-60a3-43eb-be99-5f841674db9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression\n",
            "[[696 710]\n",
            " [298 296]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.50      0.58      1406\n",
            "           1       0.29      0.50      0.37       594\n",
            "\n",
            "    accuracy                           0.50      2000\n",
            "   macro avg       0.50      0.50      0.47      2000\n",
            "weighted avg       0.58      0.50      0.52      2000\n",
            "\n",
            "Accuracy:  0.496\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = Undersampling(X, Y, 0.2)\n",
        "\n",
        "# Fit the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hyOxe4S6wGp"
      },
      "source": [
        "### GridSearchCV LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o31tkMpM6wGt",
        "outputId": "a77ba8be-033f-40ce-9052-e297d178ccfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "0.5\n",
            "LogisticRegression(C=0.01, penalty='l1', solver='liblinear')\n",
            "Logistic Regression\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "Accuracy:  0.703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thvgyoTA6wGt"
      },
      "source": [
        "#### Modifying features and testing proformance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xs-YANl6wGt",
        "outputId": "657819e1-40c2-47ea-f472-f2650acdbce0"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[22], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mLogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m), param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Get the best parameters\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    917\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    918\u001b[0m         clone(base_estimator),\n\u001b[1;32m    919\u001b[0m         X,\n\u001b[1;32m    920\u001b[0m         y,\n\u001b[1;32m    921\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    922\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    923\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    924\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    925\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    927\u001b[0m     )\n\u001b[1;32m    928\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    929\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    932\u001b[0m )\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:895\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    893\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    899\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1296\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1296\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose, prefer\u001b[38;5;241m=\u001b[39mprefer)(\n\u001b[1;32m   1297\u001b[0m     path_func(\n\u001b[1;32m   1298\u001b[0m         X,\n\u001b[1;32m   1299\u001b[0m         y,\n\u001b[1;32m   1300\u001b[0m         pos_class\u001b[38;5;241m=\u001b[39mclass_,\n\u001b[1;32m   1301\u001b[0m         Cs\u001b[38;5;241m=\u001b[39m[C_],\n\u001b[1;32m   1302\u001b[0m         l1_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio,\n\u001b[1;32m   1303\u001b[0m         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[1;32m   1304\u001b[0m         tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[1;32m   1305\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   1306\u001b[0m         solver\u001b[38;5;241m=\u001b[39msolver,\n\u001b[1;32m   1307\u001b[0m         multi_class\u001b[38;5;241m=\u001b[39mmulti_class,\n\u001b[1;32m   1308\u001b[0m         max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[1;32m   1309\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[1;32m   1310\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1311\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[1;32m   1312\u001b[0m         coef\u001b[38;5;241m=\u001b[39mwarm_start_coef_,\n\u001b[1;32m   1313\u001b[0m         penalty\u001b[38;5;241m=\u001b[39mpenalty,\n\u001b[1;32m   1314\u001b[0m         max_squared_sum\u001b[38;5;241m=\u001b[39mmax_squared_sum,\n\u001b[1;32m   1315\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m   1316\u001b[0m         n_threads\u001b[38;5;241m=\u001b[39mn_threads,\n\u001b[1;32m   1317\u001b[0m     )\n\u001b[1;32m   1318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m class_, warm_start_coef_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classes_, warm_start_coef)\n\u001b[1;32m   1319\u001b[0m )\n\u001b[1;32m   1321\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:540\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    537\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m l1_ratio)\n\u001b[1;32m    538\u001b[0m         beta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m l1_ratio\n\u001b[0;32m--> 540\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[38;5;241m=\u001b[39m sag_solver(\n\u001b[1;32m    541\u001b[0m         X,\n\u001b[1;32m    542\u001b[0m         target,\n\u001b[1;32m    543\u001b[0m         sample_weight,\n\u001b[1;32m    544\u001b[0m         loss,\n\u001b[1;32m    545\u001b[0m         alpha,\n\u001b[1;32m    546\u001b[0m         beta,\n\u001b[1;32m    547\u001b[0m         max_iter,\n\u001b[1;32m    548\u001b[0m         tol,\n\u001b[1;32m    549\u001b[0m         verbose,\n\u001b[1;32m    550\u001b[0m         random_state,\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    552\u001b[0m         max_squared_sum,\n\u001b[1;32m    553\u001b[0m         warm_start_sag,\n\u001b[1;32m    554\u001b[0m         is_saga\u001b[38;5;241m=\u001b[39m(solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    555\u001b[0m     )\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolver must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m solver\n\u001b[1;32m    561\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:325\u001b[0m, in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent sag implementation does not handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    324\u001b[0m sag \u001b[38;5;241m=\u001b[39m sag64 \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m sag32\n\u001b[0;32m--> 325\u001b[0m num_seen, n_iter_ \u001b[38;5;241m=\u001b[39m sag(\n\u001b[1;32m    326\u001b[0m     dataset,\n\u001b[1;32m    327\u001b[0m     coef_init,\n\u001b[1;32m    328\u001b[0m     intercept_init,\n\u001b[1;32m    329\u001b[0m     n_samples,\n\u001b[1;32m    330\u001b[0m     n_features,\n\u001b[1;32m    331\u001b[0m     n_classes,\n\u001b[1;32m    332\u001b[0m     tol,\n\u001b[1;32m    333\u001b[0m     max_iter,\n\u001b[1;32m    334\u001b[0m     loss,\n\u001b[1;32m    335\u001b[0m     step_size,\n\u001b[1;32m    336\u001b[0m     alpha_scaled,\n\u001b[1;32m    337\u001b[0m     beta_scaled,\n\u001b[1;32m    338\u001b[0m     sum_gradient_init,\n\u001b[1;32m    339\u001b[0m     gradient_memory_init,\n\u001b[1;32m    340\u001b[0m     seen_init,\n\u001b[1;32m    341\u001b[0m     num_seen_init,\n\u001b[1;32m    342\u001b[0m     fit_intercept,\n\u001b[1;32m    343\u001b[0m     intercept_sum_gradient,\n\u001b[1;32m    344\u001b[0m     intercept_decay,\n\u001b[1;32m    345\u001b[0m     is_saga,\n\u001b[1;32m    346\u001b[0m     verbose,\n\u001b[1;32m    347\u001b[0m )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter_ \u001b[38;5;241m==\u001b[39m max_iter:\n\u001b[1;32m    350\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    352\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    353\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df_LR_Mod = df_LR.copy()\n",
        "\n",
        "# Remove the features that are not important\n",
        "df_LR_Mod = df_LR_Mod.drop(['Person Involved', 'Financial Institution'], axis=1)\n",
        "\n",
        "X = df_LR_Mod.drop('Source of Money', axis=1)\n",
        "Y = df_LR_Mod['Source of Money']\n",
        "\n",
        "# Create a poly feature data\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = Undersampling(X_poly, Y, 0.3)\n",
        "\n",
        "# Define the hyperparameters\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=2000), param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWV5cWNBI9xr"
      },
      "source": [
        "## Decision Tree: Nitish, Sehaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qbcv8p46wGt"
      },
      "source": [
        "### Data for DT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_gz9k5t6wGt"
      },
      "outputs": [],
      "source": [
        "df_DT = df.copy()\n",
        "# Drop person involved\n",
        "df_DT.drop('Person Involved', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOj4PAeh6wGt"
      },
      "source": [
        "### DT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x69OpAlw6wGt",
        "outputId": "8f1c21c0-2111-49e9-f1e6-e39f3e1fc437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.6715\n"
          ]
        }
      ],
      "source": [
        "# Splitting the data into features (X) and target (y)\n",
        "X = df_DT.drop('Source of Money', axis=1)\n",
        "y = df_DT['Source of Money']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training using RandomForestClassifier\n",
        "clf = DecisionTreeClassifier(random_state=42, max_depth=10) # Changed\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy score\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzPVU_PO6wGt",
        "outputId": "a04609c8-94a7-4500-919b-3b4b19fda076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[1307   82]\n",
            " [ 575   36]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.94      0.80      1389\n",
            "           1       0.31      0.06      0.10       611\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.50      0.50      0.45      2000\n",
            "weighted avg       0.58      0.67      0.59      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-g7-N3w6wGu",
        "outputId": "bbe509e5-b1ea-4972-b05b-fb198e1fc51b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "feature_importances = clf.feature_importances_\n",
        "feature_names = X.columns\n",
        "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
        "\n",
        "# Get top 10 features\n",
        "top_features = importance_df.sort_values(by='importance', ascending=False).head(10)\n",
        "\n",
        "#\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='importance', y='feature', data=top_features)\n",
        "plt.title('Top 10 Feature Importances - Optimized Decision Tree')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paUNpLRh6wGu",
        "outputId": "8c7c3bfc-17d3-48b6-a240-aceb5d337f12"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Fit a single decision tree to visualize\n",
        "tree_clf = DecisionTreeClassifier(max_depth=4)  # Limit depth for clarity\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Plot the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(tree_clf, feature_names=X.columns, class_names=['Illegal', 'Legal'], filled=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMxwAy9DI-Gj"
      },
      "source": [
        "## Random Forest: Egor, Ash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyuPWJkk6wGu"
      },
      "source": [
        "### Data for RF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHpdHftWuY9G"
      },
      "outputs": [],
      "source": [
        "# Get the data for RF model\n",
        "df_RF = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1xZqar6wGu"
      },
      "source": [
        "### This needs to be reviewed RF X and Y???"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_fcJDb6wGu",
        "outputId": "56ebc1c3-1468-4f64-dd98-444dfd98d5fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[ 183 1066]\n",
            " [ 253 1498]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.15      0.22      1249\n",
            "           1       0.58      0.86      0.69      1751\n",
            "\n",
            "    accuracy                           0.56      3000\n",
            "   macro avg       0.50      0.50      0.46      3000\n",
            "weighted avg       0.52      0.56      0.50      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define features (X) and target (y) - binary classification on 'Money Laundering Risk Score'\n",
        "X_new = df_RF.drop(columns=['Money Laundering Risk Score'])\n",
        "y_new = (df_RF['Money Laundering Risk Score'] >= 5).astype(int)  # Binary target: 1 if score >= 5, else 0\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the RandomForest classifier\n",
        "rf_clf_new = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf_new.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_new = rf_clf_new.predict(X_test_new)\n",
        "\n",
        "# Generate the confusion matrix and classification report\n",
        "conf_matrix_new = confusion_matrix(y_test_new, y_pred_new)\n",
        "class_report_new = classification_report(y_test_new, y_pred_new)\n",
        "\n",
        "# Display the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_new)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.10' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/k/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "rf_clf_balanced = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "rf_clf_balanced.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_balanced = rf_clf_balanced.predict(X_test_new)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix_balanced = confusion_matrix(y_test_new, y_pred_balanced)\n",
        "class_report_balanced = classification_report(y_test_new, y_pred_balanced)\n",
        "\n",
        "# Print results\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix_balanced)\n",
        "print(\"Classification Report:\\n\", class_report_balanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_XHnycD6wGu",
        "outputId": "2f8fbfc2-18d6-4b09-c2df-78487b527697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Confusion Matrix:\n",
            " [[   5 1244]\n",
            " [   4 1747]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.00      0.01      1249\n",
            "           1       0.58      1.00      0.74      1751\n",
            "\n",
            "    accuracy                           0.58      3000\n",
            "   macro avg       0.57      0.50      0.37      3000\n",
            "weighted avg       0.57      0.58      0.43      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define a simplified parameter grid for GridSearchCV\n",
        "simple_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the RandomForest model\n",
        "rf_clf_simplified = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "simple_grid_search = GridSearchCV(estimator=rf_clf_simplified,\n",
        "                                  param_grid=simple_param_grid,\n",
        "                                  cv=3,  # 3-fold cross-validation\n",
        "                                  verbose=1,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Fit the simplified grid search model\n",
        "simple_grid_search.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Best hyperparameters from the grid search\n",
        "best_params_simplified = simple_grid_search.best_params_\n",
        "\n",
        "# Train the best model on the training set\n",
        "best_rf_model_simplified = simple_grid_search.best_estimator_\n",
        "best_rf_model_simplified.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions with the tuned model\n",
        "y_pred_tuned_simplified = best_rf_model_simplified.predict(X_test_new)\n",
        "\n",
        "# Generate confusion matrix and classification report for the tuned model\n",
        "conf_matrix_tuned_simplified = confusion_matrix(y_test_new, y_pred_tuned_simplified)\n",
        "class_report_tuned_simplified = classification_report(y_test_new, y_pred_tuned_simplified)\n",
        "\n",
        "# Output best parameters, confusion matrix, and classification report\n",
        "print(\"Best Hyperparameters:\", best_params_simplified)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix_tuned_simplified)\n",
        "print(\"Classification Report:\\n\", class_report_tuned_simplified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiehsO8sI-ch"
      },
      "source": [
        "## SGD: Devanshi, James, Abraham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lvQ8IYnXueAV"
      },
      "outputs": [],
      "source": [
        "# Stochastic Gradient Descent\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "#SGD\n",
        "df_SGD = df.copy()\n",
        "df_SGD_S = scale_features(df_SGD, features_to_modify)\n",
        "df_SGD_N = normalize_features(df_SGD, features_to_modify)\n",
        "# Mapping to a Uniform distribution\n",
        "quantile_transformer = QuantileTransformer(output_distribution='uniform')\n",
        "df_SGD_U = df_SGD.copy()\n",
        "df_SGD_U[features_to_modify] = quantile_transformer.fit_transform(df_SGD_U[features_to_modify])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6FSW-W7h6wGu",
        "outputId": "5a450080-df04-416c-9c20-3ba1e413ac48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       1.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.85      0.50      0.41      2000\n",
            "weighted avg       0.79      0.70      0.58      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1406    0]\n",
            " [ 593    1]]\n",
            "Classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.32      0.44      1406\n",
            "           1       0.30      0.69      0.42       594\n",
            "\n",
            "    accuracy                           0.43      2000\n",
            "   macro avg       0.50      0.50      0.43      2000\n",
            "weighted avg       0.59      0.43      0.43      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[448 958]\n",
            " [184 410]]\n",
            "Classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.03      0.06      1406\n",
            "           1       0.30      0.96      0.45       594\n",
            "\n",
            "    accuracy                           0.31      2000\n",
            "   macro avg       0.49      0.50      0.26      2000\n",
            "weighted avg       0.57      0.31      0.18      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[  47 1359]\n",
            " [  21  573]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nBased on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\\nit has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\\n\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_list = [df_SGD_S, df_SGD_N, df_SGD_U]\n",
        "cr_list = []\n",
        "cm_list = []\n",
        "\n",
        "# Search for the best df for standardization and normalization\n",
        "for i in range(len(df_list)):\n",
        "    df = df_list[i]\n",
        "    X = df.drop('Source of Money', axis=1)\n",
        "    y = df['Source of Money'].values\n",
        "    X_resampled, X_test, y_resampled, y_test = Undersampling(X, y, test_size=0.2)\n",
        "    SGD_classifier = SGDClassifier(random_state=0)\n",
        "    SGD_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred_lr_b = SGD_classifier.predict(X_test)\n",
        "    lr_cr = classification_report(y_test, y_pred_lr_b)\n",
        "    lr_cm = confusion_matrix(y_test, y_pred_lr_b)\n",
        "    cr_list.append(lr_cr)\n",
        "    cm_list.append(lr_cm)\n",
        "\n",
        "# Print the classification reports and confusion matrices\n",
        "for i in range(len(cr_list)):\n",
        "    print(\"Classification Reports:\")\n",
        "    print(cr_list[i])\n",
        "    print('Confusion Matrix:')\n",
        "    print(cm_list[i])\n",
        "\n",
        "'''\n",
        "Based on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\n",
        "it has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
            "540 fits failed out of a total of 2700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "100 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'squared_hinge', 'log_loss', 'huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "71 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'epsilon_insensitive', 'huber', 'squared_error', 'log_loss', 'perceptron', 'modified_huber', 'hinge', 'squared_hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'huber', 'modified_huber', 'squared_epsilon_insensitive', 'squared_error', 'epsilon_insensitive', 'hinge', 'squared_hinge', 'log_loss'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "57 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'squared_hinge', 'squared_error', 'hinge', 'modified_huber', 'squared_epsilon_insensitive', 'perceptron', 'log_loss', 'huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "88 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'perceptron', 'huber', 'log_loss', 'squared_hinge', 'epsilon_insensitive', 'squared_error', 'modified_huber', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "43 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'huber', 'log_loss', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "45 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'perceptron', 'squared_epsilon_insensitive', 'modified_huber', 'hinge', 'epsilon_insensitive', 'huber', 'log_loss', 'squared_hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "56 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'hinge', 'perceptron', 'squared_hinge', 'log_loss', 'squared_error', 'modified_huber'}. Got 'log' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.49644242 0.49979058 0.49958137        nan        nan        nan\n",
            " 0.49016671 0.50104559 0.496649   0.50104537 0.49706784 0.49790795\n",
            " 0.50020942 0.50020899 0.50125479 0.4991623  0.49979058 0.49895288\n",
            "        nan        nan        nan 0.48849066 0.48095336 0.48702513\n",
            " 0.5046001  0.50439068 0.5046001  0.49916252 0.49895397 0.49622993\n",
            " 0.48785669 0.48179062 0.48471686        nan        nan        nan\n",
            " 0.48744288 0.49267695 0.48765274 0.49330347 0.49246687 0.49330347\n",
            " 0.49392999 0.49330412 0.50063616 0.49644242 0.49979058 0.49958137\n",
            "        nan        nan        nan 0.49016671 0.50104559 0.496649\n",
            " 0.50104537 0.49706784 0.49790795 0.50020942 0.50020899 0.50125479\n",
            " 0.49832548 0.49016079 0.49706982        nan        nan        nan\n",
            " 0.49287958 0.49560319 0.49727749 0.49477185 0.48911915 0.48451795\n",
            " 0.50020942 0.50167517 0.50188306 0.48681526 0.48367412 0.48409363\n",
            "        nan        nan        nan 0.49079235 0.49225788 0.48786041\n",
            " 0.4981165  0.49853578 0.49832614 0.49895244 0.49644176 0.49958115\n",
            " 0.49644242 0.49979058 0.49958137        nan        nan        nan\n",
            " 0.49016671 0.50104559 0.496649   0.50104537 0.49706784 0.49790795\n",
            " 0.50020942 0.50020899 0.50125479 0.49979079 0.4895323  0.5\n",
            "        nan        nan        nan 0.49958115 0.50041841 0.4991623\n",
            " 0.49811562 0.494137   0.49497579 0.49623431 0.49895397 0.49560428\n",
            " 0.49120967 0.48597516 0.48785713        nan        nan        nan\n",
            " 0.49037022 0.4905803  0.4847219  0.49329931 0.48807312 0.49413788\n",
            " 0.50188416 0.49497601 0.49643848 0.49706784 0.50041841 0.5\n",
            "        nan        nan        nan 0.49958115 0.49790773 0.49686171\n",
            " 0.49393415 0.4895369  0.49519091 0.50167386 0.49895288 0.49811737\n",
            " 0.4991623  0.49979058 0.4991623         nan        nan        nan\n",
            " 0.48869986 0.48492935 0.48053583 0.50292515 0.50271463 0.50355342\n",
            " 0.49853513 0.50020921 0.5        0.48116213 0.48702053 0.48430174\n",
            "        nan        nan        nan 0.49141909 0.48722995 0.48953515\n",
            " 0.49330675 0.49204846 0.49435278 0.49476746 0.49728165 0.49727968\n",
            " 0.49706784 0.50041841 0.5               nan        nan        nan\n",
            " 0.49958115 0.49790773 0.49686171 0.49393415 0.4895369  0.49519091\n",
            " 0.50167386 0.49895288 0.49811737 0.49581502 0.48827663 0.49288265\n",
            "        nan        nan        nan 0.49287958 0.4920432  0.49643804\n",
            " 0.4914215  0.48325944 0.48556551 0.5        0.49623037 0.49728077\n",
            " 0.48723433 0.49141865 0.48221056        nan        nan        nan\n",
            " 0.48597319 0.4928844  0.48911761 0.50397577 0.49769853 0.50439287\n",
            " 0.49686302 0.49330478 0.48535083 0.49706784 0.50041841 0.5\n",
            "        nan        nan        nan 0.49958115 0.49790773 0.49686171\n",
            " 0.49393415 0.4895369  0.49519091 0.50167386 0.49895288 0.49811737\n",
            " 0.49790795 0.49958049 0.49644198        nan        nan        nan\n",
            " 0.49958115 0.49727924 0.4995829  0.49769853 0.49204517 0.49853513\n",
            " 0.50146444 0.49602116 0.49706806 0.48220618 0.48911411 0.48409231\n",
            "        nan        nan        nan 0.48220574 0.492673   0.48492913\n",
            " 0.49790204 0.48618853 0.49267498 0.5008377  0.49225306 0.4941416\n",
            " 0.49999912 0.5        0.49308966        nan        nan        nan\n",
            " 0.49434884 0.4991623  0.48953252 0.49162917 0.50292777 0.49204649\n",
            " 0.49769874 0.5        0.49937217 0.49937173 0.5        0.50020942\n",
            "        nan        nan        nan 0.48974611 0.49811584 0.4853482\n",
            " 0.50250827 0.50187737 0.4989474  0.49372144 0.49979058 0.49769612\n",
            " 0.49078731 0.49832636 0.48660212        nan        nan        nan\n",
            " 0.49330062 0.48869548 0.48681461 0.50041819 0.49225766 0.50418563\n",
            " 0.49748669 0.49769634 0.5008366  0.49999912 0.5        0.49308966\n",
            "        nan        nan        nan 0.49434884 0.4991623  0.48953252\n",
            " 0.49162917 0.50292777 0.49204649 0.49769874 0.5        0.49937217\n",
            " 0.49916187 0.49497382 0.48555478        nan        nan        nan\n",
            " 0.49329843 0.49518303 0.49246183 0.48346952 0.4847276  0.48598392\n",
            " 0.49162523 0.50020942 0.49853578 0.48785516 0.50105084 0.49225481\n",
            "        nan        nan        nan 0.48890731 0.49120638 0.48597472\n",
            " 0.48806392 0.49811715 0.48597582 0.50188504 0.49769699 0.49309492\n",
            " 0.49999912 0.5        0.49308966        nan        nan        nan\n",
            " 0.49434884 0.4991623  0.48953252 0.49162917 0.50292777 0.49204649\n",
            " 0.49769874 0.5        0.49937217 0.5        0.5        0.50062762\n",
            "        nan        nan        nan 0.49329843 0.49979058 0.49790707\n",
            " 0.50230147 0.49518544 0.49518347 0.49602467 0.5        0.5\n",
            " 0.48743937 0.49979233 0.49351048        nan        nan        nan\n",
            " 0.4889071  0.49099893 0.49246511 0.4947644  0.49246752 0.48849022\n",
            " 0.50209293 0.49832439 0.49686039 0.50125654 0.49979058 0.5\n",
            "        nan        nan        nan 0.49267147 0.49979058 0.4964411\n",
            " 0.49267432 0.50083441 0.49434949 0.50000044 0.5        0.50020942\n",
            " 0.49979058 0.5        0.5               nan        nan        nan\n",
            " 0.49434927 0.5        0.49811737 0.50187912 0.49790225 0.5039681\n",
            " 0.50209052 0.5        0.49979058 0.4918329  0.5        0.50146444\n",
            "        nan        nan        nan 0.49141931 0.50188525 0.49078994\n",
            " 0.49769699 0.49183925 0.49036868 0.50063068 0.5        0.49874477\n",
            " 0.50125654 0.49979058 0.5               nan        nan        nan\n",
            " 0.49267147 0.49979058 0.4964411  0.49267432 0.50083441 0.49434949\n",
            " 0.50000044 0.5        0.50020942 0.5        0.5        0.5\n",
            "        nan        nan        nan 0.49267016 0.49979058 0.49811518\n",
            " 0.50020176 0.48953822 0.51213827 0.49979058 0.5        0.5\n",
            " 0.49539552 0.5        0.49895397        nan        nan        nan\n",
            " 0.4863962  0.49372034 0.4920478  0.48346579 0.49560648 0.49644089\n",
            " 0.50230301 0.49979058 0.49497777 0.50125654 0.49979058 0.5\n",
            "        nan        nan        nan 0.49267147 0.49979058 0.4964411\n",
            " 0.49267432 0.50083441 0.49434949 0.50000044 0.5        0.50020942\n",
            " 0.50020942 0.5        0.5               nan        nan        nan\n",
            " 0.49623037 0.5        0.49979058 0.50020942 0.49811715 0.5\n",
            " 0.50041885 0.5        0.5        0.49330172 0.5        0.49895397\n",
            "        nan        nan        nan 0.4907895  0.49769918 0.49288309\n",
            " 0.49455541 0.4914215  0.49329887 0.50460536 0.5        0.5       ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'elasticnet'}\n",
            "0.5121382724703718\n",
            "SGDClassifier(alpha=0.1, eta0=0.1, learning_rate='invscaling',\n",
            "              loss='squared_hinge', penalty='elasticnet', random_state=0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#grip search for SGD\n",
        "X = df_SGD_U.drop('Source of Money', axis=1)\n",
        "y = df_SGD_U['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "    'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
        "    'eta0': [0.01, 0.1, 1.0]\n",
        "}\n",
        "sgd = SGDClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=sgd,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=2,\n",
        "                           #scoring='precision'\n",
        "                           )\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "before_cm = confusion_matrix(y_test, y_pred)\n",
        "before_cr = classification_report(y_test, y_pred)\n",
        "before_f1 = f1_score(y_test, y_pred)\n",
        "before_precision = precision_score(y_test, y_pred)\n",
        "before_recall = recall_score(y_test, y_pred)\n",
        "before_accuracy = accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before tuning:\n",
            "Confusion Matrix:\n",
            "[[732 674]\n",
            " [310 284]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.52      0.60      1406\n",
            "           1       0.30      0.48      0.37       594\n",
            "\n",
            "    accuracy                           0.51      2000\n",
            "   macro avg       0.50      0.50      0.48      2000\n",
            "weighted avg       0.58      0.51      0.53      2000\n",
            "\n",
            "F1 Score: 0.36597938144329895\n",
            "Precision: 0.2964509394572025\n",
            "Recall: 0.4781144781144781\n",
            "Accuracy: 0.508\n"
          ]
        }
      ],
      "source": [
        "#first results\n",
        "print(\"Before tuning:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(before_cm)\n",
        "print(\"Classification Report:\")\n",
        "print(before_cr)\n",
        "print(\"F1 Score:\", before_f1)\n",
        "print(\"Precision:\", before_precision)\n",
        "print(\"Recall:\", before_recall)\n",
        "print(\"Accuracy:\", before_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EdxFiSw86wGu",
        "outputId": "99bc0ede-6ea6-4c93-d9b1-1af78a2a8cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n"
          ]
        }
      ],
      "source": [
        "#b_features_list = []\n",
        "#feature_selection_df_list = []\n",
        "X = df_SGD_N.drop('Source of Money', axis=1)\n",
        "y = df_SGD_N['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "num_feats = 10\n",
        "#for num in range(0, num_feats):\n",
        "methods = ['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n",
        "best_features, feature_selection_df = autoFeatureSelector(X_resampled, y_resampled, num_feats, methods)\n",
        "#b_features_list.append(best_features)\n",
        "#feature_selection_df_list.append(feature_selection_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#visualize the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Total', y='Feature', data=feature_selection_df)\n",
        "plt.title('Feature Selection')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EJ2wvlf06wGv",
        "outputId": "e3c83522-df77-4bae-aff0-c366ab018db6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
            "540 fits failed out of a total of 2700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "128 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'hinge', 'perceptron', 'squared_hinge', 'log_loss', 'squared_error', 'modified_huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "71 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'huber', 'log_loss', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "68 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'epsilon_insensitive', 'huber', 'squared_error', 'log_loss', 'perceptron', 'modified_huber', 'hinge', 'squared_hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "46 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'huber', 'modified_huber', 'squared_epsilon_insensitive', 'squared_error', 'epsilon_insensitive', 'hinge', 'squared_hinge', 'log_loss'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "53 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'perceptron', 'huber', 'log_loss', 'squared_hinge', 'epsilon_insensitive', 'squared_error', 'modified_huber', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "90 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'perceptron', 'squared_epsilon_insensitive', 'modified_huber', 'hinge', 'epsilon_insensitive', 'huber', 'log_loss', 'squared_hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "66 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'modified_huber', 'squared_error', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'squared_hinge', 'log_loss', 'huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "18 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'squared_hinge', 'squared_error', 'hinge', 'modified_huber', 'squared_epsilon_insensitive', 'perceptron', 'log_loss', 'huber'}. Got 'log' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.49979058 0.5006274  0.50376569        nan        nan        nan\n",
            " 0.50020942 0.50020942 0.5        0.50041863 0.49979058 0.5035567\n",
            " 0.49853578 0.50083638 0.5        0.5        0.4991623  0.5\n",
            "        nan        nan        nan 0.50920283 0.5129683  0.50962146\n",
            " 0.49685053 0.49705974 0.49685053 0.49497382 0.50020548 0.4991623\n",
            " 0.50606716 0.50125654 0.50585555        nan        nan        nan\n",
            " 0.50355561 0.5054378  0.50522793 0.50125654 0.50041731 0.50439331\n",
            " 0.49456242 0.50167495 0.49895682 0.49979058 0.5006274  0.50376569\n",
            "        nan        nan        nan 0.50020942 0.50020942 0.5\n",
            " 0.50041863 0.49979058 0.5035567  0.49853578 0.50083638 0.5\n",
            " 0.4999989  0.50146181 0.49958093        nan        nan        nan\n",
            " 0.50167539 0.50125654 0.50460733 0.50857543 0.50857543 0.50857543\n",
            " 0.50041885 0.50209183 0.5        0.49706916 0.49350851 0.49497448\n",
            "        nan        nan        nan 0.50188    0.5064836  0.50564613\n",
            " 0.49769677 0.5060641  0.50648295 0.49393437 0.49832702 0.49874718\n",
            " 0.49979058 0.5006274  0.50376569        nan        nan        nan\n",
            " 0.50020942 0.50020942 0.5        0.50041863 0.49979058 0.5035567\n",
            " 0.49853578 0.50083638 0.5        0.4991623  0.50439068 0.5\n",
            "        nan        nan        nan 0.49748713 0.5        0.50188482\n",
            " 0.49706828 0.50251046 0.50481215 0.50167342 0.49895288 0.50000044\n",
            " 0.49958071 0.50104252 0.50208701        nan        nan        nan\n",
            " 0.5133946  0.50522859 0.50041403 0.50188219 0.49560231 0.50522772\n",
            " 0.49560626 0.5020938  0.49476878 0.49979079 0.49979058 0.50313808\n",
            "        nan        nan        nan 0.50000022 0.49979058 0.5\n",
            " 0.49644242 0.49581349 0.49644308 0.50292799 0.50167364 0.50125567\n",
            " 0.5        0.50020942 0.49979058        nan        nan        nan\n",
            " 0.50920283 0.49831979 0.50480712 0.49873338 0.49685119 0.49852439\n",
            " 0.50020921 0.5        0.5008366  0.50418322 0.49539114 0.49288133\n",
            "        nan        nan        nan 0.50355627 0.50292471 0.50627484\n",
            " 0.4966525  0.49497557 0.49413832 0.50021205 0.49539486 0.5006274\n",
            " 0.49979079 0.49979058 0.50313808        nan        nan        nan\n",
            " 0.50000022 0.49979058 0.5        0.49644242 0.49581349 0.49644308\n",
            " 0.50292799 0.50167364 0.50125567 0.50062674 0.5027166  0.49643848\n",
            "        nan        nan        nan 0.50125654 0.50062367 0.50188416\n",
            " 0.50773796 0.51402046 0.50773796 0.50020942 0.50020942 0.5\n",
            " 0.50104362 0.50062564 0.49476374        nan        nan        nan\n",
            " 0.50250805 0.50585643 0.50732042 0.50271835 0.49371903 0.50104099\n",
            " 0.49309623 0.49351267 0.49560472 0.49979079 0.49979058 0.50313808\n",
            "        nan        nan        nan 0.50000022 0.49979058 0.5\n",
            " 0.49644242 0.49581349 0.49644308 0.50292799 0.50167364 0.50125567\n",
            " 0.50020942 0.50146246 0.49769765        nan        nan        nan\n",
            " 0.49769655 0.50313808 0.50271967 0.49811475 0.50125435 0.50062564\n",
            " 0.50167364 0.49937173 0.50020942 0.49916099 0.5048104  0.49727573\n",
            "        nan        nan        nan 0.50878902 0.50502048 0.50648382\n",
            " 0.49874192 0.49602182 0.49518412 0.49853929 0.50020877 0.49706784\n",
            " 0.5008377  0.49727749 0.49643716        nan        nan        nan\n",
            " 0.50041644 0.49978926 0.49873951 0.49999606 0.49476681 0.50794629\n",
            " 0.50167342 0.5        0.49958159 0.4991623  0.5        0.50020942\n",
            "        nan        nan        nan 0.51087669 0.49936822 0.49538676\n",
            " 0.50250235 0.49789787 0.50333808 0.50104603 0.5        0.49853381\n",
            " 0.50125764 0.49832636 0.49769195        nan        nan        nan\n",
            " 0.50208964 0.49832176 0.50313567 0.49476681 0.49811343 0.49476725\n",
            " 0.4983268  0.50062827 0.49560516 0.5008377  0.49727749 0.49643716\n",
            "        nan        nan        nan 0.50041644 0.49978926 0.49873951\n",
            " 0.49999606 0.49476681 0.50794629 0.50167342 0.5        0.49958159\n",
            " 0.49978992 0.49518325 0.50376503        nan        nan        nan\n",
            " 0.50167539 0.49894959 0.50313501 0.50794585 0.50857543 0.49539464\n",
            " 0.49979058 0.5        0.49916296 0.49351114 0.5035613  0.4905803\n",
            "        nan        nan        nan 0.50292646 0.49874214 0.50522815\n",
            " 0.50878705 0.49371618 0.49392889 0.49958772 0.49769699 0.49560209\n",
            " 0.5008377  0.49727749 0.49643716        nan        nan        nan\n",
            " 0.50041644 0.49978926 0.49873951 0.49999606 0.49476681 0.50794629\n",
            " 0.50167342 0.5        0.49958159 0.50020942 0.50020942 0.50460251\n",
            "        nan        nan        nan 0.49748691 0.5        0.50376481\n",
            " 0.5        0.50355692 0.50753445 0.50062762 0.5        0.49979058\n",
            " 0.49832526 0.49790751 0.50334421        nan        nan        nan\n",
            " 0.50020658 0.4926706  0.50438871 0.50460295 0.50125545 0.49392626\n",
            " 0.50523494 0.50020921 0.49728077 0.50209424 0.49979058 0.5\n",
            "        nan        nan        nan 0.49245964 0.50020942 0.49434643\n",
            " 0.49706653 0.48870161 0.49329952 0.50062827 0.5        0.49874346\n",
            " 0.49979058 0.5        0.5               nan        nan        nan\n",
            " 0.50648382 0.5        0.50146465 0.50124603 0.50585292 0.50208394\n",
            " 0.50439331 0.5        0.5        0.49853031 0.5        0.49832636\n",
            "        nan        nan        nan 0.5062779  0.50377007 0.49434686\n",
            " 0.492257   0.49560319 0.50522947 0.49644702 0.5        0.49832592\n",
            " 0.50209424 0.49979058 0.5               nan        nan        nan\n",
            " 0.49245964 0.50020942 0.49434643 0.49706653 0.48870161 0.49329952\n",
            " 0.50062827 0.5        0.49874346 0.5        0.5        0.5\n",
            "        nan        nan        nan 0.5008377  0.49979058 0.49790576\n",
            " 0.48806765 0.50857543 0.48869504 0.49979058 0.5        0.50020942\n",
            " 0.49644023 0.5        0.49895397        nan        nan        nan\n",
            " 0.50334531 0.49685864 0.49769634 0.50062148 0.49707157 0.49183597\n",
            " 0.50167758 0.49979058 0.49623431 0.50209424 0.49979058 0.5\n",
            "        nan        nan        nan 0.49245964 0.50020942 0.49434643\n",
            " 0.49706653 0.48870161 0.49329952 0.50062827 0.5        0.49874346\n",
            " 0.5        0.5        0.49999978        nan        nan        nan\n",
            " 0.49895288 0.5        0.49958115 0.5008377  0.50146465 0.50041885\n",
            " 0.50146422 0.5        0.5        0.49937173 0.5        0.49895397\n",
            "        nan        nan        nan 0.50125348 0.50167824 0.49183553\n",
            " 0.50460207 0.50146882 0.48827532 0.50523363 0.5        0.5       ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X = df_SGD_U[best_features]\n",
        "y = df_SGD_U['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "param_grid = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], \n",
        "                'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1], \n",
        "                'learning_rate': ['optimal', 'invscaling', 'adaptive'], \n",
        "                'eta0': [0.01, 0.1, 1.0] }\n",
        "sgd = SGDClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=sgd, \n",
        "                        param_grid=param_grid, \n",
        "                        cv=5, \n",
        "                        n_jobs=-1, \n",
        "                        verbose=2, \n",
        "                        #scoring='precision' \n",
        "                        )\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "#print(grid_search.best_params_)\n",
        "#print(grid_search.best_score_)\n",
        "#print(grid_search.best_estimator_)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "after_cm = confusion_matrix(y_test, y_pred)\n",
        "after_cr = classification_report(y_test, y_pred)\n",
        "after_accuracy = accuracy_score(y_test, y_pred)\n",
        "after_f1 = f1_score(y_test, y_pred)\n",
        "after_precision = precision_score(y_test, y_pred)\n",
        "after_recall = recall_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "loV9YnmZ6wGv",
        "outputId": "1818de0f-6603-4c40-eff5-cbb95fb8394b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After tuning:\n",
            "Confusion Matrix:\n",
            "[[430 976]\n",
            " [172 422]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.31      0.43      1406\n",
            "           1       0.30      0.71      0.42       594\n",
            "\n",
            "    accuracy                           0.43      2000\n",
            "   macro avg       0.51      0.51      0.43      2000\n",
            "weighted avg       0.59      0.43      0.43      2000\n",
            "\n",
            "F1 Score: 0.42369477911646586\n",
            "Precision: 0.30185979971387694\n",
            "Recall: 0.7104377104377104\n",
            "Accuracy: 0.426\n"
          ]
        }
      ],
      "source": [
        "#after tuning results\n",
        "print(\"After tuning:\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(after_cm)\n",
        "print(\"Classification Report:\")\n",
        "print(after_cr)\n",
        "print(\"F1 Score:\", after_f1)\n",
        "print(\"Precision:\", after_precision)\n",
        "print(\"Recall:\", after_recall)\n",
        "print(\"Accuracy:\", after_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before tuning:\n",
            "F1 Score: 0.36597938144329895\n",
            "Precision: 0.2964509394572025\n",
            "Recall: 0.4781144781144781\n",
            "Accuracy: 0.508\n",
            "Confusion Matrix:\n",
            "[[732 674]\n",
            " [310 284]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.52      0.60      1406\n",
            "           1       0.30      0.48      0.37       594\n",
            "\n",
            "    accuracy                           0.51      2000\n",
            "   macro avg       0.50      0.50      0.48      2000\n",
            "weighted avg       0.58      0.51      0.53      2000\n",
            "\n",
            "After tuning:\n",
            "F1 Score: 0.42369477911646586\n",
            "Precision: 0.30185979971387694\n",
            "Recall: 0.7104377104377104\n",
            "Accuracy: 0.426\n",
            "Confusion Matrix:\n",
            "[[430 976]\n",
            " [172 422]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.31      0.43      1406\n",
            "           1       0.30      0.71      0.42       594\n",
            "\n",
            "    accuracy                           0.43      2000\n",
            "   macro avg       0.51      0.51      0.43      2000\n",
            "weighted avg       0.59      0.43      0.43      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#compare the results\n",
        "print(\"Before tuning:\")\n",
        "print(\"F1 Score:\", before_f1)\n",
        "print(\"Precision:\", before_precision)\n",
        "print(\"Recall:\", before_recall)\n",
        "print(\"Accuracy:\", before_accuracy)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(before_cm)\n",
        "print(\"Classification Report:\")\n",
        "print(before_cr)\n",
        "print(\"After tuning:\")\n",
        "print(\"F1 Score:\", after_f1)\n",
        "print(\"Precision:\", after_precision)\n",
        "print(\"Recall:\", after_recall)\n",
        "print(\"Accuracy:\", after_accuracy)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(after_cm)\n",
        "print(\"Classification Report:\")\n",
        "print(after_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAx2CAYAAADe+EqtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzde5RXdaH//9fIZZDLjAoKKAh4SSAvICi3g5dMlMKDaYmaeElLT1oqdY4iKkIZalpqR1DTILwgltcIL5SW8IVKOYCdMvOYNaaMCuoMoqLA5/eHy/k1DhiDuAft8Vhrr8Xn/Xnv/XnvwVWznuy9P2WlUqkUAAAAACjQFk29AAAAAAD+9YhSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAQKNNmzYtZWVl69y++c1v1s2bNWtWjj/++Oyxxx5p0aJFysrKGvU5y5cvz9ixY9O7d++0adMmlZWV6dmzZ0aPHp3HH398U59WIV544YWce+652WOPPdK2bdu0atUqu+66a84888w89dRTTb28D927/+389a9/beqlAABNrHlTLwAA+OiaOnVqevbsWW9s++23r/vzXXfdld/85jfp27dvysvLs3Dhwg0+9muvvZaBAwfmtddey3/+539mr732yhtvvJE///nPufPOO7N48eLsueeem+xcivC73/0uI0aMSKlUyhlnnJFBgwalZcuWefLJJ3PzzTdn3333zSuvvNLUy/xQffazn82CBQvSuXPnpl4KANDEykqlUqmpFwEAfLRMmzYtJ510Uh599NH0799/vfPWrl2bLbZ458LsM844I9dcc0029FePqVOn5ktf+lIeeuihHHjgge977A/b22+/nbKysjRvvvH/nldbW5vddtstLVq0yPz589OlS5cGc37605/m85///AdZ6mbrjTfeSKtWrRp9tRwA8PHl9j0A4EPzQaLR8uXLk2S9V9S899h/+tOfcswxx6Rjx44pLy/PjjvumOOPPz6rVq2qm/O///u/GTlyZLbeeuu0atUqffr0yY9//ON6x/nVr36VsrKy3HTTTfnGN76RHXbYIeXl5fm///u/JMkvfvGLHHTQQamoqEjr1q0zZMiQ/PKXv/yn5/PDH/4w1dXVueyyy9YZpJI0CFL33ntvBg0alNatW6ddu3Y5+OCDs2DBgnpzLrroopSVleXxxx/PF77whVRWVmabbbbJmDFjsnr16jz55JM59NBD065du3Tv3j2XXXbZOs/35ptvzpgxY9KpU6dsueWW2X///bNo0aJ6cx977LEcffTR6d69e7bccst07949xxxzTP72t7/Vm/fuLXoPPvhgvvSlL2XbbbdN69ats2rVqnXevrdo0aKMGDEi2223XcrLy7P99tvns5/9bP7+97/XzXnzzTczduzY9OjRIy1btswOO+yQ008/Pa+++mq9z+7evXtGjBiR+++/P3vvvXe23HLL9OzZMz/60Y/e9+8HACieKAUAbLQ1a9Zk9erV9bZNZdCgQUmS448/PnfffXddpFqXJUuWZJ999slvfvObTJw4Mffdd18mTZqUVatW5a233kqSPPnkkxk8eHD+8Ic/5Oqrr86dd96Z3r1758QTT2wQapJk7NixqaqqyrXXXpuf/exn2W677XLzzTdn2LBhqaioyI9//OPcfvvt2WabbXLIIYf80zD14IMPplmzZjnssMM26PxvvfXWjBw5MhUVFZkxY0ZuvPHGvPLKKznggAMyb968BvOPOuqo7LXXXrnjjjvy5S9/Od///vdz9tln5/DDD89nP/vZ3HXXXfnUpz6Vc845J3feeWeD/c8777z85S9/yQ033JAbbrghzz//fA444ID85S9/qZvz17/+NbvttluuvPLKPPDAA7n00kuzdOnS7LPPPlm2bFmDY37pS19KixYtctNNN+WnP/1pWrRo0WDOypUrc/DBB+eFF17INddckzlz5uTKK6/MjjvumBUrViRJSqVSDj/88Fx++eUZPXp0fv7zn2fMmDH58Y9/nE996lP1wmPyzn8P3/jGN3L22WfnnnvuyZ577pmTTz45jzzyyAb97AGAgpQAABpp6tSppSTr3N5+++117nP66aeXGvurx8SJE0stW7asO3aPHj1Kp512WmnJkiX15n3qU58qbbXVVqUXX3xxvcc6+uijS+Xl5aWqqqp648OHDy+1bt269Oqrr5ZKpVLp4YcfLiUp7bfffvXmrVy5srTNNtuUDjvssHrja9asKe21116lfffd933PpWfPnqVOnTr903N+95jbb799aY899iitWbOmbnzFihWl7bbbrjR48OC6sfHjx5eSlK644op6x+jTp08pSenOO++sG3v77bdL2267bemII46oG3v3fPfee+/S2rVr68b/+te/llq0aFE65ZRT1rvO1atXl1577bVSmzZtSldddVXd+Lv/fRx//PEN9nn3vWeeeaZUKpVKjz32WClJ6e67717v59x///2lJKXLLrus3vjMmTNLSUrXX3993Vi3bt1KrVq1Kv3tb3+rG3vjjTdK22yzTenUU09d72cAAMVzpRQAsNGmT5+eRx99tN72QZ679F4XXHBBqqqq8qMf/Sinnnpq2rZtm2uvvTb9+vXLjBkzkiSvv/56fv3rX+eoo47Ktttuu95jPfTQQznooIPStWvXeuMnnnhiXn/99Qa3xR155JH1Xs+fPz8vv/xyTjjhhHpXhq1duzaHHnpoHn300axcuXKTnPeTTz6Z559/PqNHj653m2Lbtm1z5JFH5je/+U1ef/31evuMGDGi3utevXqlrKwsw4cPrxtr3rx5dtlllwa32yXJscceW+95T926dcvgwYPz8MMP14299tprOeecc7LLLrukefPmad68edq2bZuVK1fmiSeeaHDM9/4M12WXXXbJ1ltvnXPOOSfXXntt/vjHPzaY89BDDyV55+/qH33hC19ImzZtGlyl1qdPn+y44451r1u1apVPfOIT6zxvAKDp+PY9AGCj9erV630fdL4pdOzYMSeddFJOOumkJMkjjzyS4cOH58wzz8wxxxyTV155JWvWrFnvc5retXz58nU+n+rdbwt87+2B7537wgsvJGn43Kd/9PLLL6dNmzbrfG/HHXfMU089lZUrV653zj+udV1reHe9a9euzSuvvJLWrVvXjW+zzTb15rVs2TKtW7dOq1atGozX1tY2OG6nTp3WObZkyZK618cee2x++ctf5oILLsg+++yTioqKlJWV5TOf+UzeeOONBvtvyDfsVVZW5te//nUuvvjinHfeeXnllVfSuXPnfPnLX87555+fFi1aZPny5WnevHmD6FhWVpZOnTo1+Ltr3759g88pLy9f5xoBgKYjSgEAHyn77bdfhg0blrvvvjsvvvhittlmmzRr1qzeQ7HXpX379lm6dGmD8eeffz5J0qFDh3rj7/2WuHff/8EPfpCBAweu8zM6duy43s8/5JBD8uCDD+ZnP/tZjj766H+61iTrXe8WW2yRrbfe+n2P0VjV1dXrHHt3LTU1NZk1a1bGjx+fc889t27OqlWr8vLLL6/zmBv6TXt77LFHbrvttpRKpTz++OOZNm1aJk6cmC233DLnnntu2rdvn9WrV+ell16qF6ZKpVKqq6uzzz77NOZUAYDNhNv3AIDN0gsvvJC1a9c2GF+zZk2eeuqptG7dOltttVXdN8X95Cc/WefDtt910EEH5aGHHqqLUO+aPn16Wrduvd7Q9K4hQ4Zkq622yh//+Mf0799/nVvLli3Xu//JJ5+cTp065b/+67/y3HPPrXPOuw8g32233bLDDjvk1ltvTalUqnt/5cqVueOOO+q+kW9TmjFjRr3P+tvf/pb58+fngAMOSPJOYCqVSikvL6+33w033JA1a9ZskjWUlZVlr732yve///1stdVW+Z//+Z8k7/zdJcnNN99cb/4dd9yRlStX1r0PAHy0uFIKAPjQ/O1vf8ujjz6aJHn66aeTJD/96U+TJN27d3/fW/9uuummXHfddTn22GOzzz77pLKyMn//+99zww035A9/+EMuvPDCugj0ve99L//2b/+WAQMG5Nxzz80uu+ySF154Iffee2+uu+66tGvXLuPHj8+sWbNy4IEH5sILL8w222yTW265JT//+c9z2WWXpbKy8n3PpW3btvnBD36QE044IS+//HI+//nPZ7vttstLL72UJUuW5KWXXsqUKVPWu39lZWXuueeejBgxIn379s0ZZ5yRQYMGpWXLlnnqqady8803Z8mSJTniiCOyxRZb5LLLLssXv/jFjBgxIqeeempWrVqV7373u3n11VdzySWXNOrvYUO8+OKL+dznPpcvf/nLqampyfjx49OqVauMHTs2SVJRUZH99tsv3/3ud9OhQ4d07949v/71r3PjjTdmq6222ujPnTVrViZPnpzDDz88O+20U0qlUu688868+uqrOfjgg5MkBx98cA455JCcc845qa2tzZAhQ/L4449n/Pjx6du3b0aPHr0pfgQAQMFEKQDgQ/Pwww/XPQvqXV/4wheSJCeccEKmTZu23n0/+9nPprq6OrNnz86UKVPyyiuvpF27dtlzzz1z00035bjjjqubu9dee+V3v/tdxo8fn7Fjx2bFihXp1KlTPvWpT9WFq9122y3z58/Peeedl9NPPz1vvPFGevXqlalTpzZ4gPb6HHfccdlxxx1z2WWX5dRTT82KFSuy3XbbpU+fPht0jH333Te///3v8/3vfz+33357Lr300qxZsyZdu3bNQQcdlP/+7/+um3vsscemTZs2mTRpUkaNGpVmzZpl4MCBefjhhzN48OANWm9jfOc738mjjz6ak046KbW1tdl3331z2223Zeedd66bc+utt+bMM8/Mf/3Xf2X16tUZMmRI5syZk89+9rMb/bm77rprttpqq1x22WV5/vnn07Jly+y2226ZNm1aTjjhhCTvXEF1991356KLLsrUqVNz8cUXp0OHDhk9enS+853vNLh6CwD4aCgr/eN12gAA/Ev51a9+lQMPPDA/+clP3vch7gAAm5pnSgEAAABQOFEKAAAAgMK5fQ8AAACAwrlSCgAAAIDCiVIAAAAAFE6UAgAAAKBwzZt6AZvK2rVr8/zzz6ddu3YpKytr6uUAAAAA/EsqlUpZsWJFtt9++2yxxfqvh/rYRKnnn38+Xbt2beplAAAAAJDk2WefTZcuXdb7/scmSrVr1y7JOydcUVHRxKsBAAAA+NdUW1ubrl271rWa9fnYRKl3b9mrqKgQpQAAAACa2D97vJIHnQMAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACte8qRcAAMC/lpoJE5p6CQCwWaocP76pl1AoV0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMJtVJSaPHlyevTokVatWqVfv36ZO3fuBu33//7f/0vz5s3Tp0+fBu/dcccd6d27d8rLy9O7d+/cddddG7M0AAAAAD4CGh2lZs6cmbPOOivjxo3LokWLMnTo0AwfPjxVVVXvu19NTU2OP/74HHTQQQ3eW7BgQUaNGpXRo0dnyZIlGT16dI466qj89re/bezyAAAAAPgIKCuVSqXG7DBgwIDsvffemTJlSt1Yr169cvjhh2fSpEnr3e/oo4/OrrvummbNmuXuu+/O4sWL694bNWpUamtrc99999WNHXroodl6660zY8aMDVpXbW1tKisrU1NTk4qKisacEgAABaqZMKGplwAAm6XK8eObegmbxIY2mkZdKfXWW29l4cKFGTZsWL3xYcOGZf78+evdb+rUqXn66aczfj0/3AULFjQ45iGHHPK+xwQAAADgo6t5YyYvW7Ysa9asSceOHeuNd+zYMdXV1evc56mnnsq5556buXPnpnnzdX9cdXV1o46ZJKtWrcqqVavqXtfW1m7oaQAAAADQxDbqQedlZWX1XpdKpQZjSbJmzZoce+yxmTBhQj7xiU9skmO+a9KkSamsrKzbunbt2ogzAAAAAKApNSpKdejQIc2aNWtwBdOLL77Y4EqnJFmxYkUee+yxnHHGGWnevHmaN2+eiRMnZsmSJWnevHkeeuihJEmnTp02+JjvGjt2bGpqauq2Z599tjGnAgAAAEATalSUatmyZfr165c5c+bUG58zZ04GDx7cYH5FRUV+//vfZ/HixXXbaaedlt122y2LFy/OgAEDkiSDBg1qcMwHH3xwncd8V3l5eSoqKuptAAAAAHw0NOqZUkkyZsyYjB49Ov3798+gQYNy/fXXp6qqKqeddlqSd65geu655zJ9+vRsscUW2X333evtv91226VVq1b1xs8888zst99+ufTSSzNy5Mjcc889+cUvfpF58+Z9wNMDAAAAYHPU6Cg1atSoLF++PBMnTszSpUuz++67Z/bs2enWrVuSZOnSpamqqmrUMQcPHpzbbrst559/fi644ILsvPPOmTlzZt2VVAAAAAB8vJSVSqVSUy9iU6itrU1lZWVqamrcygcAsBmrmTChqZcAAJulyvHjm3oJm8SGNpqN+vY9AAAAAPggRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACte8qRfAul2yaFlTLwEANjvn9u3Q1EsAAGATcaUUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFC4jYpSkydPTo8ePdKqVav069cvc+fOXe/cefPmZciQIWnfvn223HLL9OzZM9///vfrzZk2bVrKysoabG+++ebGLA8AAACAzVzzxu4wc+bMnHXWWZk8eXKGDBmS6667LsOHD88f//jH7Ljjjg3mt2nTJmeccUb23HPPtGnTJvPmzcupp56aNm3a5Ctf+UrdvIqKijz55JP19m3VqtVGnBIAAAAAm7tGR6nvfe97Ofnkk3PKKackSa688so88MADmTJlSiZNmtRgft++fdO3b9+61927d8+dd96ZuXPn1otSZWVl6dSp08acAwAAAAAfMY26fe+tt97KwoULM2zYsHrjw4YNy/z58zfoGIsWLcr8+fOz//771xt/7bXX0q1bt3Tp0iUjRozIokWLGrM0AAAAAD5CGnWl1LJly7JmzZp07Nix3njHjh1TXV39vvt26dIlL730UlavXp2LLrqo7kqrJOnZs2emTZuWPfbYI7W1tbnqqqsyZMiQLFmyJLvuuus6j7dq1aqsWrWq7nVtbW1jTgUAAACAJtTo2/eSd261+0elUqnB2HvNnTs3r732Wn7zm9/k3HPPzS677JJjjjkmSTJw4MAMHDiwbu6QIUOy99575wc/+EGuvvrqdR5v0qRJmTBhwsYsHwAAAIAm1qgo1aFDhzRr1qzBVVEvvvhig6un3qtHjx5Jkj322CMvvPBCLrrooroo9V5bbLFF9tlnnzz11FPrPd7YsWMzZsyYute1tbXp2rXrhp4KAAAAAE2oUc+UatmyZfr165c5c+bUG58zZ04GDx68wccplUr1br1b1/uLFy9O586d1zunvLw8FRUV9TYAAAAAPhoaffvemDFjMnr06PTv3z+DBg3K9ddfn6qqqpx22mlJ3rmC6bnnnsv06dOTJNdcc0123HHH9OzZM0kyb968XH755fna175Wd8wJEyZk4MCB2XXXXVNbW5urr746ixcvzjXXXLMpzhEAAACAzUyjo9SoUaOyfPnyTJw4MUuXLs3uu++e2bNnp1u3bkmSpUuXpqqqqm7+2rVrM3bs2DzzzDNp3rx5dt5551xyySU59dRT6+a8+uqr+cpXvpLq6upUVlamb9++eeSRR7LvvvtuglMEAAAAYHNTViqVSk29iE2htrY2lZWVqamp+VjcynfJomVNvQQA2Oyc27dDUy+BTaDGl9UAwDpVjh/f1EvYJDa00TTqmVIAAAAAsCmIUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQuI2KUpMnT06PHj3SqlWr9OvXL3Pnzl3v3Hnz5mXIkCFp3759ttxyy/Ts2TPf//73G8y744470rt375SXl6d379656667NmZpAAAAAHwENDpKzZw5M2eddVbGjRuXRYsWZejQoRk+fHiqqqrWOb9NmzY544wz8sgjj+SJJ57I+eefn/PPPz/XX3993ZwFCxZk1KhRGT16dJYsWZLRo0fnqKOOym9/+9uNPzMAAAAANltlpVKp1JgdBgwYkL333jtTpkypG+vVq1cOP/zwTJo0aYOOccQRR6RNmza56aabkiSjRo1KbW1t7rvvvro5hx56aLbeeuvMmDFjg45ZW1ubysrK1NTUpKKiohFntHm6ZNGypl4CAGx2zu3boamXwCZQM2FCUy8BADZLlePHN/USNokNbTSNulLqrbfeysKFCzNs2LB648OGDcv8+fM36BiLFi3K/Pnzs//++9eNLViwoMExDznkkPc95qpVq1JbW1tvAwAAAOCjoVFRatmyZVmzZk06duxYb7xjx46prq5+3327dOmS8vLy9O/fP6effnpOOeWUuveqq6sbfcxJkyalsrKybuvatWtjTgUAAACAJrRRDzovKyur97pUKjUYe6+5c+fmsccey7XXXpsrr7yywW15jT3m2LFjU1NTU7c9++yzjTwLAAAAAJpK88ZM7tChQ5o1a9bgCqYXX3yxwZVO79WjR48kyR577JEXXnghF110UY455pgkSadOnRp9zPLy8pSXlzdm+QAAAABsJhp1pVTLli3Tr1+/zJkzp974nDlzMnjw4A0+TqlUyqpVq+peDxo0qMExH3zwwUYdEwAAAICPjkZdKZUkY8aMyejRo9O/f/8MGjQo119/faqqqnLaaacleee2uueeey7Tp09PklxzzTXZcccd07NnzyTJvHnzcvnll+drX/ta3THPPPPM7Lfffrn00kszcuTI3HPPPfnFL36RefPmbYpzBAAAAGAz0+goNWrUqCxfvjwTJ07M0qVLs/vuu2f27Nnp1q1bkmTp0qWpqqqqm7927dqMHTs2zzzzTJo3b56dd945l1xySU499dS6OYMHD85tt92W888/PxdccEF23nnnzJw5MwMGDNgEpwgAAADA5qasVCqVmnoRm0JtbW0qKytTU1OTioqKpl7OB3bJomVNvQQA2Oyc27dDUy+BTaBmwoSmXgIAbJYqx49v6iVsEhvaaDbq2/cAAAAA4IMQpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcBsVpSZPnpwePXqkVatW6devX+bOnbveuXfeeWcOPvjgbLvttqmoqMigQYPywAMP1Jszbdq0lJWVNdjefPPNjVkeAAAAAJu5RkepmTNn5qyzzsq4ceOyaNGiDB06NMOHD09VVdU65z/yyCM5+OCDM3v27CxcuDAHHnhgDjvssCxatKjevIqKiixdurTe1qpVq407KwAAAAA2a80bu8P3vve9nHzyyTnllFOSJFdeeWUeeOCBTJkyJZMmTWow/8orr6z3+jvf+U7uueee/OxnP0vfvn3rxsvKytKpU6fGLgcAAACAj6BGXSn11ltvZeHChRk2bFi98WHDhmX+/PkbdIy1a9dmxYoV2WabbeqNv/baa+nWrVu6dOmSESNGNLiS6r1WrVqV2traehsAAAAAHw2NilLLli3LmjVr0rFjx3rjHTt2THV19QYd44orrsjKlStz1FFH1Y317Nkz06ZNy7333psZM2akVatWGTJkSJ566qn1HmfSpEmprKys27p27dqYUwEAAACgCW3Ug87LysrqvS6VSg3G1mXGjBm56KKLMnPmzGy33XZ14wMHDsxxxx2XvfbaK0OHDs3tt9+eT3ziE/nBD36w3mONHTs2NTU1dduzzz67MacCAAAAQBNo1DOlOnTokGbNmjW4KurFF19scPXUe82cOTMnn3xyfvKTn+TTn/70+87dYostss8++7zvlVLl5eUpLy/f8MUDAAAAsNlo1JVSLVu2TL9+/TJnzpx643PmzMngwYPXu9+MGTNy4okn5tZbb81nP/vZf/o5pVIpixcvTufOnRuzPAAAAAA+Ihr97XtjxozJ6NGj079//wwaNCjXX399qqqqctpppyV557a65557LtOnT0/yTpA6/vjjc9VVV2XgwIF1V1ltueWWqaysTJJMmDAhAwcOzK677pra2tpcffXVWbx4ca655ppNdZ4AAAAAbEYaHaVGjRqV5cuXZ+LEiVm6dGl23333zJ49O926dUuSLF26NFVVVXXzr7vuuqxevTqnn356Tj/99LrxE044IdOmTUuSvPrqq/nKV76S6urqVFZWpm/fvnnkkUey7777fsDTAwAAAGBzVFYqlUpNvYhNoba2NpWVlampqUlFRUVTL+cDu2TRsqZeAgBsds7t26Gpl8AmUDNhQlMvAQA2S5Xjxzf1EjaJDW00G/XtewAAAADwQYhSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFC4jYpSkydPTo8ePdKqVav069cvc+fOXe/cO++8MwcffHC23XbbVFRUZNCgQXnggQcazLvjjjvSu3fvlJeXp3fv3rnrrrs2ZmkAAAAAfAQ0OkrNnDkzZ511VsaNG5dFixZl6NChGT58eKqqqtY5/5FHHsnBBx+c2bNnZ+HChTnwwANz2GGHZdGiRXVzFixYkFGjRmX06NFZsmRJRo8enaOOOiq//e1vN/7MAAAAANhslZVKpVJjdhgwYED23nvvTJkypW6sV69eOfzwwzNp0qQNOsYnP/nJjBo1KhdeeGGSZNSoUamtrc19991XN+fQQw/N1ltvnRkzZmzQMWtra1NZWZmamppUVFQ04ow2T5csWtbUSwCAzc65fTs09RLYBGomTGjqJQDAZqly/PimXsImsaGNplFXSr311ltZuHBhhg0bVm982LBhmT9//gYdY+3atVmxYkW22WaburEFCxY0OOYhhxzyvsdctWpVamtr620AAAAAfDQ0KkotW7Ysa9asSceOHeuNd+zYMdXV1Rt0jCuuuCIrV67MUUcdVTdWXV3d6GNOmjQplZWVdVvXrl0bcSYAAAAANKWNetB5WVlZvdelUqnB2LrMmDEjF110UWbOnJntttvuAx1z7NixqampqdueffbZRpwBAAAAAE2peWMmd+jQIc2aNWtwBdOLL77Y4Eqn95o5c2ZOPvnk/OQnP8mnP/3peu916tSp0ccsLy9PeXl5Y5YPAAAAwGaiUVdKtWzZMv369cucOXPqjc+ZMyeDBw9e734zZszIiSeemFtvvTWf/exnG7w/aNCgBsd88MEH3/eYAAAAAHx0NepKqSQZM2ZMRo8enf79+2fQoEG5/vrrU1VVldNOOy3JO7fVPffcc5k+fXqSd4LU8ccfn6uuuioDBw6suyJqyy23TGVlZZLkzDPPzH777ZdLL700I0eOzD333JNf/OIXmTdv3qY6TwAAAAA2I41+ptSoUaNy5ZVXZuLEienTp08eeeSRzJ49O926dUuSLF26NFVVVXXzr7vuuqxevTqnn356OnfuXLedeeaZdXMGDx6c2267LVOnTs2ee+6ZadOmZebMmRkwYMAmOEUAAAAANjdlpVKp1NSL2BRqa2tTWVmZmpqaVFRUNPVyPrBLFi1r6iUAwGbn3L4dmnoJbAI1EyY09RIAYLNUOX58Uy9hk9jQRrNR374HAAAAAB+EKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwGxWlJk+enB49eqRVq1bp169f5s6du965S5cuzbHHHpvddtstW2yxRc4666wGc6ZNm5aysrIG25tvvrkxywMAAABgM9foKDVz5sycddZZGTduXBYtWpShQ4dm+PDhqaqqWuf8VatWZdttt824ceOy1157rfe4FRUVWbp0ab2tVatWjV0eAAAAAB8BjY5S3/ve93LyySfnlFNOSa9evXLllVema9eumTJlyjrnd+/ePVdddVWOP/74VFZWrve4ZWVl6dSpU70NAAAAgI+nRkWpt956KwsXLsywYcPqjQ8bNizz58//QAt57bXX0q1bt3Tp0iUjRozIokWLPtDxAAAAANh8NSpKLVu2LGvWrEnHjh3rjXfs2DHV1dUbvYiePXtm2rRpuffeezNjxoy0atUqQ4YMyVNPPbXefVatWpXa2tp6GwAAAAAfDRv1oPOysrJ6r0ulUoOxxhg4cGCOO+647LXXXhk6dGhuv/32fOITn8gPfvCD9e4zadKkVFZW1m1du3bd6M8HAAAAoFiNilIdOnRIs2bNGlwV9eKLLza4euoDLWqLLbLPPvu875VSY8eOTU1NTd327LPPbrLPBwAAAODD1ago1bJly/Tr1y9z5sypNz5nzpwMHjx4ky2qVCpl8eLF6dy583rnlJeXp6Kiot4GAAAAwEdD88buMGbMmIwePTr9+/fPoEGDcv3116eqqiqnnXZakneuYHruuecyffr0un0WL16c5J2Hmb/00ktZvHhxWrZsmd69eydJJkyYkIEDB2bXXXdNbW1trr766ixevDjXXHPNJjhFAAAAADY3jY5So0aNyvLlyzNx4sQsXbo0u+++e2bPnp1u3bolSZYuXZqqqqp6+/Tt27fuzwsXLsytt96abt265a9//WuS5NVXX81XvvKVVFdXp7KyMn379s0jjzySfffd9wOcGgAAAACbq7JSqVRq6kVsCrW1tamsrExNTc3H4la+SxYta+olAMBm59y+HZp6CWwCNRMmNPUSAGCzVDl+fFMvYZPY0EazUd++BwAAAAAfhCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIXbqCg1efLk9OjRI61atUq/fv0yd+7c9c5dunRpjj322Oy2227ZYostctZZZ61z3h133JHevXunvLw8vXv3zl133bUxSwMAAADgI6DRUWrmzJk566yzMm7cuCxatChDhw7N8OHDU1VVtc75q1atyrbbbptx48Zlr732WuecBQsWZNSoURk9enSWLFmS0aNH56ijjspvf/vbxi4PAAAAgI+AslKpVGrMDgMGDMjee++dKVOm1I316tUrhx9+eCZNmvS++x5wwAHp06dPrrzyynrjo0aNSm1tbe677766sUMPPTRbb711ZsyYsUHrqq2tTWVlZWpqalJRUbHhJ7SZumTRsqZeAgBsds7t26Gpl8AmUDNhQlMvAQA2S5Xjxzf1EjaJDW00jbpS6q233srChQszbNiweuPDhg3L/PnzN26leedKqfce85BDDvlAxwQAAABg89W8MZOXLVuWNWvWpGPHjvXGO3bsmOrq6o1eRHV1daOPuWrVqqxatarudW1t7UZ/PgAAAADF2qgHnZeVldV7XSqVGox92MecNGlSKisr67auXbt+oM8HAAAAoDiNilIdOnRIs2bNGlzB9OKLLza40qkxOnXq1Ohjjh07NjU1NXXbs88+u9GfDwAAAECxGhWlWrZsmX79+mXOnDn1xufMmZPBgwdv9CIGDRrU4JgPPvjg+x6zvLw8FRUV9TYAAAAAPhoa9UypJBkzZkxGjx6d/v37Z9CgQbn++utTVVWV0047Lck7VzA999xzmT59et0+ixcvTpK89tpreemll7J48eK0bNkyvXv3TpKceeaZ2W+//XLppZdm5MiRueeee/KLX/wi8+bN2wSnCAAAAMDmptFRatSoUVm+fHkmTpyYpUuXZvfdd8/s2bPTrVu3JMnSpUtTVVVVb5++ffvW/XnhwoW59dZb061bt/z1r39NkgwePDi33XZbzj///FxwwQXZeeedM3PmzAwYMOADnBoAAAAAm6uyUqlUaupFbAq1tbWprKxMTU3Nx+JWvksWLWvqJQDAZufcvh2aeglsAjUTJjT1EgBgs1Q5fnxTL2GT2NBGs1HfvgcAAAAAH4QoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACF26goNXny5PTo0SOtWrVKv379Mnfu3Ped/+tf/zr9+vVLq1atstNOO+Xaa6+t9/60adNSVlbWYHvzzTc3ZnkAAAAAbOYaHaVmzpyZs846K+PGjcuiRYsydOjQDB8+PFVVVeuc/8wzz+Qzn/lMhg4dmkWLFuW8887L17/+9dxxxx315lVUVGTp0qX1tlatWm3cWQEAAACwWWve2B2+973v5eSTT84pp5ySJLnyyivzwAMPZMqUKZk0aVKD+ddee2123HHHXHnllUmSXr165bHHHsvll1+eI488sm5eWVlZOnXqtJGnAQAAAMBHSaOulHrrrbeycOHCDBs2rN74sGHDMn/+/HXus2DBggbzDznkkDz22GN5++2368Zee+21dOvWLV26dMmIESOyaNGixiwNAAAAgI+QRkWpZcuWZc2aNenYsWO98Y4dO6a6unqd+1RXV69z/urVq7Ns2bIkSc+ePTNt2rTce++9mTFjRlq1apUhQ4bkqaeeWu9aVq1aldra2nobAAAAAB8NG/Wg87KysnqvS6VSg7F/Nv8fxwcOHJjjjjsue+21V4YOHZrbb789n/jEJ/KDH/xgvcecNGlSKisr67auXbtuzKkAAAAA0AQaFaU6dOiQZs2aNbgq6sUXX2xwNdS7OnXqtM75zZs3T/v27de9qC22yD777PO+V0qNHTs2NTU1dduzzz7bmFMBAAAAoAk1Kkq1bNky/fr1y5w5c+qNz5kzJ4MHD17nPoMGDWow/8EHH0z//v3TokWLde5TKpWyePHidO7ceb1rKS8vT0VFRb0NAAAAgI+GRt++N2bMmNxwww350Y9+lCeeeCJnn312qqqqctpppyV55wqm448/vm7+aaedlr/97W8ZM2ZMnnjiifzoRz/KjTfemG9+85t1cyZMmJAHHnggf/nLX7J48eKcfPLJWbx4cd0xAQAAAPh4ad7YHUaNGpXly5dn4sSJWbp0aXbffffMnj073bp1S5IsXbo0VVVVdfN79OiR2bNn5+yzz84111yT7bffPldffXWOPPLIujmvvvpqvvKVr6S6ujqVlZXp27dvHnnkkey7776b4BQBAAAA2NyUld596vhHXG1tbSorK1NTU/OxuJXvkkXLmnoJALDZObdvh6ZeAptAzYQJTb0EANgsVY4f39RL2CQ2tNFs1LfvAQAAAMAHIUoBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOE2KkpNnjw5PXr0SKtWrdKvX7/MnTv3fef/+te/Tr9+/dKqVavstNNOufbaaxvMueOOO9K7d++Ul5end+/eueuuuzZmaQAAAAB8BDQ6Ss2cOTNnnXVWxo0bl0WLFmXo0KEZPnx4qqqq1jn/mWeeyWc+85kMHTo0ixYtynnnnZevf/3rueOOO+rmLFiwIKNGjcro0aOzZMmSjB49OkcddVR++9vfbvyZAQAAALDZKiuVSqXG7DBgwIDsvffemTJlSt1Yr169cvjhh2fSpEkN5p9zzjm5995788QTT9SNnXbaaVmyZEkWLFiQJBk1alRqa2tz33331c059NBDs/XWW2fGjBkbtK7a2tpUVlampqYmFRUVjTmlzdIli5Y19RIAYLNzbt8OTb0ENoGaCROaegkAsFmqHD++qZewSWxoo2nUlVJvvfVWFi5cmGHDhtUbHzZsWObPn7/OfRYsWNBg/iGHHJLHHnssb7/99vvOWd8xAQAAAPhoa96YycuWLcuaNWvSsWPHeuMdO3ZMdXX1Oveprq5e5/zVq1dn2bJl6dy583rnrO+YSbJq1aqsWrWq7nVNTU2Sd2rcx8Gbr61o6iUAwGantrZlUy+BTaD2zTebegkAsFkq+5g0jXfbzD+7Oa9RUepdZWVl9V6XSqUGY/9s/nvHG3vMSZMmZcI6Lv3u2rXr+hcOAHykuekLAPhYu+SSpl7BJrVixYpUVlau9/1GRakOHTqkWbNmDa5gevHFFxtc6fSuTp06rXN+8+bN0759+/eds75jJsnYsWMzZsyYutdr167Nyy+/nPbt279vzAJojNra2nTt2jXPPvvsx+J5dQAA/8jvOsCHoVQqZcWKFdl+++3fd16jolTLli3Tr1+/zJkzJ5/73OfqxufMmZORI0euc59BgwblZz/7Wb2xBx98MP3790+LFi3q5syZMydnn312vTmDBw9e71rKy8tTXl5eb2yrrbZqzOkAbLCKigq/qAEAH1t+1wE2tfe7Qupdjb59b8yYMRk9enT69++fQYMG5frrr09VVVVOO+20JO9cwfTcc89l+vTpSd75pr3//u//zpgxY/LlL385CxYsyI033ljvW/XOPPPM7Lfffrn00kszcuTI3HPPPfnFL36RefPmNXZ5AAAAAHwENDpKjRo1KsuXL8/EiROzdOnS7L777pk9e3a6deuWJFm6dGmqqqrq5vfo0SOzZ8/O2WefnWuuuSbbb799rr766hx55JF1cwYPHpzbbrst559/fi644ILsvPPOmTlzZgYMGLAJThEAAACAzU1Z6Z89Ch3gX9iqVasyadKkjB07tsEtwwAAH3V+1wGakigFAAAAQOG2aOoFAAAAAPCvR5QCAAAAoHCiFPCRc9FFF6Vjx44pKyvL3Xff3dTL2aQOOOCAnHXWWU29DACgQKVSKV/5yleyzTbbpKysLIsXL27qJX1g3bt3z5VXXtnUywA2c6IUUIgTTzwxZWVldVv79u1z6KGH5vHHH2/UcZ544olMmDAh1113XZYuXZrhw4d/SCtu6B/Xv67txBNP/MCfceedd+Zb3/rWB18sALBZmT9/fpo1a5ZDDz20wXv3339/pk2bllmzZtV9w/mH9Y9vf/3rX//p7zQXXXTRB/6cRx99NF/5ylc++IKBj7XmTb0A4F/HoYcemqlTpyZJqqurc/7552fEiBGpqqra4GM8/fTTSZKRI0emrKxso9fy9ttvp0WLFo3aZ+nSpXV/njlzZi688MI8+eSTdWNbbrnlRq/nXdtss80HPgYAsPn50Y9+lK997Wu54YYbUlVVlR133LHuvaeffjqdO3fO4MGDN/nnvvd3nq5du9b7nebyyy/P/fffn1/84hd1Y23btv3An7vtttt+4GMAH3+ulAIKU15enk6dOqVTp07p06dPzjnnnDz77LN56aWX6uY899xzGTVqVLbeeuu0b98+I0eOzF//+tck79y2d9hhhyVJtthii7ootXbt2kycODFdunRJeXl5+vTpk/vvv7/umO/+i+Dtt9+eAw44IK1atcrNN9+cJJk6dWp69eqVVq1apWfPnpk8efJ61//u2jt16pTKysqUlZXVvb7//vvTrVu3evPvvvvueuHsoosuSp8+fXLTTTele/fuqayszNFHH50VK1bUzXnv7Xvdu3fPd77znXzpS19Ku3btsuOOO+b666+v9znz589Pnz590qpVq/Tv37/ucz8Ol/4DwMfBypUrc/vtt+c//uM/MmLEiEybNq3uvRNPPDFf+9rXUlVVlbKysnTv3j3du3dPknzuc5+rG3vXz372s/Tr1y+tWrXKTjvtlAkTJmT16tV175eVleXaa6/NyJEj06ZNm3z729+ut5ZmzZrV+52mbdu2ad68ed3ra6+9Nv/2b/9Wb58rr7yy3hpOPPHEHH744bn88svTuXPntG/fPqeffnrefvvtujnvvX2vrKwsN9xwQz73uc+ldevW2XXXXXPvvffW+5x77703u+66a7bccssceOCB+fGPf5yysrK8+uqrjfuBAx8ZohTQJF577bXccsst2WWXXdK+ffskyeuvv54DDzwwbdu2zSOPPJJ58+albdu2OfTQQ/PWW2/lm9/8Zt2VVkuXLq37V76rrroqV1xxRS6//PI8/vjjOeSQQ/Lv//7veeqpp+p95jnnnJOvf/3reeKJJ3LIIYfkhz/8YcaNG5eLL744TzzxRL7zne/kggsuyI9//OMP7byffvrp3H333Zk1a1ZmzZqVX//617nkkkved58rrrgi/fv3z6JFi/LVr341//Ef/5E//elPSZIVK1bksMMOyx577JH/+Z//ybe+9a2cc845H9r6AYDGmzlzZnbbbbfstttuOe644zJ16tSUSqUk7/we8+4/ri1dujSPPvpoHn300STv/OPZu2NJ8sADD+S4447L17/+9fzxj3/Mddddl2nTpuXiiy+u93njx4/PyJEj8/vf/z5f+tKXPpRzevjhh/P000/n4Ycfzo9//ONMmzatXmxblwkTJuSoo47K448/ns985jP54he/mJdffjnJO/+I+PnPfz6HH354Fi9enFNPPTXjxo37UNYObD5EKaAws2bNStu2bdO2bdu0a9cu9957b2bOnJkttnjnf4puu+22bLHFFrnhhhuyxx57pFevXpk6dWqqqqryq1/9Km3bts1WW22V5P+/ail557Lzc845J0cffXR22223XHrppenTp0+Dh2ueddZZOeKII9KjR49sv/32+da3vpUrrriibuyII47I2Wefneuuu+5D+xmsXbs206ZNy+67756hQ4dm9OjR+eUvf/m++3zmM5/JV7/61eyyyy4555xz0qFDh/zqV79Kktxyyy0pKyvLD3/4w/Tu3TvDhw/Pf/7nf35o6wcAGu/GG2/Mcccdl+Sdxxm89tprdf//X1lZmXbt2tVdwbTtttvW3fq21VZb1Y0lycUXX5xzzz03J5xwQnbaaaccfPDB+da3vtXgd5djjz02X/rSl7LTTjs1uJJ7U9l6663z3//93+nZs2dGjBiRz372s//0d5oTTzwxxxxzTHbZZZd85zvfycqVK/O73/0uSXLttddmt912y3e/+93stttuOfroozfJ8zqBzZtnSgGFOfDAAzNlypQkycsvv5zJkydn+PDh+d3vfpdu3bpl4cKF+b//+7+0a9eu3n5vvvlm3bOk3qu2tjbPP/98hgwZUm98yJAhWbJkSb2x/v371/35pZdeyrPPPpuTTz45X/7yl+vGV69encrKyg90nu+ne/fu9c6vc+fOefHFF993nz333LPuz+/eMvjuPk8++WT23HPPtGrVqm7Ovvvuu4lXDQBsrCeffDK/+93vcueddyZJmjdvnlGjRuVHP/pRPv3pTzfqWAsXLsyjjz5a78qoNWvW5M0338zrr7+e1q1bJ6n/O8+H5ZOf/GSaNWtW97pz5875/e9//777/OPvNG3atEm7du3q/U6zzz771Jvvdxr4+BOlgMK0adMmu+yyS93rfv36pbKyMj/84Q/z7W9/O2vXrk2/fv1yyy23NNj3nz0s870PPS+VSg3G2rRpU/fntWvXJkl++MMfZsCAAfXm/eMvWBtqiy22qLsM/13/+FyFd7334eplZWV1a1mf99tnXef53nUAAE3nxhtvzOrVq7PDDjvUjZVKpbRo0SKvvPJKtt566w0+1tq1azNhwoQcccQRDd77x3+g+sffeRrL7zRAkUQpoMmUlZVliy22yBtvvJEk2XvvvTNz5sxst912qaio2KBjVFRUZPvtt8+8efOy33771Y3Pnz//ff91rWPHjtlhhx3yl7/8JV/84hc/2InknWi2YsWKrFy5su4XwSIeNN6zZ8/ccsstWbVqVcrLy5Mkjz322If+uQDAP7d69epMnz49V1xxRYYNG1bvvSOPPDK33HJLzjjjjHXu26JFi6xZs6be2N57750nn3yy3j/ybWrbbrttqqur60Wion6nmT17dr0xv9PAx59nSgGFWbVqVaqrq1NdXZ0nnngiX/va1/Laa6/VfaPeF7/4xXTo0CEjR47M3Llz88wzz+TXv/51zjzzzPz9739f73H/8z//M5deemlmzpyZJ598Mueee24WL16cM888833Xc9FFF2XSpEm56qqr8uc//zm///3vM3Xq1Hzve99r9LkNGDAgrVu3znnnnZf/+7//y6233vpPH/a5KRx77LFZu3ZtvvKVr+SJJ57IAw88kMsvvzxJw6vHAIBizZo1K6+88kpOPvnk7L777vW2z3/+87nxxhvXu2/37t3zy1/+MtXV1XnllVeSJBdeeGGmT5+eiy66KH/4wx/yxBNPZObMmTn//PM32ZoPOOCAvPTSS7nsssvy9NNP55prrsl99923yY6/Pqeeemr+9Kc/5Zxzzsmf//zn3H777XW/S/mdBj6+RCmgMPfff386d+6czp07Z8CAAXn00Ufzk5/8JAcccECSpHXr1nnkkUey44475ogjjkivXr3ypS99KW+88cb7Xjn19a9/Pd/4xjfyjW98I3vssUfuv//+uq8Ufj+nnHJKbrjhhkybNi177LFH9t9//0ybNi09evRo9Llts802ufnmmzN79uzssccemTFjRi666KJGH6exKioq8rOf/SyLFy9Onz59Mm7cuFx44YVJ6l/GDwAU78Ybb8ynP/3pdT6v8sgjj8zixYvzP//zP+vc94orrsicOXPStWvX9O3bN0lyyCGHZNasWZkzZ0722WefDBw4MN/73vc26cPMe/XqlcmTJ+eaa67JXnvtld/97nf55je/ucmOvz49evTIT3/609x5553Zc889M2XKlLpv33v3anDg46es5EZdgI+VW265JSeddFJqamqy5ZZbNvVyAAA2ysUXX5xrr702zz77bFMvBfiQeKYUwEfc9OnTs9NOO2WHHXbIkiVLcs455+Soo44SpACAj5TJkydnn332Sfv27fP//t//y3e/+931PnML+HgQpQA+4qqrq3PhhRemuro6nTt3zhe+8IV6XxUNAPBR8NRTT+Xb3/52Xn755ey44475xje+kbFjxzb1soAPkdv3AAAAACicB50DAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQBssGnTpqWsrKxua968ebp06ZKTTjopzz33XOHrOfHEE9O9e/dG7fPXv/41ZWVlmTZt2oeypg0xd+7cHHXUUdlhhx3SsmXLVFZWZvDgwZkyZUpWrlzZZOsqygEHHJADDjigqZcBADSxslKpVGrqRQAAHw3Tpk3LSSedlKlTp6Znz55544038sgjj2TSpEnZfvvt8/vf/z5t2rQpbD1PP/10amtr07dv3w3eZ9WqVVm0aFF23nnnbLvtth/i6tZt/PjxmThxYgYPHpyTTz45O++8c15//fXMnz8/119/fY499th8//vfL3xdRfrjH/+YJOndu3cTrwQAaEqiFACwwd6NUo8++mj69+9fN37hhRfmW9/6Vm6++eZ88YtfXOe+r7/+elq3bl3UUjdLP/nJT3LUUUfl5JNPzg9/+MOUlZXVe3/FihVZsGBBhg0b1kQr/HD5bwAA+Edu3wMAPrCBAwcmSf72t78leee2urZt2+b3v/99hg0blnbt2uWggw5Kkrz11lv59re/nZ49e6a8vDzbbrttTjrppLz00ksNjnvrrbdm0KBBadu2bdq2bZs+ffrkxhtvrHt/Xbfv/eQnP8mAAQNSWVmZ1q1bZ6eddsqXvvSluvfXd/vevHnzctBBB6Vdu3Zp3bp1Bg8enJ///Of15rx7++LDDz+c//iP/0iHDh3Svn37HHHEEXn++ef/6c9p4sSJ2XrrrXP11Vc3CFJJ0q5du3pB6s0338zYsWPTo0ePtGzZMjvssENOP/30vPrqq/X26969e0aMGJFZs2alb9++2XLLLdOrV6/MmjWrbt29evVKmzZtsu++++axxx6rt/+7f19/+MMfctBBB6VNmzbZdtttc8YZZ+T111+vN/eaa67Jfvvtl+222y5t2rTJHnvskcsuuyxvv/12vXkHHHBAdt999zzyyCMZPHhwWrduXff3sK7b96ZMmZK99torbdu2Tbt27dKzZ8+cd9559eb87//+b0aOHJmtt946rVq1Sp8+ffLjH/+43pxf/epXKSsry4wZMzJu3Lhsv/32qaioyKc//ek8+eST6/mbAQCagigFAHxg//d//5ck9W6He+utt/Lv//7v+dSnPpV77rknEyZMyNq1azNy5MhccsklOfbYY/Pzn/88l1xySebMmZMDDjggb7zxRt3+F154Yb74xS9m++23z7Rp03LXXXflhBNOqAtf67JgwYKMGjUqO+20U2677bb8/Oc/z4UXXpjVq1e/7/p//etf51Of+lRqampy4403ZsaMGWnXrl0OO+ywzJw5s8H8U045JS1atMitt96ayy67LL/61a9y3HHHve9nLF26NP/7v/+bYcOGbdDVQqVSKYcffnguv/zyjB49Oj//+c8zZsyY/PjHP86nPvWprFq1qt78JUuWZOzYsTnnnHNy5513prKyMkcccUTGjx+fG264Id/5zndyyy23pKamJiNGjKj3s06St99+O5/5zGdy0EEH5e67784ZZ5yR6667LqNGjao37+mnn86xxx6bm266KbNmzcrJJ5+c7373uzn11FPXec7HHXdcjj322MyePTtf/epX13mut912W7761a9m//33z1133ZW77747Z599dr3naz355JMZPHhw/vCHP+Tqq6/OnXfemd69e+fEE0/MZZdd1uCY5513Xv72t7/lhhtuyPXXX5+nnnoqhx12WNasWfNPf/YAQEFKAAAbaOrUqaUkpd/85jelt99+u7RixYrSrFmzSttuu22pXbt2perq6lKpVCqdcMIJpSSlH/3oR/X2nzFjRilJ6Y477qg3/uijj5aSlCZPnlwqlUqlv/zlL6VmzZqVvvjFL77vek444YRSt27d6l5ffvnlpSSlV199db37PPPMM6UkpalTp9aNDRw4sLTddtuVVqxYUTe2evXq0u67717q0qVLae3atfXO/6tf/Wq9Y1522WWlJKWlS5eu93N/85vflJKUzj333Pc9p3fdf//9pSSlyy67rN74zJkzS0lK119/fd1Yt27dSltuuWXp73//e93Y4sWLS0lKnTt3Lq1cubJu/O677y4lKd177711Y+/+fV111VX1Puviiy8uJSnNmzdvnWtcs2ZN6e233y5Nnz691KxZs9LLL79c997+++9fSlL65S9/2WC//fffv7T//vvXvT7jjDNKW2211fv+PI4++uhSeXl5qaqqqt748OHDS61bt677O3/44YdLSUqf+cxn6s27/fbbS0lKCxYseN/PAQCK40opAKDRBg4cmBYtWqRdu3YZMWJEOnXqlPvuuy8dO3asN+/II4+s93rWrFnZaqutcthhh2X16tV1W58+fdKpU6f86le/SpLMmTMna9asyemnn96ode2zzz5JkqOOOiq33377Bn0j4MqVK/Pb3/42n//859O2bdu68WbNmmX06NH5+9//3uC2r3//93+v93rPPfdMkve9iquxHnrooSTv3Fr3j77whS+kTZs2+eUvf1lvvE+fPtlhhx3qXvfq1SvJO7fK/eOVWe+Or2ut730e2LHHHpskefjhh+vGFi1alH//939P+/bt06xZs7Ro0SLHH3981qxZkz//+c/19t96663zqU996p+e67777ptXX301xxxzTO65554sW7aswZyHHnooBx10ULp27Vpv/MQTT8zrr7+eBQsW1Bsv4u8IAPhgRCkAoNGmT5+eRx99NIsWLcrzzz+fxx9/PEOGDKk3p3Xr1qmoqKg39sILL+TVV19Ny5Yt06JFi3pbdXV1XYx49/lSXbp0adS69ttvv9x9991ZvXp1jj/++HTp0iW77757ZsyYsd59XnnllZRKpXTu3LnBe9tvv32SZPny5fXG27dvX+91eXl5kjS4Je4f7bjjjkmSZ555ZoPOZfny5WnevHmDbwgsKytLp06dGqxpm222qfe6ZcuW7zv+5ptv1htv3rx5g/Pq1KlT3VqSpKqqKkOHDs1zzz2Xq666KnPnzs2jjz6aa665JknD81/Xz3RdRo8enR/96Ef529/+liOPPDLbbbddBgwYkDlz5tTNWb58+Yf+dwQAFKt5Uy8AAPjo6dWrV71v31uXdT3I+90Hg99///3r3Kddu3ZJ/v9nU/39739vcGXMPzNy5MiMHDkyq1atym9+85tMmjQpxx57bLp3755BgwY1mL/11ltniy22yNKlSxu89+7Dyzt06NCoNaxL586ds8cee+TBBx/coG+ha9++fVavXp2XXnqpXpgqlUqprq6uuypsU1m9enWWL19eL+ZUV1fXrSVJ7r777qxcuTJ33nlnunXrVjdv8eLF6zzmuv4bWJ+TTjopJ510UlauXJlHHnkk48ePz4gRI/LnP/853bp1S/v27T/0vyMAoFiulAIACjNixIgsX748a9asSf/+/Rtsu+22W5Jk2LBhadasWaZMmbLRn1VeXp79998/l156aZJ3bjtblzZt2mTAgAG58847611Fs3bt2tx8883p0qVLPvGJT2z0Ov7RBRdckFdeeSVf//rXUyqVGrz/2muv5cEHH0ySum8rvPnmm+vNueOOO7Jy5cq69zelW265pd7rW2+9NUnqvinv3cj07lVHyTuR7Ic//OEmW0ObNm0yfPjwjBs3Lm+99Vb+8Ic/JHnn5/HQQw81+JbD6dOnp3Xr1nXfAAkAfHS4UgoAKMzRRx+dW265JZ/5zGdy5plnZt99902LFi3y97//PQ8//HBGjhyZz33uc+nevXvOO++8fOtb38obb7yRY445JpWVlfnjH/+YZcuWZcKECes8/oUXXpi///3vOeigg9KlS5e8+uqrueqqq9KiRYvsv//+613XpEmTcvDBB+fAAw/MN7/5zbRs2TKTJ0/O//7v/2bGjBmNuuLn/XzhC1/IBRdckG9961v505/+lJNPPjk777xzXn/99fz2t7+t+7a7YcOG5eCDD84hhxySc845J7W1tRkyZEgef/zxjB8/Pn379s3o0aM3yZre1bJly1xxxRV57bXXss8++2T+/Pn59re/neHDh+ff/u3fkiQHH3xwWrZsmWOOOSb/9V//lTfffDNTpkzJK6+88oE++8tf/nK23HLLDBkyJJ07d051dXUmTZqUysrKuivCxo8fn1mzZuXAAw/MhRdemG222Sa33HJLfv7zn+eyyy5LZWXlB/4ZAADFEqUAgMI0a9Ys9957b6666qrcdNNNmTRpUpo3b54uXbpk//33zx577FE3d+LEidl1113zgx/8IF/84hfTvHnz7Lrrrvn617++3uMPGDAgjz32WM4555y89NJL2WqrrdK/f/889NBD+eQnP7ne/fbff/889NBDGT9+fE488cSsXbs2e+21V+69996MGDFik/4MJk6cmE9/+tP5wQ9+kHHjxmXZsmXZcsst88lPfjJjxozJqaeemuSdq5LuvvvuXHTRRZk6dWouvvjidOjQIaNHj853vvOdelcrbQotWrTIrFmz8vWvfz3f/va3s+WWW+bLX/5yvvvd79bN6dmzZ+64446cf/75OeKII9K+ffsce+yxGTNmTIYPH77Rnz106NBMmzYtt99+e1555ZV06NAh//Zv/5bp06fX3bq42267Zf78+TnvvPNy+umn54033kivXr0yderUBg+DBwA+GspK67p2HACAfxknnnhifvrTn+a1115r6qUAAP9CPFMKAAAAgMKJUgAAAAAUzu17AAAAABTOlVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAAChc86ZewKaydu3aPP/882nXrl3KysqaejkAAAAA/5JKpVJWrFiR7bffPltssf7roT42Uer5559P165dm3oZAAAAACR59tln06VLl/W+/7GJUu3atUvyzglXVFQ08WoAAAAA/jXV1tama9euda1mfT42UerdW/YqKipEKQAAAIAm9s8er+RB5wAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwjXfmJ0mT56c7373u1m6dGk++clP5sorr8zQoUPXOXfevHk555xz8qc//Smvv/56unXrllNPPTVnn312vXl33HFHLrjggjz99NPZeeedc/HFF+dzn/vcxiwPAIDNWM2ECU29BADYLFWOH9/USyhUo6+UmjlzZs4666yMGzcuixYtytChQzN8+PBUVVWtc36bNm1yxhln5JFHHskTTzyR888/P+eff36uv/76ujkLFizIqFGjMnr06CxZsiSjR4/OUUcdld/+9rcbf2YAAAAAbLbKSqVSqTE7DBgwIHvvvXemTJlSN9arV68cfvjhmTRp0gYd44gjjkibNm1y0003JUlGjRqV2tra3HfffXVzDj300Gy99daZMWPGBh2ztrY2lZWVqampSUVFRSPOCACAIrlSCgDW7eNypdSGNppGXSn11ltvZeHChRk2bFi98WHDhmX+/PkbdIxFixZl/vz52X///evGFixY0OCYhxxyyPsec9WqVamtra23AQAAAPDR0KgotWzZsqxZsyYdO3asN96xY8dUV1e/775dunRJeXl5+vfvn9NPPz2nnHJK3XvV1dWNPuakSZNSWVlZt3Xt2rUxpwIAAABAE9qob98rKyur97pUKjUYe6+5c+fmsccey7XXXpsrr7yywW15jT3m2LFjU1NTU7c9++yzjTwLAAAAAJpKo759r0OHDmnWrFmDK5hefPHFBlc6vVePHj2SJHvssUdeeOGFXHTRRTnmmGOSJJ06dWr0McvLy1NeXt6Y5QMAAACwmWjUlVItW7ZMv379MmfOnHrjc+bMyeDBgzf4OKVSKatWrap7PWjQoAbHfPDBBxt1TAAAAAA+Ohp1pVSSjBkzJqNHj07//v0zaNCgXH/99amqqsppp52W5J3b6p577rlMnz49SXLNNddkxx13TM+ePZMk8+bNy+WXX56vfe1rdcc888wzs99+++XSSy/NyJEjc8899+QXv/hF5s2btynOEQAAAIDNTKOj1KhRo7J8+fJMnDgxS5cuze67757Zs2enW7duSZKlS5emqqqqbv7atWszduzYPPPMM2nevHl23nnnXHLJJTn11FPr5gwePDi33XZbzj///FxwwQXZeeedM3PmzAwYMGATnCIAAAAAm5uyUqlUaupFbAq1tbWprKxMTU1NKioqmno5AACsR82ECU29BADYLFWOH9/US9gkNrTRbNS37wEAAADAB9Ho2/coxiWLljX1EgBgs3Nu3w5NvQQAADYRV0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAAAAhROlAAAAACicKAUAAABA4UQpAAAAAAonSgEAAABQOFEKAAAAgMKJUgAAAAAUTpQCAAAAoHCiFAAAAACFE6UAAAAAKJwoBQAAAEDhRCkAAAAACidKAQAAAFA4UQoAAACAwolSAAAAABROlAIAAACgcKIUAAAAAIUTpQAAAAAonCgFAAAAQOFEKQAAAAAKJ0oBAAAAUDhRCgAAAIDCiVIAAAAAFE6UAgAAAKBwohQAAMD/x969h3ld1/n/fwwgM54YFRBQETAVMUwQPICLq1uiKK2mJmViJqZuJ5HcVaINoQw1dbFNPOSBSCUsTc3FA5V5WNxVCazrG7l2sCGdCfEwo26iwOf3h5fzaxogBofXoHu7Xdf7uvi85/V+f55vvK6a68778/4AUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxW1UlJo1a1YGDBiQmpqaDBs2LA8//PA6195+++05/PDD07Nnz3Tr1i0jRozIfffd12LN7NmzU1VV1Wp7/fXXN2Y8AAAAADZzbY5S8+bNy8SJEzNlypQsXrw4o0aNypgxY1JXV7fW9Q899FAOP/zwzJ8/P4sWLcphhx2WD3/4w1m8eHGLdd26dUt9fX2LraamZuOuCgAAAIDNWpe2HnD55ZdnwoQJOf3005MkM2fOzH333ZerrroqM2bMaLV+5syZLV5//etfz5133pkf/ehHGTp0aPP+qqqq9O7du63jAAAAAPAu1KY7pd54440sWrQoo0ePbrF/9OjRWbhw4QadY82aNXnllVeyww47tNj/6quvpl+/ftlll10yduzYVndS/bWVK1emqampxQYAAADAu0ObotSKFSuyevXq9OrVq8X+Xr16paGhYYPOcdlll+W1117LiSee2Lxvr732yuzZs3PXXXdl7ty5qampycEHH5ynn356neeZMWNGamtrm7e+ffu25VIAAAAA6EAb9aDzqqqqFq8rlUqrfWszd+7cXHDBBZk3b1523HHH5v0HHXRQTj755Oy7774ZNWpUbr311uy5557593//93Wea/LkyWlsbGzeli1btjGXAgAAAEAHaNMzpXr06JHOnTu3uitq+fLlre6e+mvz5s3LhAkT8v3vfz8f+tCH1ru2U6dO2X///dd7p1R1dXWqq6s3fHgAAAAANhttulOqa9euGTZsWBYsWNBi/4IFCzJy5Mh1Hjd37tyceuqpueWWW3L00Uf/zfepVCpZsmRJ+vTp05bxAAAAAHiXaPO3702aNCnjx4/P8OHDM2LEiFx77bWpq6vLWWedleStj9U9++yzmTNnTpK3gtQpp5ySK664IgcddFDzXVZbbrllamtrkyTTpk3LQQcdlD322CNNTU355je/mSVLluTKK69sr+sEAAAAYDPS5ig1bty4vPDCC5k+fXrq6+szePDgzJ8/P/369UuS1NfXp66urnn9Nddck1WrVuWzn/1sPvvZzzbv/+QnP5nZs2cnSV5++eWcccYZaWhoSG1tbYYOHZqHHnooBxxwwDu8PAAAAAA2R1WVSqXS0UO0h6amptTW1qaxsTHdunXr6HHesYsWr+joEQBgs3P+0B4dPQLtoHHatI4eAQA2S7VTp3b0CO1iQxvNRn37HgAAAAC8E6IUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHEbFaVmzZqVAQMGpKamJsOGDcvDDz+8zrW33357Dj/88PTs2TPdunXLiBEjct9997Vad9ttt2XvvfdOdXV19t577/zwhz/cmNEAAAAAeBdoc5SaN29eJk6cmClTpmTx4sUZNWpUxowZk7q6urWuf+ihh3L44Ydn/vz5WbRoUQ477LB8+MMfzuLFi5vXPProoxk3blzGjx+fJ598MuPHj8+JJ56Y//7v/974KwMAAABgs1VVqVQqbTngwAMPzH777Zerrrqqed+gQYNy7LHHZsaMGRt0jve///0ZN25cvvKVryRJxo0bl6amptxzzz3Na4488shsv/32mTt37gads6mpKbW1tWlsbEy3bt3acEWbp4sWr+joEQBgs3P+0B4dPQLtoHHatI4eAQA2S7VTp3b0CO1iQxtNm+6UeuONN7Jo0aKMHj26xf7Ro0dn4cKFG3SONWvW5JVXXskOO+zQvO/RRx9tdc4jjjhig88JAAAAwLtLl7YsXrFiRVavXp1evXq12N+rV680NDRs0Dkuu+yyvPbaaznxxBOb9zU0NLT5nCtXrszKlSubXzc1NW3Q+wMAAADQ8TbqQedVVVUtXlcqlVb71mbu3Lm54IILMm/evOy4447v6JwzZsxIbW1t89a3b982XAEAAAAAHalNUapHjx7p3LlzqzuYli9f3upOp782b968TJgwIbfeems+9KEPtfhZ796923zOyZMnp7GxsXlbtmxZWy4FAAAAgA7UpijVtWvXDBs2LAsWLGixf8GCBRk5cuQ6j5s7d25OPfXU3HLLLTn66KNb/XzEiBGtznn//fev95zV1dXp1q1biw0AAACAd4c2PVMqSSZNmpTx48dn+PDhGTFiRK699trU1dXlrLPOSvLWHUzPPvts5syZk+StIHXKKafkiiuuyEEHHdR8R9SWW26Z2traJMnZZ5+dQw45JBdffHGOOeaY3Hnnnfnxj3+cRx55pL2uEwAAAIDNSJufKTVu3LjMnDkz06dPz5AhQ/LQQw9l/vz56devX5Kkvr4+dXV1zeuvueaarFq1Kp/97GfTp0+f5u3ss89uXjNy5Mh873vfy4033pgPfOADmT17dubNm5cDDzywHS4RAAAAgM1NVaVSqXT0EO2hqakptbW1aWxsfE98lO+ixSs6egQA2OycP7RHR49AO2icNq2jRwCAzVLt1KkdPUK72NBGs1HfvgcAAAAA74QoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFDcRkWpWbNmZcCAAampqcmwYcPy8MMPr3NtfX19TjrppAwcODCdOnXKxIkTW62ZPXt2qqqqWm2vv/76xowHAAAAwGauzVFq3rx5mThxYqZMmZLFixdn1KhRGTNmTOrq6ta6fuXKlenZs2emTJmSfffdd53n7datW+rr61tsNTU1bR0PAAAAgHeBNkepyy+/PBMmTMjpp5+eQYMGZebMmenbt2+uuuqqta7v379/rrjiipxyyimpra1d53mrqqrSu3fvFhsAAAAA701tilJvvPFGFi1alNGjR7fYP3r06CxcuPAdDfLqq6+mX79+2WWXXTJ27NgsXrz4HZ0PAAAAgM1Xm6LUihUrsnr16vTq1avF/l69eqWhoWGjh9hrr70ye/bs3HXXXZk7d25qampy8MEH5+mnn17nMStXrkxTU1OLDQAAAIB3h4160HlVVVWL15VKpdW+tjjooINy8sknZ999982oUaNy6623Zs8998y///u/r/OYGTNmpLa2tnnr27fvRr8/AAAAAGW1KUr16NEjnTt3bnVX1PLly1vdPfWOhurUKfvvv/9675SaPHlyGhsbm7dly5a12/sDAAAAsGm1KUp17do1w4YNy4IFC1rsX7BgQUaOHNluQ1UqlSxZsiR9+vRZ55rq6up069atxQYAAADAu0OXth4wadKkjB8/PsOHD8+IESNy7bXXpq6uLmeddVaSt+5gevbZZzNnzpzmY5YsWZLkrYeZP//881myZEm6du2avffeO0kybdq0HHTQQdljjz3S1NSUb37zm1myZEmuvPLKdrhEAAAAADY3bY5S48aNywsvvJDp06envr4+gwcPzvz589OvX78kSX19ferq6locM3To0OY/L1q0KLfcckv69euXZ555Jkny8ssv54wzzkhDQ0Nqa2szdOjQPPTQQznggAPewaUBAAAAsLmqqlQqlY4eoj00NTWltrY2jY2N74mP8l20eEVHjwAAm53zh/bo6BFoB43TpnX0CACwWaqdOrWjR2gXG9poNurb9wAAAADgnRClAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAobqOi1KxZszJgwIDU1NRk2LBhefjhh9e5tr6+PieddFIGDhyYTp06ZeLEiWtdd9ttt2XvvfdOdXV19t577/zwhz/cmNEAAAAAeBdoc5SaN29eJk6cmClTpmTx4sUZNWpUxowZk7q6urWuX7lyZXr27JkpU6Zk3333XeuaRx99NOPGjcv48ePz5JNPZvz48TnxxBPz3//9320dDwAAAIB3gapKpVJpywEHHnhg9ttvv1x11VXN+wYNGpRjjz02M2bMWO+xhx56aIYMGZKZM2e22D9u3Lg0NTXlnnvuad535JFHZvvtt8/cuXM3aK6mpqbU1tamsbEx3bp12/AL2kxdtHhFR48AAJud84f26OgRaAeN06Z19AgAsFmqnTq1o0doFxvaaNp0p9Qbb7yRRYsWZfTo0S32jx49OgsXLty4SfPWnVJ/fc4jjjhivedcuXJlmpqaWmwAAAAAvDu0KUqtWLEiq1evTq9evVrs79WrVxoaGjZ6iIaGhjafc8aMGamtrW3e+vbtu9HvDwAAAEBZG/Wg86qqqhavK5VKq32b+pyTJ09OY2Nj87Zs2bJ39P4AAAAAlNOlLYt79OiRzp07t7qDafny5a3udGqL3r17t/mc1dXVqa6u3uj3BAAAAKDjtOlOqa5du2bYsGFZsGBBi/0LFizIyJEjN3qIESNGtDrn/fff/47OCQAAAMDmq013SiXJpEmTMn78+AwfPjwjRozItddem7q6upx11llJ3vpY3bPPPps5c+Y0H7NkyZIkyauvvprnn38+S5YsSdeuXbP33nsnSc4+++wccsghufjii3PMMcfkzjvvzI9//OM88sgj7XCJAAAAAGxu2hylxo0blxdeeCHTp09PfX19Bg8enPnz56dfv35Jkvr6+tTV1bU4ZujQoc1/XrRoUW655Zb069cvzzzzTJJk5MiR+d73vpcvf/nL+dd//de8733vy7x583LggQe+g0sDAAAAYHNVValUKh09RHtoampKbW1tGhsb061bt44e5x27aPGKjh4BADY75w/t0dEj0A4ap03r6BEAYLNUO3VqR4/QLja00WzUt+8BAAAAwDshSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUt1FRatasWRkwYEBqamoybNiwPPzww+td/+CDD2bYsGGpqanJbrvtlquvvrrFz2fPnp2qqqpW2+uvv74x4wEAAACwmWtzlJo3b14mTpyYKVOmZPHixRk1alTGjBmTurq6ta7//e9/n6OOOiqjRo3K4sWL86UvfSlf+MIXctttt7VY161bt9TX17fYampqNu6qAAAAANisdWnrAZdffnkmTJiQ008/PUkyc+bM3HfffbnqqqsyY8aMVuuvvvrq7Lrrrpk5c2aSZNCgQXniiSdy6aWX5vjjj29eV1VVld69e2/kZQAAAADwbtKmO6XeeOONLFq0KKNHj26xf/To0Vm4cOFaj3n00UdbrT/iiCPyxBNP5M0332ze9+qrr6Zfv37ZZZddMnbs2CxevHi9s6xcuTJNTU0tNgAAAADeHdoUpVasWJHVq1enV69eLfb36tUrDQ0Naz2moaFhretXrVqVFStWJEn22muvzJ49O3fddVfmzp2bmpqaHHzwwXn66afXOcuMGTNSW1vbvPXt27ctlwIAAABAB9qoB51XVVW1eF2pVFrt+1vr/3L/QQcdlJNPPjn77rtvRo0alVtvvTV77rln/v3f/32d55w8eXIaGxubt2XLlm3MpQAAAADQAdr0TKkePXqkc+fOre6KWr58eau7od7Wu3fvta7v0qVLunfvvtZjOnXqlP3333+9d0pVV1enurq6LeMDAAAAsJlo051SXbt2zbBhw7JgwYIW+xcsWJCRI0eu9ZgRI0a0Wn///fdn+PDh2WKLLdZ6TKVSyZIlS9KnT5+2jAcAAADAu0SbP743adKkXHfddbnhhhuydOnSnHPOOamrq8tZZ52V5K2P1Z1yyinN688666z84Q9/yKRJk7J06dLccMMNuf7663Puuec2r5k2bVruu+++/O53v8uSJUsyYcKELFmypPmcAAAAALy3tOnje0kybty4vPDCC5k+fXrq6+szePDgzJ8/P/369UuS1NfXp66urnn9gAEDMn/+/Jxzzjm58sors9NOO+Wb3/xmjj/++OY1L7/8cs4444w0NDSktrY2Q4cOzUMPPZQDDjigHS4RAAAAgM1NVeXtp46/yzU1NaW2tjaNjY3p1q1bR4/zjl20eEVHjwAAm53zh/bo6BFoB43TpnX0CACwWaqdOrWjR2gXG9poNurb9wAAAADgnRClAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIrbqCg1a9asDBgwIDU1NRk2bFgefvjh9a5/8MEHM2zYsNTU1GS33XbL1Vdf3WrNbbfdlr333jvV1dXZe++988Mf/nBjRgMAAADgXaDNUWrevHmZOHFipkyZksWLF2fUqFEZM2ZM6urq1rr+97//fY466qiMGjUqixcvzpe+9KV84QtfyG233da85tFHH824ceMyfvz4PPnkkxk/fnxOPPHE/Pd///fGXxkAAAAAm62qSqVSacsBBx54YPbbb79cddVVzfsGDRqUY489NjNmzGi1/rzzzstdd92VpUuXNu8766yz8uSTT+bRRx9NkowbNy5NTU255557mtcceeSR2X777TN37twNmqupqSm1tbVpbGxMt27d2nJJm6WLFq/o6BEAYLNz/tAeHT0C7aBx2rSOHgEANku1U6d29AjtYkMbTZvulHrjjTeyaNGijB49usX+0aNHZ+HChWs95tFHH221/ogjjsgTTzyRN998c71r1nVOAAAAAN7durRl8YoVK7J69er06tWrxf5evXqloaFhrcc0NDSsdf2qVauyYsWK9OnTZ51r1nXOJFm5cmVWrlzZ/LqxsTHJWzXuveD1V1/p6BEAYLPT1NS1o0egHTS9/npHjwAAm6Wq90jTeLvN/K0P57UpSr2tqqqqxetKpdJq399a/9f723rOGTNmZNpabv3u27fvugcHAN7VfOgLAHhPu+iijp6gXb3yyiupra1d58/bFKV69OiRzp07t7qDafny5a3udHpb796917q+S5cu6d69+3rXrOucSTJ58uRMmjSp+fWaNWvy4osvpnv37uuNWQBt0dTUlL59+2bZsmXviefVAQD8Jb/rAJtCpVLJK6+8kp122mm969oUpbp27Zphw4ZlwYIF+chHPtK8f8GCBTnmmGPWesyIESPyox/9qMW++++/P8OHD88WW2zRvGbBggU555xzWqwZOXLkOmeprq5OdXV1i33bbbddWy4HYIN169bNL2oAwHuW33WA9ra+O6Te1uaP702aNCnjx4/P8OHDM2LEiFx77bWpq6vLWWedleStO5ieffbZzJkzJ8lb37T3rW99K5MmTcqnP/3pPProo7n++utbfKve2WefnUMOOSQXX3xxjjnmmNx555358Y9/nEceeaSt4wEAAADwLtDmKDVu3Li88MILmT59eurr6zN48ODMnz8//fr1S5LU19enrq6uef2AAQMyf/78nHPOObnyyiuz00475Zvf/GaOP/745jUjR47M9773vXz5y1/Ov/7rv+Z973tf5s2blwMPPLAdLhEAAACAzU1V5W89Ch3g/7CVK1dmxowZmTx5cquPDAMAvNv5XQfoSKIUAAAAAMV16ugBAAAAAPi/R5QCAAAAoDhRCgAAAIDiRCngXeeCCy5Ir169UlVVlTvuuKOjx2lXhx56aCZOnNjRYwAABVUqlZxxxhnZYYcdUlVVlSVLlnT0SO9Y//79M3PmzI4eA9jMiVJAEaeeemqqqqqat+7du+fII4/ML37xizadZ+nSpZk2bVquueaa1NfXZ8yYMZto4tb+cv61baeeeuo7fo/bb789X/3qV9/5sADAZmXhwoXp3LlzjjzyyFY/u/feezN79uzcfffdqa+vz+DBgzfZP74988wzf/N3mgsuuOAdv8/jjz+eM844450PDLyndenoAYD/O4488sjceOONSZKGhoZ8+ctfztixY1NXV7fB5/jtb3+bJDnmmGNSVVW10bO8+eab2WKLLdp0TH19ffOf582bl6985St56qmnmvdtueWWGz3P23bYYYd3fA4AYPNzww035POf/3yuu+661NXVZdddd23+2W9/+9v06dMnI0eObPf3/evfefr27dvid5pLL7009957b3784x8379tmm23e8fv27NnzHZ8DeO9zpxRQTHV1dXr37p3evXtnyJAhOe+887Js2bI8//zzzWueffbZjBs3Lttvv326d++eY445Js8880yStz629+EPfzhJ0qlTp+YotWbNmkyfPj277LJLqqurM2TIkNx7773N53z7XwRvvfXWHHrooampqclNN92UJLnxxhszaNCg1NTUZK+99sqsWbPWOf/bs/fu3Tu1tbWpqqpqfn3vvfemX79+LdbfcccdLcLZBRdckCFDhuS73/1u+vfvn9ra2nzsYx/LK6+80rzmrz++179//3z961/Paaedlm233Ta77rprrr322hbvs3DhwgwZMiQ1NTUZPnx48/u+F279B4D3gtdeey233npr/umf/iljx47N7Nmzm3926qmn5vOf/3zq6upSVVWV/v37p3///kmSj3zkI8373vajH/0ow4YNS01NTXbbbbdMmzYtq1atav55VVVVrr766hxzzDHZeuut87Wvfa3FLJ07d27xO80222yTLl26NL+++uqr83d/93ctjpk5c2aLGU499dQce+yxufTSS9OnT5907949n/3sZ/Pmm282r/nrj+9VVVXluuuuy0c+8pFstdVW2WOPPXLXXXe1eJ+77rore+yxR7bccsscdthh+c53vpOqqqq8/PLLbfsLB941RCmgQ7z66qu5+eabs/vuu6d79+5Jkv/93//NYYcdlm222SYPPfRQHnnkkWyzzTY58sgj88Ybb+Tcc89tvtOqvr6++V/5rrjiilx22WW59NJL84tf/CJHHHFE/vEf/zFPP/10i/c877zz8oUvfCFLly7NEUcckW9/+9uZMmVKLrzwwixdujRf//rX86//+q/5zne+s8mu+7e//W3uuOOO3H333bn77rvz4IMP5qKLLlrvMZdddlmGDx+exYsX5zOf+Uz+6Z/+Kb/+9a+TJK+88ko+/OEPZ5999snPf/7zfPWrX8155523yeYHANpu3rx5GThwYAYOHJiTTz45N954YyqVSpK3fo95+x/X6uvr8/jjj+fxxx9P8tY/nr29L0nuu+++nHzyyfnCF76QX/3qV7nmmmsye/bsXHjhhS3eb+rUqTnmmGPyy1/+MqeddtomuaYHHnggv/3tb/PAAw/kO9/5TmbPnt0itq3NtGnTcuKJJ+YXv/hFjjrqqHziE5/Iiy++mOStf0Q84YQTcuyxx2bJkiU588wzM2XKlE0yO7D5EKWAYu6+++5ss8022WabbbLtttvmrrvuyrx589Kp01v/U/S9730vnTp1ynXXXZd99tkngwYNyo033pi6urr87Gc/yzbbbJPtttsuyf9/11Ly1m3n5513Xj72sY9l4MCBufjiizNkyJBWD9ecOHFijjvuuAwYMCA77bRTvvrVr+ayyy5r3nfcccflnHPOyTXXXLPJ/g7WrFmT2bNnZ/DgwRk1alTGjx+fn/zkJ+s95qijjspnPvOZ7L777jnvvPPSo0eP/OxnP0uS3Hzzzamqqsq3v/3t7L333hkzZkz++Z//eZPNDwC03fXXX5+TTz45yVuPM3j11Veb//+/trY22267bfMdTD179mz+6Nt2223XvC9JLrzwwpx//vn55Cc/md122y2HH354vvrVr7b63eWkk07Kaaedlt12263VndztZfvtt8+3vvWt7LXXXhk7dmyOPvrov/k7zamnnpqPf/zj2X333fP1r389r732Wh577LEkydVXX52BAwfmG9/4RgYOHJiPfexj7fK8TmDz5plSQDGHHXZYrrrqqiTJiy++mFmzZmXMmDF57LHH0q9fvyxatCi/+c1vsu2227Y47vXXX29+ltRfa2pqynPPPZeDDz64xf6DDz44Tz75ZIt9w4cPb/7z888/n2XLlmXChAn59Kc/3bx/1apVqa2tfUfXuT79+/dvcX19+vTJ8uXL13vMBz7wgeY/v/2RwbePeeqpp/KBD3wgNTU1zWsOOOCAdp4aANhYTz31VB577LHcfvvtSZIuXbpk3LhxueGGG/KhD32oTedatGhRHn/88RZ3Rq1evTqvv/56/vd//zdbbbVVkpa/82wq73//+9O5c+fm13369Mkvf/nL9R7zl7/TbL311tl2221b/E6z//77t1jvdxp47xOlgGK23nrr7L777s2vhw0bltra2nz729/O1772taxZsybDhg3LzTff3OrYv/WwzL9+6HmlUmm1b+utt27+85o1a5Ik3/72t3PggQe2WPeXv2BtqE6dOjXfhv+2v3yuwtv++uHqVVVVzbOsy/qOWdt1/vUcAEDHuf7667Nq1arsvPPOzfsqlUq22GKLvPTSS9l+++03+Fxr1qzJtGnTctxxx7X62V/+A9Vf/s7TVn6nAUoSpYAOU1VVlU6dOuXPf/5zkmS//fbLvHnzsuOOO6Zbt24bdI5u3bplp512yiOPPJJDDjmkef/ChQvX+69rvXr1ys4775zf/e53+cQnPvHOLiRvRbNXXnklr732WvMvgiUeNL7XXnvl5ptvzsqVK1NdXZ0keeKJJzb5+wIAf9uqVasyZ86cXHbZZRk9enSLnx1//PG5+eab87nPfW6tx26xxRZZvXp1i3377bdfnnrqqRb/yNfeevbsmYaGhhaRqNTvNPPnz2+xz+808N7nmVJAMStXrkxDQ0MaGhqydOnSfP7zn8+rr77a/I16n/jEJ9KjR48cc8wxefjhh/P73/8+Dz74YM4+++z88Y9/XOd5//mf/zkXX3xx5s2bl6eeeirnn39+lixZkrPPPnu981xwwQWZMWNGrrjiivzP//xPfvnLX+bGG2/M5Zdf3uZrO/DAA7PVVlvlS1/6Un7zm9/klltu+ZsP+2wPJ510UtasWZMzzjgjS5cuzX333ZdLL700Seu7xwCAsu6+++689NJLmTBhQgYPHtxiO+GEE3L99dev89j+/fvnJz/5SRoaGvLSSy8lSb7yla9kzpw5ueCCC/L//t//y9KlSzNv3rx8+ctfbreZDz300Dz//PO55JJL8tvf/jZXXnll7rnnnnY7/7qceeaZ+fWvf53zzjsv//M//5Nbb721+Xcpv9PAe5coBRRz7733pk+fPunTp08OPPDAPP744/n+97+fQw89NEmy1VZb5aGHHsquu+6a4447LoMGDcppp52WP//5z+u9c+oLX/hCvvjFL+aLX/xi9tlnn9x7773NXym8Pqeffnquu+66zJ49O/vss0/+/u//PrNnz86AAQPafG077LBDbrrppsyfPz/77LNP5s6dmwsuuKDN52mrbt265Uc/+lGWLFmSIUOGZMqUKfnKV76SpOVt/ABAeddff30+9KEPrfV5lccff3yWLFmSn//852s99rLLLsuCBQvSt2/fDB06NElyxBFH5O67786CBQuy//7756CDDsrll1/erg8zHzRoUGbNmpUrr7wy++67bx577LGce+657Xb+dRkwYEB+8IMf5Pbbb88HPvCBXHXVVc3fvvf23eDAe09VxQd1Ad5Tbr755nzqU59KY2Njttxyy44eBwBgo1x44YW5+uqrs2zZso4eBdhEPFMK4F1uzpw52W233bLzzjvnySefzHnnnZcTTzxRkAIA3lVmzZqV/fffP927d89//ud/5hvf+MY6n7kFvDeIUgDvcg0NDfnKV76ShoaG9OnTJx/96EdbfFU0AMC7wdNPP52vfe1refHFF7Prrrvmi1/8YiZPntzRYwGbkI/vAQAAAFCcB50DAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAsEnNnj07VVVVzVuXLl3Sp0+ffOxjH8vTTz/d0eMlSfr3759TTz21+fUzzzyTqqqqzJ49e4OO/9Of/pTzzz8/++yzT7bZZpvU1NRkjz32yNlnn73ZXOOm9PZ/42eeeaajRwEA3kW6dPQAAMD/DTfeeGP22muvvP766/nP//zPXHjhhXnggQfy61//Ottvv31Hj7fRHnvssYwdOzaVSiWf+9znMmLEiHTt2jVPPfVUbrrpphxwwAF56aWXOnrMTeroo4/Oo48+mj59+nT0KADAu4goBQAUMXjw4AwfPjxJcuihh2b16tWZOnVq7rjjjnzqU5/q4Ok2TlNTU4455pjU1NRk4cKF2WWXXZp/duihh+bMM8/MD37wgw6ccNP685//nJqamvTs2TM9e/bs6HEAgHcZH98DADrE24HqT3/6U4v9TzzxRP7xH/8xO+ywQ2pqajJ06NDceuutrY5/9tlnc8YZZ6Rv377p2rVrdtppp5xwwgnN53v99dfzxS9+MUOGDEltbW122GGHjBgxInfeeWe7XcO3v/3tNDQ05JJLLmkRpP7SCSec0OL1XXfdlREjRmSrrbbKtttum8MPPzyPPvpoizUXXHBBqqqq8otf/CIf/ehHm+efNGlSVq1alaeeeipHHnlktt122/Tv3z+XXHJJi+N/9rOfpaqqKjfddFMmTZqU3r17Z8stt8zf//3fZ/HixS3WPvHEE/nYxz6W/v37Z8stt0z//v3z8Y9/PH/4wx9arHv7I3r3339/TjvttPTs2TNbbbVVVq5cudaP7y1evDhjx47NjjvumOrq6uy00045+uij88c//rF5zeuvv57JkydnwIAB6dq1a3beeed89rOfzcsvv9zivfv375+xY8fm3nvvzX777Zctt9wye+21V2644Yb1/vcBADZvohQA0CF+//vfJ0n23HPP5n0PPPBADj744Lz88su5+uqrc+edd2bIkCEZN25ci+c7Pfvss9l///3zwx/+MJMmTco999yTmTNnpra2tvmjcitXrsyLL76Yc889N3fccUfmzp2bv/u7v8txxx2XOXPmtMs13H///encuXM+/OEPb9D6W265Jcccc0y6deuWuXPn5vrrr89LL72UQw89NI888kir9SeeeGL23Xff3Hbbbfn0pz+df/u3f8s555yTY489NkcffXR++MMf5h/+4R9y3nnn5fbbb291/Je+9KX87ne/y3XXXZfrrrsuzz33XA499ND87ne/a17zzDPPZODAgZk5c2buu+++XHzxxamvr8/++++fFStWtDrnaaedli222CLf/e5384Mf/CBbbLFFqzWvvfZaDj/88PzpT3/KlVdemQULFmTmzJnZdddd88orryRJKpVKjj322Fx66aUZP358/uM//iOTJk3Kd77znfzDP/xDVq5c2eKcTz75ZL74xS/mnHPOyZ133pkPfOADmTBhQh566KEN+rsHADZDFQCATejGG2+sJKn813/9V+XNN9+svPLKK5V777230rt378ohhxxSefPNN5vX7rXXXpWhQ4e22FepVCpjx46t9OnTp7J69epKpVKpnHbaaZUtttii8qtf/WqD51i1alXlzTffrEyYMKEydOjQFj/r169f5ZOf/GTz69///veVJJUbb7xxvefca6+9Kr17996g91+9enVlp512quyzzz7N11GpVCqvvPJKZccdd6yMHDmyed/UqVMrSSqXXXZZi3MMGTKkkqRy++23N+978803Kz179qwcd9xxzfseeOCBSpLKfvvtV1mzZk3z/meeeaayxRZbVE4//fR1zrlq1arKq6++Wtl6660rV1xxRfP+t/87nnLKKa2Oeftnv//97yuVSqXyxBNPVJJU7rjjjnW+z7333ltJUrnkkkta7J83b14lSeXaa69t3tevX79KTU1N5Q9/+EPzvj//+c+VHXbYoXLmmWeu8z0AgM2bO6UAgCIOOuigbLHFFtl2221z5JFHZvvtt8+dd96ZLl3eesTlb37zm/z617/OJz7xiSTJqlWrmrejjjoq9fX1eeqpp5Ik99xzTw477LAMGjRove/5/e9/PwcffHC22WabdOnSJVtssUWuv/76LF26dNNe7Fo89dRTee655zJ+/Ph06vT//wq2zTbb5Pjjj89//dd/5X//939bHDN27NgWrwcNGpSqqqqMGTOmeV+XLl2y++67t/q4XZKcdNJJqaqqan7dr1+/jBw5Mg888EDzvldffTXnnXdedt9993Tp0iVdunTJNttsk9dee22tf0/HH3/837zW3XffPdtvv33OO++8XH311fnVr37Vas1Pf/rTJGnxrYdJ8tGPfjRbb711fvKTn7TYP2TIkOy6667Nr2tqarLnnnuu9boBgHcHUQoAKGLOnDl5/PHH89Of/jRnnnlmli5dmo9//OPNP3/7WVDnnntutthiixbbZz7zmSRp/jjZ888/v85nOL3t9ttvz4knnpidd945N910Ux599NE8/vjjOe200/L666+3yzXtuuuuef755/Paa6/9zbUvvPBCkqz1G+p22mmnrFmzptW39O2www4tXnft2jVbbbVVampqWu1f2zX17t17rfveniV5K1x961vfyumnn5777rsvjz32WB5//PH07Nkzf/7zn1sdvyHfsFdbW5sHH3wwQ4YMyZe+9KW8//3vz0477ZSpU6fmzTffTPLW30eXLl1aPSC9qqqq1YxJ0r1791bvU11dvdYZAYB3B9++BwAUMWjQoOaHmx922GFZvXp1rrvuuvzgBz/ICSeckB49eiRJJk+enOOOO26t5xg4cGCSpGfPni0emL02N910UwYMGJB58+a1uFvor59V9E4cccQRuf/++/OjH/0oH/vYx9a79u2oUl9f3+pnzz33XDp16pTtt9++3WZLkoaGhrXue3uWxsbG3H333Zk6dWrOP//85jVvP49rbf7y73J99tlnn3zve99LpVLJL37xi8yePTvTp0/PlltumfPPPz/du3fPqlWr8vzzz7cIU5VKJQ0NDdl///3bcqkAwLuQO6UAgA5xySWXZPvtt89XvvKVrFmzJgMHDswee+yRJ598MsOHD1/rtu222yZJxowZkwceeKD543xrU1VVla5du7aIKA0NDe367XsTJkxI79698y//8i959tln17rm7QeQDxw4MDvvvHNuueWWVCqV5p+/9tprue2225q/ka89zZ07t8V7/eEPf8jChQtz6KGHJnnr76hSqaS6urrFcdddd11Wr17dLjNUVVVl3333zb/9279lu+22y89//vMkyQc/+MEkb8XDv3Tbbbfltddea/45APDe5U4pAKBDbL/99pk8eXL+5V/+JbfccktOPvnkXHPNNRkzZkyOOOKInHrqqdl5553z4osvZunSpfn5z3+e73//+0mS6dOn55577skhhxySL33pS9lnn33y8ssv5957782kSZOy1157ZezYsbn99tvzmc98JieccEKWLVuWr371q+nTp0+efvrpdrmG2tra3HnnnRk7dmyGDh2az33ucxkxYkS6du2ap59+OjfddFOefPLJHHfccenUqVMuueSSfOITn8jYsWNz5plnZuXKlfnGN76Rl19+ORdddFG7zPSXli9fno985CP59Kc/ncbGxkydOjU1NTWZPHlykqRbt2455JBD8o1vfCM9evRI//798+CDD+b666/Pdtttt9Hve/fdd2fWrFk59thjs9tuu6VSqeT222/Pyy+/nMMPPzxJcvjhh+eII47Ieeedl6amphx88MH5xS9+kalTp2bo0KEZP358e/wVAACbMVEKAOgwn//85/Otb30r06dPz8c//vEcdthheeyxx3LhhRdm4sSJeemll9K9e/fsvffeOfHEE5uP23nnnfPYY49l6tSpueiii/LCCy+kZ8+e+bu/+7vm5zB96lOfyvLly3P11VfnhhtuyG677Zbzzz8/f/zjHzNt2rR2u4YDDjggv/zlL/Nv//ZvufXWW3PxxRdn9erV6du3bz74wQ/mW9/6VvPak046KVtvvXVmzJiRcePGpXPnzjnooIPywAMPZOTIke0209u+/vWv5/HHH8+nPvWpNDU15YADDsj3vve9vO9972tec8stt+Tss8/Ov/zLv2TVqlU5+OCDs2DBghx99NEb/b577LFHtttuu1xyySV57rnn0rVr1wwcODCzZ8/OJz/5ySRv3UF1xx135IILLsiNN96YCy+8MD169Mj48ePz9a9/vdXdWwDAe09V5S/v6QYA4F3vZz/7WQ477LB8//vfzwknnNDR4wAArJVnSgEAAABQnCgFAAAAQHE+vgcAAABAce6UAgAAAKA4UQoAAACA4kQpAAAAAIrr0tEDtJc1a9bkueeey7bbbpuqqqqOHgcAAADg/6RKpZJXXnklO+20Uzp1Wvf9UO+ZKPXcc8+lb9++HT0GAAAAAEmWLVuWXXbZZZ0/f89EqW233TbJWxfcrVu3Dp4GAAAA4P+mpqam9O3bt7nVrMt7Jkq9/ZG9bt26iVIAAAAAHexvPV7Jg84BAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAiuvS0QMAAPB/S+O0aR09AgBslmqnTu3oEYpypxQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFbbIoNWvWrAwYMCA1NTUZNmxYHn744XWuPfXUU1NVVdVqe//737+pxgMAAACgA22SKDVv3rxMnDgxU6ZMyeLFizNq1KiMGTMmdXV1a11/xRVXpL6+vnlbtmxZdthhh3z0ox/dFOMBAAAA0ME2SZS6/PLLM2HChJx++ukZNGhQZs6cmb59++aqq65a6/ra2tr07t27eXviiSfy0ksv5VOf+tSmGA8AAACADtbuUeqNN97IokWLMnr06Bb7R48enYULF27QOa6//vp86EMfSr9+/da5ZuXKlWlqamqxAQAAAPDu0O5RasWKFVm9enV69erVYn+vXr3S0NDwN4+vr6/PPffck9NPP32962bMmJHa2trmrW/fvu9obgAAAADK2WQPOq+qqmrxulKptNq3NrNnz852222XY489dr3rJk+enMbGxuZt2bJl72RcAAAAAArq0t4n7NGjRzp37tzqrqjly5e3unvqr1Uqldxwww0ZP358unbtut611dXVqa6ufsfzAgAAAFBeu98p1bVr1wwbNiwLFixosX/BggUZOXLkeo998MEH85vf/CYTJkxo77EAAAAA2Iy0+51SSTJp0qSMHz8+w4cPz4gRI3Lttdemrq4uZ511VpK3Pnr37LPPZs6cOS2Ou/7663PggQdm8ODBm2IsAAAAADYTmyRKjRs3Li+88EKmT5+e+vr6DB48OPPnz2/+Nr36+vrU1dW1OKaxsTG33XZbrrjiik0xEgAAAACbkapKpVLp6CHaQ1NTU2pra9PY2Jhu3bp19DgAAKxD47RpHT0CAGyWaqdO7egR2sWGNppN9u17AAAAALAuohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQ3CaLUrNmzcqAAQNSU1OTYcOG5eGHH17v+pUrV2bKlCnp169fqqur8773vS833HDDphoPAAAAgA7UZVOcdN68eZk4cWJmzZqVgw8+ONdcc03GjBmTX/3qV9l1113XesyJJ56YP/3pT7n++uuz++67Z/ny5Vm1atWmGA8AAACADrZJotTll1+eCRMm5PTTT0+SzJw5M/fdd1+uuuqqzJgxo9X6e++9Nw8++GB+97vfZYcddkiS9O/ff1OMBgAAAMBmoN0/vvfGG29k0aJFGT16dIv9o0ePzsKFC9d6zF133ZXhw4fnkksuyc4775w999wz5557bv785z+v831WrlyZpqamFhsAAAAA7w7tfqfUihUrsnr16vTq1avF/l69eqWhoWGtx/zud7/LI488kpqamvzwhz/MihUr8pnPfCYvvvjiOp8rNWPGjEybNq29xwcAAACggE32oPOqqqoWryuVSqt9b1uzZk2qqqpy880354ADDshRRx2Vyy+/PLNnz17n3VKTJ09OY2Nj87Zs2bJ2vwYAAAAANo12v1OqR48e6dy5c6u7opYvX97q7qm39enTJzvvvHNqa2ub9w0aNCiVSiV//OMfs8cee7Q6prq6OtXV1e07PAAAAABFtPudUl27ds2wYcOyYMGCFvsXLFiQkSNHrvWYgw8+OM8991xeffXV5n3/8z//k06dOmWXXXZp7xEBAAAA6GCb5ON7kyZNynXXXZcbbrghS5cuzTnnnJO6urqcddZZSd766N0pp5zSvP6kk05K9+7d86lPfSq/+tWv8tBDD+Wf//mfc9ppp2XLLbfcFCMCAAAA0IHa/eN7STJu3Li88MILmT59eurr6zN48ODMnz8//fr1S5LU19enrq6uef0222yTBQsW5POf/3yGDx+e7t2758QTT8zXvva1TTEeAAAAAB2sqlKpVDp6iPbQ1NSU2traNDY2plu3bh09DgAA69DoG5QBYK1qp07t6BHaxYY2mk327XsAAAAAsC6iFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFLfJotSsWbMyYMCA1NTUZNiwYXn44YfXufZnP/tZqqqqWm2//vWvN9V4AAAAAHSgTRKl5s2bl4kTJ2bKlClZvHhxRo0alTFjxqSurm69xz311FOpr69v3vbYY49NMR4AAAAAHWyTRKnLL788EyZMyOmnn55BgwZl5syZ6du3b6666qr1Hrfjjjumd+/ezVvnzp03xXgAAAAAdLB2j1JvvPFGFi1alNGjR7fYP3r06CxcuHC9xw4dOjR9+vTJBz/4wTzwwAPtPRoAAAAAm4ku7X3CFStWZPXq1enVq1eL/b169UpDQ8Naj+nTp0+uvfbaDBs2LCtXrsx3v/vdfPCDH8zPfvazHHLIIWs9ZuXKlVm5cmXz66ampva7CAAAAAA2qXaPUm+rqqpq8bpSqbTa97aBAwdm4MCBza9HjBiRZcuW5dJLL11nlJoxY0amTZvWfgMDAAAAUEy7f3yvR48e6dy5c6u7opYvX97q7qn1Oeigg/L000+v8+eTJ09OY2Nj87Zs2bKNnhkAAACAsto9SnXt2jXDhg3LggULWuxfsGBBRo4cucHnWbx4cfr06bPOn1dXV6dbt24tNgAAAADeHTbJx/cmTZqU8ePHZ/jw4RkxYkSuvfba1NXV5ayzzkry1l1Ozz77bObMmZMkmTlzZvr375/3v//9eeONN3LTTTfltttuy2233bYpxgMAAACgg22SKDVu3Li88MILmT59eurr6zN48ODMnz8//fr1S5LU19enrq6uef0bb7yRc889N88++2y23HLLvP/9789//Md/5KijjtoU4wEAAADQwaoqlUqlo4doD01NTamtrU1jY6OP8gEAbMYafVkNAKxV7dSpHT1Cu9jQRtPuz5QCAAAAgL9FlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACK69LRA7B2Fy1e0dEjAMBm5/yhPTp6BAAA2ok7pQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4jZZlJo1a1YGDBiQmpqaDBs2LA8//PAGHfef//mf6dKlS4YMGbKpRgMAAACgg22SKDVv3rxMnDgxU6ZMyeLFizNq1KiMGTMmdXV16z2usbExp5xySj74wQ9uirEAAAAA2Exskih1+eWXZ8KECTn99NMzaNCgzJw5M3379s1VV1213uPOPPPMnHTSSRkxYsSmGAsAAACAzUS7R6k33ngjixYtyujRo1vsHz16dBYuXLjO42688cb89re/zdSpU9t7JAAAAAA2M13a+4QrVqzI6tWr06tXrxb7e/XqlYaGhrUe8/TTT+f888/Pww8/nC5dNmyklStXZuXKlc2vm5qaNn5oAAAAAIraZA86r6qqavG6Uqm02pckq1evzkknnZRp06Zlzz333ODzz5gxI7W1tc1b37593/HMAAAAAJTR7lGqR48e6dy5c6u7opYvX97q7qkkeeWVV/LEE0/kc5/7XLp06ZIuXbpk+vTpefLJJ9OlS5f89Kc/Xev7TJ48OY2Njc3bsmXL2vtSAAAAANhE2v3je127ds2wYcOyYMGCfOQjH2nev2DBghxzzDGt1nfr1i2//OUvW+ybNWtWfvrTn+YHP/hBBgwYsNb3qa6uTnV1dfsODwAAAEAR7R6lkmTSpEkZP358hg8fnhEjRuTaa69NXV1dzjrrrCRv3eX07LPPZs6cOenUqVMGDx7c4vgdd9wxNTU1rfYDAAAA8N6wSaLUuHHj8sILL2T69Ompr6/P4MGDM3/+/PTr1y9JUl9fn7q6uk3x1gAAAAC8C1RVKpVKRw/RHpqamlJbW5vGxsZ069ato8d5xy5avKKjRwCAzc75Q3t09Ai0g8Zp0zp6BADYLNVOndrRI7SLDW00m+zb9wAAAABgXUQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCmA/4+9u4+ysq73//8aQWZUZFRQBBtuzEISUxvMwCytRFFL04q0MBNSjmYhdQLifBXoBjM1bCUgiaJ5E5lmZWjN6XiDh/qaBNY5mZVpYziIUDJoBQL794fL+TUOIObMZ9Dv47HWXov9mc917ffQWrnXk2tfGwAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAACiuw6LUrFmzMnDgwNTU1KS+vj6LFi3a4t777rsvhx9+eHr27Jmddtop+++/f772ta911GgAAAAAdLKuHXHSBQsWZPz48Zk1a1YOP/zwXHnllRk5cmR+85vfpF+/fm3277LLLvnkJz+ZN7/5zdlll11y33335eyzz84uu+ySs846qyNGBAAAAKATdciVUpdddlnGjBmTsWPHZvDgwZk5c2bq6uoye/bsze4/5JBDcuqpp+aAAw7IgAED8tGPfjTHHHPMVq+uAgAAAODVq92j1Pr167NkyZKMGDGi1fqIESOyePHibTrH0qVLs3jx4rzzne9s7/EAAAAA2A60+8f3Vq1alY0bN6Z3796t1nv37p0VK1Zs9djXve51eeqpp7Jhw4ZMnTo1Y8eO3eLedevWZd26dS3Pm5ubX9ngAAAAABTTYTc6r6qqavW8Uqm0WXuxRYsW5YEHHsicOXMyc+bM3HTTTVvcO2PGjNTW1rY86urq2mVuAAAAADpeu18p1atXr3Tp0qXNVVErV65sc/XUiw0cODBJcuCBB+bJJ5/M1KlTc+qpp2527+TJkzNhwoSW583NzcIUAAAAwKtEu18p1a1bt9TX16ehoaHVekNDQ4YPH77N56lUKq0+nvdi1dXV6dGjR6sHAAAAAK8O7X6lVJJMmDAho0ePztChQzNs2LDMnTs3jY2NGTduXJLnr3Javnx5rrvuuiTJFVdckX79+mX//fdPktx333255JJLct5553XEeAAAAAB0sg6JUqNGjcrq1aszffr0NDU1ZciQIVm4cGH69++fJGlqakpjY2PL/k2bNmXy5Ml59NFH07Vr17z+9a/PRRddlLPPPrsjxgMAAACgk1VVKpVKZw/RHpqbm1NbW5s1a9a8Jj7Kd9HSVZ09AgBsdyYd0quzR6AdrJk2rbNHAIDtUu2FF3b2CO1iWxtNh337HgAAAABsiSgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFNdhUWrWrFkZOHBgampqUl9fn0WLFm1x76233pqjjz46e+65Z3r06JFhw4blxz/+cUeNBgAAAEAn65AotWDBgowfPz5TpkzJ0qVLc8QRR2TkyJFpbGzc7P577703Rx99dBYuXJglS5bkqKOOynvf+94sXbq0I8YDAAAAoJNVVSqVSnuf9LDDDstb3vKWzJ49u2Vt8ODBOemkkzJjxoxtOscBBxyQUaNG5YILLtim/c3Nzamtrc2aNWvSo0ePf2nu7clFS1d19ggAsN2ZdEivzh6BdrBm2rTOHgEAtku1F17Y2SO0i21tNO1+pdT69euzZMmSjBgxotX6iBEjsnjx4m06x6ZNm7J27drsscceW9yzbt26NDc3t3oAAAAA8OrQ7lFq1apV2bhxY3r37t1qvXfv3lmxYsU2nePSSy/Ns88+mw996ENb3DNjxozU1ta2POrq6l7R3AAAAACU02E3Oq+qqmr1vFKptFnbnJtuuilTp07NggULstdee21x3+TJk7NmzZqWx+OPP/6KZwYAAACgjK7tfcJevXqlS5cuba6KWrlyZZurp15swYIFGTNmTG6++ea85z3v2ere6urqVFdXv+J5AQAAACiv3a+U6tatW+rr69PQ0NBqvaGhIcOHD9/icTfddFPOOOOM3HjjjTn++OPbeywAAAAAtiPtfqVUkkyYMCGjR4/O0KFDM2zYsMydOzeNjY0ZN25ckuc/erd8+fJcd911SZ4PUqeffnouv/zyvO1tb2u5ymqnnXZKbW1tR4wIAAAAQCfqkCg1atSorF69OtOnT09TU1OGDBmShQsXpn///kmSpqamNDY2tuy/8sors2HDhpx77rk599xzW9Y/9rGPZf78+R0xIgAAAACdqEOiVJKcc845Oeecczb7sxeHprvvvrujxgAAAABgO9Rh374HAAAAAFsiSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFdViUmjVrVgYOHJiamprU19dn0aJFW9zb1NSU0047LYMGDcoOO+yQ8ePHd9RYAAAAAGwHOiRKLViwIOPHj8+UKVOydOnSHHHEERk5cmQaGxs3u3/dunXZc889M2XKlBx00EEdMRIAAAAA25EOiVKXXXZZxowZk7Fjx2bw4MGZOXNm6urqMnv27M3uHzBgQC6//PKcfvrpqa2t7YiRAAAAANiOtHuUWr9+fZYsWZIRI0a0Wh8xYkQWL17cbq+zbt26NDc3t3oAAAAA8OrQ7lFq1apV2bhxY3r37t1qvXfv3lmxYkW7vc6MGTNSW1vb8qirq2u3cwMAAADQsTrsRudVVVWtnlcqlTZrr8TkyZOzZs2alsfjjz/ebucGAAAAoGN1be8T9urVK126dGlzVdTKlSvbXD31SlRXV6e6urrdzgcAAABAOe1+pVS3bt1SX1+fhoaGVusNDQ0ZPnx4e78cAAAAAK9C7X6lVJJMmDAho0ePztChQzNs2LDMnTs3jY2NGTduXJLnP3q3fPnyXHfddS3HLFu2LEnyzDPP5KmnnsqyZcvSrVu3vOlNb+qIEQEAAADoRB0SpUaNGpXVq1dn+vTpaWpqypAhQ7Jw4cL0798/SdLU1JTGxsZWxxxyyCEtf16yZEluvPHG9O/fP4899lhHjAgAAABAJ+qQKJUk55xzTs4555zN/mz+/Plt1iqVSkeNAgAAAMB2psO+fQ8AAAAAtkSUAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIrrsCg1a9asDBw4MDU1Namvr8+iRYu2uv+ee+5JfX19ampqsu+++2bOnDkdNRoAAAAAnaxDotSCBQsyfvz4TJkyJUuXLs0RRxyRkSNHprGxcbP7H3300Rx33HE54ogjsnTp0nz+85/Ppz71qdxyyy0dMR4AAAAAnaxDotRll12WMWPGZOzYsRk8eHBmzpyZurq6zJ49e7P758yZk379+mXmzJkZPHhwxo4dmzPPPDOXXHJJR4wHAAAAQCfr2t4nXL9+fZYsWZJJkya1Wh8xYkQWL1682WN+9rOfZcSIEa3WjjnmmMybNy/PPfdcdtxxxzbHrFu3LuvWrWt5vmbNmiRJc3PzK/0Vtgv/eGZtZ48AANud5uZunT0C7aD5H//o7BEAYLtU9RppGi+0mUqlstV97R6lVq1alY0bN6Z3796t1nv37p0VK1Zs9pgVK1Zsdv+GDRuyatWq9OnTp80xM2bMyLRp09qs19XVvYLpAYDtWdv/8gMAvIZcdFFnT9Cu1q5dm9ra2i3+vN2j1AuqqqpaPa9UKm3WXmr/5tZfMHny5EyYMKHl+aZNm/KXv/wlPXv23OrrALwczc3Nqaury+OPP54ePXp09jgAAO3Kex2gI1QqlaxduzZ9+/bd6r52j1K9evVKly5d2lwVtXLlyjZXQ71g77333uz+rl27pmfPnps9prq6OtXV1a3Wdtttt399cICt6NGjhzdqAMBrlvc6QHvb2hVSL2j3G51369Yt9fX1aWhoaLXe0NCQ4cOHb/aYYcOGtdn/k5/8JEOHDt3s/aQAAAAAeHXrkG/fmzBhQq666qpcffXVeeihh3L++eensbEx48aNS/L8R+9OP/30lv3jxo3Ln/70p0yYMCEPPfRQrr766sybNy+f/exnO2I8AAAAADpZh9xTatSoUVm9enWmT5+epqamDBkyJAsXLkz//v2TJE1NTWlsbGzZP3DgwCxcuDDnn39+rrjiivTt2zdf//rXc8opp3TEeADbrLq6OhdeeGGbjwsDALwWeK8DdKaqykt9Px8AAAAAtLMO+fgeAAAAAGyNKAUAAABAcaIUAAAAAMWJUsCrztSpU9O7d+9UVVXltttu6+xx2tWRRx6Z8ePHd/YYAEBBlUolZ511VvbYY49UVVVl2bJlnT3SKzZgwIDMnDmzs8cAtnOiFFDEGWeckaqqqpZHz549c+yxx+ZXv/rVyzrPQw89lGnTpuXKK69MU1NTRo4c2UETt/XP82/uccYZZ7zi17j11lvzhS984ZUPCwBsVxYvXpwuXbrk2GOPbfOzO++8M/Pnz8/tt9/e8u3lHfWPb4899thLvqeZOnXqK36dX/ziFznrrLNe+cDAa1rXzh4A+H/Hsccem2uuuSZJsmLFivzHf/xHTjjhhDQ2Nm7zOR555JEkyYknnpiqqqp/eZbnnnsuO+6448s6pqmpqeXPCxYsyAUXXJCHH364ZW2nnXb6l+d5wR577PGKzwEAbH+uvvrqnHfeebnqqqvS2NiYfv36tfzskUceSZ8+fTJ8+PB2f90Xv+epq6tr9Z7mkksuyZ133pn//M//bFnr3r37K37dPffc8xWfA3jtc6UUUEx1dXX23nvv7L333jn44IMzceLEPP7443nqqada9ixfvjyjRo3K7rvvnp49e+bEE0/MY489luT5j+29973vTZLssMMOLVFq06ZNmT59el73uteluro6Bx98cO68886Wc77wL4Lf+c53cuSRR6ampibXX399kuSaa67J4MGDU1NTk/333z+zZs3a4vwvzL733nuntrY2VVVVLc/vvPPO9O/fv9X+2267rVU4mzp1ag4++OB861vfyoABA1JbW5sPf/jDWbt2bcueF398b8CAAfnyl7+cM888M7vuumv69euXuXPntnqdxYsX5+CDD05NTU2GDh3a8rqvhUv/AeC14Nlnn813vvOd/Nu//VtOOOGEzJ8/v+VnZ5xxRs4777w0NjamqqoqAwYMyIABA5Ik73//+1vWXvDDH/4w9fX1qampyb777ptp06Zlw4YNLT+vqqrKnDlzcuKJJ2aXXXbJF7/4xVazdOnSpdV7mu7du6dr164tz+fMmZO3v/3trY6ZOXNmqxnOOOOMnHTSSbnkkkvSp0+f9OzZM+eee26ee+65lj0v/vheVVVVrrrqqrz//e/PzjvvnDe84Q35wQ9+0Op1fvCDH+QNb3hDdtpppxx11FG59tprU1VVlaeffvrl/YUDrxqiFNApnnnmmdxwww3Zb7/90rNnzyTJ3/72txx11FHp3r177r333tx3333p3r17jj322Kxfvz6f/exnW660ampqavlXvssvvzyXXnppLrnkkvzqV7/KMccck/e97335/e9/3+o1J06cmE996lN56KGHcswxx+Sb3/xmpkyZki996Ut56KGH8uUvfzn/5//8n1x77bUd9ns/8sgjue2223L77bfn9ttvzz333JOLLrpoq8dceumlGTp0aJYuXZpzzjkn//Zv/5bf/va3SZK1a9fmve99bw488MD88pe/zBe+8IVMnDixw+YHAF6+BQsWZNCgQRk0aFA++tGP5pprrkmlUkny/PuYF/5xrampKb/4xS/yi1/8Isnz/3j2wlqS/PjHP85HP/rRfOpTn8pvfvObXHnllZk/f36+9KUvtXq9Cy+8MCeeeGJ+/etf58wzz+yQ3+muu+7KI488krvuuivXXntt5s+f3yq2bc60adPyoQ99KL/61a9y3HHH5SMf+Uj+8pe/JHn+HxE/8IEP5KSTTsqyZcty9tlnZ8qUKR0yO7D9EKWAYm6//fZ079493bt3z6677pof/OAHWbBgQXbY4fn/K/r2t7+dHXbYIVdddVUOPPDADB48ONdcc00aGxtz9913p3v37tltt92S/P9XLSXPX3Y+ceLEfPjDH86gQYPyla98JQcffHCbm2uOHz8+J598cgYOHJi+ffvmC1/4Qi699NKWtZNPPjnnn39+rrzyyg77O9i0aVPmz5+fIUOG5Igjjsjo0aPz05/+dKvHHHfccTnnnHOy3377ZeLEienVq1fuvvvuJMkNN9yQqqqqfPOb38yb3vSmjBw5Mv/+7//eYfMDAC/fvHnz8tGPfjTJ87czeOaZZ1r++19bW5tdd9215QqmPffcs+Wjb7vttlvLWpJ86UtfyqRJk/Kxj30s++67b44++uh84QtfaPPe5bTTTsuZZ56Zfffdt82V3O1l9913zze+8Y3sv//+OeGEE3L88ce/5HuaM844I6eeemr222+/fPnLX86zzz6b+++/P0kyZ86cDBo0KF/96lczaNCgfPjDH26X+3UC2zf3lAKKOeqoozJ79uwkyV/+8pfMmjUrI0eOzP3335/+/ftnyZIl+cMf/pBdd9211XH/+Mc/Wu4l9WLNzc154okncvjhh7daP/zww/Pggw+2Whs6dGjLn5966qk8/vjjGTNmTD7xiU+0rG/YsCG1tbWv6PfcmgEDBrT6/fr06ZOVK1du9Zg3v/nNLX9+4SODLxzz8MMP581vfnNqampa9rz1rW9t56kBgH/Vww8/nPvvvz+33nprkqRr164ZNWpUrr766rznPe95WedasmRJfvGLX7S6Mmrjxo35xz/+kb/97W/Zeeedk7R+z9NRDjjggHTp0qXleZ8+ffLrX/96q8f883uaXXbZJbvuumur9zSHHnpoq/3e08BrnygFFLPLLrtkv/32a3leX1+f2trafPOb38wXv/jFbNq0KfX19bnhhhvaHPtSN8t88U3PK5VKm7Vddtml5c+bNm1Kknzzm9/MYYcd1mrfP7/B2lY77LBDy2X4L/jn+yq84MU3V6+qqmqZZUu2dszmfs8XzwEAdJ558+Zlw4YN2WeffVrWKpVKdtxxx/z1r3/N7rvvvs3n2rRpU6ZNm5aTTz65zc/++R+o/vk9z8vlPQ1QkigFdJqqqqrssMMO+fvf/54kectb3pIFCxZkr732So8ePbbpHD169Ejfvn1z33335R3veEfL+uLFi7f6r2u9e/fOPvvskz/+8Y/5yEc+8sp+kTwfzdauXZtnn3225Y1giRuN77///rnhhhuybt26VFdXJ0keeOCBDn9dAOClbdiwIdddd10uvfTSjBgxotXPTjnllNxwww355Cc/udljd9xxx2zcuLHV2lve8pY8/PDDrf6Rr73tueeeWbFiRatIVOo9zcKFC1uteU8Dr33uKQUUs27duqxYsSIrVqzIQw89lPPOOy/PPPNMyzfqfeQjH0mvXr1y4oknZtGiRXn00Udzzz335NOf/nT+/Oc/b/G8//7v/56vfOUrWbBgQR5++OFMmjQpy5Yty6c//emtzjN16tTMmDEjl19+eX73u9/l17/+da655ppcdtllL/t3O+yww7Lzzjvn85//fP7whz/kxhtvfMmbfbaH0047LZs2bcpZZ52Vhx56KD/+8Y9zySWXJGl79RgAUNbtt9+ev/71rxkzZkyGDBnS6vGBD3wg8+bN2+KxAwYMyE9/+tOsWLEif/3rX5MkF1xwQa677rpMnTo1//u//5uHHnooCxYsyH/8x3+028xHHnlknnrqqVx88cV55JFHcsUVV+SOO+5ot/Nvydlnn53f/va3mThxYn73u9/lO9/5Tst7Ke9p4LVLlAKKufPOO9OnT5/06dMnhx12WH7xi1/k5ptvzpFHHpkk2XnnnXPvvfemX79+OfnkkzN48OCceeaZ+fvf/77VK6c+9alP5TOf+Uw+85nP5MADD8ydd97Z8pXCWzN27NhcddVVmT9/fg488MC8853vzPz58zNw4MCX/bvtscceuf7667Nw4cIceOCBuemmmzJ16tSXfZ6Xq0ePHvnhD3+YZcuW5eCDD86UKVNywQUXJGl9GT8AUN68efPynve8Z7P3qzzllFOybNmy/PKXv9zssZdeemkaGhpSV1eXQw45JElyzDHH5Pbbb09DQ0MOPfTQvO1tb8tll13WrjczHzx4cGbNmpUrrrgiBx10UO6///589rOfbbfzb8nAgQPz3e9+N7feemve/OY3Z/bs2S3fvvfC1eDAa09VxQd1AV5Tbrjhhnz84x/PmjVrstNOO3X2OAAA/5IvfelLmTNnTh5//PHOHgXoIO4pBfAqd91112XffffNPvvskwcffDATJ07Mhz70IUEKAHhVmTVrVg499ND07Nkz//3f/52vfvWrW7znFvDaIEoBvMqtWLEiF1xwQVasWJE+ffrkgx/8YKuvigYAeDX4/e9/ny9+8Yv5y1/+kn79+uUzn/lMJk+e3NljAR3Ix/cAAAAAKM6NzgEAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgD4l3z9619PVVVVhgwZ0tmjvCo9+eSTmTRpUg488MB07949NTU1ecMb3pBPf/rT+f3vf9/Z43W4+fPnp6qqKo899lhnjwIAdJKqSqVS6ewhAIBXn4MPPjgPPvhgkuTnP/95DjvssE6e6NXj/vvvzwknnJBKpZJPfvKTGTZsWLp165aHH344119/ff7nf/4nf/3rXzt7zA711FNP5ZFHHskhhxyS6urqzh4HAOgEohQA8LI98MADOfTQQ3P88cfnRz/6UT7xiU9k7ty5nT3WZv3tb3/Lzjvv3NljtGhubs6gQYOy4447ZvHixXnd617XZs93v/vdfOADH+iE6Tre3//+99TU1KSqqqqzRwEAOpmP7wEAL9u8efOSJBdddFGGDx+eb3/72/nb3/7WZt/y5ctz1llnpa6uLt26dUvfvn3zgQ98IE8++WTLnqeffjqf+cxnsu+++6a6ujp77bVXjjvuuPz2t79Nktx9992pqqrK3Xff3ercjz32WKqqqjJ//vyWtTPOOCPdu3fPr3/964wYMSK77rpr3v3udydJGhoacuKJJ+Z1r3tdampqst9+++Xss8/OqlWr2sz929/+Nqeeemp69+6d6urq9OvXL6effnrWrVuXxx57LF27ds2MGTPaHHfvvfemqqoqN9988xb/7r75zW9mxYoVufjiizcbpJK0CVI/+MEPMmzYsOy8887Zddddc/TRR+dnP/tZqz1Tp05NVVVVfvWrX+WDH/xgamtrs8cee2TChAnZsGFDHn744Rx77LHZddddM2DAgFx88cWtjn/h7/n666/PhAkTsvfee2ennXbKO9/5zixdurTV3gceeCAf/vCHM2DAgOy0004ZMGBATj311PzpT39qte+Fj+j95Cc/yZlnnpk999wzO++8c9atW7fZj+8tXbo0J5xwQvbaa69UV1enb9++Of744/PnP/+5Zc8//vGPTJ48OQMHDky3bt2yzz775Nxzz83TTz/d6rUHDBiQE044IXfeeWfe8pa3ZKeddsr++++fq6++eov/2wAAZYlSAMDL8ve//z033XRTDj300AwZMiRnnnlm1q5d2ybELF++PIceemi+973vZcKECbnjjjsyc+bM1NbWtnw0be3atXn729+eK6+8Mh//+Mfzwx/+MHPmzMkb3/jGNDU1/UvzrV+/Pu973/vyrne9K9///vczbdq0JMkjjzySYcOGZfbs2fnJT36SCy64IP/3//7fvP3tb89zzz3XcvyDDz6YQw89ND//+c8zffr03HHHHZkxY0bWrVuX9evXZ8CAAXnf+96XOXPmZOPGja1e+xvf+Eb69u2b97///Vuc7yc/+Um6dOmS9773vdv0+9x444058cQT06NHj9x0002ZN29e/vrXv+bII4/Mfffd12b/hz70oRx00EG55ZZb8olPfCJf+9rXcv755+ekk07K8ccfn+9973t517velYkTJ+bWW29tc/znP//5/PGPf8xVV12Vq666Kk888USOPPLI/PGPf2zZ89hjj2XQoEGZOXNmfvzjH+crX/lKmpqacuihh2428p155pnZcccd861vfSvf/e53s+OOO7bZ8+yzz+boo4/Ok08+mSuuuCINDQ2ZOXNm+vXrl7Vr1yZJKpVKTjrppFxyySUZPXp0fvSjH2XChAm59tpr8653vSvr1q1rdc4HH3wwn/nMZ3L++efn+9//ft785jdnzJgxuffee7fp7x4A6GAVAICX4brrrqskqcyZM6dSqVQqa9eurXTv3r1yxBFHtNp35plnVnbcccfKb37zmy2ea/r06ZUklYaGhi3uueuuuypJKnfddVer9UcffbSSpHLNNde0rH3sYx+rJKlcffXVW/0dNm3aVHnuuecqf/rTnypJKt///vdbfvaud72rsttuu1VWrlz5kjN973vfa1lbvnx5pWvXrpVp06Zt9bX333//yt57773VPS/YuHFjpW/fvpUDDzywsnHjxpb1tWvXVvbaa6/K8OHDW9YuvPDCSpLKpZde2uocBx98cCVJ5dZbb21Ze+655yp77rln5eSTT27zO73lLW+pbNq0qWX9scceq+y4446VsWPHbnHODRs2VJ555pnKLrvsUrn88stb1q+55ppKksrpp5/e5pgXfvboo49WKpVK5YEHHqgkqdx2221bfJ0777yzkqRy8cUXt1pfsGBBJUll7ty5LWv9+/ev1NTUVP70pz+1rP3973+v7LHHHpWzzz57i68BAJTjSikA4GWZN29edtppp3z4wx9OknTv3j0f/OAHs2jRolbfGnfHHXfkqKOOyuDBg7d4rjvuuCNvfOMb8573vKddZzzllFParK1cuTLjxo1LXV1dunbtmh133DH9+/dPkjz00ENJnr//1D333JMPfehD2XPPPbd4/iOPPDIHHXRQrrjiipa1OXPmpKqqKmeddVa7/R4PP/xwnnjiiYwePTo77PD/v23r3r17TjnllPz85z9v87HJE044odXzwYMHp6qqKiNHjmxZ69q1a/bbb782H7dLktNOO63V/Z769++f4cOH56677mpZe+aZZzJx4sTst99+6dq1a7p27Zru3bvn2Wefbfm7/Geb+9/jxfbbb7/svvvumThxYubMmZPf/OY3bfb813/9V5LnP6b5zz74wQ9ml112yU9/+tNW6wcffHD69evX8rympiZvfOMbN/t7AwDliVIAwDb7wx/+kHvvvTfHH398KpVKnn766Tz99NMt90D65/v1PPXUU1u8Z9LL2fNy7bzzzunRo0ertU2bNmXEiBG59dZb87nPfS4//elPc//99+fnP/95kuc/kpgkf/3rX7Nx48ZtmulTn/pUfvrTn+bhhx/Oc889l29+85v5wAc+kL333nurx/Xr1y9PPfVUnn322Zd8jdWrVydJ+vTp0+Znffv2zaZNm9p8S98ee+zR6nm3bt2y8847p6amps36P/7xjzbn3dz8e++9d8ssyfPh6hvf+EbGjh2bH//4x7n//vvzi1/8InvuuWfL3+U/29z8L1ZbW5t77rknBx98cD7/+c/ngAMOSN++fXPhhRe2fLxy9erV6dq1a5tgWFVV1WbGJOnZs2eb16murt7sjABAeaIUALDNrr766lQqlXz3u9/N7rvv3vI4/vjjkyTXXntty32W9txzz1Y3qN6cbdnzQkx58f2CNnfvoiSb/Va3//mf/8mDDz6Yr371qznvvPNy5JFH5tBDD20TLfbYY4906dLlJWdKng8zPXv2zBVXXJGbb745K1asyLnnnvuSxx1zzDHZuHFjfvjDH77k3hfm29z9tZ544onssMMO2X333V/yPC/HihUrNrv2wixr1qzJ7bffns997nOZNGlS3v3ud+fQQw/NgQcemL/85S+bPee2ftPegQcemG9/+9tZvXp1li1bllGjRmX69Om59NJLkzz/97Fhw4Y89dRTrY6rVCpZsWJFevXq9XJ+VQCgk4lSAMA22bhxY6699tq8/vWvz1133dXm8ZnPfCZNTU254447kiQjR47MXXfdlYcffniL5xw5cmR+97vftXwsa3MGDBiQJPnVr37Vav0HP/jBNs/+QhSprq5utX7llVe2ev7Ct83dfPPNW4xeL6ipqclZZ52Va6+9NpdddlkOPvjgHH744S85y5gxY7L33nvnc5/7XJYvX77ZPS/cgHzQoEHZZ599cuONN6ZSqbT8/Nlnn80tt9zS8o187emmm25q9Vp/+tOfsnjx4hx55JFJnv+7rFQqbf4ur7rqqjY3fv9XVVVV5aCDDsrXvva17LbbbvnlL3+ZJC3fpHj99de32n/LLbfk2Wefbfk5APDq0LWzBwAAXh3uuOOOPPHEE/nKV77SEij+2ZAhQ/KNb3wj8+bNywknnNDyzXXveMc78vnPfz4HHnhgnn766dx5552ZMGFC9t9//4wfPz4LFizIiSeemEmTJuWtb31r/v73v+eee+7JCSeckKOOOip777133vOe92TGjBnZfffd079///z0pz/d7DfHbcn++++f17/+9Zk0aVIqlUr22GOP/PCHP0xDQ0ObvZdddlne/va357DDDsukSZOy33775cknn8wPfvCDXHnlldl1111b9p5zzjm5+OKLs2TJklx11VXbNEttbW2+//3v54QTTsghhxyST37ykxk2bFi6deuW3//+97n++uvz4IMP5uSTT84OO+yQiy++OB/5yEdywgkn5Oyzz866devy1a9+NU8//XQuuuiibf472FYrV67M+9///nziE5/ImjVrcuGFF6ampiaTJ09OkvTo0SPveMc78tWvfjW9evXKgAEDcs8992TevHnZbbfd/uXXvf322zNr1qycdNJJ2XfffVOpVHLrrbfm6aefztFHH50kOfroo3PMMcdk4sSJaW5uzuGHH55f/epXufDCC3PIIYdk9OjR7fFXAAAU4kopAGCbzJs3L926dcvHP/7xzf68V69eef/735/bb789Tz75ZPbZZ5/cf//9OeGEE3LRRRfl2GOPzXnnnZc1a9a03Pdo1113zX333ZcxY8Zk7ty5Of744/OJT3wiDz/8cPr27dty7m9961t597vfnYkTJ+aDH/xgli9fnptuummbZ99xxx3zwx/+MG984xtz9tln59RTT83KlSvzn//5n232HnTQQbn//vtTX1+fyZMn59hjj83EiRNTXV2dbt26tdq7zz775O1vf3v22GOPnHbaads8z1vf+tb8+te/zplnnpnvfOc7Oemkk3LMMcfkK1/5Svbff/8sWrSoZe9pp52W2267LatXr86oUaPy8Y9/PD169Mhdd92Vt7/97dv8mtvqy1/+cvr375+Pf/zjOfPMM9OnT5/cddddef3rX9+y58Ybb8xRRx2Vz33uczn55JPzwAMPpKGhIbW1tf/y677hDW/Ibrvtlosvvjjve9/78sEPfjC//OUvM3/+/HziE59I8vwVVLfddlsmTJiQa665Jscdd1wuueSSjB49Ov/1X//V5uotAGD7VlX55+uzAQDYZitXrkz//v1z3nnn5eKLL+7scV6Ru+++O0cddVRuvvnmlhvXAwB0JB/fAwB4mf785z/nj3/8Y7761a9mhx12yKc//enOHgkA4FXHx/cAAF6mq666KkceeWT+93//NzfccEP22Wefzh4JAOBVx8f3AAAAACjOlVIAAAAAFCdKAQAAAFCcKAUAAABAca+Zb9/btGlTnnjiiey6666pqqrq7HEAAAAA/p9UqVSydu3a9O3bNzvssOXroV4zUeqJJ55IXV1dZ48BAAAAQJLHH388r3vd67b489dMlNp1112TPP8L9+jRo5OnAQAAAPh/U3Nzc+rq6lpazZa8ZqLUCx/Z69GjhygFAAAA0Mle6vZKbnQOAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUFzXzh6Azbto6arOHgEAtjuTDunV2SMAANBOXCkFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMV1WJSaNWtWBg4cmJqamtTX12fRokVb3Hv33XenqqqqzeO3v/1tR40HAAAAQCfqkCi1YMGCjB8/PlOmTMnSpUtzxBFHZOTIkWlsbNzqcQ8//HCamppaHm94wxs6YjwAAAAAOlmHRKnLLrssY8aMydixYzN48ODMnDkzdXV1mT179laP22uvvbL33nu3PLp06dIR4wEAAADQydo9Sq1fvz5LlizJiBEjWq2PGDEiixcv3uqxhxxySPr06ZN3v/vdueuuu7a6d926dWlubm71AAAAAODVod2j1KpVq7Jx48b07t271Xrv3r2zYsWKzR7Tp0+fzJ07N7fccktuvfXWDBo0KO9+97tz7733bvF1ZsyYkdra2pZHXV1du/4eAAAAAHScrh114qqqqlbPK5VKm7UXDBo0KIMGDWp5PmzYsDz++OO55JJL8o53vGOzx0yePDkTJkxoed7c3CxMAQAAALxKtPuVUr169UqXLl3aXBW1cuXKNldPbc3b3va2/P73v9/iz6urq9OjR49WDwAAAABeHdo9SnXr1i319fVpaGhotd7Q0JDhw4dv83mWLl2aPn36tPd4AAAAAGwHOuTjexMmTMjo0aMzdOjQDBs2LHPnzk1jY2PGjRuX5PmP3i1fvjzXXXddkmTmzJkZMGBADjjggKxfvz7XX399brnlltxyyy0dMR4AAAAAnaxDotSoUaOyevXqTJ8+PU1NTRkyZEgWLlyY/v37J0mamprS2NjYsn/9+vX57Gc/m+XLl2ennXbKAQcckB/96Ec57rjjOmI8AAAAADpZVaVSqXT2EO2hubk5tbW1WbNmzWvi/lIXLV3V2SMAwHZn0iG9OnsEAABewrY2mna/pxQAAAAAvBRRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAormtnDwAAwP9b1kyb1tkjAMB2qfbCCzt7hKJcKQUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxXVYlJo1a1YGDhyYmpqa1NfXZ9GiRdt03H//93+na9euOfjggztqNAAAAAA6WYdEqQULFmT8+PGZMmVKli5dmiOOOCIjR45MY2PjVo9bs2ZNTj/99Lz73e/uiLEAAAAA2E50SJS67LLLMmbMmIwdOzaDBw/OzJkzU1dXl9mzZ2/1uLPPPjunnXZahg0b1hFjAQAAALCdaPcotX79+ixZsiQjRoxotT5ixIgsXrx4i8ddc801eeSRR3LhhRdu0+usW7cuzc3NrR4AAAAAvDq0e5RatWpVNm7cmN69e7da7927d1asWLHZY37/+99n0qRJueGGG9K1a9dtep0ZM2aktra25VFXV/eKZwcAAACgjA670XlVVVWr55VKpc1akmzcuDGnnXZapk2blje+8Y3bfP7JkydnzZo1LY/HH3/8Fc8MAAAAQBnbdlnSy9CrV6906dKlzVVRK1eubHP1VJKsXbs2DzzwQJYuXZpPfvKTSZJNmzalUqmka9eu+clPfpJ3vetdbY6rrq5OdXV1e48PAAAAQAHtfqVUt27dUl9fn4aGhlbrDQ0NGT58eJv9PXr0yK9//essW7as5TFu3LgMGjQoy5Yty2GHHdbeIwIAAADQydr9SqkkmTBhQkaPHp2hQ4dm2LBhmTt3bhobGzNu3Lgkz3/0bvny5bnuuuuyww47ZMiQIa2O32uvvVJTU9NmHQAAAIDXhg6JUqNGjcrq1aszffr0NDU1ZciQIVm4cGH69++fJGlqakpjY2NHvDQAAAAArwJVlUql0tlDtIfm5ubU1tZmzZo16dGjR2eP84pdtHRVZ48AANudSYf06uwRaAdrpk3r7BEAYLtUe+GFnT1Cu9jWRtNh374HAAAAAFsiSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcR0WpWbNmpWBAwempqYm9fX1WbRo0Rb33nfffTn88MPTs2fP7LTTTtl///3zta99raNGAwAAAKCTde2Iky5YsCDjx4/PrFmzcvjhh+fKK6/MyJEj85vf/Cb9+vVrs3+XXXbJJz/5ybz5zW/OLrvskvvuuy9nn312dtlll5x11lkdMSIAAAAAnahDrpS67LLLMmbMmIwdOzaDBw/OzJkzU1dXl9mzZ292/yGHHJJTTz01BxxwQAYMGJCPfvSjOeaYY7Z6dRUAAAAAr17tHqXWr1+fJUuWZMSIEa3WR4wYkcWLF2/TOZYuXZrFixfnne98Z3uPBwAAAMB2oN0/vrdq1aps3LgxvXv3brXeu3fvrFixYqvHvu51r8tTTz2VDRs2ZOrUqRk7duwW965bty7r1q1red7c3PzKBgcAAACgmA670XlVVVWr55VKpc3aiy1atCgPPPBA5syZk5kzZ+amm27a4t4ZM2aktra25VFXV9cucwMAAADQ8dr9SqlevXqlS5cuba6KWrlyZZurp15s4MCBSZIDDzwwTz75ZKZOnZpTTz11s3snT56cCRMmtDxvbm4WpgAAAABeJdr9Sqlu3bqlvr4+DQ0NrdYbGhoyfPjwbT5PpVJp9fG8F6uurk6PHj1aPQAAAAB4dWj3K6WSZMKECRk9enSGDh2aYcOGZe7cuWlsbMy4ceOSPH+V0/Lly3PdddclSa644or069cv+++/f5LkvvvuyyWXXJLzzjuvI8YDAAAAoJN1SJQaNWpUVq9enenTp6epqSlDhgzJwoUL079//yRJU1NTGhsbW/Zv2rQpkydPzqOPPpquXbvm9a9/fS666KKcffbZHTEeAAAAAJ2sqlKpVDp7iPbQ3Nyc2trarFmz5jXxUb6Llq7q7BEAYLsz6ZBenT0C7WDNtGmdPQIAbJdqL7yws0doF9vaaDrs2/cAAAAAYEtEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAorsOi1KxZszJw4MDU1NSkvr4+ixYt2uLeW2+9NUcffXT23HPP9OjRI8OGDcuPf/zjjhoNAAAAgE7WIVFqwYIFGT9+fKZMmZKlS5fmiCOOyMiRI9PY2LjZ/ffee2+OPvroLFy4MEuWLMlRRx2V9773vVm6dGlHjAcAAABAJ6uqVCqV9j7pYYcdlre85S2ZPXt2y9rgwYNz0kknZcaMGdt0jgMOOCCjRo3KBRdcsE37m5ubU1tbmzVr1qRHjx7/0tzbk4uWrursEQBguzPpkF6dPQLtYM20aZ09AgBsl2ovvLCzR2gX29po2v1KqfXr12fJkiUZMWJEq/URI0Zk8eLF23SOTZs2Ze3atdljjz3aezwAAAAAtgNd2/uEq1atysaNG9O7d+9W6717986KFSu26RyXXnppnn322XzoQx/a4p5169Zl3bp1Lc+bm5v/tYEBAAAAKK7DbnReVVXV6nmlUmmztjk33XRTpk6dmgULFmSvvfba4r4ZM2aktra25VFXV/eKZwYAAACgjHaPUr169UqXLl3aXBW1cuXKNldPvdiCBQsyZsyYfOc738l73vOere6dPHly1qxZ0/J4/PHHX/HsAAAAAJTR7lGqW7duqa+vT0NDQ6v1hoaGDB8+fIvH3XTTTTnjjDNy44035vjjj3/J16murk6PHj1aPQAAAAB4dWj3e0olyYQJEzJ69OgMHTo0w4YNy9y5c9PY2Jhx48Ylef4qp+XLl+e6665L8nyQOv3003P55ZfnbW97W8tVVjvttFNqa2s7YkQAAAAAOlGHRKlRo0Zl9erVmT59epqamjJkyJAsXLgw/fv3T5I0NTWlsbGxZf+VV16ZDRs25Nxzz825557bsv6xj30s8+fP74gRAQAAAOhEHRKlkuScc87JOeecs9mfvTg03X333R01BgAAAADboQ779j0AAAAA2BJRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEK8dz5AAAAFSNJREFUAAAAgOJEKQAAAACKE6UAAAAAKE6UAgAAAKA4UQoAAACA4kQpAAAAAIoTpQAAAAAoTpQCAAAAoDhRCgAAAIDiRCkAAAAAihOlAAAAAChOlAIAAACgOFEKAAAAgOJEKQAAAACKE6UAAAAAKK7DotSsWbMycODA1NTUpL6+PosWLdri3qamppx22mkZNGhQdthhh4wfP76jxgIAAABgO9AhUWrBggUZP358pkyZkqVLl+aII47IyJEj09jYuNn969aty5577pkpU6bkoIMO6oiRAAAAANiOdEiUuuyyyzJmzJiMHTs2gwcPzsyZM1NXV5fZs2dvdv+AAQNy+eWX5/TTT09tbW1HjAQAAADAdqTdo9T69euzZMmSjBgxotX6iBEjsnjx4nZ7nXXr1qW5ubnVAwAAAIBXh3aPUqtWrcrGjRvTu3fvVuu9e/fOihUr2u11ZsyYkdra2pZHXV1du50bAAAAgI7VYTc6r6qqavW8Uqm0WXslJk+enDVr1rQ8Hn/88XY7NwAAAAAdq2t7n7BXr17p0qVLm6uiVq5c2ebqqVeiuro61dXV7XY+AAAAAMpp9yulunXrlvr6+jQ0NLRab2hoyPDhw9v75QAAAAB4FWr3K6WSZMKECRk9enSGDh2aYcOGZe7cuWlsbMy4ceOSPP/Ru+XLl+e6665rOWbZsmVJkmeeeSZPPfVUli1blm7duuVNb3pTR4wIAAAAQCfqkCg1atSorF69OtOnT09TU1OGDBmShQsXpn///kmSpqamNDY2tjrmkEMOafnzkiVLcuONN6Z///557LHHOmJEAAAAADpRh0SpJDnnnHNyzjnnbPZn8+fPb7NWqVQ6ahQAAAAAtjMd9u17AAAAALAlohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFCdKAQAAAFCcKAUAAABAcaIUAAAAAMWJUgAAAAAUJ0oBAAAAUJwoBQAAAEBxohQAAAAAxYlSAAAAABQnSgEAAABQnCgFAAAAQHGiFAAAAADFiVIAAAAAFNdhUWrWrFkZOHBgampqUl9fn0WLFm11/z333JP6+vrU1NRk3333zZw5czpqNAAAAAA6WYdEqQULFmT8+PGZMmVKli5dmiOOOCIjR45MY2PjZvc/+uijOe6443LEEUdk6dKl+fznP59PfepTueWWWzpiPAAAAAA6WYdEqcsuuyxjxozJ2LFjM3jw4MycOTN1dXWZPXv2ZvfPmTMn/fr1y8yZMzN48OCMHTs2Z555Zi655JKOGA8AAACATta1vU+4fv36LFmyJJMmTWq1PmLEiCxevHizx/zsZz/LiBEjWq0dc8wxmTdvXp577rnsuOOObY5Zt25d1q1b1/J8zZo1SZLm5uZX+itsF/7xzNrOHgEAtjvNzd06ewTaQfM//tHZIwDAdqnqNdI0XmgzlUplq/vaPUqtWrUqGzduTO/evVut9+7dOytWrNjsMStWrNjs/g0bNmTVqlXp06dPm2NmzJiRadOmtVmvq6t7BdMDANuztv/lBwB4Dbnoos6eoF2tXbs2tbW1W/x5u0epF1RVVbV6XqlU2qy91P7Nrb9g8uTJmTBhQsvzTZs25S9/+Ut69uy51dcBeDmam5tTV1eXxx9/PD169OjscQAA2pX3OkBHqFQqWbt2bfr27bvVfe0epXr16pUuXbq0uSpq5cqVba6GesHee++92f1du3ZNz549N3tMdXV1qqurW63ttttu//rgAFvRo0cPb9QAgNcs73WA9ra1K6Re0O43Ou/WrVvq6+vT0NDQar2hoSHDhw/f7DHDhg1rs/8nP/lJhg4dutn7SQEAAADw6tYh3743YcKEXHXVVbn66qvz0EMP5fzzz09jY2PGjRuX5PmP3p1++ukt+8eNG5c//elPmTBhQh566KFcffXVmTdvXj772c92xHgAAAAAdLIOuafUqFGjsnr16kyfPj1NTU0ZMmRIFi5cmP79+ydJmpqa0tjY2LJ/4MCBWbhwYc4///xcccUV6du3b77+9a/nlFNO6YjxALZZdXV1LrzwwjYfFwYAeC3wXgfoTFWVl/p+PgAAAABoZx3y8T0AAAAA2BpRCgAAAIDiRCkAAAAAihOlgFedqVOnpnfv3qmqqsptt93W2eO0qyOPPDLjx4/v7DEAgIIqlUrOOuus7LHHHqmqqsqyZcs6e6RXbMCAAZk5c2ZnjwFs5/6/9u4+psr6/+P486BObg4gouXRRGQ4pITIm2zZjW6WaBiKpia0CEunpdbUHQsjCLFMcLklYoAcSCqoMYfMcOa8yfgD0lBXiEo5qDiLTSrFm0T4/uG4fh5vMBKOye/12M52zue6Pp/P++IPfe99fa7PpaKUiDhFbGwsJpPJ+Pj6+hIeHs6RI0c6NE5VVRVJSUls3ryZ+vp6Jk+e3EURX+/q+G/0iY2Nve05ioqKSE5Ovv1gRURE5D+lrKyMHj16EB4eft2x0tJSbDYbJSUlxtvLu+rm26lTp26Z0yQmJt72PBUVFcyfP//2AxaRbq3nnQ5ARP7/CA8PJycnBwC73c6qVauIiIigtrb2H49RU1MDQGRkJCaT6V/HcunSJXr16tWhPvX19cb3goICEhISqK6uNtrc3Nz+dTxt+vbte9tjiIiIyH/Pli1bWLx4MVlZWdTW1uLn52ccq6mpwWKx8Oijj3b6vNfmPIMHD3bIaVJTUyktLeXrr7822sxm823P279//9seQ0S6P62UEhGn6d27NwMGDGDAgAGEhYVhtVqpq6ujoaHBOOfXX39l9uzZ+Pj44OvrS2RkJKdOnQKuPLY3depUAFxcXIyiVEtLC++++y733XcfvXv3JiwsjNLSUmPMtjuChYWFjB8/HldXV7Zu3QpATk4OwcHBuLq6Mnz4cNLT028af1vsAwYMwNvbG5PJZPwuLS1lyJAhDudv27bNoXCWmJhIWFgYn3zyCf7+/nh7ezNnzhzOnDljnHPt43v+/v6sWbOGuLg4PD098fPz4+OPP3aYp6ysjLCwMFxdXRk9erQxb3dY+i8iItIdNDU1UVhYyMKFC4mIiMBmsxnHYmNjWbx4MbW1tZhMJvz9/fH39wdg+vTpRlub7du3M2rUKFxdXQkICCApKYnm5mbjuMlkIiMjg8jISDw8PFi9erVDLD169HDIacxmMz179jR+Z2Rk8Nhjjzn0+fDDDx1iiI2NZdq0aaSmpmKxWPD19eXVV1/l0qVLxjnXPr5nMpnIyspi+vTpuLu7M2zYMIqLix3mKS4uZtiwYbi5uTFhwgRyc3MxmUz88ccfHfuDi8hdQ0UpEbkjzp49S35+PoGBgfj6+gJw7tw5JkyYgNlsZv/+/Rw4cACz2Ux4eDh///03y5cvN1Za1dfXG3f5NmzYQFpaGqmpqRw5coRJkybx7LPPcuLECYc5rVYrS5YsoaqqikmTJpGZmUl8fDwpKSlUVVWxZs0a3n77bXJzc7vsumtqati2bRslJSWUlJSwb98+3n///Xb7pKWlMXr0aL7//nsWLVrEwoULOXbsGABnzpxh6tSphISEcOjQIZKTk7FarV0Wv4iIiHRcQUEBQUFBBAUFERMTQ05ODq2trcCVPKbt5lp9fT0VFRVUVFQAV26etbUB7Ny5k5iYGJYsWcKPP/7I5s2bsdlspKSkOMz3zjvvEBkZydGjR4mLi+uSa9qzZw81NTXs2bOH3NxcbDabQ7HtRpKSkpg1axZHjhxhypQpREdHc/r0aeDKTcSZM2cybdo0KisrWbBgAfHx8V0Su4j8d6goJSJOU1JSgtlsxmw24+npSXFxMQUFBbi4XPmn6PPPP8fFxYWsrCxCQkIIDg4mJyeH2tpa9u7di9lspk+fPsD/rVqCK8vOrVYrc+bMISgoiLVr1xIWFnbd5pqvv/46UVFRDB06lIEDB5KcnExaWprRFhUVxRtvvMHmzZu77G/Q0tKCzWZjxIgRPP7447zwwgvs3r273T5Tpkxh0aJFBAYGYrVa6devH3v37gUgPz8fk8lEZmYm999/P5MnT2bFihVdFr+IiIh0XHZ2NjExMcCV7QzOnj1r/P/v7e2Np6ensYKpf//+xqNvffr0MdoAUlJSWLlyJS+++CIBAQE89dRTJCcnX5e7zJ07l7i4OAICAq5byd1ZfHx8+Oijjxg+fDgRERE888wzt8xpYmNjef755wkMDGTNmjU0NTVRXl4OQEZGBkFBQaxbt46goCDmzJnTKft1ish/m/aUEhGnmTBhAps2bQLg9OnTpKenM3nyZMrLyxkyZAgHDx7k5MmTeHp6OvS7cOGCsZfUtf766y9+++03xo0b59A+btw4Dh8+7NA2evRo43tDQwN1dXXMmzePV155xWhvbm7G29v7tq6zPf7+/g7XZ7FY+P3339vtExoaanxve2SwrU91dTWhoaG4uroa5zz88MOdHLWIiIj8W9XV1ZSXl1NUVARAz549mT17Nlu2bGHixIkdGuvgwYNUVFQ4rIy6fPkyFy5c4Ny5c7i7uwOOOU9XeeCBB+jRo4fx22KxcPTo0Xb7XJ3TeHh44Onp6ZDTjBkzxuF85TQi3Z+KUiLiNB4eHgQGBhq/R40ahbe3N5mZmaxevZqWlhZGjRpFfn7+dX1vtVnmtZuet7a2Xtfm4eFhfG9paQEgMzOTsWPHOpx3dYL1T7m4uBjL8Ntcva9Cm2s3VzeZTEYsN9Nenxtd57VxiIiIyJ2TnZ1Nc3MzgwYNMtpaW1vp1asXjY2N+Pj4/OOxWlpaSEpKIioq6rpjV9+gujrn6SjlNCLiTCpKicgdYzKZcHFx4fz58wCMHDmSgoIC7rnnHry8vP7RGF5eXgwcOJADBw7wxBNPGO1lZWXt3l279957GTRoED/99BPR0dG3dyFcKZqdOXOGpqYmIxF0xkbjw4cPJz8/n4sXL9K7d28Avvvuuy6fV0RERG6tubmZvLw80tLSePrppx2OzZgxg/z8fF577bUb9u3VqxeXL192aBs5ciTV1dUON/k6W//+/bHb7Q5FImflNDt27HBoU04j0v1pTykRcZqLFy9it9ux2+1UVVWxePFizp49a7xRLzo6mn79+hEZGck333zDzz//zL59+1i6dCm//PLLTcddsWIFa9eupaCggOrqalauXEllZSVLly5tN57ExETee+89NmzYwPHjxzl69Cg5OTmsX7++w9c2duxY3N3deeuttzh58iSffvrpLTf77Axz586lpaWF+fPnU1VVxc6dO0lNTQWuXz0mIiIizlVSUkJjYyPz5s1jxIgRDp+ZM2eSnZ19077+/v7s3r0bu91OY2MjAAkJCeTl5ZGYmMgPP/xAVVUVBQUFrFq1qtNiHj9+PA0NDXzwwQfU1NSwceNGvvrqq04b/2YWLFjAsWPHsFqtHD9+nMLCQiOXUk4j0n2pKCUiTlNaWorFYsFisTB27FgqKir44osvGD9+PADu7u7s378fPz8/oqKiCA4OJi4ujvPnz7e7cmrJkiUsW7aMZcuWERISQmlpqfFK4fa8/PLLZGVlYbPZCAkJ4cknn8RmszF06NAOX1vfvn3ZunUrO3bsICQkhM8++4zExMQOj9NRXl5ebN++ncrKSsLCwoiPjychIQFwXMYvIiIizpednc3EiRNvuF/ljBkzqKys5NChQzfsm5aWxq5duxg8eDAPPfQQAJMmTaKkpIRdu3YxZswYHnnkEdavX9+pm5kHBweTnp7Oxo0befDBBykvL2f58uWdNv7NDB06lC+//JKioiJCQ0PZtGmT8fa9ttXgItL9mFr1oK6ISLeSn5/PSy+9xJ9//ombm9udDkdERETkX0lJSSEjI4O6uro7HYqIdBHtKSUicpfLy8sjICCAQYMGcfjwYaxWK7NmzVJBSkRERO4q6enpjBkzBl9fX7799lvWrVt30z23RKR7UFFKROQuZ7fbSUhIwG63Y7FYeO655xxeFS0iIiJyNzhx4gSrV6/m9OnT+Pn5sWzZMt588807HZaIdCE9viciIiIiIiIiIk6njc5FRERERERERMTpVJQSERERERERERGnU1FKREREREREREScTkUpERERERERERFxOhWlRERERERERETE6VSUEhERERERERERp1NRSkREREREREREnE5FKRERERERERERcToVpURERERERERExOn+B75TRAnKrUM3AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1200x3200 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the comparison of all the results\n",
        "subplots = 4\n",
        "fig, axs = plt.subplots(subplots, 1, figsize=(12, 8 * subplots))\n",
        "\n",
        "# Define colors for each bar\n",
        "before_color = 'skyblue'\n",
        "after_color = 'lightcoral'\n",
        "\n",
        "axs[0].bar(['Before Tuning', 'After Tuning'], [before_f1, after_f1], color=[before_color, after_color])\n",
        "axs[0].set_title('F1 Score Comparison')\n",
        "\n",
        "axs[1].bar(['Before Tuning', 'After Tuning'], [before_precision, after_precision], color=[before_color, after_color])\n",
        "axs[1].set_title('Precision Comparison')\n",
        "\n",
        "axs[2].bar(['Before Tuning', 'After Tuning'], [before_recall, after_recall], color=[before_color, after_color])\n",
        "axs[2].set_title('Recall Comparison')\n",
        "\n",
        "axs[3].bar(['Before Tuning', 'After Tuning'], [before_accuracy, after_accuracy], color=[before_color, after_color])\n",
        "axs[3].set_title('Accuracy Comparison')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfLZECBwI-xV"
      },
      "source": [
        "## SVM: Eric, Moosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdXWC5Pr6wGv"
      },
      "source": [
        "### Data for SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XowPRU4muiu-"
      },
      "outputs": [],
      "source": [
        "df_svm = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHBJiJen6wGv"
      },
      "outputs": [],
      "source": [
        "X = df_svm.drop(columns=['Source of Money'])\n",
        "y = df_svm['Source of Money']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz09tJyW6wGv"
      },
      "source": [
        "### Splitting the data for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG5GOJ9i6wGv"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmlRjiEu6wGv"
      },
      "source": [
        "### SVM Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5UTDI2d6wGv"
      },
      "outputs": [],
      "source": [
        "svc_model = SVC()\n",
        "svc_model.fit(X_train, y_train)\n",
        "y_pred = svc_model.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(f\"SVC Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(classification_report(y_test, y_pred,  zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXveRmRB6wGv"
      },
      "source": [
        "### GridSearchCV (Hyper Parameter tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBFFQc7E6wGv"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning using GridSearchCV\n",
        "# SVM GridSearchCV params\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['rbf'],\n",
        "    'gamma': [1,0.1,0.01,0.001,0.0001]\n",
        "}\n",
        "grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)\n",
        "grid_svm.fit(X_train, y_train)\n",
        "print(f\"Best SVM Parameters: {grid_svm.best_params_}\")\n",
        "print(f\"Best SVM Accuracy: {grid_svm.best_score_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mTwjwTnqDiD"
      },
      "source": [
        "# Conclusion and comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQg4pZMmIX90"
      },
      "source": [
        "Present your work including approach and findings during the class on September 24th or 26th, 2024. Each group will have a maximum of 15 minutes to present their project. It is advised that your PowerPoint files to be no longer than 15 slides.\n",
        "\n",
        "Prepare a written technical report of no longer than 15 pages to discuss the problem statement, various steps conducted, summary of findings and conclusions. Submit the report and the notebook file (with proper headings, explanatory comments and code sections) by the midnight of September 29th, 2024."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
