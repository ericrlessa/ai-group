{"cells":[{"cell_type":"markdown","metadata":{"id":"v52S4rZMHsK3"},"source":["# Term Project Group 1\n"]},{"cell_type":"markdown","metadata":{"id":"ceU4aZcRIdh8"},"source":["The Dataset: https://www.kaggle.com/datasets/waqi786/global-black-money-transactions-dataset"]},{"cell_type":"markdown","metadata":{"id":"ZyyUVrF_IioY"},"source":["### Explanation:\n","\n","This dataset gives a solid overview of black money transactions in different countries, focusing on financial activities tied to illegal dealings. It includes details like transaction amounts and risk scores, making it super useful for anyone looking to study financial crime trends or work on anti-money laundering tools.\n","\n","\n","### Dataset:\n","\n","Transaction ID: Unique identifier for each transaction. (e.g., TX0000001)\n","\n","Country: Country where the transaction occurred. (e.g., USA, China)\n","\n","Amount (USD): Transaction amount in US Dollars. (e.g., 150000.00)\n","\n","Transaction Type: Type of transaction. (e.g., Offshore Transfer, Property Purchase)\n","\n","Date of Transaction: The date and time of the transaction. (e.g., 2022-03-15 14:32:00)\n","\n","Person Involved: Name or identifier of the person/entity involved. (e.g., Person_1234)\n","\n","Industry: Industry associated with the transaction. (e.g., Real Estate, Finance)\n","\n","Destination Country: Country where the money was sent. (e.g., Switzerland)\n","\n","Reported by Authority: Whether the transaction was reported to authorities. (e.g., True/False)\n","\n","Source of Money: Origin of the money. (e.g., Legal, Illegal)\n","\n","Money Laundering Risk Score: Risk score indicating the likelihood of money\n","laundering (1-10). (e.g., 8)\n","\n","Shell Companies Involved: Number of shell companies used in the transaction. (e.g., 3)\n","\n","Financial Institution: Bank or financial institution involved in the transaction. (e.g., Bank_567)\n","\n","Tax Haven Country: Country where the money was transferred to a tax haven. (e.g., Cayman Islands)"]},{"cell_type":"markdown","metadata":{"id":"dFAjkKdlH4LW"},"source":["# Pre-process and clean the dataset as appropriate."]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.svm import SVC\n","# from lightgbm import LGBMClassifier\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.feature_selection import RFE, SelectFromModel, chi2, SelectKBest\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from lightgbm import LGBMClassifier\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from sklearn.model_selection import GridSearchCV\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-bNKeluqIu7D"},"source":["## Exploring the data"]},{"cell_type":"markdown","metadata":{},"source":["### Load Data"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["# Load data\n","df = pd.read_csv('Big_Black_Money_Dataset.csv')"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Transaction ID</th>\n","      <th>Country</th>\n","      <th>Amount (USD)</th>\n","      <th>Transaction Type</th>\n","      <th>Date of Transaction</th>\n","      <th>Person Involved</th>\n","      <th>Industry</th>\n","      <th>Destination Country</th>\n","      <th>Reported by Authority</th>\n","      <th>Source of Money</th>\n","      <th>Money Laundering Risk Score</th>\n","      <th>Shell Companies Involved</th>\n","      <th>Financial Institution</th>\n","      <th>Tax Haven Country</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TX0000000001</td>\n","      <td>Brazil</td>\n","      <td>3.267530e+06</td>\n","      <td>Offshore Transfer</td>\n","      <td>2013-01-01 00:00:00</td>\n","      <td>Person_1101</td>\n","      <td>Construction</td>\n","      <td>USA</td>\n","      <td>True</td>\n","      <td>Illegal</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>Bank_40</td>\n","      <td>Singapore</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TX0000000002</td>\n","      <td>China</td>\n","      <td>4.965767e+06</td>\n","      <td>Stocks Transfer</td>\n","      <td>2013-01-01 01:00:00</td>\n","      <td>Person_7484</td>\n","      <td>Luxury Goods</td>\n","      <td>South Africa</td>\n","      <td>False</td>\n","      <td>Illegal</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>Bank_461</td>\n","      <td>Bahamas</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TX0000000003</td>\n","      <td>UK</td>\n","      <td>9.416750e+04</td>\n","      <td>Stocks Transfer</td>\n","      <td>2013-01-01 02:00:00</td>\n","      <td>Person_3655</td>\n","      <td>Construction</td>\n","      <td>Switzerland</td>\n","      <td>True</td>\n","      <td>Illegal</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>Bank_387</td>\n","      <td>Switzerland</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TX0000000004</td>\n","      <td>UAE</td>\n","      <td>3.864201e+05</td>\n","      <td>Cash Withdrawal</td>\n","      <td>2013-01-01 03:00:00</td>\n","      <td>Person_3226</td>\n","      <td>Oil &amp; Gas</td>\n","      <td>Russia</td>\n","      <td>False</td>\n","      <td>Illegal</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>Bank_353</td>\n","      <td>Panama</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TX0000000005</td>\n","      <td>South Africa</td>\n","      <td>6.433784e+05</td>\n","      <td>Cryptocurrency</td>\n","      <td>2013-01-01 04:00:00</td>\n","      <td>Person_7975</td>\n","      <td>Real Estate</td>\n","      <td>USA</td>\n","      <td>True</td>\n","      <td>Illegal</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>Bank_57</td>\n","      <td>Luxembourg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  Transaction ID       Country  Amount (USD)   Transaction Type  \\\n","0   TX0000000001        Brazil  3.267530e+06  Offshore Transfer   \n","1   TX0000000002         China  4.965767e+06    Stocks Transfer   \n","2   TX0000000003            UK  9.416750e+04    Stocks Transfer   \n","3   TX0000000004           UAE  3.864201e+05    Cash Withdrawal   \n","4   TX0000000005  South Africa  6.433784e+05     Cryptocurrency   \n","\n","   Date of Transaction Person Involved      Industry Destination Country  \\\n","0  2013-01-01 00:00:00     Person_1101  Construction                 USA   \n","1  2013-01-01 01:00:00     Person_7484  Luxury Goods        South Africa   \n","2  2013-01-01 02:00:00     Person_3655  Construction         Switzerland   \n","3  2013-01-01 03:00:00     Person_3226     Oil & Gas              Russia   \n","4  2013-01-01 04:00:00     Person_7975   Real Estate                 USA   \n","\n","   Reported by Authority Source of Money  Money Laundering Risk Score  \\\n","0                   True         Illegal                            6   \n","1                  False         Illegal                            9   \n","2                   True         Illegal                            1   \n","3                  False         Illegal                            7   \n","4                   True         Illegal                            1   \n","\n","   Shell Companies Involved Financial Institution Tax Haven Country  \n","0                         1               Bank_40         Singapore  \n","1                         0              Bank_461           Bahamas  \n","2                         3              Bank_387       Switzerland  \n","3                         2              Bank_353            Panama  \n","4                         9               Bank_57        Luxembourg  "]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["# View the first few rows\n","df.head()"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["Transaction ID                 0\n","Country                        0\n","Amount (USD)                   0\n","Transaction Type               0\n","Date of Transaction            0\n","Person Involved                0\n","Industry                       0\n","Destination Country            0\n","Reported by Authority          0\n","Source of Money                0\n","Money Laundering Risk Score    0\n","Shell Companies Involved       0\n","Financial Institution          0\n","Tax Haven Country              0\n","dtype: int64"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["# Check for missing values\n","df.isnull().sum()"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["Transaction ID                  object\n","Country                         object\n","Amount (USD)                   float64\n","Transaction Type                object\n","Date of Transaction             object\n","Person Involved                 object\n","Industry                        object\n","Destination Country             object\n","Reported by Authority             bool\n","Source of Money                 object\n","Money Laundering Risk Score      int64\n","Shell Companies Involved         int64\n","Financial Institution           object\n","Tax Haven Country               object\n","dtype: object"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# Get data types\n","df.dtypes"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Amount (USD)</th>\n","      <th>Money Laundering Risk Score</th>\n","      <th>Shell Companies Involved</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1.000000e+04</td>\n","      <td>10000.000000</td>\n","      <td>10000.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>2.501818e+06</td>\n","      <td>5.526400</td>\n","      <td>4.469400</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>1.424364e+06</td>\n","      <td>2.893603</td>\n","      <td>2.879773</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.003180e+04</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.279005e+06</td>\n","      <td>3.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>2.501310e+06</td>\n","      <td>6.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>3.722416e+06</td>\n","      <td>8.000000</td>\n","      <td>7.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>4.999812e+06</td>\n","      <td>10.000000</td>\n","      <td>9.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Amount (USD)  Money Laundering Risk Score  Shell Companies Involved\n","count  1.000000e+04                 10000.000000              10000.000000\n","mean   2.501818e+06                     5.526400                  4.469400\n","std    1.424364e+06                     2.893603                  2.879773\n","min    1.003180e+04                     1.000000                  0.000000\n","25%    1.279005e+06                     3.000000                  2.000000\n","50%    2.501310e+06                     6.000000                  4.000000\n","75%    3.722416e+06                     8.000000                  7.000000\n","max    4.999812e+06                    10.000000                  9.000000"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{},"source":["## Processing the data"]},{"cell_type":"markdown","metadata":{},"source":["### Handle missing values if applicable"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# For numerical features\n","numerical_features = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n","imputer = SimpleImputer(strategy='median')\n","df[numerical_features] = imputer.fit_transform(df[numerical_features])\n","\n","# For categorical features\n","categorical_features = ['Country', 'Transaction Type', 'Person Involved', 'Industry',\n","                        'Destination Country', 'Financial Institution', 'Tax Haven Country']\n","imputer_cat = SimpleImputer(strategy='most_frequent')\n","df[categorical_features] = imputer_cat.fit_transform(df[categorical_features])"]},{"cell_type":"markdown","metadata":{},"source":["### Dropping Features and OHE"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# Drop Irrelevant Features\n","df.drop('Transaction ID', axis=1, inplace=True) # Dropped because it is unique for each transaction\n","# df.drop('Person Involved', axis=1, inplace=True) # Frequency Encoding will be implemented\n","# df.drop('Financial Institution', axis=1, inplace=True) # Implement Frequency Encoding\n","df.drop('Date of Transaction', axis=1, inplace=True) # Date of transaction is not relevant\n","\n","# Convert 'Reported by Authority' to integer\n","df['Reported by Authority'] = df['Reported by Authority'].astype(int)\n","\n","# Frequency encoding for 'Financial Institution'\n","df['Financial Institution'] = df.groupby('Financial Institution')['Financial Institution'].transform('count')\n","\n","# Frequency encoding for 'Person Involved'\n","df['Person Involved'] = df.groupby('Person Involved')['Person Involved'].transform('count')\n","\n","# Encode target variable\n","le = LabelEncoder()\n","df['Source of Money'] = le.fit_transform(df['Source of Money'])\n","\n","# One-Hot Encode nominal categorical features\n","nominal_features = ['Country', 'Transaction Type', 'Industry',\n","                    'Destination Country', 'Tax Haven Country']\n","df = pd.get_dummies(df, columns=nominal_features, drop_first=True)\n","\n","dummy_columns = df.filter(like='_').columns\n","df[dummy_columns] = df[dummy_columns].astype(int)\n","\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n","       'Source of Money', 'Money Laundering Risk Score',\n","       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n","       'Country_India', 'Country_Russia', 'Country_Singapore',\n","       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n","       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n","       'Transaction Type_Offshore Transfer',\n","       'Transaction Type_Property Purchase',\n","       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n","       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n","       'Industry_Oil & Gas', 'Industry_Real Estate',\n","       'Destination Country_China', 'Destination Country_India',\n","       'Destination Country_Russia', 'Destination Country_Singapore',\n","       'Destination Country_South Africa', 'Destination Country_Switzerland',\n","       'Destination Country_UAE', 'Destination Country_UK',\n","       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n","       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n","       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n","      dtype='object')"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["df.columns"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Amount (USD)</th>\n","      <th>Person Involved</th>\n","      <th>Reported by Authority</th>\n","      <th>Source of Money</th>\n","      <th>Money Laundering Risk Score</th>\n","      <th>Shell Companies Involved</th>\n","      <th>Financial Institution</th>\n","      <th>Country_China</th>\n","      <th>Country_India</th>\n","      <th>Country_Russia</th>\n","      <th>...</th>\n","      <th>Destination Country_South Africa</th>\n","      <th>Destination Country_Switzerland</th>\n","      <th>Destination Country_UAE</th>\n","      <th>Destination Country_UK</th>\n","      <th>Destination Country_USA</th>\n","      <th>Tax Haven Country_Cayman Islands</th>\n","      <th>Tax Haven Country_Luxembourg</th>\n","      <th>Tax Haven Country_Panama</th>\n","      <th>Tax Haven Country_Singapore</th>\n","      <th>Tax Haven Country_Switzerland</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3.267530e+06</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6.0</td>\n","      <td>1.0</td>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4.965767e+06</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>24</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>9.416750e+04</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>3.0</td>\n","      <td>12</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.864201e+05</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.0</td>\n","      <td>2.0</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6.433784e+05</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1.0</td>\n","      <td>9.0</td>\n","      <td>19</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 40 columns</p>\n","</div>"],"text/plain":["   Amount (USD)  Person Involved  Reported by Authority  Source of Money  \\\n","0  3.267530e+06                2                      1                0   \n","1  4.965767e+06                1                      0                0   \n","2  9.416750e+04                1                      1                0   \n","3  3.864201e+05                5                      0                0   \n","4  6.433784e+05                4                      1                0   \n","\n","   Money Laundering Risk Score  Shell Companies Involved  \\\n","0                          6.0                       1.0   \n","1                          9.0                       0.0   \n","2                          1.0                       3.0   \n","3                          7.0                       2.0   \n","4                          1.0                       9.0   \n","\n","   Financial Institution  Country_China  Country_India  Country_Russia  ...  \\\n","0                     17              0              0               0  ...   \n","1                     24              1              0               0  ...   \n","2                     12              0              0               0  ...   \n","3                     18              0              0               0  ...   \n","4                     19              0              0               0  ...   \n","\n","   Destination Country_South Africa  Destination Country_Switzerland  \\\n","0                                 0                                0   \n","1                                 1                                0   \n","2                                 0                                1   \n","3                                 0                                0   \n","4                                 0                                0   \n","\n","   Destination Country_UAE  Destination Country_UK  Destination Country_USA  \\\n","0                        0                       0                        1   \n","1                        0                       0                        0   \n","2                        0                       0                        0   \n","3                        0                       0                        0   \n","4                        0                       0                        1   \n","\n","   Tax Haven Country_Cayman Islands  Tax Haven Country_Luxembourg  \\\n","0                                 0                             0   \n","1                                 0                             0   \n","2                                 0                             0   \n","3                                 0                             0   \n","4                                 0                             1   \n","\n","   Tax Haven Country_Panama  Tax Haven Country_Singapore  \\\n","0                         0                            1   \n","1                         0                            0   \n","2                         0                            0   \n","3                         1                            0   \n","4                         0                            0   \n","\n","   Tax Haven Country_Switzerland  \n","0                              0  \n","1                              0  \n","2                              1  \n","3                              0  \n","4                              0  \n","\n","[5 rows x 40 columns]"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/plain":["Source of Money\n","0    7017\n","1    2983\n","Name: count, dtype: int64"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["df['Source of Money'].value_counts()"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["features_to_modify = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n","\n","def scale_features(df, features):\n","    df_S = df.copy()\n","    scaler = StandardScaler()\n","    df_S[features] = scaler.fit_transform(df[features])\n","    return df_S\n","\n","def normalize_features(df, features):\n","    df_N = df.copy()\n","    scaler = MinMaxScaler()\n","    df_N[features] = scaler.fit_transform(df[features])\n","    return df_N\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Biased data correction"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["def Undersampling(X,Y, test_size):\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = test_size, random_state=0)\n","    rus = RandomUnderSampler(random_state=0)\n","    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n","    return X_resampled, X_test, y_resampled, y_test"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Selectors (Optional):"]},{"cell_type":"markdown","metadata":{},"source":["### Feature selector functions"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["def cor_selector(X, y,num_feats):\n","    # Your code goes here (Multiple lines)\n","    cor_list = []\n","    feature_name = X.columns.tolist()\n","    for i in feature_name:\n","        cor = np.corrcoef(X[i], y)[0, 1]\n","        cor_list.append(cor)\n","    #print(np.argsort(np.abs(cor_list)))\n","    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n","    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n","    #print(cor_feature)\n","    cor_support = [True if i in cor_feature else False for i in feature_name]\n","    # Your code ends here\n","    return cor_support, cor_feature\n","\n","def chi_squared_selector(X, y, num_feats):\n","    # Your code goes here (Multiple lines)\n","    X_norm = MinMaxScaler().fit_transform(X)\n","    chi_selector = SelectKBest(chi2, k=num_feats)\n","    chi_selector.fit(X_norm, y)\n","    chi_support = chi_selector.get_support()\n","    #print(chi_support)\n","    chi_feature = X.loc[:,chi_support].columns.tolist()\n","    # Your code ends here\n","    return chi_support, chi_feature\n","\n","def rfe_selector(X, y, num_feats):\n","    # Your code goes here (Multiple lines)\n","    rfe_selector = RFE(estimator=LogisticRegression(random_state=42), n_features_to_select=num_feats, step=10, verbose=5)\n","    rfe_selector.fit(X, y)\n","    rfe_support = rfe_selector.support_\n","    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n","    # Your code ends here\n","    return rfe_support, rfe_feature\n","\n","def embedded_log_reg_selector(X, y, num_feats):\n","    # Your code goes here (Multiple lines)\n","    embedded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\", random_state = 42), max_features=num_feats)\n","    embedded_lr_selector.fit(X, y)\n","    embedded_lr_support = embedded_lr_selector.get_support()\n","    embedded_lr_feature = X.loc[:,embedded_lr_support].columns.tolist()\n","    # Your code ends here\n","    return embedded_lr_support, embedded_lr_feature\n","\n","def embedded_rf_selector(X, y, num_feats):\n","    # Your code goes here (Multiple lines)\n","    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), max_features=num_feats)\n","    embeded_rf_selector.fit(X, y)\n","    embedded_rf_support = embeded_rf_selector.get_support()\n","    embedded_rf_feature = X.loc[:,embedded_rf_support].columns.tolist()\n","    # Your code ends here\n","    return embedded_rf_support, embedded_rf_feature\n","\n","def embedded_lgbm_selector(X, y, num_feats):\n","    # Your code goes here (Multiple lines)\n","    lgbc = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n","    embeded_lgbm_selector = SelectFromModel(lgbc, max_features=num_feats)\n","    embeded_lgbm_selector.fit(X, y)\n","    embedded_lgbm_support = embeded_lgbm_selector.get_support()\n","    embedded_lgbm_feature = X.loc[:,embedded_lgbm_support].columns.tolist()\n","    # Your code ends here\n","    return embedded_lgbm_support, embedded_lgbm_feature"]},{"cell_type":"markdown","metadata":{},"source":["### Feature Selectors Combined:"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["def autoFeatureSelector(X, y, num_feats, methods=[]):\n","\n","    support_dict = {}\n","\n","    feature_name = list(X.columns)\n","    support_dict['Feature'] = feature_name\n","    \n","    if 'pearson' in methods:\n","        cor_support, cor_feature = cor_selector(X, y, num_feats)\n","        support_dict['Pearson'] = cor_support\n","    if 'chi-square' in methods:\n","        chi_support, chi_feature = chi_squared_selector(X, y, num_feats)\n","        support_dict['Chi-2'] = chi_support\n","    if 'rfe' in methods:\n","        rfe_support, rfe_feature = rfe_selector(X, y, num_feats)\n","        support_dict['RFE'] = rfe_support\n","    if 'log-reg' in methods:\n","        embedded_lr_support, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)\n","        support_dict['Logistics'] = embedded_lr_support\n","    if 'rf' in methods:\n","        embedded_rf_support, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)\n","        support_dict['Random Forest'] = embedded_rf_support\n","    if 'lgbm' in methods:\n","        embedded_lgbm_support, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)\n","        support_dict['LightGBM'] = embedded_lgbm_support \n","    \n","    # Combine all the above feature list and count the maximum set of features that got selected by all methods\n","    \n","    print(\"Combining all methods\")\n","    feature_selection_df = pd.DataFrame(support_dict)\n","    feature_selection_df['Total'] = feature_selection_df.apply(lambda row: np.sum(row[1:].astype(int)), axis=1)\n","    print(\"Sorting features\")\n","    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n","    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n","    print(\"Selecting best features\")\n","    best_features = feature_selection_df['Feature'].tolist()[:num_feats]\n","    return best_features, feature_selection_df"]},{"cell_type":"markdown","metadata":{},"source":["# Models:"]},{"cell_type":"markdown","metadata":{},"source":["- Utilize GridSearchCV to tune the parameters of each of the models.\n","- Check if better results can be obtained for any of the models.\n","- Discuss your observations regarding model performance.\n","- Randomly remove some features (or based on a certain hypothesis) and re-evaluate the models.\n","- Document your observations concerning model performances."]},{"cell_type":"markdown","metadata":{"id":"sPCRye0hI5va"},"source":["## Logistic Regression: Saif, Dwip\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data for LR"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"QXrNCYAEuQ1i"},"outputs":[{"data":{"text/plain":["Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n","       'Source of Money', 'Money Laundering Risk Score',\n","       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n","       'Country_India', 'Country_Russia', 'Country_Singapore',\n","       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n","       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n","       'Transaction Type_Offshore Transfer',\n","       'Transaction Type_Property Purchase',\n","       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n","       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n","       'Industry_Oil & Gas', 'Industry_Real Estate',\n","       'Destination Country_China', 'Destination Country_India',\n","       'Destination Country_Russia', 'Destination Country_Singapore',\n","       'Destination Country_South Africa', 'Destination Country_Switzerland',\n","       'Destination Country_UAE', 'Destination Country_UK',\n","       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n","       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n","       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n","      dtype='object')"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["# Get the data for Logistic Regression\n","df_LR = df.copy()\n","\n","# Normalize the variables that are greater than 1 as this will affect the models proformnce\n","df_LR = normalize_features(df_LR, ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved', 'Financial Institution', 'Person Involved'])\n","\n","df_LR.columns"]},{"cell_type":"markdown","metadata":{},"source":["### Simple LR Model"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression\n","[[1406    0]\n"," [ 594    0]]\n","              precision    recall  f1-score   support\n","\n","           0       0.70      1.00      0.83      1406\n","           1       0.00      0.00      0.00       594\n","\n","    accuracy                           0.70      2000\n","   macro avg       0.35      0.50      0.41      2000\n","weighted avg       0.49      0.70      0.58      2000\n","\n","0.703\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["# Implement a logistic regression model\n","X = df_LR.drop('Source of Money', axis=1)\n","Y = df_LR['Source of Money']\n","\n","# Train-Test Split\n","X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n","\n","# Fit the model\n","log_reg = LogisticRegression()\n","log_reg.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = log_reg.predict(X_test)\n","\n","# Evaluate\n","print(\"Logistic Regression\")\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","print(accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["#### Using Undersampling to train the LR Model:"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Logistic Regression\n","[[696 710]\n"," [298 296]]\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.50      0.58      1406\n","           1       0.29      0.50      0.37       594\n","\n","    accuracy                           0.50      2000\n","   macro avg       0.50      0.50      0.47      2000\n","weighted avg       0.58      0.50      0.52      2000\n","\n","0.496\n"]}],"source":["X_train, X_test, y_train, y_test = Undersampling(X, Y, 0.2)\n","\n","# Fit the model\n","log_reg = LogisticRegression()\n","log_reg.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = log_reg.predict(X_test)\n","\n","# Evaluate\n","print(\"Logistic Regression\")\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","print(accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{},"source":["### GridSearchCV LR"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'C': 10, 'penalty': 'l2'}\n","0.4886989857390085\n","LogisticRegression(C=10)\n","Logistic Regression\n","[[698 708]\n"," [296 298]]\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.50      0.58      1406\n","           1       0.30      0.50      0.37       594\n","\n","    accuracy                           0.50      2000\n","   macro avg       0.50      0.50      0.48      2000\n","weighted avg       0.58      0.50      0.52      2000\n","\n","0.498\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n","30 fits failed out of a total of 60.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","30 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1474, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.48346798        nan 0.48451072        nan 0.48702447\n","        nan 0.48869899        nan 0.48848978        nan 0.48848978]\n","  warnings.warn(\n"]}],"source":["# Define the hyperparameters\n","param_grid = {\n","    'penalty': ['l1', 'l2'],\n","    'C': [0.01, 0.1, 1, 10, 100, 1000]\n","}\n","\n","# Instantiate GridSearchCV\n","grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5)\n","\n","# Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters\n","print(grid_search.best_params_)\n","print(grid_search.best_score_)\n","print(grid_search.best_estimator_)\n","# Get the best model\n","best_model = grid_search.best_estimator_\n","\n","# Predict\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate\n","print(\"Logistic Regression\")\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","print(accuracy_score(y_test, y_pred))\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Modifying features and testing proformance"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'C': 0.01, 'penalty': 'l2'}\n","0.5002383749247915\n","LogisticRegression(C=0.01, max_iter=1000)\n","Logistic Regression\n","[[1030 1075]\n"," [ 460  435]]\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.49      0.57      2105\n","           1       0.29      0.49      0.36       895\n","\n","    accuracy                           0.49      3000\n","   macro avg       0.49      0.49      0.47      3000\n","weighted avg       0.57      0.49      0.51      3000\n","\n","0.48833333333333334\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n","30 fits failed out of a total of 60.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","30 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1474, in wrapper\n","    return fit_method(estimator, *args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n","    solver = _check_solver(self.solver, self.penalty, self.dual)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n","    raise ValueError(\n","ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [       nan 0.50023837        nan 0.49377618        nan 0.48970232\n","        nan 0.49089992        nan 0.4906604         nan 0.4906604 ]\n","  warnings.warn(\n"]}],"source":["df_LR_Mod = df_LR.copy()\n","\n","# Remove the features that are not important\n","df_LR_Mod = df_LR_Mod.drop(['Person Involved', 'Financial Institution'], axis=1)\n","\n","X = df_LR_Mod.drop('Source of Money', axis=1)\n","Y = df_LR_Mod['Source of Money']\n","\n","X_train, X_test, y_train, y_test = Undersampling(X, Y, 0.3)\n","\n","# Define the hyperparameters\n","param_grid = {\n","    'penalty': ['l1', 'l2'],\n","    'C': [0.01, 0.1, 1, 10, 100, 1000]\n","}\n","\n","# Instantiate GridSearchCV\n","grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=1000), param_grid=param_grid, cv=5)\n","\n","# Fit the model\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best parameters\n","print(grid_search.best_params_)\n","print(grid_search.best_score_)\n","print(grid_search.best_estimator_)\n","# Get the best model\n","best_model = grid_search.best_estimator_\n","\n","# Predict\n","y_pred = best_model.predict(X_test)\n","\n","# Evaluate\n","print(\"Logistic Regression\")\n","print(confusion_matrix(y_test, y_pred))\n","print(classification_report(y_test, y_pred))\n","print(accuracy_score(y_test, y_pred))"]},{"cell_type":"markdown","metadata":{"id":"oWV5cWNBI9xr"},"source":["## Decision Tree: Nitish, Sehaj"]},{"cell_type":"markdown","metadata":{},"source":["### Data for DT Model"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["df_DT = df.copy()\n","# Drop person involved\n","df_DT.drop('Person Involved', axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{},"source":["### DT Model"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Accuracy: 0.6715\n"]}],"source":["# Splitting the data into features (X) and target (y)\n","X = df_DT.drop('Source of Money', axis=1)\n","y = df_DT['Source of Money']\n","\n","# Split data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Model training using RandomForestClassifier\n","clf = DecisionTreeClassifier(random_state=42, max_depth=10) # Changed\n","clf.fit(X_train, y_train)\n","\n","# Predictions and accuracy score\n","y_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","print(f\"Model Accuracy: {accuracy:.4f}\")"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","[[1307   82]\n"," [ 575   36]]\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.94      0.80      1389\n","           1       0.31      0.06      0.10       611\n","\n","    accuracy                           0.67      2000\n","   macro avg       0.50      0.50      0.45      2000\n","weighted avg       0.58      0.67      0.59      2000\n","\n"]}],"source":["conf_matrix = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["feature_importances = clf.feature_importances_\n","feature_names = X.columns\n","importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n","\n","# Get top 10 features\n","top_features = importance_df.sort_values(by='importance', ascending=False).head(10)\n","\n","#\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x='importance', y='feature', data=top_features)\n","plt.title('Top 10 Feature Importances - Optimized Decision Tree')\n","plt.show()"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 2000x1000 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Fit a single decision tree to visualize\n","tree_clf = DecisionTreeClassifier(max_depth=4)  # Limit depth for clarity\n","tree_clf.fit(X_train, y_train)\n","\n","# Plot the decision tree\n","plt.figure(figsize=(20, 10))\n","plot_tree(tree_clf, feature_names=X.columns, class_names=['Illegal', 'Legal'], filled=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XMxwAy9DI-Gj"},"source":["## Random Forest: Egor, Ash"]},{"cell_type":"markdown","metadata":{},"source":["### Data for RF Model"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"CHpdHftWuY9G"},"outputs":[],"source":["# Get the data for RF model\n","df_RF = df.copy()"]},{"cell_type":"markdown","metadata":{},"source":["### This needs to be reviewed RF X and Y???"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix:\n","[[ 183 1066]\n"," [ 253 1498]]\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.42      0.15      0.22      1249\n","           1       0.58      0.86      0.69      1751\n","\n","    accuracy                           0.56      3000\n","   macro avg       0.50      0.50      0.46      3000\n","weighted avg       0.52      0.56      0.50      3000\n","\n"]}],"source":["# Define features (X) and target (y) - binary classification on 'Money Laundering Risk Score'\n","X_new = df_RF.drop(columns=['Money Laundering Risk Score'])\n","y_new = (df_RF['Money Laundering Risk Score'] >= 5).astype(int)  # Binary target: 1 if score >= 5, else 0\n","\n","# Split the dataset into training and testing sets\n","X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n","\n","# Initialize and train the RandomForest classifier\n","rf_clf_new = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_clf_new.fit(X_train_new, y_train_new)\n","\n","# Make predictions on the test set\n","y_pred_new = rf_clf_new.predict(X_test_new)\n","\n","# Generate the confusion matrix and classification report\n","conf_matrix_new = confusion_matrix(y_test_new, y_pred_new)\n","class_report_new = classification_report(y_test_new, y_pred_new)\n","\n","# Display the results\n","print(\"Confusion Matrix:\")\n","print(conf_matrix_new)\n","print(\"\\nClassification Report:\")\n","print(class_report_new)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 3 folds for each of 32 candidates, totalling 96 fits\n","Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n","Confusion Matrix:\n"," [[   5 1244]\n"," [   4 1747]]\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.56      0.00      0.01      1249\n","           1       0.58      1.00      0.74      1751\n","\n","    accuracy                           0.58      3000\n","   macro avg       0.57      0.50      0.37      3000\n","weighted avg       0.57      0.58      0.43      3000\n","\n"]}],"source":["# Split the dataset into training and testing sets\n","X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n","\n","# Define a simplified parameter grid for GridSearchCV\n","simple_param_grid = {\n","    'n_estimators': [100, 200],\n","    'max_depth': [10, 20],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2],\n","    'bootstrap': [True, False]\n","}\n","\n","# Initialize the RandomForest model\n","rf_clf_simplified = RandomForestClassifier(random_state=42)\n","\n","# Set up the GridSearchCV\n","simple_grid_search = GridSearchCV(estimator=rf_clf_simplified,\n","                                  param_grid=simple_param_grid,\n","                                  cv=3,  # 3-fold cross-validation\n","                                  verbose=1,\n","                                  n_jobs=-1)\n","\n","# Fit the simplified grid search model\n","simple_grid_search.fit(X_train_new, y_train_new)\n","\n","# Best hyperparameters from the grid search\n","best_params_simplified = simple_grid_search.best_params_\n","\n","# Train the best model on the training set\n","best_rf_model_simplified = simple_grid_search.best_estimator_\n","best_rf_model_simplified.fit(X_train_new, y_train_new)\n","\n","# Make predictions with the tuned model\n","y_pred_tuned_simplified = best_rf_model_simplified.predict(X_test_new)\n","\n","# Generate confusion matrix and classification report for the tuned model\n","conf_matrix_tuned_simplified = confusion_matrix(y_test_new, y_pred_tuned_simplified)\n","class_report_tuned_simplified = classification_report(y_test_new, y_pred_tuned_simplified)\n","\n","# Output best parameters, confusion matrix, and classification report\n","print(\"Best Hyperparameters:\", best_params_simplified)\n","print(\"Confusion Matrix:\\n\", conf_matrix_tuned_simplified)\n","print(\"Classification Report:\\n\", class_report_tuned_simplified)"]},{"cell_type":"markdown","metadata":{"id":"hiehsO8sI-ch"},"source":["## SGD: Devanshi, James, Abraham"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"lvQ8IYnXueAV"},"outputs":[],"source":["# Stochastic Gradient Descent\n","from sklearn.pipeline import make_pipeline\n","#SGD\n","df_SGD = df.copy()\n","df_SGD_S = scale_features(df_SGD, features_to_modify)\n","df_SGD_N = normalize_features(df_SGD, features_to_modify)\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Reports:\n","              precision    recall  f1-score   support\n","\n","           0       0.70      1.00      0.83      1406\n","           1       1.00      0.00      0.00       594\n","\n","    accuracy                           0.70      2000\n","   macro avg       0.85      0.50      0.41      2000\n","weighted avg       0.79      0.70      0.58      2000\n","\n","Confusion Matrix:\n","[[1406    0]\n"," [ 593    1]]\n","Classification Reports:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.32      0.44      1406\n","           1       0.30      0.69      0.42       594\n","\n","    accuracy                           0.43      2000\n","   macro avg       0.50      0.50      0.43      2000\n","weighted avg       0.59      0.43      0.43      2000\n","\n","Confusion Matrix:\n","[[448 958]\n"," [184 410]]\n"]},{"data":{"text/plain":["\"\\nBased on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\\nit has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\\n\""]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["df_list = [df_SGD_S, df_SGD_N]\n","cr_list = []\n","cm_list = []\n","\n","# Search for the best df for standardization and normalization\n","for i in range(len(df_list)):\n","    df = df_list[i]\n","    X = df.drop('Source of Money', axis=1)\n","    y = df['Source of Money'].values\n","    X_resampled, X_test, y_resampled, y_test = Undersampling(X, y, test_size=0.2)\n","    SGD_classifier = SGDClassifier(random_state=0)\n","    SGD_classifier.fit(X_resampled, y_resampled)\n","    y_pred_lr_b = SGD_classifier.predict(X_test)\n","    lr_cr = classification_report(y_test, y_pred_lr_b)\n","    lr_cm = confusion_matrix(y_test, y_pred_lr_b)\n","    cr_list.append(lr_cr)\n","    cm_list.append(lr_cm)\n","\n","# Print the classification reports and confusion matrices\n","for i in range(len(cr_list)):\n","    print(\"Classification Reports:\")\n","    print(cr_list[i])\n","    print('Confusion Matrix:')\n","    print(cm_list[i])\n","\n","'''\n","Based on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\n","it has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\n","'''"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n","540 fits failed out of a total of 2700.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","72 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'hinge', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive', 'log_loss', 'huber', 'squared_epsilon_insensitive'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","77 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'epsilon_insensitive', 'modified_huber', 'squared_hinge', 'huber', 'squared_epsilon_insensitive', 'hinge', 'squared_error', 'log_loss'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","57 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'huber', 'squared_epsilon_insensitive', 'modified_huber', 'squared_hinge', 'epsilon_insensitive', 'hinge', 'log_loss', 'perceptron'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","59 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'squared_hinge', 'log_loss', 'epsilon_insensitive', 'hinge', 'modified_huber', 'perceptron', 'squared_error', 'huber'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","71 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_error', 'log_loss', 'epsilon_insensitive', 'squared_hinge', 'modified_huber', 'squared_epsilon_insensitive', 'perceptron', 'huber'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","95 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_hinge', 'perceptron', 'huber', 'squared_epsilon_insensitive', 'modified_huber', 'log_loss', 'epsilon_insensitive', 'squared_error'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","63 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'squared_hinge', 'huber', 'log_loss', 'modified_huber', 'squared_epsilon_insensitive', 'perceptron', 'epsilon_insensitive', 'hinge'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","46 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'hinge', 'modified_huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive', 'huber', 'squared_hinge', 'perceptron'}. Got 'log' instead.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.49728055 0.49728055 0.50083704        nan        nan        nan\n"," 0.4962354  0.49937282 0.50083682 0.50041885 0.49979036 0.50000022\n"," 0.50104712 0.50146553 0.49790576 0.49937173 0.49979058 0.4991623\n","        nan        nan        nan 0.48828167 0.47990624 0.48723455\n"," 0.50418169 0.50397249 0.50397249 0.4966525  0.49916296 0.50020942\n"," 0.48450963 0.48262591 0.48639423        nan        nan        nan\n"," 0.48995268 0.49330369 0.49204605 0.49204868 0.49100177 0.49037372\n"," 0.49497733 0.49016013 0.49874937 0.49728055 0.49728055 0.50083704\n","        nan        nan        nan 0.4962354  0.49937282 0.50083682\n"," 0.50041885 0.49979036 0.50000022 0.50104712 0.50146553 0.49790576\n"," 0.49330478 0.49120682 0.49706982        nan        nan        nan\n"," 0.4939267  0.49560297 0.49727749 0.48325484 0.48598151 0.49393218\n"," 0.49958115 0.49937282 0.4981154  0.48702469 0.48576508 0.48200026\n","        nan        nan        nan 0.49225766 0.48974392 0.48932704\n"," 0.49769787 0.49811628 0.49811672 0.50251309 0.49267476 0.50356393\n"," 0.49728055 0.49728055 0.50083704        nan        nan        nan\n"," 0.4962354  0.49937282 0.50083682 0.50041885 0.49979036 0.50000022\n"," 0.50104712 0.50146553 0.49790576 0.49979079 0.49204232 0.50020921\n","        nan        nan        nan 0.49958115 0.49979079 0.49874346\n"," 0.49790488 0.4953942  0.49267279 0.50230104 0.50083638 0.50271923\n"," 0.49016364 0.4861848  0.4893231         nan        nan        nan\n"," 0.48974019 0.4918364  0.48430261 0.49246336 0.48828189 0.49623212\n"," 0.50104778 0.49016189 0.49790466 0.49393305 0.50313808 0.498536\n","        nan        nan        nan 0.49958115 0.50083704 0.50062762\n"," 0.4964468  0.4905849  0.49539946 0.49644351 0.49916274 0.49393043\n"," 0.49937173 0.49979058 0.49937173        nan        nan        nan\n"," 0.48849088 0.48430152 0.4817915  0.50271616 0.50292558 0.5027166\n"," 0.49853096 0.50041841 0.50313917 0.48534689 0.48618568 0.48597516\n","        nan        nan        nan 0.48953668 0.49141646 0.49204758\n"," 0.49456242 0.49162961 0.49456242 0.50084033 0.48932989 0.49309382\n"," 0.49393305 0.50313808 0.498536          nan        nan        nan\n"," 0.49958115 0.50083704 0.50062762 0.4964468  0.4905849  0.49539946\n"," 0.49644351 0.49916274 0.49393043 0.4983257  0.48932112 0.49623037\n","        nan        nan        nan 0.4939267  0.49162501 0.49518215\n"," 0.49413985 0.4989531  0.48890929 0.5        0.49769634 0.49979101\n"," 0.49058227 0.48995159 0.48639444        nan        nan        nan\n"," 0.48911345 0.48890556 0.49037219 0.50376744 0.49853513 0.50460361\n"," 0.50523451 0.49037569 0.49099915 0.49393305 0.50313808 0.498536\n","        nan        nan        nan 0.49958115 0.50083704 0.50062762\n"," 0.4964468  0.4905849  0.49539946 0.49644351 0.49916274 0.49393043\n"," 0.5        0.50020942 0.4976959         nan        nan        nan\n"," 0.49958115 0.50020833 0.49979123 0.50251046 0.49267279 0.49811715\n"," 0.49874521 0.49727793 0.50083726 0.48157988 0.49162567 0.48534908\n","        nan        nan        nan 0.48848518 0.4895334  0.48953405\n"," 0.4951839  0.48870096 0.4912112  0.50041819 0.4966536  0.48702381\n"," 0.49958115 0.49979058 0.49246117        nan        nan        nan\n"," 0.49372078 0.49330215 0.49350676 0.49183816 0.50250936 0.49121032\n"," 0.49560341 0.50020942 0.49958137 0.4991623  0.5        0.50041885\n","        nan        nan        nan 0.48995487 0.49811584 0.48430196\n"," 0.50334597 0.50208745 0.49811102 0.50188263 0.49832461 0.49916165\n"," 0.4945541  0.49832636 0.49204123        nan        nan        nan\n"," 0.49204473 0.48848628 0.48911674 0.5016743  0.49246621 0.50146553\n"," 0.50272339 0.50062827 0.4964433  0.49958115 0.49979058 0.49246117\n","        nan        nan        nan 0.49372078 0.49330215 0.49350676\n"," 0.49183816 0.50250936 0.49121032 0.49560341 0.50020942 0.49958137\n"," 0.49853403 0.49518325 0.49015751        nan        nan        nan\n"," 0.4939267  0.49664746 0.49162413 0.48598042 0.49163005 0.48932726\n"," 0.5008377  0.5        0.49623431 0.48304497 0.50105084 0.48995203\n","        nan        nan        nan 0.48848825 0.49204627 0.48702403\n"," 0.49183356 0.49916274 0.48639729 0.50230542 0.49769699 0.4991634\n"," 0.49958115 0.49979058 0.49246117        nan        nan        nan\n"," 0.49372078 0.49330215 0.49350676 0.49183816 0.50250936 0.49121032\n"," 0.49560341 0.50020942 0.49958137 0.50020942 0.5        0.50146444\n","        nan        nan        nan 0.49329843 0.5        0.49811628\n"," 0.50209183 0.49644132 0.492673   0.49748932 0.5        0.4993726\n"," 0.48932638 0.49979233 0.49120463        nan        nan        nan\n"," 0.48807006 0.49037065 0.48513856 0.49036934 0.48744244 0.48807203\n"," 0.50481588 0.49832439 0.49979233 0.50104712 0.49979058 0.5\n","        nan        nan        nan 0.494137   0.5        0.49350895\n"," 0.49267476 0.50020658 0.49288462 0.49916318 0.5        0.5\n"," 0.49979058 0.5        0.5               nan        nan        nan\n"," 0.49330347 0.5        0.49811737 0.50439111 0.49455585 0.50103551\n"," 0.49665053 0.5        0.5        0.49120266 0.5        0.50146444\n","        nan        nan        nan 0.48932551 0.50377007 0.49016035\n"," 0.49267257 0.49142018 0.48995246 0.50021249 0.5        0.49874477\n"," 0.50104712 0.49979058 0.5               nan        nan        nan\n"," 0.494137   0.5        0.49350895 0.49267476 0.50020658 0.49288462\n"," 0.49916318 0.5        0.5        0.49644351 0.5        0.5\n","        nan        nan        nan 0.4939267  0.49979058 0.49811518\n"," 0.51255734 0.48953778 0.5117203  0.49979058 0.5        0.50020942\n"," 0.49434949 0.5        0.49895397        nan        nan        nan\n"," 0.48493067 0.49685864 0.49351092 0.48639423 0.49330412 0.49518456\n"," 0.49812154 0.49979058 0.49497777 0.50104712 0.49979058 0.5\n","        nan        nan        nan 0.494137   0.5        0.49350895\n"," 0.49267476 0.50020658 0.49288462 0.49916318 0.5        0.5\n"," 0.5        0.5        0.5               nan        nan        nan\n"," 0.49664921 0.5        0.49979058 0.50020942 0.49748954 0.49518325\n"," 0.5        0.5        0.5        0.49644176 0.5        0.49895397\n","        nan        nan        nan 0.49057942 0.49686214 0.49372144\n"," 0.49016079 0.49225832 0.49162523 0.5008423  0.5        0.5       ]\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n","{'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'l2'}\n","0.5125573397007601\n","SGDClassifier(alpha=0.1, eta0=0.1, learning_rate='invscaling',\n","              loss='squared_hinge', random_state=0)\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]}],"source":["#grip search for SGD\n","X = df_SGD_N.drop('Source of Money', axis=1)\n","y = df_SGD_N['Source of Money']\n","X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n","param_grid = {\n","    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n","    'penalty': ['l2', 'l1', 'elasticnet'],\n","    'alpha': [0.0001, 0.001, 0.01, 0.1],\n","    'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n","    'eta0': [0.01, 0.1, 1.0]\n","}\n","sgd = SGDClassifier(random_state=0)\n","grid_search = GridSearchCV(estimator=sgd, \n","                           param_grid=param_grid,\n","                           cv=5, \n","                           n_jobs=-1, \n","                           verbose=2, \n","                           #scoring='precision' \n","                           )\n","grid_search.fit(X_resampled, y_resampled)\n","print(grid_search.best_params_)\n","print(grid_search.best_score_)\n","print(grid_search.best_estimator_)\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","before_cm = confusion_matrix(y_test, y_pred)\n","before_cr = classification_report(y_test, y_pred)\n","before_f1 = f1_score(y_test, y_pred)\n","before_precision = precision_score(y_test, y_pred)\n","before_recall = recall_score(y_test, y_pred)\n","before_accuracy = accuracy_score(y_test, y_pred)\n"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Before tuning:\n","Confusion Matrix:\n","[[ 395 1011]\n"," [ 159  435]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.28      0.40      1406\n","           1       0.30      0.73      0.43       594\n","\n","    accuracy                           0.41      2000\n","   macro avg       0.51      0.51      0.41      2000\n","weighted avg       0.59      0.41      0.41      2000\n","\n","F1 Score: 0.4264705882352941\n","Precision: 0.3008298755186722\n","Recall: 0.7323232323232324\n","Accuracy: 0.415\n"]}],"source":["#first results\n","print(\"Before tuning:\")\n","print(\"Confusion Matrix:\")\n","print(before_cm)\n","print(\"Classification Report:\")\n","print(before_cr)\n","print(\"F1 Score:\", before_f1)\n","print(\"Precision:\", before_precision)\n","print(\"Recall:\", before_recall)\n","print(\"Accuracy:\", before_accuracy)\n"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting estimator with 39 features.\n","Fitting estimator with 29 features.\n","Fitting estimator with 19 features.\n","[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n","[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000527 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 378\n","[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n","Combining all methods\n","Sorting features\n","Selecting best features\n"]}],"source":["#b_features_list = []\n","#feature_selection_df_list = []\n","X = df_SGD_N.drop('Source of Money', axis=1)\n","y = df_SGD_N['Source of Money']\n","X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n","num_feats = 10\n","#for num in range(0, num_feats):\n","methods = ['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n","best_features, feature_selection_df = autoFeatureSelector(X_resampled, y_resampled, num_feats, methods)\n","#b_features_list.append(best_features)\n","#feature_selection_df_list.append(feature_selection_df)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1200x800 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["#visualize the results\n","plt.figure(figsize=(12, 8))\n","sns.barplot(x='Total', y='Feature', data=feature_selection_df)\n","plt.title('Feature Selection')\n","plt.show()"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n","\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n","[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n","[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"]},{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n","  warnings.warn(\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n","540 fits failed out of a total of 2700.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","118 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_hinge', 'perceptron', 'huber', 'squared_epsilon_insensitive', 'modified_huber', 'log_loss', 'epsilon_insensitive', 'squared_error'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","43 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'hinge', 'squared_error', 'squared_hinge', 'modified_huber', 'epsilon_insensitive', 'log_loss', 'huber', 'squared_epsilon_insensitive'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","41 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'huber', 'squared_epsilon_insensitive', 'modified_huber', 'squared_hinge', 'epsilon_insensitive', 'hinge', 'log_loss', 'perceptron'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","83 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_epsilon_insensitive', 'squared_hinge', 'log_loss', 'epsilon_insensitive', 'hinge', 'modified_huber', 'perceptron', 'squared_error', 'huber'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","33 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'perceptron', 'epsilon_insensitive', 'modified_huber', 'squared_hinge', 'huber', 'squared_epsilon_insensitive', 'hinge', 'squared_error', 'log_loss'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","93 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_error', 'squared_hinge', 'huber', 'log_loss', 'modified_huber', 'squared_epsilon_insensitive', 'perceptron', 'epsilon_insensitive', 'hinge'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","95 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_error', 'log_loss', 'epsilon_insensitive', 'squared_hinge', 'modified_huber', 'squared_epsilon_insensitive', 'perceptron', 'huber'}. Got 'log' instead.\n","\n","--------------------------------------------------------------------------------\n","34 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n","    estimator._validate_params()\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n","    validate_parameter_constraints(\n","  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n","    raise InvalidParameterError(\n","sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'epsilon_insensitive', 'hinge', 'modified_huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive', 'huber', 'squared_hinge', 'perceptron'}. Got 'log' instead.\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.50146487 0.50314049 0.50083726        nan        nan        nan\n"," 0.50000022 0.50230126 0.50041819 0.50041841 0.50041885 0.49979079\n"," 0.49790795 0.5008366  0.49811584 0.49895288 0.4991623  0.5\n","        nan        nan        nan 0.5098311  0.51359635 0.50920305\n"," 0.50250082 0.50208241 0.50229162 0.50020921 0.50041841 0.50250915\n"," 0.5043944  0.49727639 0.50606738        nan        nan        nan\n"," 0.50690508 0.50146005 0.49853096 0.50292975 0.49832658 0.50585818\n"," 0.4935153  0.49476987 0.49707091 0.50146487 0.50314049 0.50083726\n","        nan        nan        nan 0.50000022 0.50230126 0.50041819\n"," 0.50041841 0.50041885 0.49979079 0.49790795 0.5008366  0.49811584\n"," 0.49999934 0.4995783  0.50083594        nan        nan        nan\n"," 0.50062827 0.50104712 0.50397906 0.50857434 0.50857434 0.50857434\n"," 0.50020942 0.50125523 0.5        0.50292799 0.50188241 0.49141843\n","        nan        nan        nan 0.50229972 0.50543648 0.50606563\n"," 0.50062258 0.50313216 0.50460076 0.49079432 0.49372166 0.49937326\n"," 0.50146487 0.50314049 0.50083726        nan        nan        nan\n"," 0.50000022 0.50230126 0.50041819 0.50041841 0.50041885 0.49979079\n"," 0.49790795 0.5008366  0.49811584 0.49958115 0.50397227 0.5\n","        nan        nan        nan 0.49727771 0.50020921 0.5008377\n"," 0.49769634 0.50209227 0.50062674 0.50020877 0.50104603 0.49832461\n"," 0.50188197 0.49246292 0.49434621        nan        nan        nan\n"," 0.50941554 0.5039749  0.49978598 0.5016697  0.49706697 0.50418081\n"," 0.49916274 0.48953559 0.49602335 0.49853425 0.5        0.50355627\n","        nan        nan        nan 0.50041885 0.49979058 0.50292865\n"," 0.49602313 0.49790773 0.49686236 0.50104624 0.49790642 0.5\n"," 0.49895288 0.5        0.49979058        nan        nan        nan\n"," 0.51004009 0.4987382  0.50334246 0.50333786 0.50208263 0.50291945\n"," 0.49979058 0.49958115 0.49979079 0.50501983 0.49644001 0.49560188\n","        nan        nan        nan 0.50460076 0.50501829 0.50815812\n"," 0.50167276 0.50063068 0.49602269 0.50063243 0.50188591 0.49999978\n"," 0.49853425 0.5        0.50355627        nan        nan        nan\n"," 0.50041885 0.49979058 0.50292865 0.49602313 0.49790773 0.49686236\n"," 0.50104624 0.49790642 0.5        0.50209161 0.50104186 0.49978641\n","        nan        nan        nan 0.5008377  0.50083332 0.50146509\n"," 0.50941116 0.50857434 0.50920195 0.50690377 0.50125545 0.50272098\n"," 0.49916143 0.49664878 0.49643913        nan        nan        nan\n"," 0.50125216 0.5075305  0.50669237 0.50878683 0.49371969 0.49539355\n"," 0.49246709 0.49916471 0.49163005 0.49853425 0.5        0.50355627\n","        nan        nan        nan 0.50041885 0.49979058 0.50292865\n"," 0.49602313 0.49790773 0.49686236 0.50104624 0.49790642 0.5\n"," 0.5        0.5041841  0.4960216         nan        nan        nan\n"," 0.49748713 0.50230126 0.50355627 0.49727749 0.5046058  0.50334597\n"," 0.50230126 0.50041885 0.50020942 0.50481084 0.49204627 0.49267125\n","        nan        nan        nan 0.50794957 0.50104186 0.50208767\n"," 0.50041556 0.49790554 0.49790444 0.49623891 0.49539618 0.49894981\n"," 0.4987428  0.5        0.4953907         nan        nan        nan\n"," 0.49999781 0.50062696 0.49978707 0.49623059 0.50209643 0.50815527\n"," 0.5        0.49979058 0.5        0.5        0.5        0.50041863\n","        nan        nan        nan 0.51066748 0.49936822 0.49454884\n"," 0.5027109  0.50271046 0.5022914  0.49643979 0.50167539 0.50104603\n"," 0.50418498 0.49832636 0.49748757        nan        nan        nan\n"," 0.50397336 0.49643694 0.50397249 0.5029304  0.49392802 0.49832767\n"," 0.50209534 0.50062827 0.49853469 0.4987428  0.5        0.4953907\n","        nan        nan        nan 0.49999781 0.50062696 0.49978707\n"," 0.49623059 0.50209643 0.50815527 0.5        0.49979058 0.5\n"," 0.49937151 0.49518325 0.50188197        nan        nan        nan\n"," 0.50104712 0.49894959 0.50041534 0.50731845 0.50857434 0.50857434\n"," 0.49937173 0.49979058 0.50209227 0.49539311 0.5035613  0.49706938\n","        nan        nan        nan 0.50522772 0.49706697 0.50397205\n"," 0.50188087 0.49225197 0.492673   0.49309733 0.49769699 0.49539661\n"," 0.4987428  0.5        0.4953907         nan        nan        nan\n"," 0.49999781 0.50062696 0.49978707 0.49623059 0.50209643 0.50815527\n"," 0.5        0.49979058 0.5        0.50020942 0.50083704 0.50711297\n","        nan        nan        nan 0.49811518 0.5        0.50460251\n"," 0.5        0.50020942 0.5        0.50041841 0.5        0.50041841\n"," 0.49497316 0.49790751 0.49664878        nan        nan        nan\n"," 0.50690289 0.49664702 0.50334443 0.49832045 0.50104624 0.49204649\n"," 0.49728274 0.50020921 0.49581349 0.50209424 0.49979058 0.5\n","        nan        nan        nan 0.49329821 0.50020942 0.49371793\n"," 0.49560166 0.50125304 0.4947644  0.49999956 0.5        0.49979058\n"," 0.49979058 0.5        0.5               nan        nan        nan\n"," 0.50732064 0.5        0.50146465 0.50124581 0.50564394 0.50166597\n"," 0.5        0.5        0.50041885 0.49811343 0.5        0.49832636\n","        nan        nan        nan 0.50565051 0.50251353 0.49665053\n"," 0.4960251  0.4960216  0.4951839  0.49288593 0.5        0.49832592\n"," 0.50209424 0.49979058 0.5               nan        nan        nan\n"," 0.49329821 0.50020942 0.49371793 0.49560166 0.50125304 0.4947644\n"," 0.49999956 0.5        0.49979058 0.5        0.5        0.5\n","        nan        nan        nan 0.50020942 0.49979058 0.49790576\n"," 0.50020088 0.50857434 0.48869482 0.50020942 0.5        0.4991623\n"," 0.49664943 0.5        0.49895397        nan        nan        nan\n"," 0.50439024 0.49685864 0.49623453 0.4993691  0.49748998 0.49518303\n"," 0.4983303  0.49979058 0.49623431 0.50209424 0.49979058 0.5\n","        nan        nan        nan 0.49329821 0.50020942 0.49371793\n"," 0.49560166 0.50125304 0.4947644  0.49999956 0.5        0.49979058\n"," 0.49832636 0.5        0.50020921        nan        nan        nan\n"," 0.49958115 0.5        0.49979058 0.5        0.50000022 0.50020942\n"," 0.49958137 0.5        0.50020942 0.49581305 0.5        0.49895397\n","        nan        nan        nan 0.50334794 0.49811803 0.49539639\n"," 0.50732239 0.50188285 0.4889025  0.4974891  0.5        0.5       ]\n","  warnings.warn(\n"]}],"source":["X = df_SGD_N[best_features]\n","y = df_SGD_N['Source of Money']\n","X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n","param_grid = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'], \n","                'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1], \n","                'learning_rate': ['optimal', 'invscaling', 'adaptive'], \n","                'eta0': [0.01, 0.1, 1.0] }\n","sgd = SGDClassifier(random_state=0)\n","grid_search = GridSearchCV(estimator=sgd, \n","                        param_grid=param_grid, \n","                        cv=5, \n","                        n_jobs=-1, \n","                        verbose=2, \n","                        #scoring='precision' \n","                        )\n","grid_search.fit(X_resampled, y_resampled)\n","#print(grid_search.best_params_)\n","#print(grid_search.best_score_)\n","#print(grid_search.best_estimator_)\n","best_model = grid_search.best_estimator_\n","y_pred = best_model.predict(X_test)\n","after_cm = confusion_matrix(y_test, y_pred)\n","after_cr = classification_report(y_test, y_pred)\n","after_accuracy = accuracy_score(y_test, y_pred)\n","after_f1 = f1_score(y_test, y_pred)\n","after_precision = precision_score(y_test, y_pred)\n","after_recall = recall_score(y_test, y_pred)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["After tuning:\n","Confusion Matrix:\n","[[1007  399]\n"," [ 452  142]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.72      0.70      1406\n","           1       0.26      0.24      0.25       594\n","\n","    accuracy                           0.57      2000\n","   macro avg       0.48      0.48      0.48      2000\n","weighted avg       0.56      0.57      0.57      2000\n","\n","F1 Score: 0.25022026431718064\n","Precision: 0.26247689463955637\n","Recall: 0.23905723905723905\n","Accuracy: 0.5745\n"]}],"source":["#after tuning results\n","print(\"After tuning:\")\n","print(\"Confusion Matrix:\")\n","print(after_cm)\n","print(\"Classification Report:\")\n","print(after_cr)\n","print(\"F1 Score:\", after_f1)\n","print(\"Precision:\", after_precision)\n","print(\"Recall:\", after_recall)\n","print(\"Accuracy:\", after_accuracy)\n"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Before tuning:\n","F1 Score: 0.4264705882352941\n","Precision: 0.3008298755186722\n","Recall: 0.7323232323232324\n","Accuracy: 0.415\n","Confusion Matrix:\n","[[ 395 1011]\n"," [ 159  435]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.28      0.40      1406\n","           1       0.30      0.73      0.43       594\n","\n","    accuracy                           0.41      2000\n","   macro avg       0.51      0.51      0.41      2000\n","weighted avg       0.59      0.41      0.41      2000\n","\n","After tuning:\n","F1 Score: 0.25022026431718064\n","Precision: 0.26247689463955637\n","Recall: 0.23905723905723905\n","Accuracy: 0.5745\n","Confusion Matrix:\n","[[1007  399]\n"," [ 452  142]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.69      0.72      0.70      1406\n","           1       0.26      0.24      0.25       594\n","\n","    accuracy                           0.57      2000\n","   macro avg       0.48      0.48      0.48      2000\n","weighted avg       0.56      0.57      0.57      2000\n","\n"]}],"source":["#compare the results\n","print(\"Before tuning:\")\n","print(\"F1 Score:\", before_f1)\n","print(\"Precision:\", before_precision)\n","print(\"Recall:\", before_recall)\n","print(\"Accuracy:\", before_accuracy)\n","print(\"Confusion Matrix:\")\n","print(before_cm)\n","print(\"Classification Report:\")\n","print(before_cr)\n","print(\"After tuning:\")\n","print(\"F1 Score:\", after_f1)\n","print(\"Precision:\", after_precision)\n","print(\"Recall:\", after_recall)\n","print(\"Accuracy:\", after_accuracy)\n","print(\"Confusion Matrix:\")\n","print(after_cm)\n","print(\"Classification Report:\")\n","print(after_cr)"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1200x3200 with 4 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Visualize the comparison of all the results\n","subplots = 4\n","fig, axs = plt.subplots(subplots, 1, figsize=(12, 8 * subplots))\n","\n","# Define colors for each bar\n","before_color = 'skyblue'\n","after_color = 'lightcoral'\n","\n","axs[0].bar(['Before Tuning', 'After Tuning'], [before_f1, after_f1], color=[before_color, after_color])\n","axs[0].set_title('F1 Score Comparison')\n","\n","axs[1].bar(['Before Tuning', 'After Tuning'], [before_precision, after_precision], color=[before_color, after_color])\n","axs[1].set_title('Precision Comparison')\n","\n","axs[2].bar(['Before Tuning', 'After Tuning'], [before_recall, after_recall], color=[before_color, after_color])\n","axs[2].set_title('Recall Comparison')\n","\n","axs[3].bar(['Before Tuning', 'After Tuning'], [before_accuracy, after_accuracy], color=[before_color, after_color])\n","axs[3].set_title('Accuracy Comparison')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nfLZECBwI-xV"},"source":["## SVM: Eric, Moosa"]},{"cell_type":"markdown","metadata":{},"source":["### Data for SVM"]},{"cell_type":"code","execution_count":78,"metadata":{"id":"XowPRU4muiu-"},"outputs":[],"source":["df_svm = df.copy()"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["X = df_svm.drop(columns=['Source of Money'])\n","y = df_svm['Source of Money']"]},{"cell_type":"markdown","metadata":{},"source":["### Splitting the data for training and testing"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["# Split the data into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["### SVM Model Training and Prediction"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[2105    0]\n"," [ 895    0]]\n","SVC Accuracy: 0.7017\n","              precision    recall  f1-score   support\n","\n","           0       0.70      1.00      0.82      2105\n","           1       1.00      0.00      0.00       895\n","\n","    accuracy                           0.70      3000\n","   macro avg       0.85      0.50      0.41      3000\n","weighted avg       0.79      0.70      0.58      3000\n","\n"]}],"source":["svc_model = SVC()\n","svc_model.fit(X_train, y_train)\n","y_pred = svc_model.predict(X_test)\n","\n","print(confusion_matrix(y_test, y_pred))\n","print(f\"SVC Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n","print(classification_report(y_test, y_pred,  zero_division=1))"]},{"cell_type":"markdown","metadata":{},"source":["### GridSearchCV (Hyper Parameter tuning)"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best SVM Parameters: {'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}\n","Best SVM Accuracy: 0.7017142857142857\n"]}],"source":["# Hyperparameter Tuning using GridSearchCV\n","# SVM GridSearchCV params\n","param_grid_svm = {\n","    'C': [0.1, 1, 10],\n","    'kernel': ['rbf'],\n","    'gamma': [1,0.1,0.01,0.001,0.0001]\n","}\n","grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)\n","grid_svm.fit(X_train, y_train)\n","print(f\"Best SVM Parameters: {grid_svm.best_params_}\")\n","print(f\"Best SVM Accuracy: {grid_svm.best_score_}\")"]},{"cell_type":"markdown","metadata":{"id":"6mTwjwTnqDiD"},"source":["# Conclusion and comparison\n"]},{"cell_type":"markdown","metadata":{"id":"WQg4pZMmIX90"},"source":["Present your work including approach and findings during the class on September 24th or 26th, 2024. Each group will have a maximum of 15 minutes to present their project. It is advised that your PowerPoint files to be no longer than 15 slides.\n","\n","Prepare a written technical report of no longer than 15 pages to discuss the problem statement, various steps conducted, summary of findings and conclusions. Submit the report and the notebook file (with proper headings, explanatory comments and code sections) by the midnight of September 29th, 2024."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
