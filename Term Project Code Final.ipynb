{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v52S4rZMHsK3"
      },
      "source": [
        "# Term Project Group 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceU4aZcRIdh8"
      },
      "source": [
        "The Dataset: https://www.kaggle.com/datasets/waqi786/global-black-money-transactions-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyyUVrF_IioY"
      },
      "source": [
        "### Explanation:\n",
        "\n",
        "This dataset gives a solid overview of black money transactions in different countries, focusing on financial activities tied to illegal dealings. It includes details like transaction amounts and risk scores, making it super useful for anyone looking to study financial crime trends or work on anti-money laundering tools.\n",
        "\n",
        "\n",
        "### Dataset:\n",
        "\n",
        "Transaction ID: Unique identifier for each transaction. (e.g., TX0000001)\n",
        "\n",
        "Country: Country where the transaction occurred. (e.g., USA, China)\n",
        "\n",
        "Amount (USD): Transaction amount in US Dollars. (e.g., 150000.00)\n",
        "\n",
        "Transaction Type: Type of transaction. (e.g., Offshore Transfer, Property Purchase)\n",
        "\n",
        "Date of Transaction: The date and time of the transaction. (e.g., 2022-03-15 14:32:00)\n",
        "\n",
        "Person Involved: Name or identifier of the person/entity involved. (e.g., Person_1234)\n",
        "\n",
        "Industry: Industry associated with the transaction. (e.g., Real Estate, Finance)\n",
        "\n",
        "Destination Country: Country where the money was sent. (e.g., Switzerland)\n",
        "\n",
        "Reported by Authority: Whether the transaction was reported to authorities. (e.g., True/False)\n",
        "\n",
        "Source of Money: Origin of the money. (e.g., Legal, Illegal)\n",
        "\n",
        "Money Laundering Risk Score: Risk score indicating the likelihood of money\n",
        "laundering (1-10). (e.g., 8)\n",
        "\n",
        "Shell Companies Involved: Number of shell companies used in the transaction. (e.g., 3)\n",
        "\n",
        "Financial Institution: Bank or financial institution involved in the transaction. (e.g., Bank_567)\n",
        "\n",
        "Tax Haven Country: Country where the money was transferred to a tax haven. (e.g., Cayman Islands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFAjkKdlH4LW"
      },
      "source": [
        "# Pre-process and clean the dataset as appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piBKVR5r6wGk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NfzVSt-I6wGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28428b2e-7b93-4544-aaa9-8118eb7c1050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "# from lightgbm import LGBMClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE, SelectFromModel, chi2, SelectKBest\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bNKeluqIu7D"
      },
      "source": [
        "## Exploring and visualizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgPt-hfO6wGk"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GTUfUTkk6wGl"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ericrlessa/global-illicit-money-transactions/refs/heads/main/Big_Black_Money_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "RM-m9W_C797X",
        "outputId": "c1c4ccbc-4b97-4c7c-c339-ac1078c9962c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualise some features\n",
        "plt.pie(df['Reported by Authority'].value_counts(), labels=df['Source of Money'].unique(), autopct='%1.1f%%')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "XvQRDQYP83Aa",
        "outputId": "e81671e7-474f-4d80-dc47-6fc5e424c28e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Axes: xlabel='Shell Companies Involved', ylabel='Amount (USD)'>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.lineplot(x=df['Shell Companies Involved'], y=df['Amount (USD)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OOIer9R86wGl",
        "outputId": "29b4429b-5f0a-4105-8359-f7866bb6ddbd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Transaction ID</th>\n",
              "      <th>Country</th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Transaction Type</th>\n",
              "      <th>Date of Transaction</th>\n",
              "      <th>Person Involved</th>\n",
              "      <th>Industry</th>\n",
              "      <th>Destination Country</th>\n",
              "      <th>Reported by Authority</th>\n",
              "      <th>Source of Money</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <th>Financial Institution</th>\n",
              "      <th>Tax Haven Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TX0000000001</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>3.267530e+06</td>\n",
              "      <td>Offshore Transfer</td>\n",
              "      <td>2013-01-01 00:00:00</td>\n",
              "      <td>Person_1101</td>\n",
              "      <td>Construction</td>\n",
              "      <td>USA</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Bank_40</td>\n",
              "      <td>Singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TX0000000002</td>\n",
              "      <td>China</td>\n",
              "      <td>4.965767e+06</td>\n",
              "      <td>Stocks Transfer</td>\n",
              "      <td>2013-01-01 01:00:00</td>\n",
              "      <td>Person_7484</td>\n",
              "      <td>Luxury Goods</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>Bank_461</td>\n",
              "      <td>Bahamas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TX0000000003</td>\n",
              "      <td>UK</td>\n",
              "      <td>9.416750e+04</td>\n",
              "      <td>Stocks Transfer</td>\n",
              "      <td>2013-01-01 02:00:00</td>\n",
              "      <td>Person_3655</td>\n",
              "      <td>Construction</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Bank_387</td>\n",
              "      <td>Switzerland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TX0000000004</td>\n",
              "      <td>UAE</td>\n",
              "      <td>3.864201e+05</td>\n",
              "      <td>Cash Withdrawal</td>\n",
              "      <td>2013-01-01 03:00:00</td>\n",
              "      <td>Person_3226</td>\n",
              "      <td>Oil &amp; Gas</td>\n",
              "      <td>Russia</td>\n",
              "      <td>False</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>Bank_353</td>\n",
              "      <td>Panama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TX0000000005</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>6.433784e+05</td>\n",
              "      <td>Cryptocurrency</td>\n",
              "      <td>2013-01-01 04:00:00</td>\n",
              "      <td>Person_7975</td>\n",
              "      <td>Real Estate</td>\n",
              "      <td>USA</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>Bank_57</td>\n",
              "      <td>Luxembourg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Transaction ID       Country  Amount (USD)   Transaction Type  \\\n",
              "0   TX0000000001        Brazil  3.267530e+06  Offshore Transfer   \n",
              "1   TX0000000002         China  4.965767e+06    Stocks Transfer   \n",
              "2   TX0000000003            UK  9.416750e+04    Stocks Transfer   \n",
              "3   TX0000000004           UAE  3.864201e+05    Cash Withdrawal   \n",
              "4   TX0000000005  South Africa  6.433784e+05     Cryptocurrency   \n",
              "\n",
              "   Date of Transaction Person Involved      Industry Destination Country  \\\n",
              "0  2013-01-01 00:00:00     Person_1101  Construction                 USA   \n",
              "1  2013-01-01 01:00:00     Person_7484  Luxury Goods        South Africa   \n",
              "2  2013-01-01 02:00:00     Person_3655  Construction         Switzerland   \n",
              "3  2013-01-01 03:00:00     Person_3226     Oil & Gas              Russia   \n",
              "4  2013-01-01 04:00:00     Person_7975   Real Estate                 USA   \n",
              "\n",
              "   Reported by Authority Source of Money  Money Laundering Risk Score  \\\n",
              "0                   True         Illegal                            6   \n",
              "1                  False         Illegal                            9   \n",
              "2                   True         Illegal                            1   \n",
              "3                  False         Illegal                            7   \n",
              "4                   True         Illegal                            1   \n",
              "\n",
              "   Shell Companies Involved Financial Institution Tax Haven Country  \n",
              "0                         1               Bank_40         Singapore  \n",
              "1                         0              Bank_461           Bahamas  \n",
              "2                         3              Bank_387       Switzerland  \n",
              "3                         2              Bank_353            Panama  \n",
              "4                         9               Bank_57        Luxembourg  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "gYV9SDr56wGl",
        "outputId": "9f15bd63-6e3d-40d5-e23c-748f0c8d15f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transaction ID                 0\n",
              "Country                        0\n",
              "Amount (USD)                   0\n",
              "Transaction Type               0\n",
              "Date of Transaction            0\n",
              "Person Involved                0\n",
              "Industry                       0\n",
              "Destination Country            0\n",
              "Reported by Authority          0\n",
              "Source of Money                0\n",
              "Money Laundering Risk Score    0\n",
              "Shell Companies Involved       0\n",
              "Financial Institution          0\n",
              "Tax Haven Country              0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "id": "LYEVd61j6wGm",
        "outputId": "260725a0-45d3-4172-b73b-20e5229554fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Transaction ID                  object\n",
              "Country                         object\n",
              "Amount (USD)                   float64\n",
              "Transaction Type                object\n",
              "Date of Transaction             object\n",
              "Person Involved                 object\n",
              "Industry                        object\n",
              "Destination Country             object\n",
              "Reported by Authority             bool\n",
              "Source of Money                 object\n",
              "Money Laundering Risk Score      int64\n",
              "Shell Companies Involved         int64\n",
              "Financial Institution           object\n",
              "Tax Haven Country               object\n",
              "dtype: object"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "kANCm_zF6wGm",
        "outputId": "33e75b1b-38bc-4a78-9dc6-59fe2e9b2fed"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.000000e+04</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.501818e+06</td>\n",
              "      <td>5.526400</td>\n",
              "      <td>4.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.424364e+06</td>\n",
              "      <td>2.893603</td>\n",
              "      <td>2.879773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.003180e+04</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.279005e+06</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.501310e+06</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.722416e+06</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.999812e+06</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Amount (USD)  Money Laundering Risk Score  Shell Companies Involved\n",
              "count  1.000000e+04                 10000.000000              10000.000000\n",
              "mean   2.501818e+06                     5.526400                  4.469400\n",
              "std    1.424364e+06                     2.893603                  2.879773\n",
              "min    1.003180e+04                     1.000000                  0.000000\n",
              "25%    1.279005e+06                     3.000000                  2.000000\n",
              "50%    2.501310e+06                     6.000000                  4.000000\n",
              "75%    3.722416e+06                     8.000000                  7.000000\n",
              "max    4.999812e+06                    10.000000                  9.000000"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znkDwy846wGm"
      },
      "source": [
        "## Processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7YWgMej6wGm"
      },
      "source": [
        "### Handle missing values if applicable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R1AAOvjD6wGm"
      },
      "outputs": [],
      "source": [
        "# For numerical features\n",
        "numerical_features = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[numerical_features] = imputer.fit_transform(df[numerical_features])\n",
        "\n",
        "# For categorical features\n",
        "categorical_features = ['Country', 'Transaction Type', 'Person Involved', 'Industry',\n",
        "                        'Destination Country', 'Financial Institution', 'Tax Haven Country']\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "df[categorical_features] = imputer_cat.fit_transform(df[categorical_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5cXCduR6wGm"
      },
      "source": [
        "### Dropping Features and OHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4tgHlLPf6wGm"
      },
      "outputs": [],
      "source": [
        "# Drop Irrelevant Features\n",
        "df.drop('Transaction ID', axis=1, inplace=True) # Dropped because it is unique for each transaction\n",
        "# df.drop('Person Involved', axis=1, inplace=True) # Frequency Encoding will be implemented\n",
        "# df.drop('Financial Institution', axis=1, inplace=True) # Implement Frequency Encoding\n",
        "df.drop('Date of Transaction', axis=1, inplace=True) # Date of transaction is not relevant\n",
        "\n",
        "# Convert 'Reported by Authority' to integer\n",
        "df['Reported by Authority'] = df['Reported by Authority'].astype(int)\n",
        "\n",
        "# Frequency encoding for 'Financial Institution'\n",
        "df['Financial Institution'] = df.groupby('Financial Institution')['Financial Institution'].transform('count')\n",
        "\n",
        "# Frequency encoding for 'Person Involved'\n",
        "df['Person Involved'] = df.groupby('Person Involved')['Person Involved'].transform('count')\n",
        "\n",
        "# Encode target variable\n",
        "le = LabelEncoder()\n",
        "df['Source of Money'] = le.fit_transform(df['Source of Money'])\n",
        "\n",
        "# One-Hot Encode nominal categorical features\n",
        "nominal_features = ['Country', 'Transaction Type', 'Industry',\n",
        "                    'Destination Country', 'Tax Haven Country']\n",
        "df = pd.get_dummies(df, columns=nominal_features, drop_first=True)\n",
        "\n",
        "dummy_columns = df.filter(like='_').columns\n",
        "df[dummy_columns] = df[dummy_columns].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRGGPdSZ6wGm",
        "outputId": "b50b172e-b0f2-40e8-86e3-31928874c553"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n",
              "       'Source of Money', 'Money Laundering Risk Score',\n",
              "       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n",
              "       'Country_India', 'Country_Russia', 'Country_Singapore',\n",
              "       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n",
              "       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n",
              "       'Transaction Type_Offshore Transfer',\n",
              "       'Transaction Type_Property Purchase',\n",
              "       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n",
              "       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n",
              "       'Industry_Oil & Gas', 'Industry_Real Estate',\n",
              "       'Destination Country_China', 'Destination Country_India',\n",
              "       'Destination Country_Russia', 'Destination Country_Singapore',\n",
              "       'Destination Country_South Africa', 'Destination Country_Switzerland',\n",
              "       'Destination Country_UAE', 'Destination Country_UK',\n",
              "       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n",
              "       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n",
              "       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "W6CsiyNz6wGm",
        "outputId": "17179b88-34fe-42f4-f1f2-ad06c9bb390b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Person Involved</th>\n",
              "      <th>Reported by Authority</th>\n",
              "      <th>Source of Money</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <th>Financial Institution</th>\n",
              "      <th>Country_China</th>\n",
              "      <th>Country_India</th>\n",
              "      <th>Country_Russia</th>\n",
              "      <th>...</th>\n",
              "      <th>Destination Country_South Africa</th>\n",
              "      <th>Destination Country_Switzerland</th>\n",
              "      <th>Destination Country_UAE</th>\n",
              "      <th>Destination Country_UK</th>\n",
              "      <th>Destination Country_USA</th>\n",
              "      <th>Tax Haven Country_Cayman Islands</th>\n",
              "      <th>Tax Haven Country_Luxembourg</th>\n",
              "      <th>Tax Haven Country_Panama</th>\n",
              "      <th>Tax Haven Country_Singapore</th>\n",
              "      <th>Tax Haven Country_Switzerland</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.267530e+06</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.965767e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.416750e+04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.864201e+05</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.433784e+05</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 40 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Amount (USD)  Person Involved  Reported by Authority  Source of Money  \\\n",
              "0  3.267530e+06                2                      1                0   \n",
              "1  4.965767e+06                1                      0                0   \n",
              "2  9.416750e+04                1                      1                0   \n",
              "3  3.864201e+05                5                      0                0   \n",
              "4  6.433784e+05                4                      1                0   \n",
              "\n",
              "   Money Laundering Risk Score  Shell Companies Involved  \\\n",
              "0                          6.0                       1.0   \n",
              "1                          9.0                       0.0   \n",
              "2                          1.0                       3.0   \n",
              "3                          7.0                       2.0   \n",
              "4                          1.0                       9.0   \n",
              "\n",
              "   Financial Institution  Country_China  Country_India  Country_Russia  ...  \\\n",
              "0                     17              0              0               0  ...   \n",
              "1                     24              1              0               0  ...   \n",
              "2                     12              0              0               0  ...   \n",
              "3                     18              0              0               0  ...   \n",
              "4                     19              0              0               0  ...   \n",
              "\n",
              "   Destination Country_South Africa  Destination Country_Switzerland  \\\n",
              "0                                 0                                0   \n",
              "1                                 1                                0   \n",
              "2                                 0                                1   \n",
              "3                                 0                                0   \n",
              "4                                 0                                0   \n",
              "\n",
              "   Destination Country_UAE  Destination Country_UK  Destination Country_USA  \\\n",
              "0                        0                       0                        1   \n",
              "1                        0                       0                        0   \n",
              "2                        0                       0                        0   \n",
              "3                        0                       0                        0   \n",
              "4                        0                       0                        1   \n",
              "\n",
              "   Tax Haven Country_Cayman Islands  Tax Haven Country_Luxembourg  \\\n",
              "0                                 0                             0   \n",
              "1                                 0                             0   \n",
              "2                                 0                             0   \n",
              "3                                 0                             0   \n",
              "4                                 0                             1   \n",
              "\n",
              "   Tax Haven Country_Panama  Tax Haven Country_Singapore  \\\n",
              "0                         0                            1   \n",
              "1                         0                            0   \n",
              "2                         0                            0   \n",
              "3                         1                            0   \n",
              "4                         0                            0   \n",
              "\n",
              "   Tax Haven Country_Switzerland  \n",
              "0                              0  \n",
              "1                              0  \n",
              "2                              1  \n",
              "3                              0  \n",
              "4                              0  \n",
              "\n",
              "[5 rows x 40 columns]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "y3M1fT0C6wGn",
        "outputId": "48534882-bb57-41ed-ff12-bd34c0d4dc48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Source of Money\n",
              "0    7017\n",
              "1    2983\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['Source of Money'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FVJFdDDr6wGn"
      },
      "outputs": [],
      "source": [
        "features_to_modify = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n",
        "\n",
        "def scale_features(df, features):\n",
        "    df_S = df.copy()\n",
        "    scaler = StandardScaler()\n",
        "    df_S[features] = scaler.fit_transform(df[features])\n",
        "    return df_S\n",
        "\n",
        "def normalize_features(df, features):\n",
        "    df_N = df.copy()\n",
        "    scaler = MinMaxScaler()\n",
        "    df_N[features] = scaler.fit_transform(df[features])\n",
        "    return df_N\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvwpmmQE6wGn"
      },
      "source": [
        "### Biased data correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dKMUlEYs6wGn"
      },
      "outputs": [],
      "source": [
        "def Undersampling(X,Y, test_size):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = test_size, random_state=0)\n",
        "    rus = RandomUnderSampler(random_state=0)\n",
        "    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "    return X_resampled, X_test, y_resampled, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLpCpmLS6wGn"
      },
      "source": [
        "## Feature Selectors (Optional):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR3MQUR16wGn"
      },
      "source": [
        "### Feature selector functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WYkFROWN6wGn"
      },
      "outputs": [],
      "source": [
        "def cor_selector(X, y,num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    cor_list = []\n",
        "    feature_name = X.columns.tolist()\n",
        "    for i in feature_name:\n",
        "        cor = np.corrcoef(X[i], y)[0, 1]\n",
        "        cor_list.append(cor)\n",
        "    #print(np.argsort(np.abs(cor_list)))\n",
        "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
        "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
        "    #print(cor_feature)\n",
        "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
        "    # Your code ends here\n",
        "    return cor_support, cor_feature\n",
        "\n",
        "def chi_squared_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    X_norm = MinMaxScaler().fit_transform(X)\n",
        "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
        "    chi_selector.fit(X_norm, y)\n",
        "    chi_support = chi_selector.get_support()\n",
        "    #print(chi_support)\n",
        "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return chi_support, chi_feature\n",
        "\n",
        "def rfe_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    rfe_selector = RFE(estimator=LogisticRegression(random_state=42), n_features_to_select=num_feats, step=10, verbose=5)\n",
        "    rfe_selector.fit(X, y)\n",
        "    rfe_support = rfe_selector.support_\n",
        "    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return rfe_support, rfe_feature\n",
        "\n",
        "def embedded_log_reg_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    embedded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\", random_state = 42), max_features=num_feats)\n",
        "    embedded_lr_selector.fit(X, y)\n",
        "    embedded_lr_support = embedded_lr_selector.get_support()\n",
        "    embedded_lr_feature = X.loc[:,embedded_lr_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_lr_support, embedded_lr_feature\n",
        "\n",
        "def embedded_rf_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), max_features=num_feats)\n",
        "    embeded_rf_selector.fit(X, y)\n",
        "    embedded_rf_support = embeded_rf_selector.get_support()\n",
        "    embedded_rf_feature = X.loc[:,embedded_rf_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_rf_support, embedded_rf_feature\n",
        "\n",
        "def embedded_lgbm_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    lgbc = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
        "    embeded_lgbm_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
        "    embeded_lgbm_selector.fit(X, y)\n",
        "    embedded_lgbm_support = embeded_lgbm_selector.get_support()\n",
        "    embedded_lgbm_feature = X.loc[:,embedded_lgbm_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_lgbm_support, embedded_lgbm_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdq2lh2f6wGn"
      },
      "source": [
        "### Feature Selectors Combined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uwkj0SqO6wGn"
      },
      "outputs": [],
      "source": [
        "def autoFeatureSelector(X, y, num_feats, methods=[]):\n",
        "\n",
        "    support_dict = {}\n",
        "\n",
        "    feature_name = list(X.columns)\n",
        "    support_dict['Feature'] = feature_name\n",
        "\n",
        "    if 'pearson' in methods:\n",
        "        cor_support, cor_feature = cor_selector(X, y, num_feats)\n",
        "        support_dict['Pearson'] = cor_support\n",
        "    if 'chi-square' in methods:\n",
        "        chi_support, chi_feature = chi_squared_selector(X, y, num_feats)\n",
        "        support_dict['Chi-2'] = chi_support\n",
        "    if 'rfe' in methods:\n",
        "        rfe_support, rfe_feature = rfe_selector(X, y, num_feats)\n",
        "        support_dict['RFE'] = rfe_support\n",
        "    if 'log-reg' in methods:\n",
        "        embedded_lr_support, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)\n",
        "        support_dict['Logistics'] = embedded_lr_support\n",
        "    if 'rf' in methods:\n",
        "        embedded_rf_support, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)\n",
        "        support_dict['Random Forest'] = embedded_rf_support\n",
        "    if 'lgbm' in methods:\n",
        "        embedded_lgbm_support, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)\n",
        "        support_dict['LightGBM'] = embedded_lgbm_support\n",
        "\n",
        "    # Combine all the above feature list and count the maximum set of features that got selected by all methods\n",
        "\n",
        "    print(\"Combining all methods\")\n",
        "    feature_selection_df = pd.DataFrame(support_dict)\n",
        "    feature_selection_df['Total'] = feature_selection_df.apply(lambda row: np.sum(row[1:].astype(int)), axis=1)\n",
        "    print(\"Sorting features\")\n",
        "    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
        "    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
        "    print(\"Selecting best features\")\n",
        "    best_features = feature_selection_df['Feature'].tolist()[:num_feats]\n",
        "    return best_features, feature_selection_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S14DFsWA6wGn"
      },
      "source": [
        "# Models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB2eHh_Q6wGn"
      },
      "source": [
        "- Utilize GridSearchCV to tune the parameters of each of the models.\n",
        "- Check if better results can be obtained for any of the models.\n",
        "- Discuss your observations regarding model performance.\n",
        "- Randomly remove some features (or based on a certain hypothesis) and re-evaluate the models.\n",
        "- Document your observations concerning model performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPCRye0hI5va"
      },
      "source": [
        "## Logistic Regression: Saif, Dwip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUwZsu36wGn"
      },
      "source": [
        "### Data for LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXrNCYAEuQ1i",
        "outputId": "b317d58c-0b75-43a5-b073-bcca0db39f2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n",
              "       'Source of Money', 'Money Laundering Risk Score',\n",
              "       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n",
              "       'Country_India', 'Country_Russia', 'Country_Singapore',\n",
              "       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n",
              "       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n",
              "       'Transaction Type_Offshore Transfer',\n",
              "       'Transaction Type_Property Purchase',\n",
              "       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n",
              "       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n",
              "       'Industry_Oil & Gas', 'Industry_Real Estate',\n",
              "       'Destination Country_China', 'Destination Country_India',\n",
              "       'Destination Country_Russia', 'Destination Country_Singapore',\n",
              "       'Destination Country_South Africa', 'Destination Country_Switzerland',\n",
              "       'Destination Country_UAE', 'Destination Country_UK',\n",
              "       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n",
              "       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n",
              "       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the data for Logistic Regression\n",
        "df_LR = df.copy()\n",
        "\n",
        "# Normalize the variables that are greater than 1 as this will affect the models proformnce\n",
        "df_LR = normalize_features(df_LR, ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved', 'Financial Institution', 'Person Involved'])\n",
        "\n",
        "df_LR.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xoujTX6wGn"
      },
      "source": [
        "### Simple LR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQVmIY1D6wGn",
        "outputId": "b29c2ba5-dc0c-4c4d-c1df-ade1dfee889c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression\n",
            "[[1279  110]\n",
            " [ 558   53]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.92      0.79      1389\n",
            "           1       0.33      0.09      0.14       611\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.51      0.50      0.46      2000\n",
            "weighted avg       0.58      0.67      0.59      2000\n",
            "\n",
            "Accuracy:  0.666\n"
          ]
        }
      ],
      "source": [
        "# Implement a logistic regression model\n",
        "X = df_LR.drop('Source of Money', axis=1)\n",
        "Y = df_LR['Source of Money']\n",
        "\n",
        "# Create a poly feature data\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698W25HN6wGo"
      },
      "source": [
        "#### Using Undersampling to train the LR Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpHJFNju6wGo",
        "outputId": "b748e910-60a3-43eb-be99-5f841674db9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression\n",
            "[[696 710]\n",
            " [298 296]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.50      0.58      1406\n",
            "           1       0.29      0.50      0.37       594\n",
            "\n",
            "    accuracy                           0.50      2000\n",
            "   macro avg       0.50      0.50      0.47      2000\n",
            "weighted avg       0.58      0.50      0.52      2000\n",
            "\n",
            "Accuracy:  0.496\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = Undersampling(X, Y, 0.2)\n",
        "\n",
        "# Fit the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hyOxe4S6wGp"
      },
      "source": [
        "### GridSearchCV LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o31tkMpM6wGt",
        "outputId": "a77ba8be-033f-40ce-9052-e297d178ccfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "0.5\n",
            "LogisticRegression(C=0.01, penalty='l1', solver='liblinear')\n",
            "Logistic Regression\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "Accuracy:  0.703\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thvgyoTA6wGt"
      },
      "source": [
        "#### Modifying features and testing proformance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xs-YANl6wGt",
        "outputId": "657819e1-40c2-47ea-f472-f2650acdbce0"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[49], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mLogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m), param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Get the best parameters\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:968\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    962\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    963\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    964\u001b[0m     )\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    972\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1543\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1543\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:914\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    910\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    911\u001b[0m         )\n\u001b[0;32m    912\u001b[0m     )\n\u001b[1;32m--> 914\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    934\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1350\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m     n_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1350\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1356\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1375\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:547\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    544\u001b[0m         alpha \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m l1_ratio)\n\u001b[0;32m    545\u001b[0m         beta \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m C) \u001b[38;5;241m*\u001b[39m l1_ratio\n\u001b[1;32m--> 547\u001b[0m     w0, n_iter_i, warm_start_sag \u001b[38;5;241m=\u001b[39m \u001b[43msag_solver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarm_start_sag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolver must be one of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnewton-cg\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m}, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m solver\n\u001b[0;32m    568\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:324\u001b[0m, in \u001b[0;36msag_solver\u001b[1;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mZeroDivisionError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent sag implementation does not handle \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe case step_size * alpha_scaled == 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m     )\n\u001b[0;32m    323\u001b[0m sag \u001b[38;5;241m=\u001b[39m sag64 \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat64 \u001b[38;5;28;01melse\u001b[39;00m sag32\n\u001b[1;32m--> 324\u001b[0m num_seen, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43msag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta_scaled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43msum_gradient_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_memory_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_seen_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_sum_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintercept_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_saga\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter_ \u001b[38;5;241m==\u001b[39m max_iter:\n\u001b[0;32m    349\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe max_iter was reached which means the coef_ did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    351\u001b[0m         ConvergenceWarning,\n\u001b[0;32m    352\u001b[0m     )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "df_LR_Mod = df_LR.copy()\n",
        "\n",
        "# Remove the features that are not important\n",
        "df_LR_Mod = df_LR_Mod.drop(['Person Involved', 'Financial Institution'], axis=1)\n",
        "\n",
        "X = df_LR_Mod.drop('Source of Money', axis=1)\n",
        "Y = df_LR_Mod['Source of Money']\n",
        "\n",
        "# Create a poly feature data\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = Undersampling(X_poly, Y, 0.3)\n",
        "\n",
        "# Define the hyperparameters\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=2000), param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWV5cWNBI9xr"
      },
      "source": [
        "## Decision Tree: Nitish, Sehaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qbcv8p46wGt"
      },
      "source": [
        "### Data for DT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_gz9k5t6wGt"
      },
      "outputs": [],
      "source": [
        "df_DT = df.copy()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOj4PAeh6wGt"
      },
      "source": [
        "### DT Simple Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x69OpAlw6wGt",
        "outputId": "8f1c21c0-2111-49e9-f1e6-e39f3e1fc437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.6710\n"
          ]
        }
      ],
      "source": [
        "# Splitting the data into features (X) and target (y)\n",
        "X = df_DT.drop('Source of Money', axis=1)\n",
        "y = df_DT['Source of Money']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training using RandomForestClassifier\n",
        "clf = DecisionTreeClassifier(random_state=42, max_depth=10) # Changed\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy score\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzPVU_PO6wGt",
        "outputId": "a04609c8-94a7-4500-919b-3b4b19fda076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[1298   91]\n",
            " [ 567   44]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.93      0.80      1389\n",
            "           1       0.33      0.07      0.12       611\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.51      0.50      0.46      2000\n",
            "weighted avg       0.58      0.67      0.59      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CHdVsCoWTdJ"
      },
      "source": [
        "### GridSearchCV DT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emmabtUHWTdK",
        "outputId": "feeb36f5-770a-4eb7-9a3c-7eadc927c5f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 768 candidates, totalling 2304 fits\n",
            "Best Parameters: {'class_weight': None, 'criterion': 'entropy', 'max_depth': 3, 'max_leaf_nodes': 5, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Model Accuracy: 0.6935\n",
            "Confusion Matrix:\n",
            "[[1384    5]\n",
            " [ 608    3]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      1.00      0.82      1389\n",
            "           1       0.38      0.00      0.01       611\n",
            "\n",
            "    accuracy                           0.69      2000\n",
            "   macro avg       0.53      0.50      0.41      2000\n",
            "weighted avg       0.60      0.69      0.57      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
            "768 fits failed out of a total of 2304.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "768 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"c:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got None instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1052: UserWarning: One or more of the test scores are non-finite: [0.70337504 0.70337504        nan 0.70337504 0.70337504        nan\n",
            " 0.70299994 0.70299994        nan 0.70274998 0.70274998        nan\n",
            " 0.70350002 0.70350002        nan 0.70350002 0.70350002        nan\n",
            " 0.70350002 0.70350002        nan 0.70274998 0.70274998        nan\n",
            " 0.70337504 0.70337504        nan 0.70337504 0.70337504        nan\n",
            " 0.70299994 0.70299994        nan 0.70274998 0.70274998        nan\n",
            " 0.70337504 0.70337504        nan 0.70337504 0.70337504        nan\n",
            " 0.70299994 0.70299994        nan 0.70274998 0.70274998        nan\n",
            " 0.69800024 0.69812522        nan 0.69800015 0.69825021        nan\n",
            " 0.69875024 0.69875024        nan 0.6980001  0.6980001         nan\n",
            " 0.70312493 0.70312493        nan 0.70312493 0.70312493        nan\n",
            " 0.70350002 0.70350002        nan 0.70249991 0.70249991        nan\n",
            " 0.70050016 0.70050016        nan 0.70050016 0.70050016        nan\n",
            " 0.70087526 0.70087526        nan 0.70049979 0.70049979        nan\n",
            " 0.69975007 0.69962504        nan 0.69962499 0.69975002        nan\n",
            " 0.69999999 0.69999999        nan 0.6980001  0.6980001         nan\n",
            " 0.68250012 0.68250035        nan 0.68299987 0.68587512        nan\n",
            " 0.67975022 0.67975022        nan 0.67974915 0.67974915        nan\n",
            " 0.70312493 0.70312493        nan 0.70312493 0.70312493        nan\n",
            " 0.70350002 0.70350002        nan 0.70249991 0.70249991        nan\n",
            " 0.70174991 0.70174991        nan 0.70174991 0.70174991        nan\n",
            " 0.70075013 0.70075013        nan 0.70049979 0.70049979        nan\n",
            " 0.70024982 0.70024982        nan 0.70024982 0.70024982        nan\n",
            " 0.69837472 0.69837472        nan 0.69687416 0.69687416        nan\n",
            " 0.57862558 0.59537532        nan 0.60674978 0.61137477        nan\n",
            " 0.60725061 0.60725061        nan 0.63062625 0.63062625        nan\n",
            " 0.70312493 0.70312493        nan 0.70312493 0.70312493        nan\n",
            " 0.70350002 0.70350002        nan 0.70249991 0.70249991        nan\n",
            " 0.70174991 0.70174991        nan 0.70174991 0.70174991        nan\n",
            " 0.70075013 0.70075013        nan 0.70049979 0.70049979        nan\n",
            " 0.70024982 0.70024982        nan 0.70024982 0.70024982        nan\n",
            " 0.69837472 0.69837472        nan 0.69687416 0.69687416        nan\n",
            " 0.7036251  0.7036251         nan 0.7036251  0.7036251         nan\n",
            " 0.70350007 0.70350007        nan 0.7032501  0.7032501         nan\n",
            " 0.70400015 0.70400015        nan 0.70400015 0.70400015        nan\n",
            " 0.70400015 0.70400015        nan 0.70350007 0.70350007        nan\n",
            " 0.7036251  0.7036251         nan 0.7036251  0.7036251         nan\n",
            " 0.70350007 0.70350007        nan 0.7032501  0.7032501         nan\n",
            " 0.7036251  0.7036251         nan 0.7036251  0.7036251         nan\n",
            " 0.70350007 0.70350007        nan 0.7032501  0.7032501         nan\n",
            " 0.70212515 0.70225013        nan 0.70237512 0.70237512        nan\n",
            " 0.7025001  0.7025001         nan 0.69925051 0.69925051        nan\n",
            " 0.70400015 0.70400015        nan 0.70400015 0.70400015        nan\n",
            " 0.70400015 0.70400015        nan 0.70350007 0.70350007        nan\n",
            " 0.70312498 0.70312498        nan 0.70312498 0.70312498        nan\n",
            " 0.70237516 0.70237516        nan 0.70200016 0.70200016        nan\n",
            " 0.70187504 0.70175001        nan 0.70225008 0.70187499        nan\n",
            " 0.70225004 0.70225004        nan 0.70062533 0.70062533        nan\n",
            " 0.68762565 0.68875065        nan 0.68712547 0.68750043        nan\n",
            " 0.68812558 0.68812558        nan 0.6810003  0.6810003         nan\n",
            " 0.70400015 0.70400015        nan 0.70400015 0.70400015        nan\n",
            " 0.70400015 0.70400015        nan 0.70350007 0.70350007        nan\n",
            " 0.70300027 0.70300027        nan 0.70300027 0.70300027        nan\n",
            " 0.70162535 0.70162535        nan 0.70112494 0.70112494        nan\n",
            " 0.70037532 0.70037532        nan 0.70000023 0.70000023        nan\n",
            " 0.70075023 0.70075023        nan 0.6989996  0.6989996         nan\n",
            " 0.57150077 0.58037658        nan 0.59912555 0.59812591        nan\n",
            " 0.60525058 0.60525058        nan 0.62112491 0.62112491        nan\n",
            " 0.70400015 0.70400015        nan 0.70400015 0.70400015        nan\n",
            " 0.70400015 0.70400015        nan 0.70350007 0.70350007        nan\n",
            " 0.70300027 0.70300027        nan 0.70300027 0.70300027        nan\n",
            " 0.70162535 0.70162535        nan 0.70112494 0.70112494        nan\n",
            " 0.70037532 0.70037532        nan 0.70000023 0.70000023        nan\n",
            " 0.70075023 0.70075023        nan 0.6989996  0.6989996         nan\n",
            " 0.50527485 0.50527485        nan 0.50527485 0.50527485        nan\n",
            " 0.50527485 0.50527485        nan 0.50502488 0.50502488        nan\n",
            " 0.64188277 0.64188277        nan 0.64188277 0.64188277        nan\n",
            " 0.64188277 0.64188277        nan 0.64188277 0.64188277        nan\n",
            " 0.50527485 0.50527485        nan 0.50527485 0.50527485        nan\n",
            " 0.50527485 0.50527485        nan 0.50502488 0.50502488        nan\n",
            " 0.50527485 0.50527485        nan 0.50527485 0.50527485        nan\n",
            " 0.50527485 0.50527485        nan 0.50502488 0.50502488        nan\n",
            " 0.53285565 0.53285565        nan 0.53298064 0.53298064        nan\n",
            " 0.53223045 0.53223045        nan 0.53335536 0.53335536        nan\n",
            " 0.52422836 0.52422836        nan 0.52422836 0.52422836        nan\n",
            " 0.52422836 0.52422836        nan 0.52422836 0.52422836        nan\n",
            " 0.44161711 0.44161711        nan 0.44161711 0.44161711        nan\n",
            " 0.44049183 0.44049183        nan 0.54135436 0.54135436        nan\n",
            " 0.53273067 0.5328557         nan 0.53298069 0.53298069        nan\n",
            " 0.53198039 0.53198039        nan 0.53323037 0.53323037        nan\n",
            " 0.56424619 0.56349628        nan 0.5619967  0.56224658        nan\n",
            " 0.54649831 0.54649831        nan 0.55149689 0.55149689        nan\n",
            " 0.52422836 0.52422836        nan 0.52422836 0.52422836        nan\n",
            " 0.52422836 0.52422836        nan 0.52422836 0.52422836        nan\n",
            " 0.56847264 0.56847264        nan 0.56847264 0.56847264        nan\n",
            " 0.56847264 0.56847264        nan 0.42349029 0.42349029        nan\n",
            " 0.52822987 0.52822987        nan 0.52822987 0.52822987        nan\n",
            " 0.52810484 0.52810484        nan 0.61212482 0.61212482        nan\n",
            " 0.58412428 0.5777491         nan 0.55825041 0.55712452        nan\n",
            " 0.5486238  0.5486238         nan 0.51699773 0.51699773        nan\n",
            " 0.52422836 0.52422836        nan 0.52422836 0.52422836        nan\n",
            " 0.52422836 0.52422836        nan 0.52422836 0.52422836        nan\n",
            " 0.56847264 0.56847264        nan 0.56847264 0.56847264        nan\n",
            " 0.56847264 0.56847264        nan 0.42349029 0.42349029        nan\n",
            " 0.52822987 0.52822987        nan 0.52822987 0.52822987        nan\n",
            " 0.52810484 0.52810484        nan 0.61212482 0.61212482        nan\n",
            " 0.50527485 0.50527485        nan 0.50527485 0.50527485        nan\n",
            " 0.50527485 0.50527485        nan 0.50502488 0.50502488        nan\n",
            " 0.64150782 0.64150782        nan 0.64150782 0.64150782        nan\n",
            " 0.64150782 0.64150782        nan 0.64188277 0.64188277        nan\n",
            " 0.50527485 0.50527485        nan 0.50527485 0.50527485        nan\n",
            " 0.50527485 0.50527485        nan 0.50502488 0.50502488        nan\n",
            " 0.50527485 0.50527485        nan 0.50527485 0.50527485        nan\n",
            " 0.50527485 0.50527485        nan 0.50502488 0.50502488        nan\n",
            " 0.51597803 0.51597803        nan 0.51610301 0.51610301        nan\n",
            " 0.51660309 0.51660309        nan 0.52935586 0.52935586        nan\n",
            " 0.6416328  0.6416328         nan 0.6416328  0.6416328         nan\n",
            " 0.64150782 0.64150782        nan 0.64188277 0.64188277        nan\n",
            " 0.44498676 0.44498676        nan 0.44498676 0.44498676        nan\n",
            " 0.44486178 0.44486178        nan 0.53635508 0.53635508        nan\n",
            " 0.51597803 0.51597803        nan 0.51610301 0.51610301        nan\n",
            " 0.51635303 0.51635303        nan 0.528231   0.528231          nan\n",
            " 0.58837689 0.58787705        nan 0.58637705 0.58650203        nan\n",
            " 0.57875272 0.57875272        nan 0.55599661 0.55599661        nan\n",
            " 0.6416328  0.6416328         nan 0.6416328  0.6416328         nan\n",
            " 0.64150782 0.64150782        nan 0.64188277 0.64188277        nan\n",
            " 0.5614722  0.5614722         nan 0.5614722  0.5614722         nan\n",
            " 0.56134722 0.56134722        nan 0.44411272 0.44411272        nan\n",
            " 0.44573761 0.44573761        nan 0.44586259 0.44586259        nan\n",
            " 0.44498751 0.44498751        nan 0.60824927 0.60824927        nan\n",
            " 0.5811241  0.56462466        nan 0.56199919 0.56249946        nan\n",
            " 0.55812514 0.55812514        nan 0.51987416 0.51987416        nan\n",
            " 0.6416328  0.6416328         nan 0.6416328  0.6416328         nan\n",
            " 0.64150782 0.64150782        nan 0.64188277 0.64188277        nan\n",
            " 0.5614722  0.5614722         nan 0.5614722  0.5614722         nan\n",
            " 0.56134722 0.56134722        nan 0.44411272 0.44411272        nan\n",
            " 0.44511245 0.44511245        nan 0.44523743 0.44523743        nan\n",
            " 0.44561267 0.44561267        nan 0.60824927 0.60824927        nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'max_depth': [3,5, 10, None],\n",
        "    'min_samples_split': [2, 5, None],\n",
        "    'min_samples_leaf': [1, 2, 4, 10],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_leaf_nodes': [None, 5,10,20],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=2)\n",
        "\n",
        "# Fit the model with GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and the best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "\n",
        "# Make predictions with the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raExB1sxWTdL"
      },
      "source": [
        "### Top  10 features ploting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-g7-N3w6wGu",
        "outputId": "bbe509e5-b1ea-4972-b05b-fb198e1fc51b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "feature_importances = clf.feature_importances_\n",
        "feature_names = X.columns\n",
        "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
        "\n",
        "# Get top 10 features\n",
        "top_features = importance_df.sort_values(by='importance', ascending=False).head(10)\n",
        "\n",
        "#\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='importance', y='feature', data=top_features)\n",
        "plt.title('Top 10 Feature Importances - Optimized Decision Tree')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NTjFl5PWTdM"
      },
      "source": [
        "### Plot the decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paUNpLRh6wGu",
        "outputId": "8c7c3bfc-17d3-48b6-a240-aceb5d337f12"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Fit a single decision tree to visualize\n",
        "tree_clf = DecisionTreeClassifier(max_depth=4)  # Limit depth for clarity\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Plot the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(tree_clf, feature_names=X.columns, class_names=['Illegal', 'Legal'], filled=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMxwAy9DI-Gj"
      },
      "source": [
        "## Random Forest: Egor, Ash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyuPWJkk6wGu"
      },
      "source": [
        "### Data for RF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHpdHftWuY9G"
      },
      "outputs": [],
      "source": [
        "# Get the data for RF model\n",
        "df_RF = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1xZqar6wGu"
      },
      "source": [
        "### This needs to be reviewed RF X and Y???"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_fcJDb6wGu",
        "outputId": "56ebc1c3-1468-4f64-dd98-444dfd98d5fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[ 183 1066]\n",
            " [ 253 1498]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.15      0.22      1249\n",
            "           1       0.58      0.86      0.69      1751\n",
            "\n",
            "    accuracy                           0.56      3000\n",
            "   macro avg       0.50      0.50      0.46      3000\n",
            "weighted avg       0.52      0.56      0.50      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define features (X) and target (y) - binary classification on 'Money Laundering Risk Score'\n",
        "X_new = df_RF.drop(columns=['Money Laundering Risk Score'])\n",
        "y_new = (df_RF['Money Laundering Risk Score'] >= 5).astype(int)  # Binary target: 1 if score >= 5, else 0\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the RandomForest classifier\n",
        "rf_clf_new = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf_new.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_new = rf_clf_new.predict(X_test_new)\n",
        "\n",
        "# Generate the confusion matrix and classification report\n",
        "conf_matrix_new = confusion_matrix(y_test_new, y_pred_new)\n",
        "class_report_new = classification_report(y_test_new, y_pred_new)\n",
        "\n",
        "# Display the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_new)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcTlOEnzWTdQ",
        "outputId": "e743adf7-a92a-4544-c46d-62411b7b0bb7"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.10.10' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/k/AppData/Local/Programs/Python/Python310/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "rf_clf_balanced = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "\n",
        "# Train the model\n",
        "rf_clf_balanced.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_balanced = rf_clf_balanced.predict(X_test_new)\n",
        "\n",
        "# Evaluate the model\n",
        "conf_matrix_balanced = confusion_matrix(y_test_new, y_pred_balanced)\n",
        "class_report_balanced = classification_report(y_test_new, y_pred_balanced)\n",
        "\n",
        "# Print results\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix_balanced)\n",
        "print(\"Classification Report:\\n\", class_report_balanced)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_XHnycD6wGu",
        "outputId": "2f8fbfc2-18d6-4b09-c2df-78487b527697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Confusion Matrix:\n",
            " [[   5 1244]\n",
            " [   4 1747]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.00      0.01      1249\n",
            "           1       0.58      1.00      0.74      1751\n",
            "\n",
            "    accuracy                           0.58      3000\n",
            "   macro avg       0.57      0.50      0.37      3000\n",
            "weighted avg       0.57      0.58      0.43      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define a simplified parameter grid for GridSearchCV\n",
        "simple_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the RandomForest model\n",
        "rf_clf_simplified = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "simple_grid_search = GridSearchCV(estimator=rf_clf_simplified,\n",
        "                                  param_grid=simple_param_grid,\n",
        "                                  cv=3,  # 3-fold cross-validation\n",
        "                                  verbose=1,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Fit the simplified grid search model\n",
        "simple_grid_search.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Best hyperparameters from the grid search\n",
        "best_params_simplified = simple_grid_search.best_params_\n",
        "\n",
        "# Train the best model on the training set\n",
        "best_rf_model_simplified = simple_grid_search.best_estimator_\n",
        "best_rf_model_simplified.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions with the tuned model\n",
        "y_pred_tuned_simplified = best_rf_model_simplified.predict(X_test_new)\n",
        "\n",
        "# Generate confusion matrix and classification report for the tuned model\n",
        "conf_matrix_tuned_simplified = confusion_matrix(y_test_new, y_pred_tuned_simplified)\n",
        "class_report_tuned_simplified = classification_report(y_test_new, y_pred_tuned_simplified)\n",
        "\n",
        "# Output best parameters, confusion matrix, and classification report\n",
        "print(\"Best Hyperparameters:\", best_params_simplified)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix_tuned_simplified)\n",
        "print(\"Classification Report:\\n\", class_report_tuned_simplified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiehsO8sI-ch"
      },
      "source": [
        "## SGD: Devanshi, James, Abraham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvQ8IYnXueAV"
      },
      "outputs": [],
      "source": [
        "# Stochastic Gradient Descent\n",
        "from sklearn.pipeline import make_pipeline\n",
        "#SGD\n",
        "df_SGD = df.copy()\n",
        "df_SGD_S = scale_features(df_SGD, features_to_modify)\n",
        "df_SGD_N = normalize_features(df_SGD, features_to_modify)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FSW-W7h6wGu",
        "outputId": "5a450080-df04-416c-9c20-3ba1e413ac48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       1.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.85      0.50      0.41      2000\n",
            "weighted avg       0.79      0.70      0.58      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1406    0]\n",
            " [ 593    1]]\n",
            "Classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.32      0.44      1406\n",
            "           1       0.30      0.69      0.42       594\n",
            "\n",
            "    accuracy                           0.43      2000\n",
            "   macro avg       0.50      0.50      0.43      2000\n",
            "weighted avg       0.59      0.43      0.43      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[448 958]\n",
            " [184 410]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nBased on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\\nit has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\\n\""
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_list = [df_SGD_S, df_SGD_N]\n",
        "cr_list = []\n",
        "cm_list = []\n",
        "\n",
        "# Search for the best df for standardization and normalization\n",
        "for i in range(len(df_list)):\n",
        "    df = df_list[i]\n",
        "    X = df.drop('Source of Money', axis=1)\n",
        "    y = df['Source of Money'].values\n",
        "    X_resampled, X_test, y_resampled, y_test = Undersampling(X, y, test_size=0.2)\n",
        "    SGD_classifier = SGDClassifier(random_state=0)\n",
        "    SGD_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred_lr_b = SGD_classifier.predict(X_test)\n",
        "    lr_cr = classification_report(y_test, y_pred_lr_b)\n",
        "    lr_cm = confusion_matrix(y_test, y_pred_lr_b)\n",
        "    cr_list.append(lr_cr)\n",
        "    cm_list.append(lr_cm)\n",
        "\n",
        "# Print the classification reports and confusion matrices\n",
        "for i in range(len(cr_list)):\n",
        "    print(\"Classification Reports:\")\n",
        "    print(cr_list[i])\n",
        "    print('Confusion Matrix:')\n",
        "    print(cm_list[i])\n",
        "\n",
        "'''\n",
        "Based on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\n",
        "it has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eMvTYXz6wGu",
        "outputId": "7c84c9ff-8f40-42d0-e638-9fdade26e588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
            "540 fits failed out of a total of 2700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'squared_error', 'modified_huber', 'huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'hinge', 'log_loss', 'perceptron'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "95 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'hinge', 'epsilon_insensitive', 'perceptron', 'squared_error', 'modified_huber', 'log_loss', 'squared_epsilon_insensitive', 'huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "36 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_error', 'squared_hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'modified_huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "77 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_hinge', 'huber', 'epsilon_insensitive', 'perceptron', 'modified_huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "77 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'hinge', 'huber', 'epsilon_insensitive', 'log_loss', 'squared_hinge', 'perceptron', 'squared_epsilon_insensitive', 'squared_error'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "79 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'log_loss', 'huber', 'epsilon_insensitive', 'perceptron', 'squared_error', 'squared_epsilon_insensitive', 'squared_hinge', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "64 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_hinge', 'modified_huber', 'squared_error', 'epsilon_insensitive', 'perceptron', 'squared_epsilon_insensitive', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "77 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'squared_error', 'hinge', 'squared_epsilon_insensitive', 'huber', 'perceptron', 'squared_hinge', 'epsilon_insensitive', 'log_loss'}. Got 'log' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.49728055 0.49728055 0.50083704        nan        nan        nan\n",
            " 0.4962354  0.49937282 0.50083682 0.50041885 0.49979036 0.50000022\n",
            " 0.50104712 0.50146553 0.49790576 0.49937173 0.49979058 0.4991623\n",
            "        nan        nan        nan 0.48828167 0.47990624 0.48723455\n",
            " 0.50418169 0.50397249 0.50397249 0.4966525  0.49916296 0.50020942\n",
            " 0.48450963 0.48262591 0.48639423        nan        nan        nan\n",
            " 0.48995268 0.49330369 0.49204605 0.49204868 0.49100177 0.49037372\n",
            " 0.49497733 0.49016013 0.49874937 0.49728055 0.49728055 0.50083704\n",
            "        nan        nan        nan 0.4962354  0.49937282 0.50083682\n",
            " 0.50041885 0.49979036 0.50000022 0.50104712 0.50146553 0.49790576\n",
            " 0.49330478 0.49120682 0.49706982        nan        nan        nan\n",
            " 0.4939267  0.49560297 0.49727749 0.48325484 0.48598151 0.49393218\n",
            " 0.49958115 0.49937282 0.4981154  0.48702469 0.48576508 0.48200026\n",
            "        nan        nan        nan 0.49225766 0.48974392 0.48932704\n",
            " 0.49769787 0.49811628 0.49811672 0.50251309 0.49267476 0.50356393\n",
            " 0.49728055 0.49728055 0.50083704        nan        nan        nan\n",
            " 0.4962354  0.49937282 0.50083682 0.50041885 0.49979036 0.50000022\n",
            " 0.50104712 0.50146553 0.49790576 0.49979079 0.49204232 0.50020921\n",
            "        nan        nan        nan 0.49958115 0.49979079 0.49874346\n",
            " 0.49790488 0.4953942  0.49267279 0.50230104 0.50083638 0.50271923\n",
            " 0.49016364 0.4861848  0.4893231         nan        nan        nan\n",
            " 0.48974019 0.4918364  0.48430261 0.49246336 0.48828189 0.49623212\n",
            " 0.50104778 0.49016189 0.49790466 0.49393305 0.50313808 0.498536\n",
            "        nan        nan        nan 0.49958115 0.50083704 0.50062762\n",
            " 0.4964468  0.4905849  0.49539946 0.49644351 0.49916274 0.49393043\n",
            " 0.49937173 0.49979058 0.49937173        nan        nan        nan\n",
            " 0.48849088 0.48430152 0.4817915  0.50271616 0.50292558 0.5027166\n",
            " 0.49853096 0.50041841 0.50313917 0.48534689 0.48618568 0.48597516\n",
            "        nan        nan        nan 0.48953668 0.49141646 0.49204758\n",
            " 0.49456242 0.49162961 0.49456242 0.50084033 0.48932989 0.49309382\n",
            " 0.49393305 0.50313808 0.498536          nan        nan        nan\n",
            " 0.49958115 0.50083704 0.50062762 0.4964468  0.4905849  0.49539946\n",
            " 0.49644351 0.49916274 0.49393043 0.4983257  0.48932112 0.49623037\n",
            "        nan        nan        nan 0.4939267  0.49162501 0.49518215\n",
            " 0.49413985 0.4989531  0.48890929 0.5        0.49769634 0.49979101\n",
            " 0.49058227 0.48995159 0.48639444        nan        nan        nan\n",
            " 0.48911345 0.48890556 0.49037219 0.50376744 0.49853513 0.50460361\n",
            " 0.50523451 0.49037569 0.49099915 0.49393305 0.50313808 0.498536\n",
            "        nan        nan        nan 0.49958115 0.50083704 0.50062762\n",
            " 0.4964468  0.4905849  0.49539946 0.49644351 0.49916274 0.49393043\n",
            " 0.5        0.50020942 0.4976959         nan        nan        nan\n",
            " 0.49958115 0.50020833 0.49979123 0.50251046 0.49267279 0.49811715\n",
            " 0.49874521 0.49727793 0.50083726 0.48157988 0.49162567 0.48534908\n",
            "        nan        nan        nan 0.48848518 0.4895334  0.48953405\n",
            " 0.4951839  0.48870096 0.4912112  0.50041819 0.4966536  0.48702381\n",
            " 0.49958115 0.49979058 0.49246117        nan        nan        nan\n",
            " 0.49372078 0.49330215 0.49350676 0.49183816 0.50250936 0.49121032\n",
            " 0.49560341 0.50020942 0.49958137 0.4991623  0.5        0.50041885\n",
            "        nan        nan        nan 0.48995487 0.49811584 0.48430196\n",
            " 0.50334597 0.50208745 0.49811102 0.50188263 0.49832461 0.49916165\n",
            " 0.4945541  0.49832636 0.49204123        nan        nan        nan\n",
            " 0.49204473 0.48848628 0.48911674 0.5016743  0.49246621 0.50146553\n",
            " 0.50272339 0.50062827 0.4964433  0.49958115 0.49979058 0.49246117\n",
            "        nan        nan        nan 0.49372078 0.49330215 0.49350676\n",
            " 0.49183816 0.50250936 0.49121032 0.49560341 0.50020942 0.49958137\n",
            " 0.49853403 0.49518325 0.49015751        nan        nan        nan\n",
            " 0.4939267  0.49664746 0.49162413 0.48598042 0.49163005 0.48932726\n",
            " 0.5008377  0.5        0.49623431 0.48304497 0.50105084 0.48995203\n",
            "        nan        nan        nan 0.48848825 0.49204627 0.48702403\n",
            " 0.49183356 0.49916274 0.48639729 0.50230542 0.49769699 0.4991634\n",
            " 0.49958115 0.49979058 0.49246117        nan        nan        nan\n",
            " 0.49372078 0.49330215 0.49350676 0.49183816 0.50250936 0.49121032\n",
            " 0.49560341 0.50020942 0.49958137 0.50020942 0.5        0.50146444\n",
            "        nan        nan        nan 0.49329843 0.5        0.49811628\n",
            " 0.50209183 0.49644132 0.492673   0.49748932 0.5        0.4993726\n",
            " 0.48932638 0.49979233 0.49120463        nan        nan        nan\n",
            " 0.48807006 0.49037065 0.48513856 0.49036934 0.48744244 0.48807203\n",
            " 0.50481588 0.49832439 0.49979233 0.50104712 0.49979058 0.5\n",
            "        nan        nan        nan 0.494137   0.5        0.49350895\n",
            " 0.49267476 0.50020658 0.49288462 0.49916318 0.5        0.5\n",
            " 0.49979058 0.5        0.5               nan        nan        nan\n",
            " 0.49330347 0.5        0.49811737 0.50439111 0.49455585 0.50103551\n",
            " 0.49665053 0.5        0.5        0.49120266 0.5        0.50146444\n",
            "        nan        nan        nan 0.48932551 0.50377007 0.49016035\n",
            " 0.49267257 0.49142018 0.48995246 0.50021249 0.5        0.49874477\n",
            " 0.50104712 0.49979058 0.5               nan        nan        nan\n",
            " 0.494137   0.5        0.49350895 0.49267476 0.50020658 0.49288462\n",
            " 0.49916318 0.5        0.5        0.49644351 0.5        0.5\n",
            "        nan        nan        nan 0.4939267  0.49979058 0.49811518\n",
            " 0.51255734 0.48953778 0.5117203  0.49979058 0.5        0.50020942\n",
            " 0.49434949 0.5        0.49895397        nan        nan        nan\n",
            " 0.48493067 0.49685864 0.49351092 0.48639423 0.49330412 0.49518456\n",
            " 0.49812154 0.49979058 0.49497777 0.50104712 0.49979058 0.5\n",
            "        nan        nan        nan 0.494137   0.5        0.49350895\n",
            " 0.49267476 0.50020658 0.49288462 0.49916318 0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.49664921 0.5        0.49979058 0.50020942 0.49748954 0.49518325\n",
            " 0.5        0.5        0.5        0.49644176 0.5        0.49895397\n",
            "        nan        nan        nan 0.49057942 0.49686214 0.49372144\n",
            " 0.49016079 0.49225832 0.49162523 0.5008423  0.5        0.5       ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
            "0.5125573397007601\n",
            "SGDClassifier(alpha=0.1, eta0=0.1, learning_rate='invscaling',\n",
            "              loss='squared_hinge', random_state=0)\n",
            "[[ 395 1011]\n",
            " [ 159  435]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.28      0.40      1406\n",
            "           1       0.30      0.73      0.43       594\n",
            "\n",
            "    accuracy                           0.41      2000\n",
            "   macro avg       0.51      0.51      0.41      2000\n",
            "weighted avg       0.59      0.41      0.41      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#grip search for SGD\n",
        "X = df_SGD_N.drop('Source of Money', axis=1)\n",
        "y = df_SGD_N['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "    'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
        "    'eta0': [0.01, 0.1, 1.0]\n",
        "}\n",
        "sgd = SGDClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=sgd,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=2,\n",
        "                           #scoring='precision'\n",
        "                           )\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdxFiSw86wGu",
        "outputId": "99bc0ede-6ea6-4c93-d9b1-1af78a2a8cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000879 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000069 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n"
          ]
        }
      ],
      "source": [
        "b_features_list = []\n",
        "feature_selection_df_list = []\n",
        "X = df_SGD_N.drop('Source of Money', axis=1)\n",
        "y = df_SGD_N['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "num_feats = 20\n",
        "for num in range(0, num_feats):\n",
        "    methods = ['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n",
        "    best_features, feature_selection_df = autoFeatureSelector(X_resampled, y_resampled, num+1, methods)\n",
        "    b_features_list.append(best_features)\n",
        "    feature_selection_df_list.append(feature_selection_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ2wvlf06wGv",
        "outputId": "e3c83522-df77-4bae-aff0-c366ab018db6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[  41 1365]\n",
            " [  22  572]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.03      0.06      1406\n",
            "           1       0.30      0.96      0.45       594\n",
            "\n",
            "    accuracy                           0.31      2000\n",
            "   macro avg       0.47      0.50      0.25      2000\n",
            "weighted avg       0.55      0.31      0.17      2000\n",
            "\n",
            "[[ 330 1076]\n",
            " [ 137  457]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.23      0.35      1406\n",
            "           1       0.30      0.77      0.43       594\n",
            "\n",
            "    accuracy                           0.39      2000\n",
            "   macro avg       0.50      0.50      0.39      2000\n",
            "weighted avg       0.59      0.39      0.38      2000\n",
            "\n",
            "[[ 244 1162]\n",
            " [ 127  467]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.17      0.27      1406\n",
            "           1       0.29      0.79      0.42       594\n",
            "\n",
            "    accuracy                           0.36      2000\n",
            "   macro avg       0.47      0.48      0.35      2000\n",
            "weighted avg       0.55      0.36      0.32      2000\n",
            "\n",
            "[[698 708]\n",
            " [316 278]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.50      0.58      1406\n",
            "           1       0.28      0.47      0.35       594\n",
            "\n",
            "    accuracy                           0.49      2000\n",
            "   macro avg       0.49      0.48      0.46      2000\n",
            "weighted avg       0.57      0.49      0.51      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[1030  376]\n",
            " [ 443  151]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.73      0.72      1406\n",
            "           1       0.29      0.25      0.27       594\n",
            "\n",
            "    accuracy                           0.59      2000\n",
            "   macro avg       0.49      0.49      0.49      2000\n",
            "weighted avg       0.58      0.59      0.58      2000\n",
            "\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "[[1296  110]\n",
            " [ 557   37]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.92      0.80      1406\n",
            "           1       0.25      0.06      0.10       594\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.48      0.49      0.45      2000\n",
            "weighted avg       0.57      0.67      0.59      2000\n",
            "\n",
            "[[1255  151]\n",
            " [ 551   43]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.89      0.78      1406\n",
            "           1       0.22      0.07      0.11       594\n",
            "\n",
            "    accuracy                           0.65      2000\n",
            "   macro avg       0.46      0.48      0.45      2000\n",
            "weighted avg       0.55      0.65      0.58      2000\n",
            "\n",
            "[[ 173 1233]\n",
            " [  80  514]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.12      0.21      1406\n",
            "           1       0.29      0.87      0.44       594\n",
            "\n",
            "    accuracy                           0.34      2000\n",
            "   macro avg       0.49      0.49      0.32      2000\n",
            "weighted avg       0.57      0.34      0.28      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 365 1041]\n",
            " [ 169  425]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.26      0.38      1406\n",
            "           1       0.29      0.72      0.41       594\n",
            "\n",
            "    accuracy                           0.40      2000\n",
            "   macro avg       0.49      0.49      0.39      2000\n",
            "weighted avg       0.57      0.40      0.39      2000\n",
            "\n",
            "[[1405    1]\n",
            " [ 593    1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.50      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.60      0.50      0.41      2000\n",
            "weighted avg       0.64      0.70      0.58      2000\n",
            "\n",
            "[[1132  274]\n",
            " [ 479  115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.81      0.75      1406\n",
            "           1       0.30      0.19      0.23       594\n",
            "\n",
            "    accuracy                           0.62      2000\n",
            "   macro avg       0.50      0.50      0.49      2000\n",
            "weighted avg       0.58      0.62      0.60      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "cm_list = []\n",
        "cr_list = []\n",
        "accuracy_list = []\n",
        "f1_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "for i in range(0,len(b_features_list)):\n",
        "    X = df_SGD_N[b_features_list[i]]\n",
        "    y = df_SGD_N['Source of Money']\n",
        "    X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "    SGD_classifier = SGDClassifier(random_state=0)\n",
        "    SGD_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred = SGD_classifier.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cr = classification_report(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(cm)\n",
        "    print(cr)\n",
        "    cm_list.append(cm)\n",
        "    cr_list.append(cr)\n",
        "    accuracy_list.append(accuracy)\n",
        "    f1_list.append(f1)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loV9YnmZ6wGv",
        "outputId": "1818de0f-6603-4c40-eff5-cbb95fb8394b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Accuracy\n",
            "2\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "Best Model F1\n",
            "0\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "Best Model Precision\n",
            "17\n",
            "[[1405    1]\n",
            " [ 593    1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.50      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.60      0.50      0.41      2000\n",
            "weighted avg       0.64      0.70      0.58      2000\n",
            "\n",
            "Best Model Recall\n",
            "0\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#select the best model\n",
        "best_model_acc = np.argmax(accuracy_list)\n",
        "best_model_f1 = np.argmax(f1_list)\n",
        "best_model_precision = np.argmax(precision_list)\n",
        "best_model_recall = np.argmax(recall_list)\n",
        "print(\"Best Model Accuracy\")\n",
        "print(best_model_acc)\n",
        "print(cm_list[best_model_acc])\n",
        "print(cr_list[best_model_acc])\n",
        "print(\"Best Model F1\")\n",
        "print(best_model_f1)\n",
        "print(cm_list[best_model_f1])\n",
        "print(cr_list[best_model_f1])\n",
        "print(\"Best Model Precision\")\n",
        "print(best_model_precision)\n",
        "print(cm_list[best_model_precision])\n",
        "print(cr_list[best_model_precision])\n",
        "print(\"Best Model Recall\")\n",
        "print(best_model_recall)\n",
        "print(cm_list[best_model_recall])\n",
        "print(cr_list[best_model_recall])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlHuXkq06wGv",
        "outputId": "19dfb275-0159-4f81-c890-2b124b877846"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#visualize the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Total', y='Feature', data=feature_selection_df_list[best_model])\n",
        "plt.title('Feature Selection')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHBoid3O6wGv",
        "outputId": "fdbac012-06a8-4192-88a6-81bca102f0a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "{'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
            "0.5161131678678613\n",
            "SGDClassifier(eta0=0.01, learning_rate='adaptive', random_state=0)\n",
            "[[ 302 1104]\n",
            " [ 132  462]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.21      0.33      1406\n",
            "           1       0.30      0.78      0.43       594\n",
            "\n",
            "    accuracy                           0.38      2000\n",
            "   macro avg       0.50      0.50      0.38      2000\n",
            "weighted avg       0.58      0.38      0.36      2000\n",
            "\n",
            "0.382\n",
            "0.42777777777777776\n",
            "0.2950191570881226\n",
            "0.7777777777777778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
            "540 fits failed out of a total of 2700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "81 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'squared_error', 'modified_huber', 'huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'hinge', 'log_loss', 'perceptron'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "92 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'hinge', 'epsilon_insensitive', 'perceptron', 'squared_error', 'modified_huber', 'log_loss', 'squared_epsilon_insensitive', 'huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "85 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'squared_error', 'hinge', 'squared_epsilon_insensitive', 'huber', 'perceptron', 'squared_hinge', 'epsilon_insensitive', 'log_loss'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "94 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_hinge', 'modified_huber', 'squared_error', 'epsilon_insensitive', 'perceptron', 'squared_epsilon_insensitive', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "70 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'hinge', 'huber', 'epsilon_insensitive', 'log_loss', 'squared_hinge', 'perceptron', 'squared_epsilon_insensitive', 'squared_error'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "28 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_error', 'squared_hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'modified_huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "19 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'log_loss', 'huber', 'epsilon_insensitive', 'perceptron', 'squared_error', 'squared_epsilon_insensitive', 'squared_hinge', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "71 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_hinge', 'huber', 'epsilon_insensitive', 'perceptron', 'modified_huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.5        0.50502092 0.5               nan        nan        nan\n",
            " 0.49937173 0.50104712 0.50146597 0.49937173 0.50104712 0.50146597\n",
            " 0.49623453 0.50125523 0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51443953 0.51443953 0.51443953\n",
            " 0.51443953 0.51443953 0.51443953 0.50272251 0.49979058 0.5\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.50753029 0.49895288 0.49895288 0.5        0.50502092 0.5\n",
            "        nan        nan        nan 0.49937173 0.50104712 0.50146597\n",
            " 0.49937173 0.50104712 0.50146597 0.49623453 0.50125523 0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.49560209 0.49979058 0.49979058 0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.49895354 0.50125654 0.50020942\n",
            " 0.5        0.50502092 0.5               nan        nan        nan\n",
            " 0.49937173 0.50104712 0.50146597 0.49937173 0.50104712 0.50146597\n",
            " 0.49623453 0.50125523 0.5        0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.50899582 0.51192469 0.50878661\n",
            " 0.50899582 0.51192469 0.50878661 0.51192753 0.5        0.51276654\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.51046288 0.5        0.4991623  0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.4991634  0.51192469 0.51192469\n",
            " 0.4991634  0.51192469 0.51192469 0.49623321 0.50020942 0.50125654\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51443953 0.51443953 0.51443953 0.51443953 0.51443953 0.51443953\n",
            " 0.49560209 0.5        0.5        0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.49978926 0.5        0.49895288\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.4991634  0.51192469 0.51192469 0.4991634  0.51192469 0.51192469\n",
            " 0.49623321 0.50020942 0.50125654 0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.50209424 0.5        0.5\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.49916428 0.5        0.50125654 0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.4991634  0.51192469 0.51192469\n",
            " 0.4991634  0.51192469 0.51192469 0.49623321 0.50020942 0.50125654\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.50899582 0.5041841  0.50878661 0.50899582 0.5041841  0.50878661\n",
            " 0.50941664 0.4991623  0.50020942 0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.50230301 0.50020942 0.50272251\n",
            " 0.51611317 0.5        0.51213411        nan        nan        nan\n",
            " 0.5        0.5        0.50418848 0.5        0.5        0.50418848\n",
            " 0.4991623  0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51527635 0.50376635 0.51443953\n",
            " 0.51527635 0.50376635 0.51443953 0.50209424 0.5        0.5\n",
            " 0.51611317 0.49895288 0.51255296        nan        nan        nan\n",
            " 0.51611317 0.51255296 0.51611317 0.51611317 0.51255296 0.51611317\n",
            " 0.50418301 0.5        0.5        0.51611317 0.5        0.51213411\n",
            "        nan        nan        nan 0.5        0.5        0.50418848\n",
            " 0.5        0.5        0.50418848 0.4991623  0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.51255296 0.51611317 0.51611317 0.51255296 0.51611317\n",
            " 0.50272251 0.5        0.5        0.50376635 0.5        0.49895288\n",
            "        nan        nan        nan 0.51611317 0.50983329 0.51611317\n",
            " 0.51611317 0.50983329 0.51611317 0.49330193 0.5        0.5\n",
            " 0.51611317 0.5        0.51213411        nan        nan        nan\n",
            " 0.5        0.5        0.50418848 0.5        0.5        0.50418848\n",
            " 0.4991623  0.5        0.5        0.51611317 0.5        0.51611317\n",
            "        nan        nan        nan 0.508159   0.50648536 0.50732218\n",
            " 0.508159   0.50648536 0.50732218 0.50836864 0.5        0.4991623\n",
            " 0.51611317 0.5        0.50460251        nan        nan        nan\n",
            " 0.51611317 0.50292953 0.51611317 0.51611317 0.50669522 0.51611317\n",
            " 0.49937107 0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.5        0.4991623\n",
            " 0.51611317 0.5        0.5        0.4991623  0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51527635 0.5        0.49937173 0.51527635 0.5        0.49937173\n",
            " 0.50272251 0.5        0.5        0.50711275 0.49979058 0.49979058\n",
            "        nan        nan        nan 0.51611317 0.5        0.49937173\n",
            " 0.51611317 0.5        0.49937173 0.50334553 0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.5        0.4991623  0.51611317 0.5        0.5\n",
            " 0.4991623  0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.5        0.49937173\n",
            " 0.51611317 0.5        0.49937173 0.50314136 0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.5        0.50020942 0.51611317 0.5        0.50020942\n",
            " 0.49162501 0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.5        0.4991623\n",
            " 0.51611317 0.5        0.5        0.4991623  0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.50774059 0.5        0.5        0.50774059 0.5        0.5\n",
            " 0.50188241 0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.50020942 0.50062827\n",
            " 0.51611317 0.5        0.50020942 0.50041863 0.5        0.5       ]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X = df_SGD_N[best_features]\n",
        "y = df_SGD_N['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "param_grid = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        "                'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "                'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
        "                'eta0': [0.01, 0.1, 1.0] }\n",
        "sgd = SGDClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=sgd,\n",
        "                        param_grid=param_grid,\n",
        "                        cv=5,\n",
        "                        n_jobs=-1,\n",
        "                        verbose=2,\n",
        "                        #scoring='precision'\n",
        "                        )\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "#print(grid_search.best_params_)\n",
        "#print(grid_search.best_score_)\n",
        "#print(grid_search.best_estimator_)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "print(cr)\n",
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w0T1cN_6wGv",
        "outputId": "8ec688fb-0559-454a-cc7b-7fd24f451f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 302 1104]\n",
            " [ 132  462]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.21      0.33      1406\n",
            "           1       0.30      0.78      0.43       594\n",
            "\n",
            "    accuracy                           0.38      2000\n",
            "   macro avg       0.50      0.50      0.38      2000\n",
            "weighted avg       0.58      0.38      0.36      2000\n",
            "\n",
            "0.382\n",
            "0.42777777777777776\n",
            "0.2950191570881226\n",
            "0.7777777777777778\n"
          ]
        }
      ],
      "source": [
        "print(cm)\n",
        "print(cr)\n",
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfLZECBwI-xV"
      },
      "source": [
        "## SVM: Eric, Moosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def printMetrics(y_test, y_pred):\n",
        "  print(confusion_matrix(y_test, y_pred,  labels=[0, 1]))\n",
        "  print(f\"SVC Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "  print(classification_report(y_test, y_pred, zero_division=0, labels=[0, 1]))"
      ],
      "metadata": {
        "id": "VjAN63BxXCru"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdXWC5Pr6wGv"
      },
      "source": [
        "### Data for SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XowPRU4muiu-"
      },
      "outputs": [],
      "source": [
        "df_svm = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jHBJiJen6wGv"
      },
      "outputs": [],
      "source": [
        "X = df_svm.drop(columns=['Source of Money'])\n",
        "y = df_svm['Source of Money']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz09tJyW6wGv"
      },
      "source": [
        "### Splitting the data for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jG5GOJ9i6wGv"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Linear SVM Classification"
      ],
      "metadata": {
        "id": "2rOYQVv6WdK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "linearSVC = LinearSVC(max_iter=10000, dual=True, class_weight='balanced')\n",
        "\n",
        "linearSVC.fit(X_train, y_train)\n",
        "\n",
        "y_pred = linearSVC.predict(X_test)\n",
        "\n",
        "printMetrics(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFOK4NffWncG",
        "outputId": "32838f72-16c8-459a-8ba1-51f96d729495"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2105    0]\n",
            " [ 895    0]]\n",
            "SVC Accuracy: 0.7017\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.82      2105\n",
            "           1       0.00      0.00      0.00       895\n",
            "\n",
            "    accuracy                           0.70      3000\n",
            "   macro avg       0.35      0.50      0.41      3000\n",
            "weighted avg       0.49      0.70      0.58      3000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Nonlinear SVM Classification"
      ],
      "metadata": {
        "id": "BC5PTWtHX1Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_SVM(X_train_, X_test_, y_train_, y_test_):\n",
        "  svc_model = SVC(kernel='rbf',  class_weight='balanced')\n",
        "  svc_model.fit(X_train_, y_train_)\n",
        "  y_pred = svc_model.predict(X_test_)\n",
        "\n",
        "  printMetrics(y_test_, y_pred)"
      ],
      "metadata": {
        "id": "Q_R7K7VmXkeI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Undersampling"
      ],
      "metadata": {
        "id": "XZIPAXaLc2Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_und, X_test_und, y_train_und, y_test_und = Undersampling(X,y, test_size=0.3)\n",
        "train_SVM(X_train_und, X_test_und, y_train_und, y_test_und)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAZEpaSpYDT6",
        "outputId": "4446ec3b-ef96-47ad-c56d-44da79cbb873"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1376  729]\n",
            " [ 581  314]]\n",
            "SVC Accuracy: 0.5633\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.65      0.68      2105\n",
            "           1       0.30      0.35      0.32       895\n",
            "\n",
            "    accuracy                           0.56      3000\n",
            "   macro avg       0.50      0.50      0.50      3000\n",
            "weighted avg       0.58      0.56      0.57      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Feature Selector"
      ],
      "metadata": {
        "id": "3VYCJ2YNc63e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "methods = ['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n",
        "\n",
        "best_features, feature_selection_df = autoFeatureSelector(X, y, 5, methods)\n",
        "\n",
        "X_selected = X.loc[:, best_features]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "train_SVM(X_train_feat, X_test_feat, y_train_feat, y_test_feat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVSfCnnmZJmO",
        "outputId": "9bbee778-9f48-4ddb-db3f-e373b8b9a874"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2983, number of negative: 7017\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000926 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 379\n",
            "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.298300 -> initscore=-0.855406\n",
            "[LightGBM] [Info] Start training from score -0.855406\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "[[1296  794]\n",
            " [ 596  314]]\n",
            "SVC Accuracy: 0.5367\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.62      0.65      2090\n",
            "           1       0.28      0.35      0.31       910\n",
            "\n",
            "    accuracy                           0.54      3000\n",
            "   macro avg       0.48      0.48      0.48      3000\n",
            "weighted avg       0.56      0.54      0.55      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXveRmRB6wGv"
      },
      "source": [
        "### GridSearchCV (Hyper Parameter tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBFFQc7E6wGv"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning using GridSearchCV\n",
        "# SVM GridSearchCV params\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['rbf'],\n",
        "    'gamma': [1,0.1,0.01,0.001,0.0001]\n",
        "}\n",
        "grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)\n",
        "grid_svm.fit(X_train, y_train)\n",
        "print(f\"Best SVM Parameters: {grid_svm.best_params_}\")\n",
        "print(f\"Best SVM Accuracy: {grid_svm.best_score_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mTwjwTnqDiD"
      },
      "source": [
        "# Conclusion and comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQg4pZMmIX90"
      },
      "source": [
        "Present your work including approach and findings during the class on September 24th or 26th, 2024. Each group will have a maximum of 15 minutes to present their project. It is advised that your PowerPoint files to be no longer than 15 slides.\n",
        "\n",
        "Prepare a written technical report of no longer than 15 pages to discuss the problem statement, various steps conducted, summary of findings and conclusions. Submit the report and the notebook file (with proper headings, explanatory comments and code sections) by the midnight of September 29th, 2024."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}