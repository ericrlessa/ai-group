{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v52S4rZMHsK3"
      },
      "source": [
        "# Term Project Group 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceU4aZcRIdh8"
      },
      "source": [
        "The Dataset: https://www.kaggle.com/datasets/waqi786/global-black-money-transactions-dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyyUVrF_IioY"
      },
      "source": [
        "### Explanation:\n",
        "\n",
        "This dataset gives a solid overview of black money transactions in different countries, focusing on financial activities tied to illegal dealings. It includes details like transaction amounts and risk scores, making it super useful for anyone looking to study financial crime trends or work on anti-money laundering tools.\n",
        "\n",
        "\n",
        "### Dataset:\n",
        "\n",
        "Transaction ID: Unique identifier for each transaction. (e.g., TX0000001)\n",
        "\n",
        "Country: Country where the transaction occurred. (e.g., USA, China)\n",
        "\n",
        "Amount (USD): Transaction amount in US Dollars. (e.g., 150000.00)\n",
        "\n",
        "Transaction Type: Type of transaction. (e.g., Offshore Transfer, Property Purchase)\n",
        "\n",
        "Date of Transaction: The date and time of the transaction. (e.g., 2022-03-15 14:32:00)\n",
        "\n",
        "Person Involved: Name or identifier of the person/entity involved. (e.g., Person_1234)\n",
        "\n",
        "Industry: Industry associated with the transaction. (e.g., Real Estate, Finance)\n",
        "\n",
        "Destination Country: Country where the money was sent. (e.g., Switzerland)\n",
        "\n",
        "Reported by Authority: Whether the transaction was reported to authorities. (e.g., True/False)\n",
        "\n",
        "Source of Money: Origin of the money. (e.g., Legal, Illegal)\n",
        "\n",
        "Money Laundering Risk Score: Risk score indicating the likelihood of money\n",
        "laundering (1-10). (e.g., 8)\n",
        "\n",
        "Shell Companies Involved: Number of shell companies used in the transaction. (e.g., 3)\n",
        "\n",
        "Financial Institution: Bank or financial institution involved in the transaction. (e.g., Bank_567)\n",
        "\n",
        "Tax Haven Country: Country where the money was transferred to a tax haven. (e.g., Cayman Islands)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFAjkKdlH4LW"
      },
      "source": [
        "# Pre-process and clean the dataset as appropriate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piBKVR5r6wGk"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "NfzVSt-I6wGk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import SVC\n",
        "# from lightgbm import LGBMClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFE, SelectFromModel, chi2, SelectKBest\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bNKeluqIu7D"
      },
      "source": [
        "## Exploring and visualizing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgPt-hfO6wGk"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GTUfUTkk6wGl"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/ericrlessa/global-illicit-money-transactions/refs/heads/main/Big_Black_Money_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualise some features\n",
        "plt.pie(df['Reported by Authority'].value_counts(), labels=df['Source of Money'].unique(), autopct='%1.1f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RM-m9W_C797X",
        "outputId": "c1c4ccbc-4b97-4c7c-c339-ac1078c9962c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=df['Shell Companies Involved'])"
      ],
      "metadata": {
        "id": "XvQRDQYP83Aa",
        "outputId": "e81671e7-474f-4d80-dc47-6fc5e424c28e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Shell Companies Involved', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OOIer9R86wGl",
        "outputId": "29b4429b-5f0a-4105-8359-f7866bb6ddbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Transaction ID       Country  Amount (USD)   Transaction Type  \\\n",
              "0   TX0000000001        Brazil  3.267530e+06  Offshore Transfer   \n",
              "1   TX0000000002         China  4.965767e+06    Stocks Transfer   \n",
              "2   TX0000000003            UK  9.416750e+04    Stocks Transfer   \n",
              "3   TX0000000004           UAE  3.864201e+05    Cash Withdrawal   \n",
              "4   TX0000000005  South Africa  6.433784e+05     Cryptocurrency   \n",
              "\n",
              "   Date of Transaction Person Involved      Industry Destination Country  \\\n",
              "0  2013-01-01 00:00:00     Person_1101  Construction                 USA   \n",
              "1  2013-01-01 01:00:00     Person_7484  Luxury Goods        South Africa   \n",
              "2  2013-01-01 02:00:00     Person_3655  Construction         Switzerland   \n",
              "3  2013-01-01 03:00:00     Person_3226     Oil & Gas              Russia   \n",
              "4  2013-01-01 04:00:00     Person_7975   Real Estate                 USA   \n",
              "\n",
              "   Reported by Authority Source of Money  Money Laundering Risk Score  \\\n",
              "0                   True         Illegal                            6   \n",
              "1                  False         Illegal                            9   \n",
              "2                   True         Illegal                            1   \n",
              "3                  False         Illegal                            7   \n",
              "4                   True         Illegal                            1   \n",
              "\n",
              "   Shell Companies Involved Financial Institution Tax Haven Country  \n",
              "0                         1               Bank_40         Singapore  \n",
              "1                         0              Bank_461           Bahamas  \n",
              "2                         3              Bank_387       Switzerland  \n",
              "3                         2              Bank_353            Panama  \n",
              "4                         9               Bank_57        Luxembourg  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ebf5fe9-6b92-49de-8337-057ebf46e9de\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Transaction ID</th>\n",
              "      <th>Country</th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Transaction Type</th>\n",
              "      <th>Date of Transaction</th>\n",
              "      <th>Person Involved</th>\n",
              "      <th>Industry</th>\n",
              "      <th>Destination Country</th>\n",
              "      <th>Reported by Authority</th>\n",
              "      <th>Source of Money</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <th>Financial Institution</th>\n",
              "      <th>Tax Haven Country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TX0000000001</td>\n",
              "      <td>Brazil</td>\n",
              "      <td>3.267530e+06</td>\n",
              "      <td>Offshore Transfer</td>\n",
              "      <td>2013-01-01 00:00:00</td>\n",
              "      <td>Person_1101</td>\n",
              "      <td>Construction</td>\n",
              "      <td>USA</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>Bank_40</td>\n",
              "      <td>Singapore</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TX0000000002</td>\n",
              "      <td>China</td>\n",
              "      <td>4.965767e+06</td>\n",
              "      <td>Stocks Transfer</td>\n",
              "      <td>2013-01-01 01:00:00</td>\n",
              "      <td>Person_7484</td>\n",
              "      <td>Luxury Goods</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>False</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>Bank_461</td>\n",
              "      <td>Bahamas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TX0000000003</td>\n",
              "      <td>UK</td>\n",
              "      <td>9.416750e+04</td>\n",
              "      <td>Stocks Transfer</td>\n",
              "      <td>2013-01-01 02:00:00</td>\n",
              "      <td>Person_3655</td>\n",
              "      <td>Construction</td>\n",
              "      <td>Switzerland</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Bank_387</td>\n",
              "      <td>Switzerland</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TX0000000004</td>\n",
              "      <td>UAE</td>\n",
              "      <td>3.864201e+05</td>\n",
              "      <td>Cash Withdrawal</td>\n",
              "      <td>2013-01-01 03:00:00</td>\n",
              "      <td>Person_3226</td>\n",
              "      <td>Oil &amp; Gas</td>\n",
              "      <td>Russia</td>\n",
              "      <td>False</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>Bank_353</td>\n",
              "      <td>Panama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TX0000000005</td>\n",
              "      <td>South Africa</td>\n",
              "      <td>6.433784e+05</td>\n",
              "      <td>Cryptocurrency</td>\n",
              "      <td>2013-01-01 04:00:00</td>\n",
              "      <td>Person_7975</td>\n",
              "      <td>Real Estate</td>\n",
              "      <td>USA</td>\n",
              "      <td>True</td>\n",
              "      <td>Illegal</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>Bank_57</td>\n",
              "      <td>Luxembourg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ebf5fe9-6b92-49de-8337-057ebf46e9de')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ebf5fe9-6b92-49de-8337-057ebf46e9de button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ebf5fe9-6b92-49de-8337-057ebf46e9de');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8c956bac-5df0-4611-bbac-5aa4449ba311\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8c956bac-5df0-4611-bbac-5aa4449ba311')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8c956bac-5df0-4611-bbac-5aa4449ba311 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"Transaction ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"TX0000006253\",\n          \"TX0000004685\",\n          \"TX0000001732\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"USA\",\n          \"China\",\n          \"Russia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Amount (USD)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1424364.282674346,\n        \"min\": 10031.796729514868,\n        \"max\": 4999812.408537276,\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          2270918.122876275,\n          3092635.118538095,\n          3328773.146613001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Transaction Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Stocks Transfer\",\n          \"Property Purchase\",\n          \"Cash Withdrawal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Date of Transaction\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"2013-09-18 12:00:00\",\n          \"2013-07-15 04:00:00\",\n          \"2013-03-14 03:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Person Involved\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6320,\n        \"samples\": [\n          \"Person_947\",\n          \"Person_1038\",\n          \"Person_3893\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Industry\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Construction\",\n          \"Luxury Goods\",\n          \"Casinos\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Destination Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Singapore\",\n          \"South Africa\",\n          \"UK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Reported by Authority\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Source of Money\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Legal\",\n          \"Illegal\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Money Laundering Risk Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 10,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          10,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Shell Companies Involved\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 0,\n        \"max\": 9,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          7,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Financial Institution\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 499,\n        \"samples\": [\n          \"Bank_193\",\n          \"Bank_180\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tax Haven Country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Singapore\",\n          \"Bahamas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x783682c62bc0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_0['Amount (USD)'].plot(kind='hist', bins=20, title='Amount (USD)')\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-df6e33aa-de0d-4dae-bdb8-b4d5705bdc95\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAioAAAG5CAYAAACgKh/DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAjCElEQVR4nO3dfVSUdf7/8dcI7lTKjaIuJIwsImSmIirrQddNzVK/pRa23ZlS\n",
              "i6C1qy1Z3tXZdFuxs55Wt46KWriuu2Z5txy12ixrNS1EwxvcVEwcKpTSADVFwfn94a85O4sUQ1zM\n",
              "B3g+zplznIvPXNd75pg9z8zFXDaXy+USAACAgVr4egAAAICaECoAAMBYhAoAADAWoQIAAIxFqAAA\n",
              "AGMRKgAAwFiECgAAMBahAgAAjEWoAAAAYxEqAJqMY8eO6Wc/+5nOnTvnk+OfP39enTp10pEjR3xy\n",
              "fKApIlSAZiQrK0s2m03Tp0/39Si19v7778tms6mysvIH1z711FOaMmWKWrduLUl67rnnNGDAgGrr\n",
              "VqxYofDwcPf948eP64EHHtCNN96o1q1b68Ybb9SIESNUXFzsMUPr1q0VEBCgNm3aKD4+XjNnztTX\n",
              "X3/t3k+rVq2Unp6uJ5988sc+bQD/H6ECNCOLFi1SSEiIXn31VVVUVPh6nHp17NgxvfXWWxo/frzX\n",
              "jx0xYoQCAgJ08OBBnTt3Tp988onuu+8+2Ww2j3WlpaU6e/asvvrqKy1evFh79+5Vz549deLECfea\n",
              "cePGaevWrbyrAtQTQgVoJnbv3q3c3FytWrVKZWVleuONNzx+npycrPvuu0+TJk1SSEiI2rVrpwUL\n",
              "FqioqEh33HGHAgICdPPNN2vXrl3ux1RVVelPf/qTYmJiFBQUpD59+ujNN990//x/37mQqr/Lceut\n",
              "t2rKlCl68MEHFRQUpIiICC1evFiS5HQ6NXz4cElScHCwWrdurblz517z+W3YsEG9e/dWmzZtvHpd\n",
              "Tp8+rU8//VQTJ05U27ZtJUk//elPNX78eIWGhl7zMf7+/vr5z3+uDRs2qFWrVnr22WfdP2vTpo36\n",
              "9u2rDRs2eDUHgGsjVIBmYtGiRYqLi9OwYcN09913a9GiRdXWbNy4UUOGDFFJSYmWL1+u9PR0jRs3\n",
              "TvPnz1dpaamGDh2q5ORk9/oFCxZo4cKFeu2113T69GlNnTpVo0aN0t69e72abcWKFUpJSdE333yj\n",
              "BQsW6De/+Y0KCgrkcDjc4VNaWqpz585p5syZ19xHbm6ubrnlFq+OK0khISHq3r270tLSlJWVpf37\n",
              "9+vKlSu1euz111+vpKQkvfPOOx7be/Tood27d3s9C4DqCBWgGfjmm2+0Zs0apaamSpJSU1O1a9cu\n",
              "7du3z2PdgAEDNGbMGPn5+Wn06NEKCgrS7bffru7du8vPz0/jxo3TkSNHVFZWJklaunSpnnrqKcXH\n",
              "x8vf31/333+/hg8frqVLl3o1X1JSkgYPHqwWLVooKSlJbdu21Z49e7zax5kzZxQUFOTVY76zbds2\n",
              "DR8+XIsXL1ZCQoLatWunqVOn1urjMYfDodOnT3tsCwoK0pkzZ+o0CwBPhArQDHx3Eu1DDz0kSRo0\n",
              "aJCio6OrvasSFhbmcb9Vq1Ye21q1aiVJOnv2rCSpqKhInTt39nhMdHS0nE6nV/PdeOON1Y773TFq\n",
              "q23btu6A+k7Lli11+fLlamsvX76sli1buu+HhIRozpw5ysnJUVlZmV599VUtW7ZMGRkZP3hcp9Op\n",
              "kJAQj21lZWXuj5EA/DiECtDEuVwuLVmyRJcuXVJMTIxCQ0MVFhamzz//XH//+99VXl5e531HRETo\n",
              "2LFjHtuOHTsmh8MhSQoICND58+c9fv7ll196dYwWLWr3z1Tv3r2Vn5/vsS0qKkrHjx9XVVWVx/aj\n",
              "R49WC6zv2O12jR49WrfddtsPfoR18eJFrV+/XkOHDvXYfuDAAfXp06dWcwP4foQK0MS98847Onr0\n",
              "qP71r38pLy/Pfdu/f78k6a9//Wud952SkqL58+crLy9PlZWVev3117VlyxalpKRIknr16qWzZ89q\n",
              "zZo1unLlit5///1qJ/H+kO9OaD18+PD3rhs9erRyc3NVWlrq3jZ8+HD5+/trxowZKi8vV1VVlbZt\n",
              "26bly5fr0UcflXT1Y7Hp06dr//79qqioUFVVld59911t27ZNAwcOvOaxKisrtXv3biUlJam8vFxz\n",
              "5sxx/6y0tFQ5OTkaPXq0V88TwLURKkATt3jxYt12220aNGiQQkND3bcuXbooJSXF/Rs2dZGenq7H\n",
              "H39cY8aMUdu2bfXCCy9o/fr17ncToqKi9PLLL2vq1KkKDg5WZmamHnnkEa+OERMTo9/+9rcaNGiQ\n",
              "goODNW/evGuu69Kli26//XatWLHCvS04OFhbt27V4cOHFRsbq5CQEP3ud7/T/Pnz9eCDD0qSfvKT\n",
              "n+jrr7/Wvffeq3bt2ikkJERTpkzRtGnTqn0fSnBwsAICAtSuXTulpqaqe/fu2rdvnyIjI91rVq5c\n",
              "qSFDhuimm27y6nkCuDaby+Vy+XoIAKgPx44d05AhQ3Tw4EH3l741pPPnz6tbt256++23FRsb2+DH\n",
              "B5oiQgUAABiLj34AAICxCBUAAGAsQgUAABiLUAEAAMYiVAAAgLEIFQAAYKwmESoLFy709QgAAMAC\n",
              "TSJUTpw44esRAACABZpEqAAAgKaJUAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQAQAAxiJUAACAsQgV\n",
              "AABgLEIFAAAYi1ABAADGIlQAAICxCBUAAGAsS0Nl8uTJioyMlM1mU15eXo3rXnnlFXXp0kWdO3fW\n",
              "hAkTdPnyZSvHAgAAjYSloTJmzBjt2LFDnTp1qnHN8ePH9eyzz2r79u0qKCjQqVOntHTpUivHAgAA\n",
              "jYSloTJw4ECFh4d/75q1a9dq5MiRCg0Nlc1m08SJE7V69eoa11dUVKi8vNzjVlVVVd+jAwAAA/j7\n",
              "egCn0+nxjktkZKScTmeN6zMyMjR79myPbf369bNktsjpmy3Zb2NVOO//fD0C0KxZ+W8S/303flb9\n",
              "/fD1341GdzLtjBkzVFZW5nFLSEjw9VgAAMACPn9HxeFw6NixY+77hYWFcjgcNa632+2y2+0e2/z8\n",
              "/CybDwAA+I7P31FJSkpSdna2Tp48KZfLpSVLluj+++/39VgAAMAAloZKWlqawsPD9fnnn+uOO+5Q\n",
              "dHS0JCklJUXZ2dmSpKioKM2ePVv9+/dXdHS02rdvr7S0NCvHAgAAjYSlH/1kZmZec/vy5cs97k+Y\n",
              "MEETJkywchQAANAI+fyjHwAAgJoQKgAAwFiECgAAMBahAgAAjEWoAAAAYxEqAADAWIQKAAAwFqEC\n",
              "AACMRagAAABjESoAAMBYhAoAADAWoQIAAIxFqAAAAGMRKgAAwFiECgAAMBahAgAAjEWoAAAAYxEq\n",
              "AADAWIQKAAAwFqECAACMRagAAABjESoAAMBYhAoAADAWoQIAAIxFqAAAAGMRKgAAwFiECgAAMBah\n",
              "AgAAjEWoAAAAYxEqAADAWIQKAAAwFqECAACMRagAAABjESoAAMBYhAoAADAWoQIAAIxFqAAAAGMR\n",
              "KgAAwFiECgAAMBahAgAAjEWoAAAAYxEqAADAWIQKAAAwFqECAACMRagAAABjESoAAMBYhAoAADAW\n",
              "oQIAAIxFqAAAAGMRKgAAwFiECgAAMBahAgAAjEWoAAAAYxEqAADAWIQKAAAwFqECAACMRagAAABj\n",
              "ESoAAMBYhAoAADAWoQIAAIxFqAAAAGNZGipHjx5VYmKiYmJi1LdvX+Xn51dbc+XKFaWnp+vmm29W\n",
              "jx49NGjQIBUUFFg5FgAAaCQsDZW0tDSlpqbqyJEjmjZtmpKTk6utyc7O1ocffqh9+/Zp//79GjJk\n",
              "iGbOnGnlWAAAoJGwLFRKSkqUm5ursWPHSpKSkpJUVFRU7d0Sm82miooKXbx4US6XS+Xl5QoPD7dq\n",
              "LAAA0Ij4W7XjoqIihYWFyd//6iFsNpscDoecTqeio6Pd6+666y5t27ZNoaGhCggIUMeOHfXBBx/U\n",
              "uN+KigpVVFR4bKuqqrLmSQAAAJ/y+cm0ubm5OnjwoL744gt9+eWXGjJkiCZOnFjj+oyMDAUFBXnc\n",
              "cnJyGnBiAADQUCwLlYiICBUXF6uyslKS5HK55HQ65XA4PNatXLlSgwcPVnBwsFq0aKHx48dr27Zt\n",
              "Ne53xowZKisr87glJCRY9TQAAIAPWRYqHTp0UHx8vFatWiVJWrduncLDwz0+9pGkqKgovffee7p0\n",
              "6ZIkadOmTbrllltq3K/dbldgYKDHzc/Pz6qnAQAAfMiyc1QkKTMzU8nJyZo7d64CAwOVlZUlSUpJ\n",
              "SdHIkSM1cuRIPf744/rPf/6jnj17qmXLlgoNDdWSJUusHAsAADQSloZKbGysdu3aVW378uXL3X+2\n",
              "2+1atmyZlWMAAIBGyucn0wIAANSEUAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQAQAAxiJUAACAsQgV\n",
              "AABgLEIFAAAYi1ABAADGIlQAAICxCBUAAGAsQgUAABiLUAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQ\n",
              "AQAAxiJUAACAsQgVAABgLEIFAAAYi1ABAADGIlQAAICxCBUAAGAsQgUAABiLUAEAAMYiVAAAgLEI\n",
              "FQAAYCxCBQAAGItQAQAAxiJUAACAsQgVAABgLEIFAAAYi1ABAADGIlQAAICxCBUAAGAsQgUAABiL\n",
              "UAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQAQAAxiJUAACAsQgVAABgLEIFAAAYi1ABAADGIlQAAICx\n",
              "CBUAAGAsQgUAABiLUAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQAQAAxiJUAACAsQgVAABgLEIFAAAY\n",
              "i1ABAADGIlQAAICxCBUAAGAsQgUAABiLUAEAAMayNFSOHj2qxMRExcTEqG/fvsrPz7/mugMHDujW\n",
              "W29V165d1bVrV61fv97KsQAAQCPhb+XO09LSlJqaquTkZK1du1bJycnavXu3x5pvv/1Wo0aN0sqV\n",
              "KzVgwABVVVXpzJkzVo4FAAAaCcveUSkpKVFubq7Gjh0rSUpKSlJRUZEKCgo81v3jH/9Qv379NGDA\n",
              "AEmSn5+f2rdvX+N+KyoqVF5e7nGrqqqy6mkAAAAfsixUioqKFBYWJn//q2/a2Gw2ORwOOZ1Oj3WH\n",
              "Dh2S3W7XnXfeqbi4OI0bN05fffVVjfvNyMhQUFCQxy0nJ8eqpwEAAHzI61B5++2363WAyspKbd26\n",
              "VZmZmfrkk0/UsWNHTZo0qcb1M2bMUFlZmcctISGhXmcCAABm8DpU5syZo9jYWC1cuFDl5eU1rouI\n",
              "iFBxcbEqKyslSS6XS06nUw6Hw2Odw+HQoEGD1LFjR9lsNo0dO1YfffRRjfu12+0KDAz0uPn5+Xn7\n",
              "NAAAQCPgdah8+OGHeu2113Tw4EHFxMToscce06FDh6qt69Chg+Lj47Vq1SpJ0rp16xQeHq7o6GiP\n",
              "db/61a+0e/dud/Rs2bJFPXv2rMtzAQAATUydzlHp1auXli1bprfeekubNm1Sjx49NHToUB04cMBj\n",
              "XWZmpjIzMxUTE6N58+YpKytLkpSSkqLs7GxJV99RmTlzphITE9WjRw+99957WrJkyY98WgAAoCmo\n",
              "068nb926VS+99JIOHDigxx9/XL/+9a/1/vvv6+677/b4rZ7Y2Fjt2rWr2uOXL1/ucf/hhx/Www8/\n",
              "XJdRAABAE+Z1qHTt2lXt2rXT5MmTdc8997jPDxkzZoxeeeWVeh8QAAA0X16HyqpVq9S7d+9r/uzN\n",
              "N9/80QMBAAB8x+tzVPbs2ePxzbGnT5/WsmXL6nUoAAAAqQ6hsmjRIrVt29Z9PyQkRIsWLarXoQAA\n",
              "AKQ6hIrL5aq2ja+wBwAAVvA6VMLCwvT666+7769Zs0ZhYWH1OhQAAIBUh5NpFyxYoFGjRunpp5+W\n",
              "JN1www365z//We+DAQAAeB0qN910kw4dOqTDhw9LuvpdKXyFPQAAsEKdvvDNZrMpODhYlZWV+uKL\n",
              "LySp2jV8AAAAfiyvQ2XFihWaPHmyWrZsqRYtrp7iYrPZVFJSUu/DAQCA5s3rUPnDH/6g3bt3KzY2\n",
              "1op5AAAA3Lz+rZ927doRKQAAoEF4HSqjR4/WggULVFJSovLycvcNAACgvnn90c+sWbMkSenp6bLZ\n",
              "bHK5XLLZbHzpGwAAqHdeh8qVK1esmAMAAKAarz/6ka5emPBvf/ubJKm0tFTFxcX1OhQAAIBUx4sS\n",
              "Pvroo3ruueckXb168oMPPljfcwEAAHgfKkuXLtVHH32kwMBASVLnzp311Vdf1ftgAAAAXoeK3W7X\n",
              "9ddf77HN379OX3ALAADwvbwOlfbt2+vIkSOy2WySrn5TLV+fDwAArFCnqyc/8MAD+vTTTxUREaHA\n",
              "wEBt2rTJitkAAEAz53WoREdH6+OPP9bhw4flcrm4ejIAALCM16HidDolSa1atZIkrp4MAAAs43Wo\n",
              "9O7d2/2NtBcvXtS3336rkJAQrp4MAADqndeh8r+/irx+/Xrt27ev3gYCAAD4Tp2+mfa/3XPPPdq8\n",
              "eXN9zAIAAODB63dU/vtKyVVVVfr444+5ejIAALCE16ESHBzsPkfFz89PXbp00V/+8hcrZgMAAM0c\n",
              "V08GAADG+tHnqAAAAFjF63dUWrRo4f76/P/mcrlks9lUVVVVL4MBAAB4HSpz5szRhQsXNGnSJEnS\n",
              "kiVLdP311+uJJ56o79kAAEAz53WobNiwQXv27HHff/7559W7d2/NmjWrXgcDAADw+hyVs2fPenwL\n",
              "bUlJic6ePVuvQwEAAEh1eEflySefVM+ePTVixAhJ0ltvvaXnnnuuvucCAADwPlTS0tLUv39/bdu2\n",
              "TZKUnp6ubt261ftgAAAAXoeKJIWEhKh79+669dZbVVlZqUuXLuknP/lJfc8GAACaOa/PUVm7dq36\n",
              "9eunRx55RJKUn5+v0aNH1/dcAAAA3odKRkaG9u7dq+DgYElSz549deLEifqeCwAAwPtQ8fPzU0hI\n",
              "iMc2PvYBAABW8DpUAgICdOrUKfe307777rtq27ZtvQ8GAADg9cm0L7zwgoYPH67PPvtMAwYM0PHj\n",
              "x7V582YrZgMAAM2cV6Fy5coVVVVVadu2bdq5c6dcLpcSExPd56sAAADUJ69CpUWLFkpNTdW+ffs0\n",
              "fPhwq2YCAACQVIdzVLp06aKCggIrZgEAAPDg9TkqZ86cUVxcnBITE9W6dWv39vXr19frYAAAALUO\n",
              "ldTUVC1dulTjx4/XyJEj1aZNGyvnAgAAqH2o5ObmSpLGjx+v+Ph47d2717KhAAAApDqcoyJJLper\n",
              "vucAAACoptbvqFy4cEEHDhyQy+XSxYsX3X/+To8ePSwZEAAANF9ehcrIkSPd9//7zzabTZ999ln9\n",
              "TgYAAJq9WodKYWGhhWMAAABUV6dzVAAAABoCoQIAAIxFqAAAAGMRKgAAwFiECgAAMBahAgAAjEWo\n",
              "AAAAYxEqAADAWIQKAAAwlqWhcvToUSUmJiomJkZ9+/ZVfn5+jWtdLpcGDx6s4OBgK0cCAACNiKWh\n",
              "kpaWptTUVB05ckTTpk1TcnJyjWv//Oc/q3PnzlaOAwAAGhnLQqWkpES5ubkaO3asJCkpKUlFRUUq\n",
              "KCiotjY/P18bN27U9OnTf3C/FRUVKi8v97hVVVXV+/wAAMD3LAuVoqIihYWFyd//6nUPbTabHA6H\n",
              "nE6nx7rLly9rwoQJyszMlJ+f3w/uNyMjQ0FBQR63nJwcS54DAADwLZ+fTDt79mzdc8896tq1a63W\n",
              "z5gxQ2VlZR63hIQEi6cEAAC+4G/VjiMiIlRcXKzKykr5+/vL5XLJ6XTK4XB4rPvggw/kdDr18ssv\n",
              "q7KyUuXl5YqMjNTu3bvVvn37avu12+2y2+0e22rzTgwAAGh8LHtHpUOHDoqPj9eqVaskSevWrVN4\n",
              "eLiio6M91m3fvl0nTpxQYWGhduzYocDAQBUWFl4zUgAAQPNi6Uc/mZmZyszMVExMjObNm6esrCxJ\n",
              "UkpKirKzs608NAAAaAIs++hHkmJjY7Vr165q25cvX37N9ZGRkSotLbVyJAAA0Ij4/GRaAACAmhAq\n",
              "AADAWIQKAAAwFqECAACMRagAAABjESoAAMBYhAoAADAWoQIAAIxFqAAAAGMRKgAAwFiECgAAMBah\n",
              "AgAAjEWoAAAAYxEqAADAWIQKAAAwFqECAACMRagAAABjESoAAMBYhAoAADAWoQIAAIxFqAAAAGMR\n",
              "KgAAwFiECgAAMBahAgAAjEWoAAAAYxEqAADAWIQKAAAwFqECAACMRagAAABjESoAAMBYhAoAADAW\n",
              "oQIAAIxFqAAAAGMRKgAAwFiECgAAMBahAgAAjEWoAAAAYxEqAADAWIQKAAAwFqECAACMRagAAABj\n",
              "ESoAAMBYhAoAADAWoQIAAIxFqAAAAGMRKgAAwFiECgAAMBahAgAAjEWoAAAAYxEqAADAWIQKAAAw\n",
              "FqECAACMRagAAABjESoAAMBYhAoAADAWoQIAAIxFqAAAAGMRKgAAwFiECgAAMBahAgAAjEWoAAAA\n",
              "YxEqAADAWJaGytGjR5WYmKiYmBj17dtX+fn51da89957SkhI0M0336xu3brp6aef1pUrV6wcCwAA\n",
              "NBKWhkpaWppSU1N15MgRTZs2TcnJydXWtGnTRq+99poOHTqkPXv2aOfOnVq5cqWVYwEAgEbCslAp\n",
              "KSlRbm6uxo4dK0lKSkpSUVGRCgoKPNb16tVLUVFRkqTrrrtOcXFxKiwsrHG/FRUVKi8v97hVVVVZ\n",
              "9TQAAIAPWRYqRUVFCgsLk7+/vyTJZrPJ4XDI6XTW+JiTJ09q7dq1uvPOO2tck5GRoaCgII9bTk5O\n",
              "vc8PAAB8z5iTacvLy3XXXXfp6aefVp8+fWpcN2PGDJWVlXncEhISGnBSAADQUPyt2nFERISKi4tV\n",
              "WVkpf39/uVwuOZ1OORyOamvPnj2rYcOGadSoUUpPT//e/drtdtntdo9tfn5+9To7AAAwg2XvqHTo\n",
              "0EHx8fFatWqVJGndunUKDw9XdHS0x7pz585p2LBhGjZsmJ555hmrxgEAAI2QpR/9ZGZmKjMzUzEx\n",
              "MZo3b56ysrIkSSkpKcrOzpYkLVy4UDk5OVq/fr3i4uIUFxenP/7xj1aOBQAAGgnLPvqRpNjYWO3a\n",
              "tava9uXLl7v/PGvWLM2aNcvKMQAAQCNlzMm0AAAA/4tQAQAAxiJUAACAsQgVAABgLEIFAAAYi1AB\n",
              "AADGIlQAAICxCBUAAGAsQgUAABiLUAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQAQAAxiJUAACAsQgV\n",
              "AABgLEIFAAAYi1ABAADGIlQAAICxCBUAAGAsQgUAABiLUAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQ\n",
              "AQAAxiJUAACAsQgVAABgLEIFAAAYi1ABAADGIlQAAICxCBUAAGAsQgUAABiLUAEAAMYiVAAAgLEI\n",
              "FQAAYCxCBQAAGItQAQAAxiJUAACAsQgVAABgLEIFAAAYi1ABAADGIlQAAICxCBUAAGAsQgUAABiL\n",
              "UAEAAMYiVAAAgLEIFQAAYCxCBQAAGItQAQAAxiJUAACAsQgVAABgLEIFAAAYi1ABAADGIlQAAICx\n",
              "CBUAAGAsQgUAABiLUAEAAMYiVAAAgLEIFQAAYCxCBQAAGMvSUDl69KgSExMVExOjvn37Kj8//5rr\n",
              "XnnlFXXp0kWdO3fWhAkTdPnyZSvHAgAAjYSloZKWlqbU1FQdOXJE06ZNU3JycrU1x48f17PPPqvt\n",
              "27eroKBAp06d0tKlS60cCwAANBKWhUpJSYlyc3M1duxYSVJSUpKKiopUUFDgsW7t2rUaOXKkQkND\n",
              "ZbPZNHHiRK1evbrG/VZUVKi8vNzjVlVVZdXTAAAAPuRv1Y6LiooUFhYmf/+rh7DZbHI4HHI6nYqO\n",
              "jnavczqd6tSpk/t+ZGSknE5njfvNyMjQ7NmzPbbddNNNSk9P92q+qqoq5eTkKCEhQX5+ftdcc49X\n",
              "e2z60tPfrfNja/N6o37xmjeshni9rfw36cf89+0r/B33ZNXfj+/+bljxenfq1ElTpkz53jWWhYpV\n",
              "ZsyYUS1K7Ha77Ha7V/spLy9XUFCQ3n77bQUGBtbniLgGXu+Gx2vesHi9Gx6vecPy1ettWahERESo\n",
              "uLhYlZWV8vf3l8vlktPplMPh8FjncDh07Ngx9/3CwsJqa/5bXaIEAAA0Tpado9KhQwfFx8dr1apV\n",
              "kqR169YpPDzc42Mf6eq5K9nZ2Tp58qRcLpeWLFmi+++/36qxAABAI2Lpb/1kZmYqMzNTMTExmjdv\n",
              "nrKysiRJKSkpys7OliRFRUVp9uzZ6t+/v6Kjo9W+fXulpaVZORYAAGgkLD1HJTY2Vrt27aq2ffny\n",
              "5R73J0yYoAkTJlg5SjV2u12///3v+RipgfB6Nzxe84bF693weM0blq9eb5vL5XI16BEBAABqia/Q\n",
              "BwAAxiJUAACAsQgVAABgLEIFAAAYq1mGSm2v6oz6MXnyZEVGRspmsykvL8/X4zR5Fy9e1OjRoxUT\n",
              "E6OePXtq6NCh1a6xhfp3++23q0ePHoqLi9MvfvELffLJJ74eqVnIysqSzWbTxo0bfT1KkxcZGanY\n",
              "2FjFxcUpLi5Oa9asaZDjNstQqc1VnVF/xowZox07dnhc0wnWSk1N1eHDh7Vv3z6NGjVKKSkpvh6p\n",
              "yXv99de1f/9+5eXlKT09nX9XGkBhYaGWLVumfv36+XqUZmPNmjXKy8tTXl6e7rvvvgY5ZrMLldpe\n",
              "1Rn1Z+DAgQoPD/f1GM3GddddpxEjRshms0mS+vXrp8LCQt8O1QwEBwe7/1xWVuZ+/WGNK1euKCUl\n",
              "RS+99BLfo9LENbqLEv5Ytb2qM9BULFy4UKNGjfL1GM3CuHHjtG3bNknSli1bfDxN0/biiy+qf//+\n",
              "6t27t69HaVbGjRsnl8ulhIQEzZs3T+3bt7f8mM3uHRWgOZk7d64KCgqUkZHh61GahZUrV6qoqEjP\n",
              "P/+8pk2b5utxmqyDBw9q3bp1euaZZ3w9SrPy73//W/v379fevXvVrl07jR8/vkGO2+zeUantVZ2B\n",
              "xm7+/Plav369tm7dqhtuuMHX4zQr48eP18SJE3X69GmFhIT4epwmZ/v27SosLFSXLl0kSSdPnlRq\n",
              "aqqKi4s1adIkH0/XdH33/8mWLVvqiSeeUExMTIMct9m9o1LbqzoDjdmLL76o1atX65133vE4dwLW\n",
              "KC0t1Zdffum+v3HjRoWEhKht27Y+nKrpmjRpkoqLi1VYWKjCwkL169dPS5cuJVIsdP78eZWWlrrv\n",
              "r169Wr169WqQYze7d1Skq1d1Tk5O1ty5cxUYGOi+qjOskZaWps2bN+vkyZO64447FBAQwMnLFvr8\n",
              "88/15JNPKioqSoMGDZJ09WJiH3/8sY8na7rKysp077336sKFC2rRooXat2+vTZs2cUItmoxTp04p\n",
              "KSlJVVVVcrlcioqK0sqVKxvk2FyUEAAAGKvZffQDAAAaD0IFAAAYi1ABAADGIlQAAICxCBUAAGAs\n",
              "QgUAANTK5MmTFRkZKZvNpry8vFo95ptvvtFDDz2kmJgYdevWTdOnT/fqmIQKAAColTFjxmjHjh3q\n",
              "1KlTrR/z6KOPqlevXjpy5Ijy8/P1xBNPeHVMQgUAANTKwIEDFR4eXm377t27NXjwYPXp00e9evXS\n",
              "G2+8IUkqKChQbm6u0tPT3WtDQ0O9Omaz/GZaAABQP0pLS5WamqotW7YoLCxMX3/9teLj45WYmKhD\n",
              "hw4pPDxckyZNUm5urkJCQvTCCy949fX7hAoAAKiznTt36rPPPtPw4cM9th8+fFiVlZXKycnR3Llz\n",
              "lZmZqTfffFN33nmnCgsL1bJly1rtn1ABAAB15nK51K1bN+3cubPaz3Jzc9WxY0f3dceGDx+uS5cu\n",
              "6cSJE7W+GDDnqAAAgDpLTEzU8ePHtXXrVve2vLw8Xbp0Sb1791ZgYKD2798vScrJyZHL5VJERESt\n",
              "989FCQEAQK2kpaVp8+bNOnnypEJCQhQQEKCCggLt3btXU6dO1enTp3X58mU5HA5t3LhR1113nfbs\n",
              "2aPHHntMFy5ckN1u1/z58/XLX/6y1sckVAAAgLH46AcAABiLUAEAAMYiVAAAgLEIFQAAYCxCBQAA\n",
              "GItQAQAAxiJUAACAsQgVAABgLEIFAAAYi1ABAADG+n+y+ls+DMaroAAAAABJRU5ErkJggg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-df6e33aa-de0d-4dae-bdb8-b4d5705bdc95\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-df6e33aa-de0d-4dae-bdb8-b4d5705bdc95\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_1['Money Laundering Risk Score'].plot(kind='hist', bins=20, title='Money Laundering Risk Score')\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-ce199573-d7a5-4c67-a786-e13951afa5da\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjEAAAGrCAYAAAAxesZMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAwWElEQVR4nO3de1hV1b7/8c8SDE25iEKQCBxFKG9cFGJjWe2OaZ1SD9ZJ7eIN\n",
              "xS7beuwc3W61nZbR2ZVlnW1ise32HLWt2GZr1tYuJyyPl0wrKxMSAUOxVFBT4jJ+f/i0fq0jqMvW\n",
              "YjHo/Xqe8TzMOcYa8zuWaJ/mnGtNhzHGCAAAwDJtfF0AAADAhSDEAAAAKxFiAACAlQgxAADASoQY\n",
              "AABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAaDGuvPJKPfzww79ojtLSUnXs2FHffPONZ4r6\n",
              "BTp27Kj333//nONeeuklRUVFeb8goJUhxAAX4JprrpHD4VBubq7L/mPHjikwMFAOh0NFRUU+qu7c\n",
              "SkpKWnyNFyo6OlrHjx9X9+7dvXaMn96/Dh06qGPHjurSpYuGDBmiTz/91GXc8ePHdc0113ilhiVL\n",
              "lqhfv34KDg5WSEiI+vXrp+eee84rxwJaKkIMcIF69ep1Roh59dVXFRMT46OK8OOPPzbr8Xbu3Knj\n",
              "x4+ruLhYISEhGj58eLMcd8WKFfrDH/6gP//5zzpy5IgOHjyovLw8de3a1SvHM8aorq7OK3MDvwQh\n",
              "BrhAN998sw4ePKjNmzc79z3//PPKzs4+Y+zatWvVv39/BQcHKz4+Xk8++aQaGhqc/Q6HQ88995wG\n",
              "Dhyojh07qm/fvtq4caPLHK+88ooSExMVHBys3r17a/ny5ZKkhoYGxcbG6uWXX3YZ/+STTyo5OfmC\n",
              "1vbnP/9Zffr0UVBQkCIiInTnnXfqu+++c/aPGzdOd9xxh8trrrnmGs2ePfu811RXV6fp06crIiJC\n",
              "YWFhmjlz5hl17N+/X2PGjFHXrl0VHh6u0aNH69ChQy7HvO+++zRq1Ch16tRJU6dOPeMs00+XanJz\n",
              "cxUbG6vg4GDdeuutqq6uds6zZ88eXXvttQoKCtLll1+uF154QQ6HQyUlJef1fgUHB+vOO+9USUmJ\n",
              "y/vkcDi0YcMGSacvc914440KDQ1VcHCw+vTpo8LCwkbn27Jli7p166Znnnmm0f6NGzcqIyNDV111\n",
              "ldq0aaOAgAClpqYqMzPTOebkyZOaPXu24uPjFRgYqO7du7v8jrz00kvOP+M+ffq49P30Hubl5Skx\n",
              "MVEXX3yxtm3bplOnTukPf/iDevTooU6dOmnQoEH65JNPzus9ArzCAHDb1VdfbWbNmmUeeughM27c\n",
              "OGOMMYWFhSY6OtoUFxcbSWbPnj3GGGO2bNli2rZta1asWGFqa2vNtm3bTGRkpHn66aed80ky/fr1\n",
              "M3v27DG1tbXm/vvvN9HR0c7+pUuXmm7dupmtW7ea+vp6U1hYaAIDA01hYaExxpj58+ebjIwM5/iG\n",
              "hgbTs2dP8/zzzzda/969e11q/L9Wrlxpdu/eberr601JSYlJS0szo0aNcvaPHTvW3H777Y2+J+e7\n",
              "pvnz55uYmBiza9cuc+rUKTN79mzj7+9v/vjHPxpjjDl16pRJSEgwDz74oDl+/Lg5duyYueOOO8w/\n",
              "//M/uxzz4osvNmvXrjX19fXmxIkTZ6xt6dKlxs/PzzzwwAPmhx9+MN9++62Ji4szDz30kDHGmNra\n",
              "WhMfH28mTZpkTpw4YcrLy016erqRZPbu3Xte79/3339vRo4caSIiIkxdXZ3Le7B+/XpjjDFjxowx\n",
              "WVlZ5uTJk6a+vt589dVX5ptvvnHW2LVrV2OMMcuXLzfh4eFmzZo1jR77pz+ftm3bmunTp5u3337b\n",
              "HDp06Iwxt99+u0lNTTVffPGFaWhoMPv37zcff/yx8/WBgYFmw4YNpq6uzqxfv9506NDBrF692mV9\n",
              "GRkZprS01NTV1ZlTp06ZsWPHmuuuu86UlZWZ2tpa89xzz5mwsDBz5MiRJmsFvIkQA1yAn/6DXVZW\n",
              "ZgIDA82RI0fMmDFjzCOPPHLGf+AmT55sRowY4fL6BQsWmISEBOe2JPPyyy87tz///HMjyRw4cMAY\n",
              "Y0zfvn3N4sWLXebIysoyEydONMYYc+DAAXPRRReZzz//3BhjzDvvvGM6duxoqqurG63/XCHm/8rP\n",
              "zzehoaHO7fMNMWdbU1xcnHn22Wed/XV1dSYsLMwZYlatWmUuvfRS09DQ4BxTXl5uJJmysjLnMX8e\n",
              "rhpb29KlS01AQID58ccfnWP+/d//3QwdOtQYczp8tmnTxuW9+vvf/35eISYwMNAEBgYaSaZ79+5m\n",
              "y5YtLuN+HmLGjRtnbrrpJvP555+7rOmnGrt27WoefvhhExMTY3bu3NnocX9u3bp15tZbbzWXXnqp\n",
              "cTgcJi0tzXz44YfGGGMOHTpkJJmtW7c2+trrr7/ePPDAAy77pk6daoYMGeKyvrfeesvZ/9133xlJ\n",
              "5quvvnJ5XVxcnHn11VfPWS/gDVxOAn6BqKgoXXvttXryySf1t7/9TRMnTjxjTFlZmXr06OGyLy4u\n",
              "TqWlpS77Lr30UufPHTp0kHT6RmHp9OWOBx98UCEhIc62bNkyffvtt5KkSy65RJmZmc57dHJzczVm\n",
              "zBgFBgZe0Lry8/OVkZGh8PBwBQUF6c4779Thw4dVX1/v1jxnW1N5ebn+6Z/+ydnv5+en6Oho5/ae\n",
              "PXt08OBBderUybnm3r17KyAgwOW9+/kcTenSpYvatm3rUstPdezfv1+hoaEu71VsbOx5rW/79u2q\n",
              "rq7Wrl27JEmfffZZk2OffPJJxcXFKTMzU5dcconGjx+vgwcPOvuPHDmiBQsW6O6771a/fv3Oeeyh\n",
              "Q4fq9ddf1/79+7V3717FxsbqxhtvVFVVlfbu3StJSkhIaPS15/s7+fP39qfLc1dccYXL7+H+/ftV\n",
              "Xl5+znoBbyDEAL/Q3Xffrccee0w33HCDIiMjz+jv1q2biouLXfYVFxe7/Af7XCIiIrRo0SIdPXrU\n",
              "2Y4fP64333zTpY5XX31V+/bt0+rVqzVlypQLWk95ebluvfVW/e53v1Npaamqq6v16quvSjp9g6ck\n",
              "BQYG6sSJEy6v+ylQna+oqCiXe07q6+tVVlbm3I6IiFBMTIzLmo8ePapTp04pIyPDOa5Nm1/2z1jX\n",
              "rl11+PBhZ6iRpH379rk1R69evbR48WJNnTq1yfehc+fOevrpp7V792598sknKikp0bRp05z9nTp1\n",
              "0nvvvacFCxboT3/6k1vHj4mJ0ezZs1VVVaXi4mJnCPv6668bHX++v5M/f28jIiIkSZ9++qnLn8cP\n",
              "P/yg3//+927VC3gKIQb4hYYMGaL169fr6aefbrR/woQJWrt2rVatWqX6+np98skneuKJJzR58uTz\n",
              "PsYDDzygRx55RFu3blVDQ4Nqamq0detWffzxx84xgwYNUteuXZWZmamkpKTzuqn3xx9/1KlTp5yt\n",
              "pqZGx48fV0NDg7p06aJ27dppz549ysnJcXndgAED9N577+mrr75SbW2tnnnmGef//Z+vsWPH6qmn\n",
              "ntJXX32lmpoazZs3T4cPH3b2Z2Zmqra2VnPmzFFVVZUkqbKyUitWrHDrOOeSnp6uHj16aPr06frh\n",
              "hx/07bff6rHHHnN7nsGDB2vAgAH64x//2Gj/8uXLVVxcrIaGBgUGBiogIED+/v4uY1JSUrRx40Yt\n",
              "WrRI06dPb/JYf/nLX7RixQpVVlZKkg4dOqQFCxYoLCxMl19+ucLCwjR69Gjde++92r17tySpoqJC\n",
              "27dvlyRlZWXpL3/5i95//33V19fr3XffVV5e3ll/J2NiYjRixAjde++9zpB37NgxrVu3ThUVFef/\n",
              "RgEeRIgBfiGHw6HrrruuyS8ru+KKK7Ry5UrNnz9fnTp10q233qqpU6fq/vvvP+9j3H///Xr44Yc1\n",
              "ZcoUhYaGqmvXrvqP//iPM86G3H333dq+fft5n4Xp3bu32rdv72w9evTQZZddppycHN11110KDAzU\n",
              "2LFjz/gk0u23365Ro0YpIyND3bp109GjRzVw4MDzXo8kzZgxQ5mZmbr66qsVFRWlH3/8UVdccYWz\n",
              "PzAwUJs2bVJpaan69u2roKAgZWRk6IMPPnDrOOfi7++vv//97/ryyy91ySWX6LrrrtOYMWMkSe3a\n",
              "tXNrrkceeURLly7Vl19+eUbfzp079dvf/laBgYHq0aOHQkJC9OSTT54xrmfPnvrwww/15ptvauLE\n",
              "iY1ewgsNDdWSJUvUp08fdejQQf369dPRo0e1YcMGtW/fXpL0wgsv6Oqrr9YNN9ygjh07auDAgc7L\n",
              "Xrfeequeeuop3XPPPQoJCdHvfvc7LVy40OXTTY357//+b/Xv31+DBw9WYGCgEhIS9MILLzjP0AHN\n",
              "zWH47QNajXXr1mn06NH69ttvdfHFF/u6HGu98cYbGjVqlE6ePCmHw+HrcgA0gTMxQCvxww8/6E9/\n",
              "+pOys7MJMG7atGmTvv76axljtHv3bj300EMaM2YMAQZo4QgxQCvw/PPPq0uXLpKkWbNm+bga+1RU\n",
              "VOj6669Xhw4ddN111yk9Pb3Je5wAtBxcTgIAAFbiTAwAALASIQYAAFiJEAMAAKxEiAEAAFZq9SFm\n",
              "4cKFvi4BAAB4QasPMe4+AwUAANih1YcYAADQOhFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABY\n",
              "iRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlr4WYU6dOacSIEYqPj1diYqIGDx6s\n",
              "oqKiRseuWbNGl112mXr27KnMzExVV1efVx8AAPj18uqZmMmTJ2v37t3auXOnhg8frqysrDPGHD9+\n",
              "XBMnTtQbb7yhPXv26NJLL9Ujjzxyzj4AAPDr5rUQ065dO914441yOBySpPT0dJWUlJwxbt26dUpO\n",
              "TtZll10mSbrnnnu0bNmyc/Y1pqamRtXV1S6tvr7ewysDAAAtgX9zHWjhwoUaPnz4GftLS0sVExPj\n",
              "3I6NjVVFRYXq6urO2ufvf2bpOTk5mjt3rsu+9PR0D67i/4v9/VqvzCtJJY//i9fmBgCgtWiWG3sf\n",
              "e+wxFRUVKScnx6vHmTlzpqqqqlxaWlqaV48JAAB8w+sh5sknn1R+fr7WrVuniy+++Iz+6Oho7du3\n",
              "z7ldUlKiyMhI+fv7n7WvMQEBAQoKCnJpfn5+nl8UAADwOa+GmAULFmjZsmVav369QkJCGh0zdOhQ\n",
              "bd++XV999ZUkadGiRRo1atQ5+wAAwK+b1+6JKS8v14MPPqju3bvr2muvlXT6TMnmzZv10EMP6dJL\n",
              "L9WUKVMUGBioF198USNGjFBdXZ369Omjl19+WZLO2gcAAH7dHMYY4+sivGnatGlasGCBx+flxl4A\n",
              "AHyLb+wFAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYi\n",
              "xAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACA\n",
              "lQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgA\n",
              "AGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJW8GmKmTp2q2NhYORwO7dixo9Ex\n",
              "S5cuVVJSkrN16dJFmZmZkqSSkhL5+fm59BcXF3uzZAAAYAl/b05+yy23aPr06bryyiubHDN+/HiN\n",
              "Hz/eud2nTx/dfvvtzu3AwMAmAxAAAPj18mqIGTRokFvjN2/erMrKSg0bNuyCjldTU6OamhqXffX1\n",
              "9Rc0FwAAaNla1D0xeXl5uvPOO9W2bVvnvhMnTig1NVUpKSmaN2/eWUNJTk6OgoODXdqWLVuao3QA\n",
              "ANDMWkyIOXHihJYvX66JEyc690VGRmr//v3aunWrNmzYoMLCQj311FNNzjFz5kxVVVW5tLS0tOYo\n",
              "HwAANLMWE2L++te/qnfv3urVq5dzX0BAgMLDwyVJoaGhmjBhggoLC5ucIyAgQEFBQS7Nz8/P67UD\n",
              "AIDm12JCTF5enstZGEmqrKxUbW2tpNP3u+Tn5ys5OdkX5QEAgBbGqyEmOztbUVFRKi8v15AhQxQX\n",
              "FydJysrKUkFBgXPc7t27tWPHDt12220ur9+4caOSk5OVmJiolJQURUREaNasWd4sGQAAWMJhjDG+\n",
              "LsKbpk2bpgULFnh83tjfr/X4nD8pefxfvDY3AACtRYu5nAQAAOAOQgwAALASIQYAAFiJEAMAAKxE\n",
              "iAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAA\n",
              "KxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEA\n",
              "AMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVC\n",
              "DAAAsBIhBgAAWMmrIWbq1KmKjY2Vw+HQjh07Gh3z/vvvq3379kpKSnK2kydPOvvz8vLUs2dP9ejR\n",
              "Q5MmTVJtba03SwYAAJbwaoi55ZZbtHHjRsXExJx1XEJCgnbs2OFs7du3lyTt3btXc+bMUWFhoYqK\n",
              "inTw4EEtWbLEmyUDAABLeDXEDBo0SFFRURf8+pUrV2rYsGGKiIiQw+HQlClTtGzZsibH19TUqLq6\n",
              "2qXV19df8PEBAEDL1SLuiSkuLlZKSopSU1O1aNEi5/7S0lKXszixsbEqLS1tcp6cnBwFBwe7tC1b\n",
              "tni1dgAA4Bs+DzEpKSkqLy/X9u3btXr1ai1evFivv/76Bc01c+ZMVVVVubS0tDQPVwwAAFoCn4eY\n",
              "oKAgBQcHS5KioqI0evRoFRYWSpKio6O1b98+59iSkhJFR0c3OVdAQICCgoJcmp+fn3cXAAAAfMLn\n",
              "IaaiokINDQ2SpGPHjmnNmjVKTk6WJI0cOVIFBQU6cOCAjDFavHixRo0a5ctyAQBAC+HVEJOdna2o\n",
              "qCiVl5dryJAhiouLkyRlZWWpoKBAkrRq1Sr17dtXiYmJSk9P1+DBgzV+/HhJUvfu3TV37lwNHDhQ\n",
              "cXFxCgsLU3Z2tjdLBgAAlnAYY4yvi/CmadOmacGCBR6fN/b3az0+509KHv8Xr80NAEBr4fPLSQAA\n",
              "ABeCEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESI\n",
              "AQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAAr\n",
              "EWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAA\n",
              "wEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAVvJqiJk6dapiY2PlcDi0Y8eORse8++67SktL\n",
              "U69evdS7d29Nnz5dDQ0NkqSSkhL5+fkpKSnJ2YqLi71ZMgAAsIRXQ8wtt9yijRs3KiYmpskxnTp1\n",
              "0vLly/XFF1/o448/1kcffaRXXnnF2R8YGKgdO3Y4W48ePbxZMgAAsIS/NycfNGjQOcckJyc7f27X\n",
              "rp2SkpJUUlJyQcerqalRTU2Ny776+voLmgsAALRsLeqemAMHDmjlypW66aabnPtOnDih1NRUpaSk\n",
              "aN68eWcNJTk5OQoODnZpW7ZsaY7SAQBAM2sxIaa6ulo333yzpk+frgEDBkiSIiMjtX//fm3dulUb\n",
              "NmxQYWGhnnrqqSbnmDlzpqqqqlxaWlpacy0BAAA0oxYRYo4dO6ahQ4dq+PDhmjZtmnN/QECAwsPD\n",
              "JUmhoaGaMGGCCgsLm5wnICBAQUFBLs3Pz8/r9QMAgObn8xBz/PhxDR06VEOHDtXs2bNd+iorK1Vb\n",
              "Wyvp9P0u+fn5LvfQAACAXy+vhpjs7GxFRUWpvLxcQ4YMUVxcnCQpKytLBQUFkqSFCxdqy5Ytys/P\n",
              "d36Mev78+ZKkjRs3Kjk5WYmJiUpJSVFERIRmzZrlzZIBAIAlHMYY4+sivGnatGlasGCBx+eN/f1a\n",
              "j8/5k5LH/8VrcwMA0Fr4/HISAADAhSDEAAAAKxFiAACAlQgxAADASm6HmLffftsbdQAAALjF7RAz\n",
              "b948JSQkaOHChaqurvZGTQAAAOfkdoj58MMPtXz5cn3++eeKj4/XPffcoy+++MIbtQEAADTpgu6J\n",
              "SU5O1gsvvKC33npLa9asUb9+/TR48GB99tlnnq4PAACgURcUYjZs2KDhw4crMzNT9957rw4cOKDs\n",
              "7Gz967/+q6frAwAAaJS/uy+4/PLL1aVLF02dOlWZmZnOByzecsstysvL83iBAAAAjXE7xLz22mvq\n",
              "379/o33r1q37xQUBAACcD7cvJ3388cc6fPiwc/v777/XCy+84NGiAAAAzsXtELNo0SKFhoY6tzt3\n",
              "7qxFixZ5tCgAAIBzcTvENPbQ6/r6eo8UAwAAcL7cDjGRkZF6/fXXndsrVqxQZGSkR4sCAAA4F7dv\n",
              "7H3mmWc0fPhwTZ8+XZJ08cUX629/+5vHCwMAADgbt0PMZZddpi+++EK7d++WJCUkJDg/Zg0AANBc\n",
              "3A4xkuRwOBQSEqK6ujrt379fkhQdHe3RwgAAAM7G7RDz0ksvaerUqWrbtq3atDl9S43D4VBlZaXH\n",
              "iwMAAGiK2yHmkUce0datW5WQkOCNegAAAM6L259O6tKlCwEGAAD4nNshZsSIEXrmmWdUWVmp6upq\n",
              "ZwMAAGhObl9OmjVrliRp2rRpcjgcMsbI4XDwhXcAAKBZuR1iGhoavFEHAACAW9y+nCSdfgjkq6++\n",
              "Kkk6evSoKioqPFoUAADAuVzQAyAnTJighx9+WNLpp1iPGTPG03UBAACcldshZsmSJfrf//1fBQUF\n",
              "SZJ69OihQ4cOebwwAACAs3E7xAQEBKh9+/Yu+/z9L+iLfwEAAC6Y2yEmLCxMX3/9tRwOh6TT3+DL\n",
              "IwcAAEBzu6CnWI8ePVpfffWVunXrpqCgIK1Zs8YbtQEAADTJ7RATFxenzZs3a/fu3TLG8BRrAADg\n",
              "E26HmNLSUklShw4dJImnWAMAAJ9wO8T079/f+U29p06d0g8//KDOnTvzFGsAANCs3A4x//fj1Pn5\n",
              "+dq5c6fHCgIAADgfF/SNvT+XmZmptWvXeqIWAACA8+Z2iPn5k6uPHDmit956q8mnWE+dOlWxsbFy\n",
              "OBzasWNHk3Pm5eWpZ8+e6tGjhyZNmqTa2trz6gMAAL9eboeYkJAQderUSSEhIQoPD9e0adP07LPP\n",
              "Njr2lltu0caNGxUTE9PkfHv37tWcOXNUWFiooqIiHTx4UEuWLDlnHwAA+HVzO8Q0NDSovr5eDQ0N\n",
              "qq2t1RdffKGhQ4c2OnbQoEGKioo663wrV67UsGHDFBERIYfDoSlTpmjZsmXn7GtMTU2Ny5mi6upq\n",
              "1dfXu7tEAABgAZ8/L6C0tNTlTE1sbKzzY9xn62tMTk6O5s6d67IvPT3dwxUDwIWL/b137iEsefxf\n",
              "vDKvt/F+NI/W+j67fSamTZs28vPzO6P9tN+XZs6cqaqqKpeWlpbm05oAAIB3uH0mZt68eTp58qTu\n",
              "vvtuSdLixYvVvn17PfDAAxdUQHR0tIqLi53bJSUlzi/OO1tfYwICAhQQEOCyz9fBCgAAeIfbZ2JW\n",
              "r16t+fPnKyoqSlFRUXr00UeVn5+vDh06OL/F1x0jR45UQUGBDhw4IGOMFi9erFGjRp2zDwAA/Lq5\n",
              "HWKOHTvm8u28lZWVOnbsWKNjs7OzFRUVpfLycg0ZMkRxcXGSpKysLBUUFEiSunfvrrlz52rgwIGK\n",
              "i4tTWFiYsrOzz9kHAAB+3dy+nPTggw8qMTFRN954oyTprbfe0sMPP9zo2Nzc3Eb3v/jiiy7bkyZN\n",
              "0qRJkxode7Y+AADw6+V2iMnOztbAgQP13nvvSZKmTZum3r17e7wwAACAs7mgj1h37txZffv21TXX\n",
              "XKO6ujr9+OOPuuiiizxdGwAAQJPcvidm5cqVSk9P1/jx4yVJu3bt0ogRIzxdFwAAwFm5HWJycnK0\n",
              "fft2hYSESJISExO1b98+T9cFAABwVm6HGD8/P3Xu3NllH5eSAABAc3M7xAQGBurgwYNyOBySpHfe\n",
              "eUehoaEeLwwAAOBs3L6x9z//8z91ww036JtvvtGVV16pvXv3au1a7zyTAQAAoCluhZifnmD93nvv\n",
              "6aOPPpIxRhkZGc77YwAAAJqLWyGmTZs2mjx5snbu3KkbbrjBWzUBAACck9v3xPTs2VNFRUXeqAUA\n",
              "AOC8uX1PzOHDh5WUlKSMjAx17NjRuT8/P9+jhQEAAJzNeYeYyZMna8mSJRo7dqyGDRumTp06ebMu\n",
              "AACAszrvELNt2zZJ0tixY5WSkqLt27d7rSgAAIBzcfueGEkyxni6DgAAALec95mYkydP6rPPPpMx\n",
              "RqdOnXL+/JN+/fp5pUAAAIDGuBVihg0b5tz++c8Oh0PffPONZysDAAA4i/MOMSUlJV4sAwAAwD0X\n",
              "dE8MAACArxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIA\n",
              "AICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJW8GmL27Nmj\n",
              "jIwMxcfHKzU1Vbt27TpjzNKlS5WUlORsXbp0UWZmpiSppKREfn5+Lv3FxcXeLBkAAFjC35uTZ2dn\n",
              "a/LkyRo3bpxWrlypcePGaevWrS5jxo8fr/Hjxzu3+/Tpo9tvv925HRgYqB07dnizTAAAYCGvnYmp\n",
              "rKzUtm3bdMcdd0iSRo4cqbKyMhUVFTX5ms2bN6uyslLDhg27oGPW1NSourrapdXX11/QXAAAoGXz\n",
              "WogpKytTZGSk/P1Pn+xxOByKjo5WaWlpk6/Jy8vTnXfeqbZt2zr3nThxQqmpqUpJSdG8efPOGkpy\n",
              "cnIUHBzs0rZs2eK5RQEAgBajxdzYe+LECS1fvlwTJ0507ouMjNT+/fu1detWbdiwQYWFhXrqqaea\n",
              "nGPmzJmqqqpyaWlpac1RPgAAaGZeCzHdunVTRUWF6urqJEnGGJWWlio6OrrR8X/961/Vu3dv9erV\n",
              "y7kvICBA4eHhkqTQ0FBNmDBBhYWFTR4zICBAQUFBLs3Pz8+DqwIAAC2F10JMeHi4UlJS9Nprr0mS\n",
              "Vq1apaioKMXFxTU6Pi8vz+UsjHT6vpra2lpJp+93yc/PV3JysrdKBgAAFvHq5aTc3Fzl5uYqPj5e\n",
              "jz/+uJYuXSpJysrKUkFBgXPc7t27tWPHDt12220ur9+4caOSk5OVmJiolJQURUREaNasWd4sGQAA\n",
              "WMKrH7FOSEjQpk2bztj/4osvnjHu2LFjZ4zLzMx0fmcMAADAz7WYG3sBAADcQYgBAABWIsQAAAAr\n",
              "EWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAA\n",
              "wEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIM\n",
              "AACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJ\n",
              "EAMAAKxEiAEAAFYixAAAACt5NcTs2bNHGRkZio+PV2pqqnbt2nXGmPfff1/t27dXUlKSs508edLZ\n",
              "n5eXp549e6pHjx6aNGmSamtrvVkyAACwhFdDTHZ2tiZPnqyvv/5aM2bM0Lhx4xodl5CQoB07djhb\n",
              "+/btJUl79+7VnDlzVFhYqKKiIh08eFBLlizxZskAAMASXgsxlZWV2rZtm+644w5J0siRI1VWVqai\n",
              "oqLznmPlypUaNmyYIiIi5HA4NGXKFC1btqzJ8TU1NaqurnZp9fX1v3gtAACg5fFaiCkrK1NkZKT8\n",
              "/f0lSQ6HQ9HR0SotLT1jbHFxsVJSUpSamqpFixY595eWliomJsa5HRsb2+jrf5KTk6Pg4GCXtmXL\n",
              "Fg+uCgAAtBQ+v7E3JSVF5eXl2r59u1avXq3Fixfr9ddfv6C5Zs6cqaqqKpeWlpbm4YoBAEBL4LUQ\n",
              "061bN1VUVKiurk6SZIxRaWmpoqOjXcYFBQUpODhYkhQVFaXRo0ersLBQkhQdHa19+/Y5x5aUlJzx\n",
              "+p8LCAhQUFCQS/Pz8/P00gAAQAvgtRATHh6ulJQUvfbaa5KkVatWKSoqSnFxcS7jKioq1NDQIEk6\n",
              "duyY1qxZo+TkZEmn76MpKCjQgQMHZIzR4sWLNWrUKG+VDAAALOLVy0m5ubnKzc1VfHy8Hn/8cS1d\n",
              "ulSSlJWVpYKCAkmnw03fvn2VmJio9PR0DR48WOPHj5ckde/eXXPnztXAgQMVFxensLAwZWdne7Nk\n",
              "AABgCX9vTp6QkKBNmzadsf/FF190/nzffffpvvvua3KOSZMmadKkSV6pDwAA2MvnN/YCAABcCEIM\n",
              "AACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFiJ\n",
              "EAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAA\n",
              "ViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIA\n",
              "AICVCDEAAMBKhBgAAGAlQgwAALASIQYAAFjJqyFmz549ysjIUHx8vFJTU7Vr164zxrz77rtKS0tT\n",
              "r1691Lt3b02fPl0NDQ2SpJKSEvn5+SkpKcnZiouLvVkyAACwhFdDTHZ2tiZPnqyvv/5aM2bM0Lhx\n",
              "484Y06lTJy1fvlxffPGFPv74Y3300Ud65ZVXnP2BgYHasWOHs/Xo0cObJQMAAEt4LcRUVlZq27Zt\n",
              "uuOOOyRJI0eOVFlZmYqKilzGJScnq3v37pKkdu3aKSkpSSUlJRd0zJqaGlVXV7u0+vr6X7QOAADQ\n",
              "MnktxJSVlSkyMlL+/v6SJIfDoejoaJWWljb5mgMHDmjlypW66aabnPtOnDih1NRUpaSkaN68eWcN\n",
              "JTk5OQoODnZpW7Zs8dyiAABAi9Fibuytrq7WzTffrOnTp2vAgAGSpMjISO3fv19bt27Vhg0bVFhY\n",
              "qKeeeqrJOWbOnKmqqiqXlpaW1lxLAAAAzchrIaZbt26qqKhQXV2dJMkYo9LSUkVHR58x9tixYxo6\n",
              "dKiGDx+uadOmOfcHBAQoPDxckhQaGqoJEyaosLCwyWMGBAQoKCjIpfn5+Xl4ZQAAoCXwWogJDw9X\n",
              "SkqKXnvtNUnSqlWrFBUVpbi4OJdxx48f19ChQzV06FDNnj3bpa+yslK1tbWSTt/vkp+fr+TkZG+V\n",
              "DAAALOLVy0m5ubnKzc1VfHy8Hn/8cS1dulSSlJWVpYKCAknSwoULtWXLFuXn5zs/Rj1//nxJ0saN\n",
              "G5WcnKzExESlpKQoIiJCs2bN8mbJAADAEv7enDwhIUGbNm06Y/+LL77o/HnWrFlNBpPMzExlZmZ6\n",
              "rT4AAGCvFnNjLwAAgDsIMQAAwEqEGAAAYCVCDAAAsBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFi\n",
              "AACAlQgxAADASoQYAABgJUIMAACwEiEGAABYiRADAACsRIgBAABWIsQAAAArEWIAAICVCDEAAMBK\n",
              "hBgAAGAlQgwAALASIQYAAFiJEAMAAKxEiAEAAFYixAAAACsRYgAAgJUIMQAAwEqEGAAAYCVCDAAA\n",
              "sBIhBgAAWIkQAwAArESIAQAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJa+GmD179igjI0Px8fFK\n",
              "TU3Vrl27Gh2Xl5ennj17qkePHpo0aZJqa2vPqw8AAPx6eTXEZGdna/Lkyfr66681Y8YMjRs37owx\n",
              "e/fu1Zw5c1RYWKiioiIdPHhQS5YsOWcfAAD4dfNaiKmsrNS2bdt0xx13SJJGjhypsrIyFRUVuYxb\n",
              "uXKlhg0bpoiICDkcDk2ZMkXLli07Z19jampqVF1d7dLq6+u9tUQAAOBD/t6auKysTJGRkfL3P30I\n",
              "h8Oh6OholZaWKi4uzjmutLRUMTExzu3Y2FiVlpaes68xOTk5mjt3rsu+yy67TNOmTfPImn5SX1+v\n",
              "yC1blJaWJj8/P4/OLUnTpr3j8TndVV9fry1eXKOvsT772brGzPMc5+76WsK/G+7y5r+lLeH9aEm/\n",
              "o+f7e+eO+vp6/eY3j3ptfTExMbr//vvPOsZrIcYXZs6ceUZgCQgIUEBAgEePU11dreDgYL399tsK\n",
              "Cgry6NwtRWtfI+uzX2tfY2tfn9T618j6vM9rIaZbt26qqKhQXV2d/P39ZYxRaWmpoqOjXcZFR0er\n",
              "uLjYuV1SUuIcc7a+xngjsAAAgJbJa/fEhIeHKyUlRa+99pokadWqVYqKinK5lCSdvlemoKBABw4c\n",
              "kDFGixcv1qhRo87ZBwAAft28+umk3Nxc5ebmKj4+Xo8//riWLl0qScrKylJBQYEkqXv37po7d64G\n",
              "DhyouLg4hYWFKTs7+5x9AADg182r98QkJCRo06ZNZ+x/8cUXXbYnTZqkSZMmNTrH2fp8JSAgQH/8\n",
              "4x9b9aWr1r5G1me/1r7G1r4+qfWvkfV5n8MYY3x2dAAAgAvEYwcAAICVCDEAAMBKhBgAAGAlQgwA\n",
              "ALASIcZNU6dOVWxsrBwOh3bs2OHrcjzu1KlTGjFihOLj45WYmKjBgwef8bwr211//fXq16+fkpKS\n",
              "dNVVV+mTTz7xdUlesXTpUjkcDr3xxhu+LsXjYmNjlZCQoKSkJCUlJWnFihW+LsmjampqdN9996ln\n",
              "z57q27ev8xl0rcX333/v/LNLSkpSfHy8/P39dfjwYV+X5jFvvvmmUlJSlJSUpD59+ujll1/2dUke\n",
              "99Zbb2nAgAHq16+f0tPTtXPnzuYvwsAt//M//2PKyspMTEyM+eSTT3xdjsedPHnSrF271jQ0NBhj\n",
              "jHnuuefM1Vdf7duiPOzIkSPOn/Pz802/fv18V4yX7N271/zmN78x6enpZvXq1b4ux+Na69+/nzzw\n",
              "wAPmvvvuc/49rKio8HFF3vXEE0+Ym266yddleExDQ4Pp1KmT2blzpzHm9N/HgIAAU11d7ePKPOfw\n",
              "4cMmNDTUfP7558YYYz744APTu3fvZq+DMzFuGjRokKKionxdhte0a9dON954oxwOhyQpPT1dJSUl\n",
              "vi3Kw0JCQpw/V1VVOdfaWjQ0NCgrK0vPPfdcq/1+itbsxIkTysvL0/z5852/mxERET6uyrvy8vI0\n",
              "ceJEX5fhUQ6HQ0ePHpV0+hlDnTt3blV/H4uLi9W5c2f17t1bknTVVVeptLRU27dvb9Y6CDE4q4UL\n",
              "F2r48OG+LsPj7rrrLnXr1k1z5szRq6++6utyPGrBggUaOHCg+vfv7+tSvOquu+5S3759NXHiRB06\n",
              "dMjX5XhMcXGxQkND9dhjj2nAgAG66qqr9M47vn8is7d89NFHOnLkiG666SZfl+IxDodDK1asUGZm\n",
              "pmJiYnTllVfq5Zdf1kUXXeTr0jymZ8+e+v777/XRRx9JkgoKCnTs2LFm/59eQgya9Nhjj6moqEg5\n",
              "OTm+LsXjXnnlFZWVlenRRx/VjBkzfF2Ox3z++edatWqVZs+e7etSvOqDDz7Qp59+qu3bt6tLly4a\n",
              "O3asr0vymLq6Ou3bt0+9evXStm3b9Oyzz+q2227TwYMHfV2aV+Tl5emuu+6Sv79Xv0C+WdXV1enR\n",
              "Rx9Vfn6+9u3bp3feeUd33nmnvvvuO1+X5jHBwcFauXKlZs6cqf79++sf//iHevXq1fx/js1+AauV\n",
              "aO3X5J944gnTv39/l/tHWqt27dqZ7777ztdleMSiRYtMRESEiYmJMTExMSYgIMCEhYWZRYsW+bo0\n",
              "r/n2229Nx44dfV2Gxxw6dMi0adPG1NXVOfcNGDDArF+/3odVecexY8dMx44dzZdffunrUjxq69at\n",
              "pmfPni77BgwYYP7xj3/4qCLvO3XqlAkJCTF79uxp1uNyJgZnWLBggZYtW6b169e73D/SGhw9elTf\n",
              "fvutc/uNN95Q586dFRoa6sOqPOfuu+9WRUWFSkpKVFJSovT0dC1ZskR33323r0vzmBMnTjjvNZCk\n",
              "ZcuWKTk52XcFeViXLl103XXX6e2335Yk7d27V3v37tXll1/u48o8b8WKFUpMTNRll13m61I8qlu3\n",
              "bqqoqNCXX34pSSoqKlJxcbESEhJ8XJlnVVRUOH9+5JFH9Nvf/lZxcXHNWkPrOX/XTLKzs7V27Vod\n",
              "OHBAQ4YMUWBgYKv6CHJ5ebkefPBBde/eXddee62k0w/52rx5s48r84yqqirdeuutOnnypNq0aaOw\n",
              "sDCtWbOm1d3c25odPHhQI0eOVH19vYwx6t69u1555RVfl+VRixcv1sSJEzVjxgy1adNGubm56tq1\n",
              "q6/L8ri8vLwW94BfT7jkkku0ZMkS/du//ZvatGmjhoYG/dd//Zeio6N9XZpHPfTQQyosLFRdXZ1+\n",
              "85vfKC8vr9lr4AGQAADASlxOAgAAViLEAAAAKxFiAACAlQgxAADASoQYAABgJUIMAACwEiEGAABY\n",
              "iRADAACsRIgBAABWIsQAAAAr/T9kMWNxHfhQEgAAAABJRU5ErkJggg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-ce199573-d7a5-4c67-a786-e13951afa5da\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-ce199573-d7a5-4c67-a786-e13951afa5da\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_2['Shell Companies Involved'].plot(kind='hist', bins=20, title='Shell Companies Involved')\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-665ef88a-8820-4851-b72b-6af385e9abbb\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAioAAAGrCAYAAADuNLxTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAlIUlEQVR4nO3de3DU9b3/8dcmqatoLtwJJAuSkIjKRTA5TOAgirTiT4ETbKsY\n",
              "SiohkdqCB6YCIkfQKnTmHCuVsQQCqcgIHgO1GUCtF1CwKAGRS1BIkLABkag0F6qJZPP5/cGwhzXE\n",
              "krhf9hPzfMzsTPa7n3zz3l2Bp9/9ZtdljDECAACwUFioBwAAAGgKoQIAAKxFqAAAAGsRKgAAwFqE\n",
              "CgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECXCJbtmyRy+VSfX19i/cxf/58DRs2zH99xIgR\n",
              "euSRR4IxXqs0evRoPf7446EeIyjKysrkcrlUWlrq2M945JFHNGLECMf2DziBUAGC5MiRI7rnnnvU\n",
              "vXt3XXXVVerevbtuv/12nThxIqRz7du3T3fffbdiY2N11VVXqVevXrrnnnv0wQcfhHSuYHjllVc0\n",
              "b948R/admZmpjIwMR/YN4OIRKkCQ3H777YqMjNT+/ft1+vRp7d69Wz//+c/lcrlCNtOWLVuUmpqq\n",
              "rl27avv27aqpqdGHH36oUaNG6aWXXgrZXABwsQgVIAi+/PJLffzxx7r//vvVoUMHSVLXrl01adIk\n",
              "devWLWDtyy+/rKSkJEVGRmrUqFE6fvy4/7ba2lo9/PDDSkhIUPv27TV8+HDt3r27xXPl5OTorrvu\n",
              "0uLFi9WrVy+5XC7FxMTovvvu08KFC/3r/vznP+v6669XVFSUrr/+ej333HP+2869JLFy5Ur1799f\n",
              "V155pYYNG6Zjx45pyZIl6tmzp2JiYpSTkyOfz+f/PpfLpaeeekopKSm66qqrlJqaqp07d/pv37Jl\n",
              "i9LS0tSxY0e1b99et9xyiz788MOA210ulwoKCpp8vL790tfx48c1YcIE9ejRQ126dNE999yjzz//\n",
              "3H/7kiVLlJCQoMjISHXt2lWZmZkX/ViOGDFC06dP14QJExQdHa34+Hj96U9/kiQ1NDQoPj5ezz//\n",
              "fMD3PPXUU+rfv/9FPc7nq6qqUrt27bR169aA7dOmTdOYMWP811etWqUBAwYoOjpa1113ndauXRuw\n",
              "/vnnn1efPn0UGRmp9PR0VVZWXvT9BaxhAARFv379zI033mhWrlxp9uzZY3w+X8DtmzdvNpLMhAkT\n",
              "TGVlpamsrDRpaWnmF7/4hX/NpEmTzMiRI015ebk5c+aMeeaZZ0znzp3NP/7xD2OMMY8++qgZOnSo\n",
              "f/1NN91k5s6de8F5Dh06ZCSZv/3tb985d0FBgYmMjDRvvPGGqa+vN6+//rq58sorzV/+8hdjjDFH\n",
              "jhwxksyoUaPMyZMnTU1NjRk6dKhJSkoyDz30kKmtrTUlJSUmOjravPDCC/79SjIJCQmmuLjY1NbW\n",
              "mkcffdR06tTJVFZWGmOM2bZtm3n33XdNXV2dqa6uNlOmTDEej8fU1dVd9ON1/v2vra01ycnJZubM\n",
              "meb06dOmpqbGZGRkmFtvvdX/eFxxxRVm3759xhhjampqzNtvv93k4zJp0iRz7733BvysqKgo8+ab\n",
              "bxqfz2cKCgpMWFiYKSkpMcYYM2/ePHPTTTcF7KNv375m8eLFzXqcz+1v4sSJZtKkSf59ff3116Z9\n",
              "+/bmr3/9qzHGmPz8fBMfH2+KioqMz+czW7duNZGRkWbr1q3GGGPeffddExERYQoLC82ZM2dMYWGh\n",
              "ufzyyxvNCNiOUAGC5IsvvjDz5s0zKSkpxu12m/bt25uZM2ea2tpaY8z//cN79OhR//csWbLEXHPN\n",
              "Nf7vl2Q+/vjjgP0mJiaa559/3hjTvFDZtm2bkWQOHDjwnXP/+Mc/Ng8++GDAtmnTppmf/OQnxpj/\n",
              "+wf0nXfe8d/+9NNPm3bt2pn6+nr/tjvuuCNgP5LMH//4R/91n89nunXrZlatWnXBOU6dOmUkmb17\n",
              "9xpj/vXj9e37v27dOtO9e3fT0NDgv/3YsWNGkikvLzeffPKJufzyy83atWtNVVXVdz4mxlw4VH75\n",
              "y18GrOnUqZNZu3at/3E6P1y2bdtm3G63+fLLL40xF/84n/v+t99+27Rr184/6+rVq01sbKz/Me/X\n",
              "r59ZunRpwP6ysrLM5MmT/V+np6cH3J6enk6ooNXhpR8gSDp27KjHHntMO3bsUFVVlVauXKnly5cH\n",
              "vMQiSd27d/d/feWVV6qmpkaS/L/t8W//9m+KiYnxX44fP65jx441e54uXbpI0r/83vLyciUkJARs\n",
              "S0xMlNfrDdgWGxsbMHfnzp0VHh5+wftyztVXX+3/OiwsTD179lR5ebkkae/evbrzzjvVo0cPRUVF\n",
              "+ddWVFQE7KOpx+vbSkpKdPLkSbVv397/2F133XVyu93yer26+uqrtXbtWuXn58vj8SglJUVr1qz5\n",
              "zsfm286f5dvz9OrVSyNHjtSKFSskSXl5eUpPT/e/FHixj/M5w4cPV1xcnH/GvLw8ZWZm+h/zkpIS\n",
              "zZw5M+C/lTVr1ujTTz+VdPZ5P//xl9ToOtAaECqAA9xut8aNG6dbb731on+75ty5LHv37lVlZaX/\n",
              "8tVXX2n27NnNnqFPnz5KSkpqdN7Et8XHx+vw4cMB2w4fPiyPx9Psn/ltZWVl/q8bGhrk9XoVFxcn\n",
              "SfrpT3+qhIQE7d+/X9XV1Tpy5IgkyRjTop/VrVs39ezZM+Cxq6ysVG1trdLS0iRJY8eO1auvvqov\n",
              "vvhCv/3tb3Xvvffq0KFD3+9OnicrK0vPPfecTp06pZdeeklZWVn+21ryOE+ePFl5eXkqLS3VO++8\n",
              "o8mTJwfc32effTbgvp4+fVqbNm2SJMXFxQU8/pIaXQdaA0IFCIJ//OMfmj17tvbu3au6ujr5fD69\n",
              "+eab2rx5s4YPH35R++jZs6fGjRunBx54QEePHpUk1dTU6JVXXmnxrzjn5ubqpZde0owZM3T06FEZ\n",
              "Y1RdXa1Vq1Zp7ty5ks7+47py5Upt2bJFPp9Pb731llasWKHs7OwW/czzLV68WB999JG++eYbPfHE\n",
              "E/rmm2/8J4NWVVUpKipK0dHROnXqlGbOnPm9flZ6errOnDmjefPmqaqqStLZozMvvviiJOngwYPa\n",
              "tGmTTp8+rYiICEVHR0tSwFGh72vcuHE6c+aM/yTqm2++2X9bSx7nSZMmac+ePfrP//xP3XTTTQFH\n",
              "ZB588EE9/vjjKioqUkNDg+rq6lRUVKRdu3b5v7ewsFAbN26Uz+fTxo0b/REDtCaEChAEl112mb74\n",
              "4gv99Kc/VadOndSxY0dNnz5ds2bNatY/wC+88IIGDx6sUaNGKTIyUsnJyVq+fHmLjzKMGDFC77//\n",
              "vo4fP67U1FRFRkaqf//+evXVV3XXXXdJOntk43/+53/0q1/9SjExMfrNb36jxYsXKz09vUU/83xT\n",
              "p07VxIkT1aFDBxUWFmrTpk2KiYmRJK1cuVIvvfSSIiMjNWTIEI0ePfp7/azIyEht375dXq9X/fr1\n",
              "U1RUlNLS0vTOO+9Ikj+Wzr3UNHPmTK1atarRyzHfx2WXXaaJEydqw4YNuu+++wJ+Nb0lj3PXrl11\n",
              "xx13aMOGDQFHZyRp+vTpmj9/vv83zXr06KHf/va3+uc//ylJGjZsmJYtW6bp06crJiZGK1as0H33\n",
              "3Re0+wpcKi7T0r8BAeA7uFwuvf7667r11ltDPQqAVowjKgAAwFqECgAAsFZEqAcA8MPEq8oAgoEj\n",
              "KgAAwFqECgAAsBahAgAArEWoAAAAa/0gQmXx4sWhHgEAADjgBxEq595uHAAA/LD8IEIFAAD8MBEq\n",
              "AADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBah\n",
              "AgAArOVoqEybNk29evWSy+XShx9+2OS6FStWqE+fPkpISNCUKVN05swZJ8cCAACthKOhctddd2nb\n",
              "tm3q2bNnk2uOHDmiefPmaevWrSotLdXJkye1bNkyJ8cCAACthKOhMnz4cMXFxX3nmoKCAo0ZM0bd\n",
              "unWTy+XS/fffrzVr1jS5vq6uTtXV1QEXn88X7NEBAIAFIkI9gNfrDTji0qtXL3m93ibXL1y4UAsW\n",
              "LAjYNmTIEEdm6zV7oyP7laSyRf/Pkf22xpkl5+ZmZgBtxQ/176RWdzLtnDlzVFVVFXBJTU0N9VgA\n",
              "AMABIT+i4vF4dPjwYf/1srIyeTyeJte73W653e6AbeHh4Y7NBwAAQifkR1TGjx+vwsJCffbZZzLG\n",
              "aOnSpbr77rtDPRYAALCAo6GSk5OjuLg4HTt2TD/5yU+UmJgoScrKylJhYaEkqXfv3lqwYIGGDh2q\n",
              "xMREde7cWTk5OU6OBQAAWglHX/rJzc294Pa8vLyA61OmTNGUKVOcHAUAALRCIX/pBwAAoCmECgAA\n",
              "sBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAA\n",
              "AGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoA\n",
              "ALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagA\n",
              "AABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQK\n",
              "AACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWo\n",
              "AAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqO\n",
              "hkpJSYnS0tKUlJSklJQUFRcXN1rT0NCgGTNm6Nprr1X//v118803q7S01MmxAABAK+FoqOTk5Cg7\n",
              "O1uHDh3SrFmzlJmZ2WhNYWGh3n33Xe3Zs0d79+7VyJEj9fDDDzs5FgAAaCUcC5WKigrt3LlTGRkZ\n",
              "kqTx48ervLy80dESl8uluro61dbWyhij6upqxcXFOTUWAABoRSKc2nF5ebliY2MVEXH2R7hcLnk8\n",
              "Hnm9XiUmJvrX3Xnnndq8ebO6deumyMhI9ejRQ2+//XaT+62rq1NdXV3ANp/P58ydAAAAIRXyk2l3\n",
              "7typ/fv36/jx4/r00081cuRI3X///U2uX7hwoaKjowMuO3bsuIQTAwCAS8WxUImPj9eJEydUX18v\n",
              "STLGyOv1yuPxBKxbtWqVbrnlFsXExCgsLEyTJk3S5s2bm9zvnDlzVFVVFXBJTU116m4AAIAQcixU\n",
              "unTpokGDBmn16tWSpHXr1ikuLi7gZR9J6t27t9566y198803kqQNGzbo+uuvb3K/brdbUVFRAZfw\n",
              "8HCn7gYAAAghx85RkaTc3FxlZmbqySefVFRUlPLz8yVJWVlZGjNmjMaMGaMHHnhAH330kQYMGKAf\n",
              "/ehH6tatm5YuXerkWAAAoJVwNFSSk5O1ffv2Rtvz8vL8X7vdbi1fvtzJMQAAQCsV8pNpAQAAmkKo\n",
              "AAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqE\n",
              "CgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxF\n",
              "qAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBa\n",
              "hAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACs\n",
              "RagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADA\n",
              "WoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAA\n",
              "rEWoAAAAazkaKiUlJUpLS1NSUpJSUlJUXFx8wXX79u3TiBEj1LdvX/Xt21fr1693ciwAANBKRDi5\n",
              "85ycHGVnZyszM1MFBQXKzMxUUVFRwJqvvvpKY8eO1apVqzRs2DD5fD6dOnXKybEAAEAr4dgRlYqK\n",
              "Cu3cuVMZGRmSpPHjx6u8vFylpaUB61544QUNGTJEw4YNkySFh4erc+fOTe63rq5O1dXVARefz+fU\n",
              "3QAAACHkWKiUl5crNjZWERFnD9q4XC55PB55vd6AdQcOHJDb7dYdd9yhgQMH6he/+IU+//zzJve7\n",
              "cOFCRUdHB1x27Njh1N0AAAAh1OxQee2114I6QH19vd544w3l5uZq9+7d6tGjh6ZOndrk+jlz5qiq\n",
              "qirgkpqaGtSZAACAHZodKo899piSk5O1ePFiVVdXN7kuPj5eJ06cUH19vSTJGCOv1yuPxxOwzuPx\n",
              "6Oabb1aPHj3kcrmUkZGh9957r8n9ut1uRUVFBVzCw8ObezcAAEAr0OxQeffdd7V27Vrt379fSUlJ\n",
              "+tWvfqUDBw40WtelSxcNGjRIq1evliStW7dOcXFxSkxMDFj3s5/9TEVFRf7o2bRpkwYMGNCS+wIA\n",
              "AH5gWnSOyg033KDly5fr1Vdf1YYNG9S/f3+NGjVK+/btC1iXm5ur3NxcJSUladGiRcrPz5ckZWVl\n",
              "qbCwUNLZIyoPP/yw0tLS1L9/f7311ltaunTp97xbAADgh6BFv578xhtv6JlnntG+ffv0wAMPaPLk\n",
              "ydqyZYv+4z/+I+C3epKTk7V9+/ZG35+XlxdwfeLEiZo4cWJLRgEAAD9gzQ6Vvn37qlOnTpo2bZrS\n",
              "09P954fcddddWrFiRdAHBAAAbVezQ2X16tUaPHjwBW975ZVXvvdAAAAA5zT7HJVdu3YFvHPsl19+\n",
              "qeXLlwd1KAAAAKkFofLss8+qQ4cO/usdO3bUs88+G9ShAAAApBaEijGm0Tbewh4AADih2aESGxur\n",
              "//3f//Vff/HFFxUbGxvUoQAAAKQWnEz79NNPa+zYsXrooYckSe3atdNf//rXoA8GAADQ7FC55ppr\n",
              "dODAAR08eFDS2fdK4S3sAQCAE1r0hm8ul0sxMTGqr6/X8ePHJanRZ/gAAAB8X80OlT//+c+aNm2a\n",
              "fvSjHyks7OwpLi6XSxUVFUEfDgAAtG3NDpXHH39cRUVFSk5OdmIeAAAAv2b/1k+nTp2IFAAAcEk0\n",
              "O1TGjRunp59+WhUVFaqurvZfAAAAgq3ZL/3MnTtXkjRjxgy5XC4ZY+RyuXjTNwAAEHTNDpWGhgYn\n",
              "5gAAAGik2S/9SGc/mPD555+XJFVWVurEiRNBHQoAAEBq4YcS3nfffZo/f76ks5+ePGHChGDPBQAA\n",
              "0PxQWbZsmd577z1FRUVJkhISEvT5558HfTAAAIBmh4rb7dYVV1wRsC0iokVvcAsAAPCdmh0qnTt3\n",
              "1qFDh+RyuSSdfada3j4fAAA4oUWfnnzPPffo448/Vnx8vKKiorRhwwYnZgMAAG1cs0MlMTFR77//\n",
              "vg4ePChjDJ+eDAAAHNPsUPF6vZKkK6+8UpL49GQAAOCYZofK4MGD/e9IW1tbq6+++kodO3bk05MB\n",
              "AEDQNTtUvv2ryOvXr9eePXuCNhAAAMA5LXpn2vOlp6dr48aNwZgFAAAgQLOPqJz/Sck+n0/vv/8+\n",
              "n54MAAAc0exQiYmJ8Z+jEh4erj59+uiPf/yjE7MBAIA2jk9PBgAA1vre56gAAAA4pdlHVMLCwvxv\n",
              "n38+Y4xcLpd8Pl9QBgMAAGh2qDz22GP6+uuvNXXqVEnS0qVLdcUVV+jBBx8M9mwAAKCNa3ao/OUv\n",
              "f9GuXbv813/3u99p8ODBmjt3blAHAwAAaPY5KjU1NQHvQltRUaGampqgDgUAACC14IjKzJkzNWDA\n",
              "AN1+++2SpFdffVXz588P9lwAAADND5WcnBwNHTpUmzdvliTNmDFD1113XdAHAwAAaHaoSFLHjh3V\n",
              "r18/jRgxQvX19frmm2902WWXBXs2AADQxjX7HJWCggINGTJEv/zlLyVJxcXFGjduXLDnAgAAaH6o\n",
              "LFy4UB988IFiYmIkSQMGDNDRo0eDPRcAAEDzQyU8PFwdO3YM2MbLPgAAwAnNDpXIyEidPHnS/+60\n",
              "b775pjp06BD0wQAAAJp9Mu3vf/97jR49Wp988omGDRumI0eOaOPGjU7MBgAA2rhmhUpDQ4N8Pp82\n",
              "b96sv//97zLGKC0tzX++CgAAQDA1K1TCwsKUnZ2tPXv2aPTo0U7NBAAAIKkF56j06dNHpaWlTswC\n",
              "AAAQoNnnqJw6dUoDBw5UWlqarrrqKv/29evXB3UwAACAiw6V7OxsLVu2TJMmTdKYMWPUvn17J+cC\n",
              "AAC4+FDZuXOnJGnSpEkaNGiQPvjgA8eGAgAAkFpwjookGWOCPQcAAEAjF31E5euvv9a+fftkjFFt\n",
              "ba3/63P69+/vyIAAAKDtalaojBkzxn/9/K9dLpc++eST4E4GAADavIsOlbKyMgfHAAAAaKxF56gA\n",
              "AABcCoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqOhkpJSYnS\n",
              "0tKUlJSklJQUFRcXN7nWGKNbbrlFMTExTo4EAABaEUdDJScnR9nZ2Tp06JBmzZqlzMzMJtf+4Q9/\n",
              "UEJCgpPjAACAVsaxUKmoqNDOnTuVkZEhSRo/frzKy8tVWlraaG1xcbFefvllzZ49+1/ut66uTtXV\n",
              "1QEXn88X9PkBAEDoORYq5eXlio2NVUTE2c89dLlc8ng88nq9AevOnDmjKVOmKDc3V+Hh4f9yvwsX\n",
              "LlR0dHTAZceOHY7cBwAAEFohP5l2wYIFSk9PV9++fS9q/Zw5c1RVVRVwSU1NdXhKAAAQChFO7Tg+\n",
              "Pl4nTpxQfX29IiIiZIyR1+uVx+MJWPf222/L6/VqyZIlqq+vV3V1tXr16qWioiJ17ty50X7dbrfc\n",
              "bnfAtos5EgMAAFofx46odOnSRYMGDdLq1aslSevWrVNcXJwSExMD1m3dulVHjx5VWVmZtm3bpqio\n",
              "KJWVlV0wUgAAQNvi6Es/ubm5ys3NVVJSkhYtWqT8/HxJUlZWlgoLC5380QAA4AfAsZd+JCk5OVnb\n",
              "t29vtD0vL++C63v16qXKykonRwIAAK1IyE+mBQAAaAqhAgAArEWoAAAAaxEqAADAWoQKAACwFqEC\n",
              "AACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEq\n",
              "AADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBah\n",
              "AgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsR\n",
              "KgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAW\n",
              "oQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABr\n",
              "ESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArOVoqJSUlCgtLU1JSUlKSUlR\n",
              "cXFxozVvvfWWUlNTde211+q6667TQw89pIaGBifHAgAArYSjoZKTk6Ps7GwdOnRIs2bNUmZmZqM1\n",
              "7du319q1a3XgwAHt2rVLf//737Vq1SonxwIAAK2EY6FSUVGhnTt3KiMjQ5I0fvx4lZeXq7S0NGDd\n",
              "DTfcoN69e0uSLr/8cg0cOFBlZWVN7reurk7V1dUBF5/P59TdAAAAIeRYqJSXlys2NlYRERGSJJfL\n",
              "JY/HI6/X2+T3fPbZZyooKNAdd9zR5JqFCxcqOjo64LJjx46gzw8AAELPmpNpq6urdeedd+qhhx7S\n",
              "jTfe2OS6OXPmqKqqKuCSmpp6CScFAACXSoRTO46Pj9eJEydUX1+viIgIGWPk9Xrl8Xgara2pqdFt\n",
              "t92msWPHasaMGd+5X7fbLbfbHbAtPDw8qLMDAAA7OHZEpUuXLho0aJBWr14tSVq3bp3i4uKUmJgY\n",
              "sO706dO67bbbdNttt+mRRx5xahwAANAKOfrST25urnJzc5WUlKRFixYpPz9fkpSVlaXCwkJJ0uLF\n",
              "i7Vjxw6tX79eAwcO1MCBA/XEE084ORYAAGglHHvpR5KSk5O1ffv2Rtvz8vL8X8+dO1dz5851cgwA\n",
              "ANBKWXMyLQAAwLcRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABr\n",
              "ESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACw\n",
              "FqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAAsBahAgAArEWoAAAA\n",
              "axEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAAAGsRKgAAwFqECgAA\n",
              "sBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoAALAWoQIAAKxFqAAA\n",
              "AGsRKgAAwFqECgAAsBahAgAArEWoAAAAaxEqAADAWoQKAACwFqECAACsRagAAABrESoAAMBahAoA\n",
              "ALAWoQIAAKxFqAAAAGs5GiolJSVKS0tTUlKSUlJSVFxcfMF1K1asUJ8+fZSQkKApU6bozJkzTo4F\n",
              "AABaCUdDJScnR9nZ2Tp06JBmzZqlzMzMRmuOHDmiefPmaevWrSotLdXJkye1bNkyJ8cCAACthGOh\n",
              "UlFRoZ07dyojI0OSNH78eJWXl6u0tDRgXUFBgcaMGaNu3brJ5XLp/vvv15o1a5rcb11dnaqrqwMu\n",
              "Pp/PqbsBAABCyGWMMU7seNeuXZowYYIOHjzo35aamqpFixbplltu8W/7zW9+o+7du2vOnDmSpAMH\n",
              "Dui2226T1+u94H7nz5+vBQsWBGy75pprNHr06KDO7/P5tGPHDqWmpio8PDyo+0bL8JzYhefDLjwf\n",
              "duH5uDg9e/bU9OnTv3NNxCWaJWjmzJmjGTNmBGxzu91yu91B/TnV1dWKjo7Wa6+9pqioqKDuGy3D\n",
              "c2IXng+78HzYhecjeBwLlfj4eJ04cUL19fWKiIiQMUZer1cejydgncfj0eHDh/3Xy8rKGq05nxNR\n",
              "AgAA7OTYOSpdunTRoEGDtHr1aknSunXrFBcXp8TExIB148ePV2FhoT777DMZY7R06VLdfffdTo0F\n",
              "AABaEUd/6yc3N1e5ublKSkrSokWLlJ+fL0nKyspSYWGhJKl3795asGCBhg4dqsTERHXu3Fk5OTlO\n",
              "jgUAAFoJR89RSU5O1vbt2xttz8vLC7g+ZcoUTZkyxclRms3tduvRRx/lZSaL8JzYhefDLjwfduH5\n",
              "CB7HfusHAADg++It9AEAgLUIFQAAYC1CBQAAWItQAQAA1iJUmnCxn/wM59XW1mrcuHFKSkrSgAED\n",
              "NGrUqEafGYXQyM/Pl8vl0ssvvxzqUdq0uro6/frXv1afPn3Ur18//2esITQ2bdqkQYMGaeDAgbr+\n",
              "+uv13HPPhXqkVq3VvYX+pXLuk58zMzNVUFCgzMxMFRUVhXqsNis7O1ujR4+Wy+XSkiVLlJWVpS1b\n",
              "toR6rDatrKxMy5cv15AhQ0I9Sps3e/ZsuVwuHTp0SC6XS5999lmoR2qzjDHKyMjQli1b1L9/f5WV\n",
              "lemaa65Renq6IiMjQz1eq8QRlQu42E9+xqVx+eWX6/bbb5fL5ZIkDRkyRGVlZaEdqo1raGhQVlaW\n",
              "nnnmGd4nIsT++c9/asWKFXriiSf8f0a6desW4qnaNpfLpcrKSklnP/OnY8eO/Dn5HgiVCygvL1ds\n",
              "bKwiIs4ecHK5XPJ4PE1+ojMurcWLF2vs2LGhHqNNe+qppzR06FANHjw41KO0eYcPH1aHDh305JNP\n",
              "6sYbb9S///u/68033wz1WG2Wy+XSiy++qPT0dPXs2VPDhg3Tc889p8suuyzUo7VavPSDVuXJJ59U\n",
              "aWkpfxGH0P79+7Vu3Tq98847oR4Fkurr63X06FFde+21WrRokXbv3q1Ro0apuLhYXbt2DfV4bU59\n",
              "fb1+97vfaf369Ro+fLiKioo0ZswY7du3T506dQr1eK0SR1Qu4PxPfpbU5Cc/49L67//+b61fv16v\n",
              "vPKK2rVrF+px2qytW7eqrKxMffr0Ua9evfTee+8pOztbf/rTn0I9Wpvk8XgUFhame++9V5J0ww03\n",
              "6Oqrr9a+fftCPFnb9OGHH+rTTz/V8OHDJUkpKSmKi4vT7t27QzxZ60WoXMDFfvIzLp2nnnpKa9as\n",
              "0euvv66YmJhQj9OmTZ06VSdOnFBZWZnKyso0ZMgQLVu2TFOnTg31aG1Sp06dNHLkSL322muSpCNH\n",
              "jujIkSPq27dviCdrm879j+5HH30kSSotLdXhw4eVnJwc4slaLz7rpwkHDx5UZmamvvzyS0VFRSk/\n",
              "P1/9+vUL9Vht0rFjxxQfH6/evXv7z5p3u916//33QzwZJGnEiBF68MEHNW7cuFCP0mZ98sknmjx5\n",
              "sr744guFhYXpv/7rvzR+/PhQj9VmrVmzRk8++aTCwsLU0NCgOXPmaMKECaEeq9UiVAAAgLV46QcA\n",
              "AFiLUAEAANYiVAAAgLUIFQAAYC1CBQAAWItQAQAA1iJUAACAtQgVAABgLUIFAABYi1ABAADW+v/p\n",
              "+kdLA+E8zgAAAABJRU5ErkJggg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-665ef88a-8820-4851-b72b-6af385e9abbb\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-665ef88a-8820-4851-b72b-6af385e9abbb\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x78367ea64940>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Categorical distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "_df_3.groupby('Transaction ID').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-8329377e-b325-4eee-a81d-b97c6141eeca\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAm0AAAGZCAYAAADb4AYsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAg1klEQVR4nO3df1iV9f3H8dfxILi1wBymLjtQiC0ROJSWzsrQb8OrWevaZqdM\n",
              "p7WMNGQNyi7nrrGRm8tNraCu0CuHCRpmGbv6Xdrl1bgur6zAflAR2uFYyvqBwqL5Cz7fP7LTmOLO\n",
              "cRxuP4fn47ruP24+t/C+PqdrPHfOuTkuY4wRAAAATmn9nB4AAAAA/x3RBgAAYAGiDQAAwAJEGwAA\n",
              "gAWINgAAAAsQbQAAABYg2gAAACxAtAEAAFiAaAMAALAA0RZl7rvvPqdHAAAAEUC0RZmmpianRwAA\n",
              "ABFAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAALEG0AAAAWINoAAAAsQLQBAABY\n",
              "gGgDAACwANEGAABgAaINAADAAkQbAACABYg2AAAACxBtAAAAFiDaAAAALOAyxhinh0DPybgyXuNn\n",
              "nuH0GAAARJWy65ucHoFn2gAAAGxAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAAL\n",
              "EG0AAAAWINoAAAAsQLQBAABYgGgDAACwANEGAABgAaINAADAAkQbAACABSIWbV6vV16vV6NGjZLb\n",
              "7Q6e+3w+bdu2TR6PR/v37w9eP23aNBUVFUmSOjs7NX/+fKWkpGjEiBEqLS0NXhfta7/73e80ePDg\n",
              "4H7dcMMNJ/sQAACAKBITqW9cV1cnSfL7/fJ6vcHzr02fPl15eXmqqKhQZWWlGhsbtW7dOklSRUWF\n",
              "6uvr1dDQoNbWVmVlZSk7O1tpaWlRvyZJN9xwg+69995IPTQAAMBCjr08WlxcrB07dqikpESFhYVa\n",
              "s2aN+vfvL0mqqqrSnDlz5Ha7NWjQIPl8Pq1fv75PrAEAAByPY9EWGxurlStXKj8/X3PnzlVGRkZw\n",
              "LRAIKCkpKXienJysQCDQJ9Yk6bHHHlNmZqYmTZqkl19+uds9PHjwoNra2rocnZ2m2+sBAIC9HL0R\n",
              "YdOmTRo+fPgxL532Zbfeeqv8fr927Nihu+++Wz6fT01NTce9dsmSJUpISOhyfLrzUC9PDAAAeoNj\n",
              "0VZTU6MNGzaotrZWfr9flZWVwTWPx9MlVPx+vzweT59YGzp0aPBl4gkTJigrK0uvvfbacfdw4cKF\n",
              "am1t7XIMTok97rUAAMBujkRbe3u7Zs+erbKyMiUmJqq8vFyFhYVqbm6W9NWdpKtWrVJHR4daWlpU\n",
              "VVUln8/XJ9Y++uij4D598MEHqqurU3p6+nH3MS4uTvHx8V2Ofv1cPfdAAQCAU0bE7h49kQULFig7\n",
              "O1s5OTmSpMzMTM2bN0+5ubmqrq7WzJkztX37dqWmpsrlcqmgoCAYLtG+tmjRIr3++uuKiYmR2+3W\n",
              "Aw88oJEjR/bGwwIAAE5hLmMM71yPIhlXxmv8zDOcHgMAgKhSdv3x31/em/hEBAAAAAsQbQAAABYg\n",
              "2gAAACxAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAALEG0AAAAWINoAAAAsQLQB\n",
              "AABYgGgDAACwQIzTA6Bn/d/3b9by65c7PQYAAOhhPNMGAABgAaINAADAAkQbAACABYg2AAAACxBt\n",
              "AAAAFiDaAAAALEC0AQAAWIBoAwAAsADRBgAAYAGiDQAAwAJEGwAAgAWINgAAAAsQbQAAABYg2gAA\n",
              "ACxAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAALEG0AAAAWINoAAAAsQLQBAABY\n",
              "gGgDAACwANEGAABgAaINAADAAkQbAACABYg2AAAACxBtAAAAFiDaAAAALEC0AQAAWIBoAwAAsADR\n",
              "BgAAYAGiDQAAwAJEGwAAgAWINgAAAAvEOD0Aelb7mtfU/NyfnR4DAICoMrT+TqdH4Jk2AAAAGxBt\n",
              "AAAAFiDaAAAALEC0AQAAWIBoAwAAsADRBgAAYAGiDQAAwAJEGwAAgAWINgAAAAsQbQAAABYg2gAA\n",
              "ACxAtAEAAFiAaAMAALAA0QYAAGCBiESb1+uV1+vVqFGj5Ha7g+c+n0/btm2Tx+PR/v37g9dPmzZN\n",
              "RUVFkqTOzk7Nnz9fKSkpGjFihEpLS4PXRfva1z755BMNGTJE11xzTZg7DwAAolVMJL5pXV2dJMnv\n",
              "98vr9QbPvzZ9+nTl5eWpoqJClZWVamxs1Lp16yRJFRUVqq+vV0NDg1pbW5WVlaXs7GylpaVF/drX\n",
              "cnNzNXXqVH3++eeReHgAAICFHHl5tLi4WDt27FBJSYkKCwu1Zs0a9e/fX5JUVVWlOXPmyO12a9Cg\n",
              "QfL5fFq/fn2fWJOkhx9+WOecc44uvfTSSD8MAADAIo5EW2xsrFauXKn8/HzNnTtXGRkZwbVAIKCk\n",
              "pKTgeXJysgKBQJ9Y+/DDD/XQQw/pD3/4w3/dQ0k6ePCg2trauhwdpjOkfwsAAOzi2I0ImzZt0vDh\n",
              "w4956bSvMsbopptuUmlpqb71rW+F9G+WLFmihISELkfdv3ZHeFIAAOAER6KtpqZGGzZsUG1trfx+\n",
              "vyorK4NrHo9HTU1NwXO/3y+PxxP1a21tbXrzzTfl8/mUnJysO+64Qy+88IImT57c7T4uXLhQra2t\n",
              "XQ7vt87u9noAAGCvXo+29vZ2zZ49W2VlZUpMTFR5ebkKCwvV3Nws6as7SVetWqWOjg61tLSoqqpK\n",
              "Pp8v6tcSEhL0+eefy+/3y+/36y9/+Yt++MMfavPmzd3uZVxcnOLj47scbhd/xQUAgGgUkbtHT2TB\n",
              "ggXKzs5WTk6OJCkzM1Pz5s1Tbm6uqqurNXPmTG3fvl2pqalyuVwqKChQenq6JEX9GgAAQHdcxhjj\n",
              "9BDoObnfvUy/H3KV02MAABBVhtbf6fQIfCICAACADYg2AAAACxBtAAAAFiDaAAAALEC0AQAAWIBo\n",
              "AwAAsADRBgAAYAGiDQAAwAJEGwAAgAWINgAAAAsQbQAAABYg2gAAACxAtAEAAFggxukB0LNOmzVG\n",
              "Q5ff6fQYAACgh/FMGwAAgAWINgAAAAsQbQAAABYg2gAAACxAtAEAAFiAaAMAALAA0QYAAGABog0A\n",
              "AMACRBsAAIAFiDYAAAALEG0AAAAWINoAAAAsQLQBAABYgGgDAACwANEGAABgAaINAADAAjGhXmiM\n",
              "0XPPPad33nlHkjR69Gjl5OTI5XJFbDgAAAB8JaRo279/vyZPnqzPPvtMWVlZMsaopKREgwcP1ubN\n",
              "m5WQkBDpOQEAAPq0kF4evfvuu3XhhRdq586devLJJ1VdXa3GxkaNGTNGxcXFkZ4RAACgzwsp2l54\n",
              "4QWtWLFCMTHfPDHXv39/LVu2TM8//3zEhgMAAMBXQoo2Y4xOO+20Y75+vK8BAACg54UUbXFxcd2u\n",
              "xcbG9tgwAAAAOL6QbkR49913dcEFFxzzdWOMGhoaenwoAAAAdBVStD377LORngMAAAAnEFK0TZw4\n",
              "MdJzAAAA4ARCiraCgoITri9fvrxHhgEAAMDxhRRt/PFcAAAAZ4UUbUVFRZGeAwAAACfAB8YDAABY\n",
              "gGgDAACwANEGAABgAaINAADAAiHdiPC1N954Q7/+9a+1a9cuHTlyJPj1Xbt29fhgAAAA+EZY0TZr\n",
              "1izl5eVp/PjxcrvdkZoJAAAA/yGsaHO73crNzY3ULAAAAOhGWO9pmzBhgl577bVIzQIAAIBuuIwx\n",
              "JtSL09PT9f7772vEiBEaMGBA8OtvvPFGRIZD+MZccI0mTbzZ6TEAAIgqS1dMdXqE8F4eLS0tjdQc\n",
              "AAAAOIGwom3ixImSpD179kiSvve97/X8RAAAADhGWO9pe/fdd5WWlhY80tPT9d5770VqNgAAABwV\n",
              "VrTNmzdPixYt0r59+7Rv3z4tWrRIc+fOjdRsAAAAOCqsaNu3b5+mT58ePL/uuuu0b9++Hh8KAAAA\n",
              "XYUVbW63W/X19cHz+vp6/sguAABALwjrRoQ//vGPuuyyy5SRkSFJeuutt1RZWRmRwQAAAPCNsKIt\n",
              "JydH9fX1evXVVyVJ48aNU2JiYkQGAwAAwDfCijZJOvPMMzV1qvN/YA4AAKAvCSnaJk6cqK1bt+qM\n",
              "M86Qy+UKft0YI5fLpZaWlogNCAAAgBCj7dFHH5Uk1dXVRXIWAAAAdCOku0eHDRsmSVq7dq2SkpK6\n",
              "HGvXro3ogAAAAAjzT3488cQTIX0NAAAAPSukl0eff/55Pffcc/r4449VUFAQ/Hpra2vEBgMAAMA3\n",
              "QnqmbcCAARo4cKD69eunhISE4DF69Ohun2nzer3yer0aNWqU3G538Nzn82nbtm3yeDzav39/8Ppp\n",
              "06apqKhIktTZ2an58+crJSVFI0aMUGlpafC6aF974IEHlJ6eLq/Xq9GjR+v+++8P5SECAABRLuS7\n",
              "RydOnKhrrrlGmZmZIX3jr29a8Pv98nq9x9zEMH36dOXl5amiokKVlZVqbGzUunXrJEkVFRWqr69X\n",
              "Q0ODWltblZWVpezsbKWlpUX92owZM3TbbbdJktra2jR69GhdeumlysrKCvEhBQAA0Sis97SVlJTo\n",
              "888/D55/9tlnys3NPakfXFxcrB07dqikpESFhYVas2aN+vfvL0mqqqrSnDlz5Ha7NWjQIPl8Pq1f\n",
              "v75PrCUkJAT3qL29XYcPH+52Dw8ePKi2trYuh+nsPKnHAwAAnNrCirbXX39d3/3ud4PniYmJ2r59\n",
              "+0n94NjYWK1cuVL5+fmaO3du8KOxJCkQCCgpKSl4npycrEAg0CfWJGnjxo1KS0tTcnKy7rjjjm6f\n",
              "ZVuyZEmXl6sTEhK09x8Nx70WAADYLaxoO3LkSJdzY4wOHTp00j9806ZNGj58OH//7T/87Gc/0zvv\n",
              "vKP3339fFRUVev/994973cKFC9Xa2trlGDZkZC9PCwAAekNY0TZu3Djl5eWpqalJfr9f8+fP17hx\n",
              "407qB9fU1GjDhg2qra2V3+/v8sHzHo9HTU1NwXO/3y+Px9Mn1v5dcnKyLr74Yj311FPH3cO4uDjF\n",
              "x8d3OVz9wnpIAQCAJcL6Db9s2TJ9+eWXGjt2rC6++GIdPHhQK1asCPuHtre3a/bs2SorK1NiYqLK\n",
              "y8tVWFio5uZmSV/dSbpq1Sp1dHSopaVFVVVV8vl8fWKtvr4+uE+ffvqptmzZ0uWlYwAA0DeF9YHx\n",
              "8fHxWr169f/8QxcsWKDs7Gzl5ORIkjIzMzVv3jzl5uaqurpaM2fO1Pbt25WamiqXy6WCggKlp6dL\n",
              "UtSv3XfffXrllVcUGxsrY4xuv/12XXHFFf/zngMAALu5jDEmnH+wZ88evf322zpw4EDwa1dffXWP\n",
              "D4aTM+aCazRp4s1OjwEAQFRZumKq0yOE90zb6tWrVVxcrJaWFqWmpmrHjh0aN24c0QYAABBhYb2n\n",
              "bcWKFaqtrVVKSopef/11bdmyRSNHcrciAABApIUVbbGxsTrjjDOCf/rjsssu4891AAAA9IKwXh6N\n",
              "i4uTMUYjR47Uvffeq6SkJH3xxReRmg0AAABHhRVtixcvVltbm5YuXapbb71V+/bt04MPPhip2QAA\n",
              "AHBUWNE2adIkSV99PuaLL74YkYEAAABwrLDe0/bb3/5W+/fvlzFGP/rRj5SYmKjHH388UrMBAADg\n",
              "qLCirbq6WgMHDtRLL72kmJgY1dTUaPHixZGaDQAAAEeFFW39jn6u5datWzVt2jSdd955crlcERkM\n",
              "AAAA3wjrPW2nnXaa7rnnHj366KOqqamRMUaHDh2K1GwAAAA4Kqxn2srLy7V3714tXbpUQ4YM0c6d\n",
              "OzVjxoxIzQYAAICjwv7sUZza+OxRAAB6nnWfPer3+3XPPfdo586dwU9FkKQtW7b0+GAAAAD4RljR\n",
              "du2112ry5MnKy8uT2+2O1Ez4H1x2+blautz5/zcAAAB6VljRduDAAS1ZsiRSswAAAKAbYd2IMHr0\n",
              "aAUCgUjNAgAAgG6E9Uzbp59+qszMTI0fP14DBgwIfv2JJ57o8cEAAADwjbCibcaMGfyJDwAAAAeE\n",
              "FW2zZs2K1BwAAAA4gbCiTZI2bNiguro6HThwIPi15cuX9+hQAAAA6CqsGxHy8/O1du1alZeXy+Vy\n",
              "aePGjWptbY3UbAAAADgqrGh7+eWXVV1drcGDB2vZsmV69dVX9dFHH0VqNgAAABwVVrQNGDBA/fr1\n",
              "k8vl0uHDhzV06FDt2bMnUrMBAADgqLDe03b66afryy+/1CWXXKIZM2Zo6NCh+va3vx2p2QAAAHBU\n",
              "WM+0rV+/XjExMfrzn/+sjIwM9e/fXxs3bozUbAAAADgq5GfaOjo6dMcdd2jt2rWSpEWLFkVsKAAA\n",
              "AHQV8jNtbrdbDQ0NkZwFAAAA3QjrPW3Z2dm65ZZbNHv2bH3nO98Jfj0jI6PHBwMAAMA3Qoq266+/\n",
              "XuvXr1dVVZUk6cUXXwyuuVwu7dq1KzLTAQAAQFKI0fbee+9Jkj788MOIDgMAAIDjC+k9bS6XK9Jz\n",
              "AAAA4ARCeqbtzTff1KBBg475ujFGLpdLLS0tPT4YAAAAvhFStJ133nl65plnIj0LAAAAuhFStMXF\n",
              "xSkpKSnSswAAAKAbIb2nzRgT6TkAAABwAiFFW21tbaTnAAAAwAmE9dmjAAAAcAbRBgAAYAGiDQAA\n",
              "wAJEGwAAgAWINgAAAAsQbQAAABYg2gAAACxAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAF\n",
              "iDYAAAALEG0AAAAWcBljjNNDoOfcmObSwrG0OAAAPWlkeYfTI/BMGwAAgA2INgAAAAsQbQAAABYg\n",
              "2gAAACxAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAALEG0AAAAWINoAAAAsQLQB\n",
              "AABYgGgDAACwQESizev1yuv1atSoUXK73cFzn8+nbdu2yePxaP/+/cHrp02bpqKiIklSZ2en5s+f\n",
              "r5SUFI0YMUKlpaXB66J97f7779fo0aOVnp6ujIwMVVRUnOxDAAAAokxMJL5pXV2dJMnv98vr9QbP\n",
              "vzZ9+nTl5eWpoqJClZWVamxs1Lp16yRJFRUVqq+vV0NDg1pbW5WVlaXs7GylpaVF/VpaWppqamqU\n",
              "kJCg3bt3KysrS+PHj1dKSkokHiYAAGARR14eLS4u1o4dO1RSUqLCwkKtWbNG/fv3lyRVVVVpzpw5\n",
              "crvdGjRokHw+n9avX98n1iZPnqyEhARJ0tlnn62hQ4dq9+7d3e7jwYMH1dbW1uXo6Oy5xwkAAJw6\n",
              "HIm22NhYrVy5Uvn5+Zo7d64yMjKCa4FAQElJScHz5ORkBQKBPrH271566SXt27dPY8eOPe4eStKS\n",
              "JUuUkJDQ5Xjzs24vBwAAFnPsRoRNmzZp+PDhx7x0Cumtt97SjTfeqKqqKp122mndXrdw4UK1trZ2\n",
              "OTISe3FQAADQaxyJtpqaGm3YsEG1tbXy+/2qrKwMrnk8HjU1NQXP/X6/PB5Pn1iTpPr6ek2dOlWr\n",
              "V6/WJZdccsJ9jIuLU3x8fJfDzf3AAABEpV7/Fd/e3q7Zs2errKxMiYmJKi8vV2FhoZqbmyV9dSfp\n",
              "qlWr1NHRoZaWFlVVVcnn8/WJtXfffVdXXnmlVq5cqSuuuKI3Hg4AAGCJiNw9eiILFixQdna2cnJy\n",
              "JEmZmZmaN2+ecnNzVV1drZkzZ2r79u1KTU2Vy+VSQUGB0tPTJSnq1/Lz89Xa2qq77rpLd911lyTp\n",
              "nnvuCe4VAADou1zGGOP0EOg5N6a5tHAsr5ECANCTRpZ3OD0Cn4gAAABgA6INAADAAkQbAACABYg2\n",
              "AAAACxBtAAAAFiDaAAAALEC0AQAAWIBoAwAAsADRBgAAYAGiDQAAwAJEGwAAgAWINgAAAAsQbQAA\n",
              "ABaIcXoA9Kwzcn6lkcuXOz0GAADoYTzTBgAAYAGiDQAAwAJEGwAAgAWINgAAAAsQbQAAABYg2gAA\n",
              "ACxAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAALEG0AAAAWINoAAAAsQLQBAABY\n",
              "gGgDAACwANEGAABgAaINAADAAkQbAACABYg2AAAACxBtAAAAFiDaAAAALEC0AQAAWIBoAwAAsADR\n",
              "BgAAYAGiDQAAwAJEGwAAgAWINgAAAAsQbQAAABYg2gAAACxAtAEAAFiAaAMAALAA0QYAAGABog0A\n",
              "AMACRBsAAIAFiDYAAAALuIwxxukh0HO+kzNWA6+b7PQYAABElY9u/JPTI/BMGwAAgA2INgAAAAsQ\n",
              "bQAAABYg2gAAACxAtAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAALEG0AAAAWINoA\n",
              "AAAsQLQBAABYgGgDAACwQMSizev1yuv1atSoUXK73cFzn8+nbdu2yePxaP/+/cHrp02bpqKiIklS\n",
              "Z2en5s+fr5SUFI0YMUKlpaXB66J97emnn9aFF16ouLg43X777Se5+wAAINrEROob19XVSZL8fr+8\n",
              "Xm/w/GvTp09XXl6eKioqVFlZqcbGRq1bt06SVFFRofr6ejU0NKi1tVVZWVnKzs5WWlpa1K+lpqZq\n",
              "9erVeuyxx/TFF19E6uEBAACWcezl0eLiYu3YsUMlJSUqLCzUmjVr1L9/f0lSVVWV5syZI7fbrUGD\n",
              "Bsnn82n9+vV9Ym3kyJHKzMxUTMx/7+mDBw+qra2ty6EO0yOPDwAAOLU4Fm2xsbFauXKl8vPzNXfu\n",
              "XGVkZATXAoGAkpKSgufJyckKBAJ9Yi0cS5YsUUJCQpfj0Id7w/4+AADg1OfojQibNm3S8OHDj3np\n",
              "FKFZuHChWltbuxyx5wxzeiwAABABjkVbTU2NNmzYoNraWvn9flVWVgbXPB6Pmpqagud+v18ej6dP\n",
              "rIUjLi5O8fHxXQ65XWF/HwAAcOpzJNra29s1e/ZslZWVKTExUeXl5SosLFRzc7Okr+4kXbVqlTo6\n",
              "OtTS0qKqqir5fL4+sQYAAHA8Ebt79EQWLFig7Oxs5eTkSJIyMzM1b9485ebmqrq6WjNnztT27duV\n",
              "mpoql8ulgoICpaenS1LUr23evFmzZs1SW1ubjDHauHGjHnzwQV199dW98dAAAIBTlMsYw+2GUeQ7\n",
              "OWM18LrJTo8BAEBU+ejGPzk9Ap+IAAAAYAOiDQAAwAJEGwAAgAWINgAAAAsQbQAAABYg2gAAACxA\n",
              "tAEAAFiAaAMAALAA0QYAAGABog0AAMACRBsAAIAFiDYAAAALEG0AAAAWiHF6APSsW9Iu1fIb/+T0\n",
              "GAAAoIfxTBsAAIAFiDYAAAALEG0AAAAWINoAAAAsQLQBAABYgGgDAACwANEGAABgAaINAADAAkQb\n",
              "AACABYg2AAAACxBtAAAAFiDaAAAALEC0AQAAWIBoAwAAsADRBgAAYAGiDQAAwAJEGwAAgAVinB4A\n",
              "PefgwYN69tln1dHRIbfb7fQ4fUZHR4deffVVXXTRRex7L2PvncG+O4e9d0Zv7HtSUpJ++ctfnvAa\n",
              "lzHGROSno9e1tbUpISFBra2tio+Pd3qcPoN9dw577wz23TnsvTNOlX3n5VEAAAALEG0AAAAWINoA\n",
              "AAAsQLRFkbi4OBUVFSkuLs7pUfoU9t057L0z2HfnsPfOOFX2nRsRAAAALMAzbQAAABYg2gAAACxA\n",
              "tAEAAFiAaLPQBx98oB/84AcaOXKkxo4dq3feeee41z388MNKTU1VSkqK5syZo8OHD/fypNEllH3f\n",
              "smWLLrroIo0aNUppaWlasGCBOjs7HZg2eoT637skGWM0adIkDRw4sPcGjGKh7v1bb72lyy+/XOef\n",
              "f77OP/98PfHEE708aXQJZd87OztVUFCgUaNGKSMjQ9nZ2WpsbHRg2uiRn5+v5ORkuVwu1dXVdXud\n",
              "o79bDayTnZ1t/vrXvxpjjHnsscfMmDFjjrlm165dZtiwYWbv3r2ms7PTXHXVVaa0tLSXJ40uoez7\n",
              "G2+8YXbu3GmMMeZf//qXmTBhQvDf4OSEsu9fW7Zsmbn55ptNQkJC7wwX5ULZ+/b2dnPOOeeYV155\n",
              "xRhjzJEjR8wnn3zSm2NGnVD2fdOmTeaiiy4yhw4dMsYYc/fdd5tp06b15phRZ+vWrWb37t0mKSnJ\n",
              "1NbWHvcap3+3Em2W+cc//mFOP/10c/jwYWOMMZ2dnWbIkCHmgw8+6HLd0qVLTW5ubvD86aefNhMm\n",
              "TOjVWaNJqPv+n2677TZTVFTUCxNGp3D2/e233zaXXnqpaWxsJNp6QKh7v2rVKnP99dc7MWJUCnXf\n",
              "n3zySZOZmWna2tpMZ2enufPOO82vfvUrJ0aOOieKNqd/t/LyqGV2796tYcOGKSYmRpLkcrnk8XgU\n",
              "CAS6XBcIBJSUlBQ8T05OPuYahC7Uff93zc3N2rhxo6ZOndpbY0adUPf98OHDmjNnjsrKyvgQ7R4S\n",
              "6t7X19crLi5OU6dOldfr1c9//nN9+umnTowcFULd96uuukqXX365hg4dqmHDhmnz5s0qLi52YuQ+\n",
              "xenfrUQbEAFtbW266qqrtGDBAo0ZM8bpcaLe73//e/3kJz/R+eef7/Qofc6RI0f00ksvqaysTLW1\n",
              "tTrrrLM0d+5cp8eKeq+99prefvttffzxx9qzZ48mT56sW2+91emxEGFEm2XOPvts7d27V0eOHJH0\n",
              "1RuvA4GAPB5Pl+s8Ho+ampqC536//5hrELpQ912S/vnPf2rKlCn68Y9/rIKCgt4eNaqEuu9bt25V\n",
              "SUmJkpOTdckll6itrU3Jyck84/M/COd/a7Kzs3XWWWfJ5XJpxowZ2rZtmxMjR4VQ9/2RRx4J3nTT\n",
              "r18/zZo1Sy+//LITI/cpTv9uJdosc+aZZ+qCCy5QRUWFJOnxxx/X8OHDNWLEiC7X/fSnP9Xf/vY3\n",
              "NTc3yxijhx56SNddd50TI0eFUPf9iy++0JQpUzRlyhT95je/cWLUqBLqvr/yyitqamqS3+/X3//+\n",
              "d8XHx8vv92vw4MFOjB0VQt37a6+9Vtu3b1dbW5sk6ZlnnlFmZmavzxstQt33c889V1u2bNGhQ4ck\n",
              "SU899ZRGjx7d6/P2NY7/bu21d8+hx7z33ntm3LhxJjU11Vx44YXmzTffNMYY84tf/MJUV1cHr1u5\n",
              "cqU599xzzbnnnmtuuumm4F1GODmh7PvixYtNTEyMyczMDB6LFy92cmzrhfrf+9c+/PBDbkToIaHu\n",
              "/SOPPGLS0tJMenq6mTJligkEAk6NHBVC2fcDBw6Ym2++2Xz/+9836enp5oorrgjeuY6Tc8stt5iz\n",
              "zjrLuN1uc+aZZ5qUlBRjzKn1u5XPHgUAALAAL48CAABYgGgDAACwANEGAABgAaINAADAAkQbAACA\n",
              "BYg2AAAACxBtAAAAFiDaAAAALEC0AQAAWIBoAwAAsADRBgAAYIH/B1gWmrjA6/gnAAAAAElFTkSu\n",
              "QmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-8329377e-b325-4eee-a81d-b97c6141eeca\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-8329377e-b325-4eee-a81d-b97c6141eeca\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "_df_4.groupby('Country').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-65a2c707-48c8-4601-b590-8df192e147af\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAl0AAAGZCAYAAACzE4QUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAeyUlEQVR4nO3de3BW9Z348U8uNuhPINQVoVSIoNwvEQxeoaBFsEproSqt1Bte\n",
              "AClarTLMdtfFusuOul6q25XdQSy27lgEqd3WVlccy3ZxBSVSKygqiBW1KpLEVi5Jzu8PxtQItIkk\n",
              "3wefvF4zzwzPyXlOPs93HPOec07yFGRZlgUAAK2qMNcDAAC0BaILACAB0QUAkIDoAgBIQHQBACQg\n",
              "ugAAEhBdAAAJiC4AgAREFwBAAqJrP3L77bfnegQAoJWIrv3Iq6++musRAIBWIroAABIQXQAACYgu\n",
              "AIAERBcAQAKiCwAgAdEFAJCA6AIASEB0AQAkILoAABIQXQAACYguAIAERBcAQAKiCwAgAdEFAJCA\n",
              "6AIASEB0AQAkUJBlWZbrIdhl8Jc6xPHf7JTrMQAgr8z7+qu5HiEinOkCAEhCdAEAJCC6AAASEF0A\n",
              "AAmILgCABEQXAEACogsAIAHRBQCQgOgCAEhAdAEAJCC6AAASEF0AAAmILgCABEQXAEACoqsZCgoK\n",
              "YuvWrY22lZWVRWVlZUREjBo1KpYuXRoREfX19TFt2rQYOXJkVFVVpR0UANjvFOd6gHy0c+fOOO+8\n",
              "8+L999+PX/3qV3HggQfmeiQAIMec6WphH3zwQZx55plRVFQUDz74oOACACJCdLW4b33rW1FaWhr3\n",
              "3ntvFBfv/UTi9u3bo7q6utGjvj5LOCkAkJLoagEFBQUN/x47dmwsW7Ysfvvb3/7F18ydOzc6duzY\n",
              "6PH2yztae1QAIEdEVzMceuih8e677zba9s4770Tnzp0bnp911llx++23x6mnntpwg/2ezJ49O6qq\n",
              "qho9Du31mdYaHQDIMdHVDGPHjo158+Y1PF+4cGH07Nkzunbt2mi/s88+O+68884YN25crF69eo/H\n",
              "KikpiQ4dOjR6FBYW7HFfAODTz28vNsNtt90WV155ZQwePDgKCwujS5cusWjRoj3u+7WvfS0KCwtj\n",
              "3Lhx8Ytf/CKGDRuWeFoAYH9SkGWZu7f3E4O/1CGO/2anXI8BAHll3tdfzfUIEeHyIgBAEqILACAB\n",
              "0QUAkIDoAgBIQHQBACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACRQnOsB\n",
              "+LMv9r04bvn6LbkeAwBoBc50AQAkILoAABIQXQAACYguAIAERBcAQAKiCwAgAdEFAJCA6AIASEB0\n",
              "AQAkILoAABIQXQAACYguAIAERBcAQAKiCwAgAdEFAJCA6AIASEB0AQAkILoAABIQXQAACYguAIAE\n",
              "RBcAQAKiCwAgAdEFAJCA6AIASEB0AQAkILoAABIQXQAACYguAIAERBcAQAKiCwAgAdEFAJCA6AIA\n",
              "SEB0AQAkILoAABIQXQAACYguAIAEinM9AH/2xx+uijd/eVOuxwCAvNLl+WtyPUJEONMFAJCE6AIA\n",
              "SEB0AQAkILoAABIQXQAACYguAIAERBcAQAKiCwAgAdEFAJCA6AIASEB0AQAkILoAABIQXQAACYgu\n",
              "AIAERNfHFBQUxNatWxttKysri8rKyobnWZbFEUccEaecckqj/TZu3BhFRUVRXl7e8Dj22GMTTA0A\n",
              "7O+Kcz3Ap9Fjjz0WpaWlsWbNmtiwYUMcccQRDV9r3759o0ADAIhwpusTmT9/flxyySXxjW98I+6+\n",
              "++5cjwMAfAo409VMW7ZsiV/+8pfxb//2b7Fp06Y4/fTTY86cOVFYuKtfa2pqory8vGH/AQMGxI9/\n",
              "/OPdjrN9+/bYvn17o211WX2rzg4A5I7oaqKCgoKIiPjxj38cp512WpSWlkZpaWkcdthh8atf/SpO\n",
              "O+20iGj65cW5c+fGnDlzGm0b1q57i88NAOwfXF78mEMPPTTefffdRtveeeed6Ny5c0TsurS4bNmy\n",
              "KCsri7KystiwYUPMnz+/2d9n9uzZUVVV1ehRfuDhLfIeAID9jzNdHzN27NiYN29e3HjjjRERsXDh\n",
              "wujZs2d07do1nn766Xj77bdj8+bNDZcTt27dGocffni8/fbbzfo+JSUlUVJS0mhbUYEGBoB8Jbo+\n",
              "5rbbbosrr7wyBg8eHIWFhdGlS5dYtGhRROw6yzVp0qSG4IqIKC0tjTFjxsS9994bEyZM2O2eroiI\n",
              "5cuXR/v27VO+DQBgP1OQZVmW6yHY5bJDRsacw8bnegwAyCtdnr8m1yNEhHu6AACSEF0AAAmILgCA\n",
              "BEQXAEACogsAIAHRBQCQgOgCAEhAdAEAJCC6AAASEF0AAAmILgCABEQXAEACogsAIIHiXA/An/2/\n",
              "84+JLrfsH5+EDgC0LGe6AAASEF0AAAmILgCABEQXAEACogsAIAHRBQCQgOgCAEhAdAEAJCC6AAAS\n",
              "EF0AAAmILgCABEQXAEACogsAIAHRBQCQgOgCAEhAdAEAJCC6AAASEF0AAAmILgCABEQXAEACogsA\n",
              "IAHRBQCQgOgCAEhAdAEAJCC6AAASEF0AAAmILgCABEQXAEACogsAIAHRBQCQgOgCAEhAdAEAJCC6\n",
              "AAASEF0AAAmILgCABJodXccdd1zcd999sXPnztaYBwAgLxVkWZY15wWPPPJI/OAHP4iVK1fGRRdd\n",
              "FFOnTo1u3bq11nxtyjFDz4yTv3BxrscAgLxy461n5HqEiPgEZ7pOPfXUWLp0aaxYsSLq6uqioqIi\n",
              "zjrrrPjNb37TGvMBAOSFT3xP13vvvRdvvfVWFBYWRteuXWPGjBkxY8aMlpwNACBvNDu6/vM//zNO\n",
              "PPHEmDx5chx33HGxfv36+P73vx+rVq2Kn//8560xIwDAp15xc19w3333xZw5c+KLX/xio+1FRUXx\n",
              "/e9/v8UGAwDIJ80601VXVxelpaW7BdeHxo8f3yJDAQDkm2ZFV1FRUbz44outNQsAQN5q9uXF0aNH\n",
              "x6WXXhoXXHBBHHzwwQ3bBw8e3KKDAQDkk2ZH1/333x8REY8++mjDtoKCgnjllVdabioAgDzT7Oja\n",
              "sGFDa8wBAJDXmv0nI84888wmbQMA4M+aHV2bNm3abdvLL7/cIsMAAOSrJl9enDdvXtx1113x4osv\n",
              "xtChQxu2V1VVxYABA1plOACAfNHk6Bo3blz06dMnpk2bFrfeemvD9g4dOvjNRQCAv6LJ0dWjR4/o\n",
              "0aNHrF27tjXnAQDIS82+p2vjxo0xbdq0OPXUU+Pkk09ueHzckiVLYtiwYVFeXh59+/aNk08+Oerr\n",
              "6/dp2H/4h3+Ibdu2NTy/4IIL4rbbbmvy62tqauLggw+OKVOmNNr+8ssvx9ChQ+Poo4+OBQsW7Pa6\n",
              "zZs3x4gRIz7x3AAAzf6TEWeffXaccsopMWPGjCgqKtrjPm+88UZceuml8fTTT0ePHj0iIuKZZ56J\n",
              "goKCfRp2zpw5ceWVV0a7du0+0evvv//+GDZsWCxZsiRuv/32hj/u+sADD0RFRUXMmzdvt9fU1tbG\n",
              "5z73uVi+fPk+zQ4AtG3NPtO1bdu2mDt3bnz5y1+O008/veHxUW+99VYUFRXFZz/72YZtQ4cObYiu\n",
              "VatWxQknnBCDBw+O4cOHx29+85uI2HUWrbS0tOE177//fsNrpk6dGhERI0aMiPLy8vjDH/4QERFr\n",
              "166NU045JXr37h0TJkyIHTt27HX2+fPnx6xZs2LkyJENf+R14cKFceutt8aSJUuivLw8nn/++Rg1\n",
              "alTMnDkzjj/++Dj11FN3m2vFihVx0kknxZAhQ2Lw4MHx05/+NCIivvOd70RFRUWUl5fHyJEj44UX\n",
              "XtjrLNu3b4/q6upGj2wfzwQCAPuvZkfXwIED9/hnIz5q8ODBcdJJJ0WPHj3iq1/9atx0003x+uuv\n",
              "R0TEjh07YsKECXHdddfFmjVr4pZbbomJEyfG+++//xePedddd0VExPLly6OysjI6d+4cERGVlZXx\n",
              "s5/9LNauXRtvvfVWLF68eI+vf/755+O1116LsWPHxpQpU2L+/PkREXHeeefF1KlT49xzz43Kysro\n",
              "379/RES8+OKL8etf/zqWLVvW6DhbtmyJM888M+bOnRvPPvtsVFZWNlx6nDVrVqxcuTIqKytj+vTp\n",
              "ccUVV+z1/cydOzc6duzY6PHGWz7XEgDyVbMvL7799tsxZMiQOP744xtd5luyZEnDvwsLC2Px4sWx\n",
              "bt26eOKJJ+Lhhx+Of/zHf4xVq1bFBx98EIWFhTF27NiIiDjppJPisMMOi8rKyvj85z/f7Dfw1a9+\n",
              "NQ466KCIiBg+fPhe/2bY/Pnz47zzzouioqL40pe+FJdddlmsXbs2+vXrt8f9J0+eHAcccMBu21es\n",
              "WBF9+vRpCK3CwsKGM3qPPvpo3HHHHVFTUxP19fWxZcuWvc49e/bsuOqqqxptGz1y8l9/wwDAp1Kz\n",
              "o2vy5MkxeXLT4qBv377Rt2/fuOyyy2LcuHHx0EMPxZgxY3bb78NLiMXFxVFXV9ew/aM3ze/NR8Ov\n",
              "qKgoamtrd9tn586dce+998YBBxwQ9913X0RE/OlPf4r58+fHzTffvMfjfvTDvJti06ZNMWPGjFi5\n",
              "cmX06tUr1qxZEyNHjtzr/iUlJVFSUtJoW0Fhs088AgCfEs2OrvPPP/+v7vP666/Hxo0b48QTT4yI\n",
              "iPfeey82bNgQvXr1ij59+kR9fX08+uijMWbMmPjf//3fePPNN6O8vDzatWsXWZbF888/H/3794+F\n",
              "Cxc2Om779u2jqqqq0f1VTfHQQw9Fz54948knn2zYtnbt2hg1alTMnTu3Wcc64YQTYv369bF8+fIY\n",
              "MWJE1NfXx9atW6OqqioOOOCA6Nq1a2RZFnfeeWezjgsA5LdmR9dFF120x+133313w79ra2vj+uuv\n",
              "jw0bNsRBBx0UtbW1cf7558dXvvKViNh1KXLmzJlx9dVXR7t27eKBBx5oOLN0xx13xBlnnBGHHHJI\n",
              "fO1rX2v0Pa6++uoYM2ZMHHTQQfHII480eeb58+fHueee22hbv379olu3bvGzn/2syceJiOjUqVM8\n",
              "+OCDcfXVV0dNTU0UFhbG9773vRg/fnxMmjQpBgwYEIcccojPowQAGinIsixrzgv+9V//teHf27Zt\n",
              "i8WLF8fQoUOd2WkBxww9M07+wsW5HgMA8sqNt56R6xEi4hOc6br88ssbPZ82bVp8+ctfbrGBAADy\n",
              "0T7fud2uXbv4/e9/3xKzAADkrWaf6fronzmoq6uLVatWxcCBA1t0KACAfNPs6OrYseOfX1xcHDNn\n",
              "zowJEya06FAAAPmm2dF13XXXtcYcAAB5rdn3dNXU1MTll18evXv3jt69e8eMGTOipqamNWYDAMgb\n",
              "zY6u6dOnR21tbfzkJz+JRYsWRX19fUyfPr01ZgMAyBvNvry4Zs2aePbZZxue/+AHP4ghQ4a06FAA\n",
              "APmm2We66urqGl1OrKmpafR5iQAA7O4TffbicccdF+ecc05ERPzkJz+JCy+8sMUHAwDIJ02Orurq\n",
              "6tiyZUtcc801MXDgwHjsscciYtc9XpMnT261AQEA8kGTLy9ee+218fTTT0dExGmnnRY333xz3Hzz\n",
              "zdGlS5eYNWtWqw0IAJAPmhxdTz31VEycOHG37RMmTIhf//rXLToUAEC+afLlxdra2r1+rbBwnz/C\n",
              "kYgYOapn3HjL/vFJ6ABAy2pyLe3cuTOqq6t3215VVRU7d+5s0aEAAPJNk6Nr0qRJ8c1vfjPee++9\n",
              "hm3vvfdeXHjhhTFp0qRWGQ4AIF80Obq++93vRmlpaRx++OFx9NFHx9FHHx2HH354tG/fPv7u7/6u\n",
              "NWcEAPjUa/I9XUVFRfHDH/4w/v7v/z6eeeaZiIgYOnRo9OrVq9WGAwDIF83+46i9evUSWgAAzeTX\n",
              "DgEAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACQgugAAEhBdAAAJiC4AgAREFwBA\n",
              "AqILACAB0QUAkIDoAgBIQHQBACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQB\n",
              "ACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIoCDLsizXQ7DLhQMKYnaFDgaAltT7\n",
              "nrpcjxARznQBACQhugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACQgugAAEhBd\n",
              "AAAJiC4AgAREFwBAAqILACCBNhVdtbW1MWfOnOjbt28MHDgwysvL49JLL42lS5dGeXn5Hl+zefPm\n",
              "GDFiRNpBAYC8U5zrAVKaMmVKbNmyJVasWBGdOnWKLMvigQceiC1btuz1NZ/73Odi+fLlCacEAPJR\n",
              "mznT9dJLL8WiRYtiwYIF0alTp4iIKCgoiLPOOit69uwZtbW1MX369BgyZEgMGDAgVq1aFRERGzdu\n",
              "jNLS0objFBQUxD/90z/F8OHD44gjjogFCxY0fO073/lOVFRURHl5eYwcOTJeeOGFvc6zffv2qK6u\n",
              "bvSoq2+d9w4A5F6bia5nnnkmjjrqqPibv/mbPX593bp1cf7558ezzz4b3/rWt+Jv//Zv93qskpKS\n",
              "eOqpp+Lhhx+OmTNnRm1tbUREzJo1K1auXBmVlZUxffr0uOKKK/Z6jLlz50bHjh0bPda8s2/vEQDY\n",
              "f7WZ6PprjjzyyDj22GMjIuL444+Pl19+ea/7nnvuuRER0bdv3yguLo4333wzIiIeffTROP7442Pg\n",
              "wIFx/fXXR2Vl5V6PMXv27Kiqqmr0GLznHgQA8kCbuadr6NChsX79+nj33XfjkEMO2e3r7dq1a/h3\n",
              "UVFRw9mrPdnTvps2bYoZM2bEypUro1evXrFmzZoYOXLkXo9RUlISJSUljbYVSWAAyFtt5sf8kUce\n",
              "GRMnTowpU6bE1q1bIyIiy7JYvHhxvPLKK/t8/KqqqjjggAOia9eukWVZ3Hnnnft8TAAgf7SZ6IqI\n",
              "uPvuu2PIkCFx7LHHxoABA6J///7xyCOPxGc/+9l9PvagQYNi0qRJMWDAgKioqIju3bu3wMQAQL4o\n",
              "yLIsy/UQ7HLhgIKYXdGmOhgAWl3ve+pyPUJEtLEzXQAAuSK6AAASEF0AAAmILgCABEQXAEACogsA\n",
              "IAHRBQCQgOgCAEhAdAEAJCC6AAASEF0AAAmILgCABEQXAEACxbkegD/rNPbb0fuWW3I9BgDQCpzp\n",
              "AgBIQHQBACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACQgugAAEhBdAAAJ\n",
              "iC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUA\n",
              "kIDoAgBIQHQBACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACQgugAAEhBd\n",
              "AAAJFGRZluV6CHY5eGxFlE46JddjAEBe+f2F/5zrESLCmS4AgCREFwBAAqILACAB0QUAkIDoAgBI\n",
              "QHQBACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACTQpqKrrKws+vTpE+Xl\n",
              "5dGvX7/4xje+EX/84x9b7Pjl5eVRU1PT8L0qKytb7NgAwKdbm4quiIj7778/Kisr43e/+11UVVXF\n",
              "Pffcs9s+dXV1n+jYlZWV0b59+32cEADIR20uuj60Y8eO+NOf/hSdOnWKe+65J0aPHh0TJ06MQYMG\n",
              "xVNPPRW33HJLVFRURHl5eVRUVMSKFSsiYldYlZeXNzw6dOgQc+bMiYiIgoKC2Lp1a5O+//bt26O6\n",
              "urrRI+qy1nq7AECOFed6gNTOOeecOPDAA2Pjxo0xbNiwOPvss+NHP/pR/N///V+sXr06+vTpExER\n",
              "Rx55ZFx11VUREfHkk0/GBRdcEOvWrYvy8vKGy4aPP/54XHLJJXHxxRc3e465c+c2xNqHDuj5uX17\n",
              "cwDAfqvNnen68PLiO++8E2VlZTFr1qyIiDjhhBMagisiYvXq1fGFL3whBg4cGFOnTo0XXnghPvjg\n",
              "g4avP/fcc3HhhRfG0qVLo1u3bs2eY/bs2VFVVdXo8Zkjuu77GwQA9ktt7kzXh4qLi2PixIlxzTXX\n",
              "xKBBg+Lggw9u+NqOHTtiwoQJ8fjjj0dFRUVUV1dHx44dY/v27XHggQfG5s2b48wzz4wFCxbEwIED\n",
              "P9H3LykpiZKSksYbiwr25S0BAPuxNnem66OWLVvW6OzWh7Zt2xY7duyI7t27R0TEHXfc0fC1mpqa\n",
              "OP3002POnDkxevToZLMCAJ9ubS66zjnnnCgvL4+BAwfG2rVr4/bbb99tnw4dOsQNN9wQw4cPj2HD\n",
              "hsVnPvOZhq8tWbIk1q1bFzfddFPDzfR33XVXyrcAAHwKFWRZ5lfm9hMHj62I0kmn5HoMAMgrv7/w\n",
              "n3M9QkS0wTNdAAC5ILoAABIQXQAACYguAIAERBcAQAKiCwAgAdEFAJCA6AIASEB0AQAkILoAABIQ\n",
              "XQAACYguAIAERBcAQALFuR6AP7t0wIi4ZT/5JHQAoGU50wUAkIDoAgBIQHQBACQgugAAEhBdAAAJ\n",
              "iC4AgAREFwBAAqILACAB0QUAkIDoAgBIQHQBACQgugAAEhBdAAAJiC4AgAREFwBAAqILACAB0QUA\n",
              "kEBxrgdgl+3bt8fDDz8cdXV1UVRUlOtx2pS6urp46qmnYvjw4dY+IeueO9Y+N6x7bqRY9x49esQV\n",
              "V1zxV/cryLIsa5UJaJbq6uro2LFjVFVVRYcOHXI9Tpti7XPDuueOtc8N654b+9O6u7wIAJCA6AIA\n",
              "SEB0AQAkILr2EyUlJXHddddFSUlJrkdpc6x9blj33LH2uWHdc2N/Wnc30gMAJOBMFwBAAqILACAB\n",
              "0QUAkIDoSmz9+vVxwgknRO/evaOioiJ+97vf7XG/+fPnx1FHHRW9evWKSy65JHbu3Jl40vzSlHVf\n",
              "tmxZDB8+PPr37x8DBgyIa6+9Nurr63MwbX5p6n/zERFZlsXJJ58cpaWl6QbMU01d99/+9rcxatSo\n",
              "6NevX/Tr1y+WLFmSeNL805S1r6+vj6uuuir69+8fgwcPjtGjR8dLL72Ug2nzx8yZM6OsrCwKCgqi\n",
              "srJyr/vl9OdrRlKjR4/OFixYkGVZli1atCg75phjdtvnlVdeybp27Zq98cYbWX19fTZ+/Pjszjvv\n",
              "TDxpfmnKuj/zzDPZyy+/nGVZln3wwQfZiSee2PAaPrmmrP2H/uVf/iW7+OKLs44dO6YZLo81Zd3/\n",
              "+Mc/ZkcccUS2fPnyLMuyrLa2NvvDH/6Qcsy81JS1f/DBB7Phw4dnO3bsyLIsy773ve9lZ511Vsox\n",
              "884TTzyRvfbaa1mPHj2y1atX73GfXP98FV0JvfXWW1n79u2znTt3ZlmWZfX19dlhhx2WrV+/vtF+\n",
              "N954Y3bZZZc1PP/5z3+enXjiiUlnzSdNXfePu/zyy7PrrrsuwYT5qzlr/9xzz2UjRozIXnrpJdG1\n",
              "j5q67v/xH/+Rff3rX8/FiHmrqWu/dOnSbMiQIVl1dXVWX1+fXXPNNdm3v/3tXIycd/5SdOX656vL\n",
              "iwm99tpr0bVr1ygu3vU54wUFBdG9e/fYtGlTo/02bdoUPXr0aHheVla22z40XVPX/aPefPPNeOCB\n",
              "B+KMM85INWZeaura79y5My655JKYN2+eDwJuAU1d9+effz5KSkrijDPOiPLy8jjvvPPi7bffzsXI\n",
              "eaOpaz9+/PgYNWpUdOnSJbp27RqPPfZYXH/99bkYuU3J9c9X0QUfU11dHePHj49rr702jjnmmFyP\n",
              "0ybMmTMnJkyYEP369cv1KG1KbW1t/Pd//3fMmzcvVq9eHd26dYtp06bleqw2YdWqVfHcc8/F66+/\n",
              "Hps3b45TTjklpk6dmuuxaGWiK6HDDz883njjjaitrY2IXTcNb9q0Kbp3795ov+7du8err77a8Hzj\n",
              "xo277UPTNXXdIyJqampi3Lhx8ZWvfCWuuuqq1KPmnaau/RNPPBF33HFHlJWVxUknnRTV1dVRVlbm\n",
              "rMsn1Jz/14wePTq6desWBQUFMXny5HjyySdzMXLeaOraL1y4sOGXRgoLC+P888+Pxx9/PBcjtym5\n",
              "/vkquhLq3LlzDB06NH70ox9FRMTixYvj85//fBx55JGN9ps4cWI89NBD8eabb0aWZXHXXXfFpEmT\n",
              "cjFyXmjqur///vsxbty4GDduXHz3u9/Nxah5p6lrv3z58nj11Vdj48aN8T//8z/RoUOH2LhxYxx6\n",
              "6KG5GPtTr6nrfvbZZ8fKlSujuro6IiJ+8YtfxJAhQ5LPm0+auvY9e/aMZcuWxY4dOyIi4r/+679i\n",
              "4MCByedta3L+8zXZ3WNkWZZl69aty4477rjsqKOOyoYNG5atWbMmy7IsmzJlSvbTn/60Yb9///d/\n",
              "z3r27Jn17Nkzu+iiixp+w4VPpinrfsMNN2TFxcXZkCFDGh433HBDLsfOC039b/5DGzZscCN9C2jq\n",
              "ui9cuDAbMGBANmjQoGzcuHHZpk2bcjVy3mjK2m/bti27+OKLs759+2aDBg3KxowZ0/Db03wyl156\n",
              "adatW7esqKgo69y5c9arV68sy/avn68+exEAIAGXFwEAEhBdAAAJiC4AgAREFwBAAqILACAB0QUA\n",
              "kIDoAgBIQHQBACQgugAAEhBdAAAJiC4AgAT+P9tGm4AaVnPaAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-65a2c707-48c8-4601-b590-8df192e147af\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-65a2c707-48c8-4601-b590-8df192e147af\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "_df_5.groupby('Transaction Type').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-2ebb58e6-451c-456c-b351-292b19eee948\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAnkAAAGZCAYAAAD1pSe+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAuDElEQVR4nO3deXQUZd7+/6vpQBiWALIPo8geszYhIQlCIKwRZRFEEBlAouDD\n",
              "IMwjqIOIAo9HlFUHRERZFERUyICKjmyJLOICJIisYRPQsAyEAEqCIfX7gx/1JRKgO6Snye37dU6f\n",
              "k6q6q+pzpyhzedfSDsuyLAEAAMAoJXxdAAAAAIoeIQ8AAMBAhDwAAAADEfIAAAAMRMgDAAAwECEP\n",
              "AADAQIQ8AAAAAxHyAAAADETIAwAAMBAh7w/stdde83UJAADASwh5f2A//vijr0sAAABeQsgDAAAw\n",
              "ECEPAADAQIQ8AAAAAxHyAAAADETIAwAAMBAhDwAAwECEPAAAAAMR8gAAAAxEyAMAADAQIQ8AAMBA\n",
              "hDwAAAADEfIAAAAMRMgDAAAwECEPAADAQIQ8AAAAAxHyAAAADETIAwAAMBAhDwAAwECEPAAAAAMR\n",
              "8gAAAAxEyAMAADCQn68LgO/88s4mHf33RF+XAQCAcWrseMrXJTCSBwAAYCJCHgAAgIEIeQAAAAYi\n",
              "5AEAABiIkAcAAGAgQh4AAICBCHkAAAAGIuQBAAAYiJAHAABgIEIeAACAgQh5AAAABiLkAQAAGIiQ\n",
              "BwAAYCBCHgAAgIEIeQAAAAYi5AEAABiIkAcAAGAgQh4AAICBbomQl5SUpCZNmsjlcikwMFCtW7dW\n",
              "Xl6eJOnVV1/V0aNHb2r7/fv316uvvlqodTt27CiXyyWXyyWHw6HQ0FC5XC61aNHipmpyR2JiooKC\n",
              "gnT//fd7fV8AAMAsfr4uICMjQwMHDtTmzZtVu3ZtSdKWLVvkcDgkXQp5rVq1Uo0aNXxS32effWb/\n",
              "7HA4tG7dOlWsWDFfm9zcXPn5Fe2v8tixY1q0aJHOnDkjp9Pp9nqXw3GJErdEfgcAAD7i8yRw7Ngx\n",
              "OZ1O3Xbbbfa8iIgIORwOjRs3Tj///LN69uwpl8ultLQ0nTt3TgMGDFBISIhCQkI0duxYe72ffvpJ\n",
              "DzzwgEJDQxUWFqbRo0dftb9169YpKChImzZt0okTJ9S+fXu7/SOPPOJ23a1atdLQoUMVGxur9u3b\n",
              "Kzc3Vx06dFBkZKSCg4PVu3dv/fLLL5KklJQUhYSEaPDgwQoPD1dwcLA2bdokSQXWcPr0acXHxys7\n",
              "O1tNmjTRyy+/LEmaNGmSmjZtqoiICCUkJOjHH3+UJI0ZM0bdu3dXhw4dFBISooyMDM8PBAAAMIrP\n",
              "R/LCwsLUvHlz1a5dWy1btlSzZs3Uu3dv1apVS88//7zmzJmjDz74QC6XS5L0zDPPKCcnR99//73O\n",
              "nz+v5s2bKzAwUD179lSfPn3Uvn17LV68WNKlAHWlDz74QOPHj9fy5ctVp04dTZ06VXXq1NGKFSsk\n",
              "SadOnfKo9j179mjt2rUqWbKkLMvSwoULVblyZVmWpcGDB2vatGn6xz/+IUnatWuXZs+erRkzZmjm\n",
              "zJkaNWqUvvjiCy1YsOCqGipWrKjPPvvMDraStHDhQu3evVsbN26U0+nU/PnzNXjwYC1fvlyStHHj\n",
              "RqWmpqp69eoF1pqTk6OcnJx88y5aeR71FwAAFB8+H8krUaKElixZoq+++koJCQnasGGDgoODtXfv\n",
              "3gLbr1q1So899phKlCihsmXLqm/fvlq5cqXOnTun9evXa/jw4XbbqlWr2j/Pnz9fkydPVnJysurU\n",
              "qSNJiomJ0eeff67hw4dr2bJlKlu2rEe19+nTRyVLlpQkWZalqVOnqnHjxgoLC9Py5cvtgCZJ9evX\n",
              "V3R0tCQpNjZW+/bt86iGpUuXatWqVfa9ixMmTNChQ4fs5R07drxmwJOk8ePHq0KFCvk+aecPe9Rf\n",
              "AABQfPg85F0WGBioQYMGaenSpYqJidHHH3/s1nqX7927kbCwMJ06dUrbtm2z58XGxiotLU3R0dFK\n",
              "SkpSVFSULl686HbN5cqVs39euHCh1qxZoy+//FLbtm3TiBEjlJ2dbS8vXbq0/bPT6VRubq5HNViW\n",
              "pZEjRyotLU1paWnatm1bvr5cWUtBRo4cqaysrHwf159ud7uvAACgePF5yPvpp5+0YcMGezozM1MH\n",
              "DhxQvXr1JEkBAQHKysqyl7dt21azZ8+WZVn65ZdfNH/+fLVv317lypVTXFycJk+ebLe98nJteHi4\n",
              "PvnkEw0YMED//ve/JUkHDhxQuXLl9OCDD2ratGnas2ePzp07V6h+ZGZmqkqVKgoICNDZs2c1b948\n",
              "t9Zzt4auXbtq5syZ9iXl3377TampqW7X5+/vr4CAgHwfp8Pnhx8AAHiJz+/Jy83N1bhx43TgwAGV\n",
              "KVNGubm56tevn7p06SJJGjp0qB577DGVKVNG8+bN0+jRozV06FCFhoZKknr06KEHH3xQ0qVLsk88\n",
              "8YSCg4NVsmRJdenSJd+DGXfddZe++OILdezYUS+99JLOnDmjKVOm2CNrEydOVIUKFQrVj759+2rZ\n",
              "smVq1KiRqlatqhYtWtgPRlxPSkpKgTVkZmbma/fwww/r5MmTio+Pt39vAwYMUOPGjQtVLwAAMJvD\n",
              "sizL10XANwZVjtPY6p18XQYAAMapseMpX5fg+8u1AAAAKHqEPAAAAAMR8gAAAAxEyAMAADAQIQ8A\n",
              "AMBAhDwAAAADEfIAAAAMRMgDAAAwECEPAADAQIQ8AAAAAxHyAAAADETIAwAAMBAhDwAAwECEPAAA\n",
              "AAMR8gAAAAxEyAMAADAQIQ8AAMBAhDwAAAAD+fm6APhO2X6RqjHlKV+XAQAAvICRPAAAAAMR8gAA\n",
              "AAxEyAMAADAQIQ8AAMBAhDwAAAADEfIAAAAMRMgDAAAwECEPAADAQIQ8AAAAAxHyAAAADETIAwAA\n",
              "MBAhDwAAwECEPAAAAAMR8gAAAAxEyAMAADAQIQ8AAMBAhDwAAAADEfIAAAAMRMgDAAAwECEPAADA\n",
              "QIQ8AAAAAxHyAAAADETIAwAAMBAhDwAAwECEPAAAAAMR8gAAAAxEyAMAADAQIQ8AAMBAhDwAAAAD\n",
              "EfIAAAAMRMgDAAAwECEPAADAQIQ8AAAAAxHyAAAADETIAwAAMBAhDwAAwECEPAAAAAMR8gAAAAxE\n",
              "yAMAADAQIQ8AAMBAhDwAAAADEfIAAAAMRMgDAAAwECEPAADAQIQ8AAAAAzksy7J8XQR8IzKiq1q3\n",
              "fNTXZQBww4Sp9/m6BADFDCN5AAAABiLkAQAAGIiQBwAAYCBCHgAAgIEIeQAAAAYi5AEAABiIkAcA\n",
              "AGAgQh4AAICBCHkAAAAGIuQBAAAYiJAHAABgIEIeAACAgQh5AAAABiLkAQAAGIiQBwAAYCBCHgAA\n",
              "gIEIeQAAAAYi5AEAABiIkAcAAGAgr4W8Cxcu6JlnnlH9+vV11113KTQ0VO+8806+Ni+88IICAwMV\n",
              "HR1d4HRBxowZo7///e/eKvsqHTt2lMvlksvlksPhUGhoqFwul1q0aOH1fScmJiooKEj333+/1/cF\n",
              "AADM4uetDffv3185OTnaunWrypYtq4MHD+qee+5Rbm6uEhMTJUkTJkzQ/v37VbNmzQKnvSU3N1d+\n",
              "fu51/bPPPrN/djgcWrdunSpWrFjo7bnr2LFjWrRokc6cOSOn0+n2enl5eZKkEiUYpAUA4I/MK0kg\n",
              "PT1dS5cu1axZs1S2bFlJ0p133qnJkydr7NixkqRmzZopOztb7du319ChQ6+aTk9P1913363w8HCF\n",
              "hobqueees7efkZGhTp06KSgoSK1bt9apU6ckSRcvXtRTTz2lkJAQhYSE6IknntCFCxckXQqdAwYM\n",
              "UFxcnEJCQiRJ8+fPV3R0tCIiIhQXF6etW7e63cdWrVpp6NChio2NVfv27ZWbm6sOHTooMjJSwcHB\n",
              "6t27t3755RdJUkpKikJCQjR48GCFh4crODhYmzZtkiSdOHFC7du3V2hoqMLCwvTII4/o9OnTio+P\n",
              "V3Z2tpo0aaKXX35ZkjRp0iQ1bdpUERERSkhI0I8//ijp0uhm9+7d1aFDB4WEhCgjI+OqenNycnTm\n",
              "zJl8H+v/D4QAAMA8XhnJS01NVYMGDVS5cuV882NjY3X48GGdOHFCX3311VUjY1dODxs2TPfdd59G\n",
              "jhwpSXaQk6RvvvlGmzdvVuXKldWrVy+9+eabGjlypGbNmqXvvvtOmzdvltPpVOfOnTV16lQ988wz\n",
              "kqTNmzdr/fr1Kl++vDZs2KD3339fa9eulb+/v9atW6fevXtr+/btbvdzz549Wrt2rUqWLCnLsrRw\n",
              "4UJVrlxZlmVp8ODBmjZtmv7xj39Iknbt2qXZs2drxowZmjlzpkaNGqUvvvhCCxYsUJ06dbRixQq7\n",
              "nxUrVtRnn30ml8ultLQ0SdLChQu1e/dubdy4UU6nU/Pnz9fgwYO1fPlySdLGjRuVmpqq6tWrF1jr\n",
              "+PHj7YB9Wc0ajdzuKwAAKF48DnkZGRnavXu3WrVqpdzcXOXl5alUqVJFXlhcXJyeeuopnTt3Ti1b\n",
              "tlTbtm3tZQkJCXaAjI2N1bZt2yRJq1atUv/+/eXv7y9Jeuyxx/T666/bIa9Hjx4qX768JGnZsmXa\n",
              "unVrvvv/Tp06pfPnz+tPf/qTWzX26dNHJUuWlCRZlqWpU6dq+fLlys3NVVZWlpo1a2a3rV+/vr2v\n",
              "2NhYTZo0SZIUExOjqVOnavjw4YqLi1NCQkKB+1q6dKm+++47NWnSRNKlUcsrdezY8ZoBT5JGjhyp\n",
              "J598Mt+8+Lg+bvUTAAAUPx5drl28eLFiYmLUv39/SdL27dvVtWvXq9o1btxY6enpOnnyZL75Gzdu\n",
              "1O23366qVavecF/du3fXhg0b1KhRI02fPl333Xefvax06dL2z06nU7m5uQVuw+Fw5JsuV66c/bNl\n",
              "WerXr5/S0tLsT0ZGhtsB7/fbW7hwodasWaMvv/xS27Zt04gRI5SdnX3DmmNjY5WWlqbo6GglJSUp\n",
              "KirqqgB3ud6RI0fatW7bts0Ot7+vpSD+/v4KCAjI93Fw3x4AAMby6K/8+PHjtWXLFlWqVEmSFB4e\n",
              "bt8XdqUGDRqoU6dOGjhwoH799VdJ0sGDBzV8+HCNHj3arX2lp6erevXq6tu3ryZMmKCvv/76huu0\n",
              "bdtW7777ri5cuKDc3Fy9/fbbat++fYFtO3furAULFujQoUOSLj2wcPk+ucLIzMxUlSpVFBAQoLNn\n",
              "z2revHlurXfgwAGVK1dODz74oKZNm6Y9e/bo3LlzV7Xr2rWrZs6caV+2/u2335SamlroegEAgNk8\n",
              "ulzrdDqvus/uWpdq3333XT333HMKDQ1VqVKl5HQ69dRTT2nAgAFu7Wvx4sVasGCBSpUqpby8PM2c\n",
              "OfOG6wwcOFD79u1TRESEpEsPR1zrdSstWrTQhAkTdP/99ys3N1cXLlzQvffeq8jISLfq+72+fftq\n",
              "2bJlatSokapWraoWLVoUGIB/LyUlRVOmTLFH9yZOnKgKFSooMzMzX7uHH35YJ0+eVHx8vKRLT/QO\n",
              "GDBAjRs3LlS9AADAbA7Lsix3G7dp00YLFy7UPffcoy1btmj16tV66aWXtHr1am/WCC+JjOiq1i0f\n",
              "9XUZANwwYep9N24EAFfwaCTvlVde0T333KP9+/erefPmOnDggP10JwAAAG4dHoW8yMhIJScn66uv\n",
              "vpJlWWrWrNlVLwYGAACA73n8eGVWVpZOnjypzMxMnT171hs1AQAA4CZ5FPIWLlyoxo0bKykpSYsX\n",
              "L1ZERIQWLVrkrdoAAABQSB5drh03bpw2bdqkOnXqSLr0WpSEhAT16tXLK8UBAACgcDwayStTpowd\n",
              "8KRL30dbpkyZIi8KAAAAN8ejkHfvvfdqzJgxOnLkiA4fPqxx48apU6dO9hfeAwAA4Nbg0XvySlzn\n",
              "a7AcDkeBX8eFWxfvyQOKD96TB8BTHt2Tl5eX5606AAAAUIQ8ulz75ptv6vz5896qBQAAAEXEo5C3\n",
              "du1a1alTR//7v/+rvXv3eqsmAAAA3CSPQt57772nrVu3qnLlymrTpo3uueceffbZZ96qDQAAAIXk\n",
              "8TdeVK9eXc8995zeeecdbd++XX369FFgYKBWr17tjfoAAABQCG6FvPT0dElSdna23n77bTVu3Fij\n",
              "Ro3SxIkTdeLECS1YsECJiYleLRQAAADuc+vp2p49e2rLli2688471a5dO82aNUtRUVH28sjISLVr\n",
              "185rRQIAAMAzboW8y6/SS01NVc2aNQts89ZbbxVdVQAAALgpboW8rKwsffLJJ7rWe5M7d+5cpEUB\n",
              "AADg5rj1jRfly5dXVFRUgSHP4XBozZo1XikO3vXkk09qypQpvi4DAAB4gVsjefXr1yfIAQAAFCMe\n",
              "v0IFAAAAtz63Qh733AEAABQvboW8sWPHersOAAAAFCEu1wIAABiIkAcAAGAgQh4AAICB3HqFymVb\n",
              "tmzRs88+q/379ys3N9eev3///iIvDAAAAIXnUcjr16+fhgwZotjYWDmdTm/VBAAAgJvkUchzOp0a\n",
              "NGiQt2oBAABAEfHonry7775bmzZt8lYtAAAAKCIejeStXbtWb731lurXr6/SpUvb87ds2VLkhQEA\n",
              "AKDwPAp506dP91YdAAAAKEIehbyWLVtKkn7++WdJ0p///OeirwgAAAA3zaN78nbu3Kng4GD7Exoa\n",
              "ql27dnmrNgAAABSSRyFv8ODBGjVqlDIzM5WZmalRo0bpf/7nf7xVGwAAAArJo5CXmZmp3r1729O9\n",
              "evVSZmZmkRcFAACAm+NRyHM6ndqxY4c9vWPHDl6KDAAAcAvy6MGLl156SXFxcQoLC5Mkbdu2Te+9\n",
              "955XCgMAAEDheRTyOnTooB07dujbb7+VJMXExKhKlSpeKQwAAACF51HIk6Rq1arpvvvu80YtAAAA\n",
              "KCJuhbyWLVvqyy+/VKVKleRwOOz5lmXJ4XDo1KlTXisQAAAAnnMr5C1atEiSlJaW5s1aAAAAUETc\n",
              "erq2Zs2akqT58+erdu3a+T7z58/3aoEAAADwnEevUElKSnJrHgAAAHzLrcu1X3zxhf7973/rp59+\n",
              "0pNPPmnPz8rK8lphAAAAKDy3Ql7p0qVVsWJFlShRQhUqVLDn33777Ro9erTXigMAAEDhuP10bcuW\n",
              "LdW1a1eFh4d7uyYAAADcJI/uyZs2bZpOnjxpT//nP//RoEGDirwoAAAA3ByPQt7mzZtVuXJle7pK\n",
              "lSr67rvvirwoAAAA3ByPQl5ubm6+acuydOHChSItCAAAADfPo5AXExOjIUOG6Mcff9TBgwf1xBNP\n",
              "KCYmxlu1AQAAoJA8CnmTJ0/Wr7/+qqioKEVHRysnJ0dTp071Vm0AAAAoJLeerr0sICBAc+bM8VYt\n",
              "AAAAKCIehTxJ+vnnn/XDDz8oOzvbnte5c+ciLQoAAAA3x6OQN2fOHI0bN06nTp1SgwYNtHXrVsXE\n",
              "xBDyAAAAbjEe3ZM3depUpaamql69etq8ebPWrFmjhg0beqs2AAAAFJJHIa9UqVKqVKmS/SqVuLg4\n",
              "paWleaMuAAAA3ASPLtf6+/vLsiw1bNhQr776qmrXrq1z5855qzYAAAAUkkch78UXX9SZM2c0YcIE\n",
              "Pf7448rMzNSMGTO8VRsAAAAKyaOQ17p1a0lShQoVtHLlSq8UBAAAgJvn0T15zz//vE6fPi3LsnTv\n",
              "vfeqSpUqWrJkibdqAwAAQCF5FPKWLVumihUratWqVfLz89OGDRv04osveqs2AAAAFJJHIa9EiUvN\n",
              "v/zyS/Xo0UONGjWSw+HwSmEAAAAoPI/uyStbtqxeeeUVLVq0SBs2bJBlWbpw4YK3agMAAEAheTSS\n",
              "N2/ePGVkZGjChAmqXr269u3bpz59+nirNgAAABSSw7Isy9dFwDceCXZoZJRHOR+AjzScd9HXJQAo\n",
              "Zjy6XHvw4EG98sor2rdvn/2tF5K0Zs2aIi8MAAAAhedRyHvwwQfVpk0bDRkyRE6n01s1AQAA4CZ5\n",
              "FPKys7M1fvx4b9UCAACAIuLRDVkhISE6dOiQt2oBAABAEfFoJO/EiRMKDw9XbGysSpcubc9PSkoq\n",
              "8sIAAABQeB6FvD59+vDKFAAAgGLAo5DXr18/b9UBAACAIuRRyJOkDz/8UGlpacrOzrbnTZkypUiL\n",
              "AgAAwM3x6MGLoUOHav78+Zo3b54cDocWL16srKwsb9UGAACAQvIo5CUnJ2vZsmWqWrWqJk+erG+/\n",
              "/VZHjhzxVm0AAAAoJI9CXunSpVWiRAk5HA799ttvqlGjhn7++Wdv1QYAAIBC8uievPLly+vXX39V\n",
              "8+bN1adPH9WoUUNlypTxVm0AAAAoJI9G8t5//335+flp4sSJCgsLU8mSJbV48WJv1QYAAIBCcnsk\n",
              "7+LFixoxYoTmz58vSRo1apTXigIAAMDNcXskz+l0as+ePd6sBQAAAEXEo3vy4uPjNXDgQPXv31/l\n",
              "ypWz54eFhRV5YQAAACg8t0LeQw89pPfff18ffPCBJGnlypX2MofDof3793unOgAAABSKWyFv165d\n",
              "kqQDBw54tRgAAAAUDbfuyXM4HF4tIjc3V2PHjlVgYKBCQkLkcrk0cOBAnT59usj2MWbMmHxfxQYA\n",
              "AGAyh2VZ1o0a+fn5KSAg4Kr5lmXJ4XDo1KlTN1VEv379dOrUKb377ruqVKmSLMvS4sWL1aRJE9Wt\n",
              "W9dul5ubKz8/j79uV9KloJqZmamKFSveVK3uKKjOm6ndWx4JdmhklEdv0QHgIw3nXfR1CQCKGbf+\n",
              "wjdq1EipqalXfdLS0pSamnpTBezdu1cfffSR5s6dq0qVKkm6FMh69OihQ4cOKTg4WImJiXK5XHrv\n",
              "vfdUvXp1/frrr/b6vXv31htvvGGv99xzz6lx48Zq2LCh3nvvPUnS448/Lklq0aKFXC6Xjh8/ruPH\n",
              "j6tbt24KDQ1VSEiI3nzzTXubO3fuVIcOHRQWFqawsDDNnDlTktSqVSstXbrUbvfAAw9o3rx5kqT+\n",
              "/ftrwIABiouLU0hIiFJSUvLV/q9//Uvp6em69957FRUVpbCwME2fPt3elsPh0EsvvaSmTZuqTp06\n",
              "mjt37nXr2bRpkwIDA3VlRm/WrJk+//zzmzoeAADADG4NLfn7+6t27dpeKWDLli1q0KCBqlSpUuDy\n",
              "nTt3asaMGZo9e7YkacWKFVqwYIEGDhyoY8eOadWqVZo1a5bd3uFwKDU1Vfv371dkZKTuvvtuzZw5\n",
              "U2+++abWrVtnj+T17NlTjRo1UlJSko4fP64mTZooPDxckZGR6tKli8aOHauHHnpIkvSf//zHrb5s\n",
              "3rxZ69evV/ny5ZWSkpKv9osXLyo6OloLFixQYGCgfv31V8XExCg6OlpRUVGSLv2ev/32W+3atUtR\n",
              "UVH661//KkkF1lOlShVVrlxZK1euVPv27ZWamqoTJ04oISGhwNpycnKUk5OTb97FPLe6BQAAiiG3\n",
              "RvLcuKLrNXXr1lXLli3t6WHDhun111+XJL311lt66KGH8r3O5dFHH7XXi4uL09q1awvc7qpVqzRo\n",
              "0CBJUrVq1dStWzetWrVKu3fvVnZ2th2oJF0zgP5ejx49VL58+QJr3717t7Zv365evXrJ5XKpWbNm\n",
              "Onv2rHbs2GG3f/jhhyVJgYGB8vPz09GjR69bz7Bhw+zRwNdff12DBw++5v2T48ePV4UKFfJ9vncv\n",
              "uwIAgGLIrZG8m70kez0RERFKT0/XyZMnVbly5auWXxngJKlp06YqU6aMkpOTNWvWLK1ateq623f3\n",
              "oRF32vn5+enixf93X8zvH+T4fa1XTluWpdtuu01paWnX3H7p0qXtn51Op3Jzc69bT7du3fT0008r\n",
              "NTVVH3/8sSZNmnTNtiNHjtSTTz6Zb96Q6ArX3T4AACi+fH7Xff369dW9e3clJibaT9NalqUlS5Zc\n",
              "8/17w4YNU9++fXXXXXepYcOG+ZZdvpft4MGDWrdunVq0aCFJKl++vLKysux2bdu21VtvvSVJOnHi\n",
              "hJKSktSuXTs1atRIZcqU0fvvv2+3vXy5tn79+vrmm28kXXqdzPr1693uZ6NGjRQQEJDvXru9e/fe\n",
              "8KGV69Xj5+enxx9/XJ07d9b9999/3YdK/P39FRAQkO/j9PnRBwAA3nJL/JmfM2eOwsPDFR0dreDg\n",
              "YAUFBWnFihW67bbbCmz/wAMP6Ny5cxoyZMhVyy5evKjGjRurffv2+uc//6k777xTkjR8+HC1a9fO\n",
              "fvDin//8p3bu3KnQ0FDFx8dr1KhRio6Olp+fn5YtW6a5c+cqNDRU4eHhWrJkiSTp6aefVnJyskJD\n",
              "QzVy5EhFR0e73Uc/Pz99+umnSkpKUlhYmP1Qxvnz52+43rXqkaTExET99NNPBf4uAADAH5dbr1C5\n",
              "1WzatEm9e/fWrl27VKLE/8up/83XpNwqFi9erDfeeEOrV6/2eF1eoQIUH7xCBYCnbq0Xt7nh0Ucf\n",
              "1YoVK/T222/nC3h/RAkJCdqzZ4/+9a9/+boUAABwiymWI3koGozkAcUHI3kAPMVfeAAAAAMR8gAA\n",
              "AAxEyAMAADAQIQ8AAMBAhDwAAAADEfIAAAAMRMgDAAAwECEPAADAQIQ8AAAAAxHyAAAADETIAwAA\n",
              "MBAhDwAAwECEPAAAAAMR8gAAAAxEyAMAADAQIQ8AAMBAfr4uAL5TqcP/quGUKb4uAwAAeAEjeQAA\n",
              "AAYi5AEAABiIkAcAAGAgQh4AAICBCHkAAAAGIuQBAAAYiJAHAABgIEIeAACAgQh5AAAABiLkAQAA\n",
              "GIiQBwAAYCBCHgAAgIEIeQAAAAYi5AEAABiIkAcAAGAgQh4AAICBCHkAAAAGIuQBAAAYiJAHAABg\n",
              "IEIeAACAgQh5AAAABiLkAQAAGIiQBwAAYCBCHgAAgIEIeQAAAAYi5AEAABiIkAcAAGAgQh4AAICB\n",
              "CHkAAAAGIuQBAAAYiJAHAABgIEIeAACAgQh5AAAABiLkAQAAGIiQBwAAYCBCHgAAgIEIeQAAAAYi\n",
              "5AEAABiIkAcAAGAgQh4AAICBCHkAAAAGIuQBAAAYiJAHAABgIEIeAACAgQh5AAAABnJYlmX5ugj4\n",
              "RrkOUarYq42vywDghiOPvOzrEgAUM4zkAQAAGIiQBwAAYCBCHgAAgIEIeQAAAAYi5AEAABiIkAcA\n",
              "AGAgQh4AAICBCHkAAAAGIuQBAAAYiJAHAABgIEIeAACAgQh5AAAABiLkAQAAGIiQBwAAYCBCHgAA\n",
              "gIEIeQAAAAYi5AEAABiIkAcAAGAgr4e83NxcjR07VoGBgQoJCZHL5dLAgQN1+vTpQm2vf//+evXV\n",
              "V2/YbuzYsXr00Uft6fXr18vhcCglJcWe9/jjj2v06NHatGmTevbsKUk6ffq0Xn755XzbatWqlZYu\n",
              "XepWfSNGjNCYMWPcalvUHA5HoX+vAADALF4PeYmJidq0aZM2btyoH374QampqWrXrp1OnTrl1f3G\n",
              "x8fnC3TJycmKjo6+al7r1q0VGRmpDz74QFLBIa8o5OXlKS8vr8i3CwAAUBCvhry9e/fqo48+0ty5\n",
              "c1WpUiVJl0abevToobp16+ro0aOKj49XkyZNFBwcrCFDhthB6Ouvv1aTJk3kcrkUEhKiN954w97u\n",
              "zp071aZNGzVs2FDdunXThQsXrtp3TEyMfv75Zx05ckSSlJKSoueff94OeRkZGTp06JBiY2OVkpIi\n",
              "l8sl6dLo3tmzZ+VyuRQZGWlvb/369WrRooXq1aunxx9/3J6fkZGhDh06KCgoSG3btrX3J0ljxoxR\n",
              "9+7d1aFDB4WEhCgjI0MjRoxQVFSUXC6X4uLitHv3bknSrFmzNHDgQEnSjh075HA4tGLFCknSuHHj\n",
              "NG7cOEm65voAAABX8mrI27Jlixo0aKAqVaoUuLxixYr65JNPtHnzZn3//fc6ePCgPvzwQ0nS+PHj\n",
              "NWLECKWlpemHH35Qr1697PXS0tL0ySefaOfOnTp27JiWLFly1bZLlSqlZs2aKTk5WTk5OTpw4IA6\n",
              "duyoI0eOKDs7W8nJyYqNjVXp0qXzrTdz5kyVL19eaWlp2rRpkz1/3759Sk5O1g8//KAvvvhCGzdu\n",
              "lCQNHTpUTZs21Y4dO/TOO+9o9erV+ba3ceNGvfvuu9qxY4dq1aqlZ555Rt99953S0tI0ePBgDRs2\n",
              "TJLUtm1brVq1SpK0cuVKxcbG5ptu27atJF1z/RvJycnRmTNn8n100XJrXQAAUPz4+XLneXl5euaZ\n",
              "Z7R+/XpZlqXjx48rJCREvXr1Unx8vP7v//5P6enpat26tZo3b26vd//996tMmTKSpKZNm2rfvn0F\n",
              "bv/yJdvatWuradOmki6N8G3cuFEpKSmKj493u9aePXvKz89Pfn5+crlc2rdvn2JjY7V69WpNmjRJ\n",
              "klSrVi117tw533odO3ZU9erV7emVK1dq2rRpOnv2rPLy8uzL1nXr1pUk7d+/X6tWrdL48eM1fPhw\n",
              "nTt3Tjt27LDrv9b6NzJ+/HiNHTs237ySdf/sdv8BAEDx4tWRvIiICKWnp+vkyZMFLp8yZYqOHz+u\n",
              "b775Rt9//7169+6t7OxsSdLf//53LV++XDVr1tSzzz6rwYMH2+tdOfrmdDqVm5tb4Pbj4+OVnJys\n",
              "5ORktWrVSpLUsmVLe17r1q3d7ou7+3Q4HPmmy5UrZ/986NAhDRkyRAsWLNAPP/ygRYsW2f2VLo3m\n",
              "ff7550pPT1fLli1lWZaWLFmi2NhY+fn53XD96xk5cqSysrLyfUrVqel2/wEAQPHi1ZBXv359de/e\n",
              "XYmJifZTn5eDy/79+5WZmakaNWqodOnSOnr0qD766CN73d27d6tOnTp67LHH9Oyzz+rrr7/2eP9R\n",
              "UVE6fvy43nvvvXwhb9GiRcrIyLBHx64UEBCg8+fPF3ifX0Hatm2rOXPmSLp0f97HH398zbZZWVkq\n",
              "WbKkatasKcuyNH369Ku2NXHiRLuu1q1b64UXXrAv1d5o/evx9/dXQEBAvo+cjhuvCAAAiiWvX66d\n",
              "M2eOXnzxRUVHR8vPz095eXmKi4tTmzZtNGzYMD3wwAMKDg7Wn//8ZzvMSNL06dO1Zs0alSpVSk6n\n",
              "U5MnT/Z43yVLllTz5s21detWBQYGSpIaNmyos2fPqnnz5ipZsuRV69x2223q27evwsLCVK5cuXz3\n",
              "5RXktddeU//+/RUUFKRatWpdd3QwNDRUvXr1UnBwsCpXrqyuXbvmW96mTRsdOnTI/j20a9dOkyZN\n",
              "Ups2bdxaHwAA4DKHZVncff8HVa5DlCr2auPrMgC44cgjRf9qJwBm4xsvAAAADETIAwAAMBAhDwAA\n",
              "wECEPAAAAAMR8gAAAAxEyAMAADAQIQ8AAMBAhDwAAAADEfIAAAAMRMgDAAAwECEPAADAQIQ8AAAA\n",
              "AxHyAAAADETIAwAAMBAhDwAAwECEPAAAAAMR8gAAAAxEyAMAADCQn68LgO8MDG6hKY+87OsyAACA\n",
              "FzCSBwAAYCBCHgAAgIEIeQAAAAYi5AEAABiIkAcAAGAgQh4AAICBCHkAAAAGIuQBAAAYiJAHAABg\n",
              "IEIeAACAgQh5AAAABiLkAQAAGIiQBwAAYCBCHgAAgIEIeQAAAAYi5AEAABiIkAcAAGAgQh4AAICB\n",
              "CHkAAAAGIuQBAAAYiJAHAABgIEIeAACAgfx8XQB8IycnR59//rkuXrwop9Pp63K86uLFi/r222/V\n",
              "tGlT+moQ+mqmP1JfpT9Wf+lr0aldu7aGDRt2w3YOy7KsIt87bnlnzpxRhQoVlJWVpYCAAF+X41X0\n",
              "1Uz01Ux/pL5Kf6z+0tf/Pi7XAgAAGIiQBwAAYCBCHgAAgIEIeX9Q/v7+euGFF+Tv7+/rUryOvpqJ\n",
              "vprpj9RX6Y/VX/r638eDFwAAAAZiJA8AAMBAhDwAAAADEfIAAAAMRMgzUHp6upo1a6aGDRsqKipK\n",
              "27dvL7Dd7Nmz1aBBA9WrV0+PPfaYfvvtN7eW3Urc6euaNWvUtGlTBQUFKTg4WE8//bTy8vIkSQcP\n",
              "HpTT6ZTL5bI/+/bt+293wy3u9DUlJUV/+tOf8vXn/Pnz9nKTjuvcuXPz9bNKlSrq1q2bpOJzXIcO\n",
              "Hao777xTDodDaWlp12xnwrnqTl9NOVfd6asp56o7fTXhXJWk7Oxsde3aVQ0bNlR4eLjatWunvXv3\n",
              "Ftj2008/VWBgoBo0aKBu3brpzJkzbi0rchaMEx8fb82dO9eyLMv66KOPrMjIyKva7N+/36pZs6aV\n",
              "kZFh5eXlWZ06dbKmT59+w2W3Gnf6umXLFmvfvn2WZVnW+fPnrbvvvtte58CBA1aFChX+S9XeHHf6\n",
              "mpycbIWHhxe4vmnH9feCg4OtxYsXW5ZVfI7rl19+aR0+fNiqXbu2lZqaWmAbU85Vd/pqyrnqTl9N\n",
              "OVfd6evvFcdz1bIu/Ztcvny5lZeXZ1mWZU2bNs1q2bLlVe3Onj1rVatWzdq5c6dlWZb1t7/9zRox\n",
              "YsQNl3kDIc8wx44ds8qXL2/99ttvlmVZVl5enlW9enUrPT09X7sJEyZYgwYNsqeXL19u3X333Tdc\n",
              "ditxt6+/97e//c164YUXLMsqPv+Bcbev1/vDYfJx/frrr62qVataFy5csCyr+BzXy673B9KEc/VK\n",
              "noSB4niuXqmwIc/k41rcz9Urfffdd1bt2rWvmv/hhx9aHTp0sKe3b99u1apV64bLvIHLtYY5fPiw\n",
              "atasKT8/P0mSw+HQHXfcoUOHDuVrd+jQIdWuXduevvPOO+0211t2K3G3r1c6evSoFi9erPvuu8+e\n",
              "98svvygqKkoREREaN26cLl686PXaPeVJX/ft26eIiAhFRUVpxowZ9nyTj+vs2bP117/+VSVLlrTn\n",
              "FYfj6g4TztXCKK7nqieK+7laGCadq6+99pq6dOly1fyCjl9GRoZyc3Ovu8wb/LyyVeAWdObMGXXq\n",
              "1ElPP/20IiMjJUk1a9bUTz/9pGrVqunUqVPq2bOnJk+erKefftrH1RZORESEjhw5ogoVKujIkSPq\n",
              "2LGjqlSpogcffNDXpXnNL7/8okWLFunrr7+255l2XP9oOFfNZNK5+tJLL2nv3r1avXq1r0u5Lkby\n",
              "DHP77bfn+78Cy7J06NAh3XHHHfna3XHHHfrxxx/t6YMHD9ptrrfsVuJuXyXp7NmzSkhIUJcuXfTk\n",
              "k0/a8/39/VWtWjVJ0m233aYBAwZo3bp1/50OeMDdvgYEBKhChQqSpL/85S966KGH7P6YeFwl6aOP\n",
              "PlJwcLCCgoLsecXluLrDhHPVE8X9XHWXCeeqp0w5VydNmqSkpCR9/vnnKlOmzFXLCzp+l69OXG+Z\n",
              "NxDyDFOtWjVFRERowYIFkqQlS5boL3/5i+rXr5+vXffu3fXxxx/r6NGjsixLM2fOVK9evW647Fbi\n",
              "bl/PnTunhIQEJSQk6Lnnnsu37Pjx4/ZTazk5OUpKSlLjxo3/Ox3wgLt9zcjIsJ9GPHv2rD799FO7\n",
              "P6Yd18tmz56txMTEfPOKy3F1hwnnqrtMOFfdZcK56ikTztUpU6bo/fff18qVK1WxYsUC2yQkJGjL\n",
              "li3atWuXJGnGjBn28bveMq/w2t1+8Jldu3ZZMTExVoMGDawmTZpY33//vWVZlpWYmGgtW7bMbjdr\n",
              "1iyrbt26Vt26da0BAwbYN8LeaNmtxJ2+vvjii5afn58VHh5uf1588UXLsixryZIlVnBwsBUWFmYF\n",
              "BQVZQ4YMsbKzs33Wn+txp6/Tpk2zgoKC7P688MIL9pNglmXWcb3crly5ctaZM2fyrV9cjuvAgQOt\n",
              "WrVqWU6n06pWrZpVr149y7LMPFfd6asp56o7fTXlXHX333BxP1cty7IOHz5sSbLq1q1r//ts2rSp\n",
              "ZVmWNXr0aOuNN96w2y5btsxq1KiRVa9ePatLly7W6dOn3VpW1PjuWgAAAANxuRYAAMBAhDwAAAAD\n",
              "EfIAAAAMRMgDAAAwECEPAADAQIQ8AAAAAxHyAAAADETIAwAAMBAhDwAAwECEPAAAAAMR8gAAAAz0\n",
              "/wGyVkxMjw4HMQAAAABJRU5ErkJggg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-2ebb58e6-451c-456c-b351-292b19eee948\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-2ebb58e6-451c-456c-b351-292b19eee948\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "_df_6.groupby('Date of Transaction').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-e5449b56-a517-43f0-a5b8-7eb47f739535\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAo4AAAGZCAYAAAAHNbBeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAwZUlEQVR4nO3df1jVdZ7//8cRVEYC/JFGFKgguKXJWQsv+qVI0BBXiqORZjmT\n",
              "rQjTTFZeO46tNW2JwparObntETPnU+44OmpGW3y71MpsM3+NRy2vVY+BmMiPpQJK+aG8v390zbkE\n",
              "EV4HOXKC++26znXJeb59vp6+Zq7hMe8f59gsy7IEAAAAtKFHZw8AAACAnwaCIwAAAIwQHAEAAGCE\n",
              "4AgAAAAjBEcAAAAYITgCAADACMERAAAARgiOAAAAMEJwBAAAgBGCIzrU8uXLO3sEAADgJQRHdKiT\n",
              "J0929ggAAMBLCI4AAAAwQnAEAACAEYIjAAAAjBAcAQAAYITgCAAAACMERwAAABghOAIAAMAIwREA\n",
              "AABGCI4AAAAwQnAEAACAEYIjAAAAjBAcAQAAYITgCAAAACMERwAAABghOAIAAMAIwREAAABGbJZl\n",
              "WZ09BLqOUanBun1Gv84eAwCALmXlQyc7ewRJnHEEAACAIYIjAAAAjBAcAQAAYITgCAAAACMERwAA\n",
              "ABghOAIAAMAIwREAAABGCI4AAAAwQnAEAACAEYIjAAAAjBAcAQAAYITgCAAAACMERwAAABghOAIA\n",
              "AMCIV4JjbW2tJk2apJiYGMXGxio5OVkul8tdLy8vV0pKiqKjozVy5Eh98skn7trixYs1fPhw9ejR\n",
              "Q1u2bGnSd+bMmRo1apTsdrvi4uK0ffv2y87Q3jW81Wf16tWKjo5WVFSUMjIy1NDQ0KRuWZYSExPV\n",
              "t2/fy/ZobGzUE088oaioKA0bNkwrVqwwqjV3/Phx3XHHHYqJiVFcXJy+/PJLoxoAAOjevHbGcfbs\n",
              "2Tp69KgOHjyotLQ0zZo1y12bP3++4uPjdfz4ca1Zs0bTp093B6mkpCQVFBRo7Nixl/RctmyZDh06\n",
              "JKfTqby8PKWnp6uxsbHF9du7hjf6FBYW6rnnntPOnTvlcrlUVlamvLy8S/5tUVFRrfZZu3atjhw5\n",
              "omPHjmnPnj16+eWX3cGutVpzmZmZmj17to4dO6bf//73evTRR41qAACge/NKcAwICFBqaqpsNpsk\n",
              "KT4+XkVFRe76hg0blJWVJUmKi4tTWFiYduzYIUkaM2aMIiMjW+x78dm4qqqqVmdo7xre6LNx40ZN\n",
              "nDhRoaGhstlsysrK0rp169z1L7/8Ulu2bNH8+fNb7bN+/XplZGTIz89P/fv319SpU919WqtdrLy8\n",
              "XPv27dMjjzwiSZoyZYpOnToll8vVag0AAOCq3OO4fPlypaWlSZIqKyvV0NCg0NBQd33IkCEqLi42\n",
              "6jV//nxFRUVp8uTJ2rRpk3r0uPSfcKVrdHSf4uJiDR48uMUeDQ0NysjI0MqVK+Xn53fJ37Xb7Sop\n",
              "KWmzT2u1/Px89xnfU6dO6frrr5e/v78kyWazKSIiQsXFxa3WWlJXV6fq6uomr8ZGy6O9AQAAPx1e\n",
              "D46LFy+Wy+VSTk5Oh/TLzc3ViRMntGHDBs2bN0/19fUd0rezvPDCC5o8ebJuuummFutOp1NhYWFX\n",
              "tMbEiRP1+uuvX1GPluTk5CgkJKTJq+LET/s/DwAAcHleDY5LlizR5s2bVVBQoD59+kiSBgwYIH9/\n",
              "f5WWlrqPKyoqUkREhEe9k5KSVFNTo8OHD2vbtm2y2+2y2+1atGhRu9foqD7NRURE6OTJky322LFj\n",
              "h1599VUNGTJEd911l6qrqzVkyBBVVFR41Ke12sXCw8N15swZnT9/XtKPD+UUFxcrIiKi1VpLnnnm\n",
              "GVVVVTV5DYzq5dHeAACAnw6vBcelS5dq3bp12rp16yVPCqenp8vhcEiS9u7dq9OnT2vcuHGt9mto\n",
              "aGhyr92ePXtUXl6uyMhIJSUlyel0yul0asGCBe1eo6P6NDdlyhTl5+ertLRUlmXJ4XBo2rRpkqSd\n",
              "O3fq5MmTKioq0qeffqrg4GAVFRVp4MCBl/RJT0/XqlWrdOHCBX3zzTdav369pk6d2mbtYoMGDdLo\n",
              "0aO1du1aSdKmTZt04403atiwYa3WWtK7d28FBwc3efXoYfNobwAAwE+HzbKsDr8p7euvv1Z4eLgi\n",
              "IyMVFBQk6ceQsXv3bklSWVmZZsyYocLCQvXq1UsrVqzQ+PHjJUnZ2dlyOByqqKhQUFCQAgICdODA\n",
              "AQUGBio5OVlVVVXy9/dXYGCgFi5cqMTExBZnaM8aLYW1juqzatUq5ebmSpISEhLkcDjUs2fPJscU\n",
              "FRXJbrfru+++c79nt9v1/vvvKywsTBcuXNCcOXNUUFAgm82mOXPm6Mknn5SkVmv5+fnKz893X64+\n",
              "evSoHn30UVVWVio4OFhr1qzRLbfc0mbNxKjUYN0+o5/x8QAAoG0rHzrZ9kFXgVeCI7ovgiMAAB3P\n",
              "V4Ij3xwDAAAAIwRHAAAAGCE4AgAAwAjBEQAAAEYIjgAAADBCcAQAAIARgiMAAACMEBwBAABghOAI\n",
              "AAAAIwRHAAAAGCE4AgAAwAjBEQAAAEYIjgAAADDi39kDoGtJ+odZWvrQ0s4eAwAAeAFnHAEAAGCE\n",
              "4AgAAAAjBEcAAAAYITgCAADACMERAAAARgiOAAAAMEJwBAAAgBGCIwAAAIwQHAEAAGCE4AgAAAAj\n",
              "BEcAAAAYITgCAADACMERAAAARgiOAAAAMEJwBAAAgBGCIwAAAIwQHAEAAGCE4AgAAAAjBEcAAAAY\n",
              "ITgCAADACMERAAAARgiOAAAAMEJwBAAAgBGCIwAAAIwQHAEAAGCE4AgAAAAjBEcAAAAYITgCAADA\n",
              "CMERAAAARgiOAAAAMEJwBAAAgBGCIwAAAIwQHAEAAGCE4AgAAAAjBEcAAAAYITgCAADAiH9nD4Cu\n",
              "5Yf/t0+l/9/LnT0GAABdSuiR33X2CJI44wgAAABDBEcAAAAYITgCAADACMERAAAARgiOAAAAMEJw\n",
              "BAAAgBGCIwAAAIwQHAEAAGCE4AgAAAAjBEcAAAAYITgCAADACMERAAAARgiOAAAAMEJwBAAAgBGv\n",
              "Bcfa2lpNmjRJMTExio2NVXJyslwul7teXl6ulJQURUdHa+TIkfrkk0/ctcWLF2v48OHq0aOHtmzZ\n",
              "0qTvzJkzNWrUKNntdsXFxWn79u2XnaG9a3irz+rVqxUdHa2oqChlZGSooaFBkrRr1y7Z7XbZ7XaN\n",
              "GDFCmZmZqqura7HH2bNn9dBDD2nYsGGKiYnRxo0bjWrN7d69W7GxsYqJiVFiYqJOnz5tVAMAAN2X\n",
              "V884zp49W0ePHtXBgweVlpamWbNmuWvz589XfHy8jh8/rjVr1mj69OnuIJWUlKSCggKNHTv2kp7L\n",
              "li3ToUOH5HQ6lZeXp/T0dDU2Nra4fnvX8EafwsJCPffcc9q5c6dcLpfKysqUl5cnSYqNjdXevXvl\n",
              "dDp1+PBhlZeX67XXXmuxz5IlS9S7d2+5XC598MEHevzxx1VZWdlm7WKNjY16+OGH9corr+jYsWNK\n",
              "TU3VU0891WYNAAB0b14LjgEBAUpNTZXNZpMkxcfHq6ioyF3fsGGDsrKyJElxcXEKCwvTjh07JElj\n",
              "xoxRZGRki3379u3r/nNVVVWrM7R3DW/02bhxoyZOnKjQ0FDZbDZlZWVp3bp1kqQ+ffqoZ8+ekqT6\n",
              "+nqdO3fOvW/NrV+/3j3L0KFDlZCQoLfffrvN2sX2798vf39/jR8/XpKUmZmpd999V7W1ta3WAABA\n",
              "93bV7nFcvny50tLSJEmVlZVqaGhQaGiouz5kyBAVFxcb9Zo/f76ioqI0efJkbdq0ST16XPrPuNI1\n",
              "OrpPcXGxBg8efNkeRUVFio2N1bXXXquQkBA9/vjjkqSSkhLZ7XajPq3VHA6H/vCHP7R4XFBQkIKD\n",
              "g1VSUtJqrbm6ujpVV1c3eV2wWj77CwAAfvquSnBcvHixXC6XcnJyOqRfbm6uTpw4oQ0bNmjevHmq\n",
              "r6/vkL6daciQITp48KBKS0tVV1enzZs3S5LCwsLkdDqvuH9WVpZefPHFK+5zsZycHIWEhDR5Oc+d\n",
              "6tA1AACA7/B6cFyyZIk2b96sgoIC9enTR5I0YMAA+fv7q7S01H1cUVGRIiIiPOqdlJSkmpoaHT58\n",
              "WNu2bXM/YLJo0aJ2r9FRfZqLiIjQyZMn2+xxzTXXaNq0afqv//ovj/uYrtH8uJqaGlVVVSksLKzV\n",
              "WnPPPPOMqqqqmrzsPwtvbRsAAMBPmFeD49KlS7Vu3Tpt3bq1yb2JkpSeni6HwyFJ2rt3r06fPq1x\n",
              "48a12q+hoaHJk9l79uxReXm5IiMjlZSUJKfTKafTqQULFrR7jY7q09yUKVOUn5+v0tJSWZYlh8Oh\n",
              "adOmSZJcLpf7YZv6+nq9/fbbGjVqVIt9Lp6lsLBQH3/8sSZNmtRm7WK33nqrGhoa9NFHH0mSVq5c\n",
              "qQkTJiggIKDVWnO9e/dWcHBwk5efjU94AgCgq7JZlmV5o/HXX3+t8PBwRUZGKigoSNKPQWP37t2S\n",
              "pLKyMs2YMUOFhYXq1auXVqxY4X4gIzs7Ww6HQxUVFQoKClJAQIAOHDigwMBAJScnq6qqSv7+/goM\n",
              "DNTChQuVmJjY4gztWWPgwIFe67Nq1Srl5uZKkhISEuRwONSzZ0/l5eXpj3/8o/z8/HT+/Hndc889\n",
              "eumllxQQEKCSkhKlpqa6L1f/8MMPeuyxx7Rv3z75+fkpOztbDz74YJs1h8OhkpIS9+XqXbt2KTMz\n",
              "U7W1tQoLC9Nbb72l8PDwNmttyRwwVi9cN8HoWAAAYCb0yO86ewRJXgyO6J4IjgAAdDxfCY5cVwQA\n",
              "AIARgiMAAACMEBwBAABghOAIAAAAIwRHAAAAGCE4AgAAwAjBEQAAAEYIjgAAADBCcAQAAIARgiMA\n",
              "AACMEBwBAABghOAIAAAAIwRHAAAAGPHv7AHQtQT+6jaFLv1dZ48BAAC8gDOOAAAAMEJwBAAAgBGP\n",
              "L1WfOXNGhYWFOn/+vPu9sWPHduhQAAAA8D0eBcdFixbp5ZdfVmRkpPz8/CRJNptNe/bs8cpwAAAA\n",
              "8B0eBcc33nhDJ06c0IABA7w1DwAAAHyUR/c4XnfddYRGAACAbsqjM47Jycl66qmnNH36dAUEBLjf\n",
              "HzVqVIcPBgAAAN/iUXB88803JUnvvPOO+z2bzaavvvqqY6cCAACAz/EoOBYWFnprDgAAAPg4jz+O\n",
              "Z8+ePdq2bZsk6d5779Vtt93W4UMBAADA93j0cExeXp4eeOABlZeXq6KiQlOmTNHrr7/urdkAAADg\n",
              "Qzw647hixQrt379fAwcOlCT9y7/8i+655x7NmjXLK8MBAADAd3j8lYN/D43N/wwAAICuzaPgGB0d\n",
              "rQULFqi4uFjFxcV67rnnFB0d7a3ZAAAA4EM8Co4Oh0MnTpzQ6NGjNXr0aLlcLv3nf/6nt2YDAACA\n",
              "D/HoHseBAwfqL3/5i7dmAQAAgA8zCo47duzQuHHjlJ+f32J94sSJHToUAAAAfI9RcFy7dq3GjRun\n",
              "ZcuWXVKz2WwERwAAgG7AKDiuWrVKkvTRRx95dRgAAAD4Lo8ejhkzZozRewAAAOh6PAqO58+fb/Jz\n",
              "Q0ODampqOnQgAAAA+Caj4Phv//Zv6tevnw4fPqz+/fu7X8HBwRo7dqy3ZwQAAIAPMLrHMSsrS1On\n",
              "TtWvf/1rORwO9/vBwcHq16+f14YDAACA7zAKjiEhIQoJCdHKlSs1aNAgBQQESJJqa2t16tQphYeH\n",
              "e3VIAAAAdD6P7nF84IEHmvxsWdYl7wEAAKBr8ig41tfXu882StLPfvYz1dXVdfhQAAAA8D0eBUeb\n",
              "zaby8nL3z6WlpbIsq8OHAgAAgO/x6Luq58yZo9tvv10zZsyQ9OM3yjz//PNeGQwAAAC+xaPgOHPm\n",
              "TA0dOlTvv/++JGnNmjW6++67vTIYAAAAfItHwVGSEhISlJCQ4IVRAAAA4Ms8Co7nzp3Tq6++KqfT\n",
              "qdraWvf7mzdv7vDBAAAA4Fs8ejgmIyNDRUVF+uyzzzR+/HidPHlSgwcP9tZsAAAA8CEeBceDBw/q\n",
              "tddeU3BwsJ544gl9/PHH2r9/v7dmAwAAgA/xKDj+7Gc/kyT5+/vrhx9+UFBQkCoqKrwyGAAAAHyL\n",
              "R/c49u/fX99++61SU1P185//XNdee61uvPFGb80GAAAAH+JRcHzvvffk5+enhQsX6s9//rO+/fZb\n",
              "/fKXv/TWbAAAAPAhHgXHCxcuyM/PTzabTbfffruOHDmiwMBAb82Gn6BPPv5K857+784eAwCALuWl\n",
              "Zfd39giSPLzH8c4771RNTY0qKyt19913Kzc3V7/5zW+8NRsAAAB8iEfB8fz58woKCtJ7772nX/3q\n",
              "V/r000/1P//zP96aDQAAAD7Eo+BYX18vSfr444+VmJgoSfLz8+v4qQAAAOBzPLrHcfz48br55pt1\n",
              "4cIFrVy5Ut9++638/T3+1kIAAAD8BHmU+l599VUdPHhQkZGR6tmzpy5cuKBVq1Z5azYAAAD4EI+C\n",
              "o81m06hRo1RaWqrvvvtOkjRgwABvzAUAAAAf41Fw/NOf/qQ5c+aoZ8+e6tHjx9sjbTabysvLvTIc\n",
              "AAAAfIdHwXHhwoXau3evhg8f7q15AAAA4KM8eqr62muvJTQCAAB0Ux4Fx0mTJumVV15ReXm5qqur\n",
              "3S8AAAB0fR5dql6wYIEkae7cubLZbLIsSzabTRcuXPDKcAAAAPAdHgXHxsZGb80BAAAAH+fRpWoA\n",
              "AAB0Xx4Fx+PHj+u+++5TWFiY+vfv734BAACg6/MoOGZkZOjRRx9Vv379tGPHDj3wwAP653/+50uO\n",
              "q62t1aRJkxQTE6PY2FglJyfL5XK56+Xl5UpJSVF0dLRGjhypTz75xF1bvHixhg8frh49emjLli1N\n",
              "+s6cOVOjRo2S3W5XXFyctm/fftlZ27uGt/qsXr1a0dHRioqKUkZGhhoaGiRJH374ocaMGaObb75Z\n",
              "I0aM0Lx58y57S0BjY6OeeOIJRUVFadiwYVqxYoVRrbnjx4/rjjvuUExMjOLi4vTll18a1QAAQPfm\n",
              "UXCsrq7W1KlT1aNHD91yyy1auXLlZQPT7NmzdfToUR08eFBpaWmaNWuWuzZ//nzFx8fr+PHjWrNm\n",
              "jaZPn+4OUklJSSooKNDYsWMv6bls2TIdOnRITqdTeXl5Sk9Pv2zIau8a3uhTWFio5557Tjt37pTL\n",
              "5VJZWZny8vIkSf369dNf/vIXHTlyRPv379dnn32mN998s8U+a9eu1ZEjR3Ts2DHt2bNHL7/8sjvY\n",
              "tVZrLjMzU7Nnz9axY8f0+9//Xo8++qhRDQAAdG8eBceePXtKkoKCglRUVKS6ujr93//93yXHBQQE\n",
              "KDU1VTabTZIUHx+voqIid33Dhg3KysqSJMXFxSksLEw7duyQJI0ZM0aRkZEtrt+3b1/3n6uqqlqd\n",
              "tb1reKPPxo0bNXHiRIWGhspmsykrK0vr1q2TJP3jP/6ju0dAQIDsdnuTvbrY+vXrlZGRIT8/P/Xv\n",
              "319Tp05192mtdrHy8nLt27dPjzzyiCRpypQpOnXqlFwuV6u1ltTV1TX5WKbq6mpZPEAFAECX5VFw\n",
              "HDt2rCorK/Xb3/5Wt956q4YOHaq0tLQ2/97y5cvdx1VWVqqhoUGhoaHu+pAhQ1RcXGw0w/z58xUV\n",
              "FaXJkydr06ZN7q8+vNiVrtHRfYqLizV48OA2e5SWlmrjxo26//773e/Z7XaVlJS02ae1Wn5+vvuM\n",
              "76lTp3T99dfL3//HB+ptNpsiIiJUXFzcaq0lOTk5CgkJafI6U3bMo70BAAA/HcbB0bIszZ07VwMG\n",
              "DND06dPldDq1detWLVu2rNW/t3jxYrlcLuXk5FzxsJKUm5urEydOaMOGDZo3b57q6+s7pG9nq66u\n",
              "1oQJEzRv3jzddttt7vedTqfCwsKuqPfEiRP1+uuvX+mIl3jmmWdUVVXV5HX9dTEdvg4AAPANHp1x\n",
              "TE5Odv85PDxcI0eObPX4JUuWaPPmzSooKFCfPn0kSQMGDJC/v79KS0vdxxUVFSkiIsKTUZSUlKSa\n",
              "mhodPnxY27Ztk91ul91u16JFi9q9Rkf1aS4iIkInT568bI+amhqlpKQoLS1Nc+fObVefttb4u/Dw\n",
              "cJ05c0bnz5+X9OP/ISguLlZERESrtZb07t1bwcHBTV62Fs4AAwCArsH4t7zNZtONN97Y4j2NLVm6\n",
              "dKnWrVunrVu3Nrk3UZLS09PlcDgkSXv37tXp06c1bty4Vvs1NDQ0udduz549Ki8vV2RkpJKSkuR0\n",
              "OuV0Ot3fbtOeNTqqT3NTpkxRfn6+SktLZVmWHA6Hpk2bJkn6/vvvlZKSopSUFD377LOt9klPT9eq\n",
              "Vat04cIFffPNN1q/fr2mTp3aZu1igwYN0ujRo7V27VpJ0qZNm3TjjTdq2LBhrdYAAABslmVZbR10\n",
              "/PhxRUdH64EHHtDnn3+u1NRUXXPNNe760qVLmxz/9ddfKzw8XJGRkQoKCpL049mp3bt3S5LKyso0\n",
              "Y8YMFRYWqlevXlqxYoXGjx8vScrOzpbD4VBFRYWCgoIUEBCgAwcOKDAwUMnJyaqqqpK/v78CAwO1\n",
              "cOFCJSYmtjhze9YYOHCg1/qsWrVKubm5kqSEhAQ5HA717NlTixYt0r/+679qxIgR7mPT09PdwdVu\n",
              "t+v9999XWFiYLly4oDlz5qigoEA2m01z5szRk08+KUmt1vLz85Wfn+++XH306FE9+uijqqysVHBw\n",
              "sNasWaNbbrmlzZqJ20ZPUuK4WW0fCAAAjL207P62D7oKjILj6NGj9be//U0vvPBCi/Xnn3++wwfD\n",
              "TxPBEQCAjucrwdHou6r/ni0JiAAAAN2XUXCsqqrSu+++q8udnJw4cWKHDgUAAADfYxQcKyoqtGzZ\n",
              "shaDo81mIzgCAAB0A0bBcdiwYfrwww+9PQsAAAB8GB+6BwAAACNGwZFL0QAAADAKjpf7GB4AAAB0\n",
              "H1yqBgAAgBGj4Pjtt996ew4AAAD4OKPgeM8990iSHnzwQa8OAwAAAN9l9HE8586d0+7du3X48GEd\n",
              "Pnz4ks9zHDVqlFeGAwAAgO8wCo5PPfWUZs6cqcLCwkuesLbZbPrqq6+8MhwAAAB8h1FwzMzMVGZm\n",
              "ptLT0/XXv/7V2zMBAADAB9msy30B9WWcPXtWTqdTkmS329WnTx9vzIWfqLlz52rp0qWdPQYAAPAC\n",
              "ozOOf7dr1y5NnjxZ1113nWw2m8rKyrRp0ybdfvvt3poPAAAAPsKj4Pj0009r48aNuvPOOyVJn332\n",
              "mZ5++ml9/vnnXhkOAAAAvsOjDwA/d+6cOzRK0h133KHa2toOHwoAAAC+x6PgeM0112jbtm3un7dv\n",
              "367AwMAOHwoAAAC+x6NL1cuXL9eUKVPk5+cnSWpsbNTmzZu9MhgAAAB8i0fB8bbbbpPL5dLRo0cl\n",
              "ScOHD1fPnj29MhgAAAB8i0fBUZJ69uypkSNHemMWAAAA+DCP7nEEAABA90VwBAAAgJF2Bce6urqO\n",
              "ngMAAAA+zqPgeOjQIY0cOVJRUVGSpP3792vevHleGQwAAAC+xaPgOGfOHDkcDg0cOFCSNHr0aL33\n",
              "3nteGQwAAAC+xaPg+P333+uuu+5y/2yz2dSrV68OHwoAAAC+x6Pg6O/vr4aGBtlsNknSqVOn3B8G\n",
              "DgAAgK7No+D429/+VpMmTVJFRYWeffZZ3X333dzjCAAA0E149AHgjzzyiCIjI/XOO++ovr5ea9eu\n",
              "bXLpGgAAAF2XR8HxjTfe0GOPPaY77rjjkvcAAADQtXl0qXrFihWXvPcf//EfHTYMAAAAfJfRGcc9\n",
              "e/Zo165dqqio0B//+Ef3+1VVVXwYOAAAQDdhFBzPnDkjp9Ops2fP6sCBA+73g4OD9ac//clbswEA\n",
              "AMCHGAXHtLQ0paWlqaCgQPfdd5+3ZwIAAIAP8ujhmPvuu08lJSX64osvVFtb635/4sSJHT4YAAAA\n",
              "fItHwXHNmjV64YUX9M033yg6OloHDx5UfHw8wREAAKAb8Oip6qVLl+rAgQOKiorS/v379eGHHyom\n",
              "JsZbswEAAMCHeBQce/XqpX79+un8+fOSpLFjx8rpdHpjLgAAAPgYjy5V9+7dW5ZlKSYmRq+88ooG\n",
              "Dx6s77//3luzAQAAwId4FByzs7NVXV2tl156SVlZWfruu+/02muveWs2AAAA+BCPgmNiYqIkKSQk\n",
              "RFu3bvXKQAAAAPBNxvc47t27V1OnTtXIkSM1cuRITZs2Tfv27fPmbAAAAPAhRsFx165duvfeexUZ\n",
              "Gans7GwtXLhQQ4cO1b333qvdu3d7e0YAAAD4AKNL1S+99JLeeOMN/eIXv3C/94tf/ELx8fHKycnR\n",
              "li1bvDUfAAAAfITRGccvv/yySWj8u7S0NB05cqTDhwIAAIDvMQqOffr0uWwtMDCww4YBAACA7zK6\n",
              "VF1XV6fDhw/LsqxLahd/ZzUAAAC6LqPgeO7cuct+H7XNZuvQgQAAAOCbjIJjUVGRl8cAAACAr7NZ\n",
              "LV1/Btpp5gibnonz6CvQAQBAG2L+dKGzR5DkwQeAAwAAoHsjOAIAAMAIwREAAABGCI4AAAAwQnAE\n",
              "AACAEYIjAAAAjBAcAQAAYITgCAAAACMERwAAABghOAIAAMAIwREAAABGCI4AAAAwQnAEAACAEYIj\n",
              "AAAAjHgtONbW1mrSpEmKiYlRbGyskpOT5XK53PXy8nKlpKQoOjpaI0eO1CeffOKuLV68WMOHD1eP\n",
              "Hj20ZcuWJn1nzpypUaNGyW63Ky4uTtu3b7/sDO1dw1t9Vq9erejoaEVFRSkjI0MNDQ2SpKKiIiUk\n",
              "JCgkJER2u73VHmfPntVDDz2kYcOGKSYmRhs3bjSqNbd7927FxsYqJiZGiYmJOn36tFENAAB0X149\n",
              "4zh79mwdPXpUBw8eVFpammbNmuWuzZ8/X/Hx8Tp+/LjWrFmj6dOnu4NUUlKSCgoKNHbs2Et6Llu2\n",
              "TIcOHZLT6VReXp7S09PV2NjY4vrtXcMbfQoLC/Xcc89p586dcrlcKisrU15eniQpODhY2dnZ+vOf\n",
              "/9zmLEuWLFHv3r3lcrn0wQcf6PHHH1dlZWWbtYs1Njbq4Ycf1iuvvKJjx44pNTVVTz31VJs1AADQ\n",
              "vXktOAYEBCg1NVU2m02SFB8fr6KiInd9w4YNysrKkiTFxcUpLCxMO3bskCSNGTNGkZGRLfbt27ev\n",
              "+89VVVWtztDeNbzRZ+PGjZo4caJCQ0Nls9mUlZWldevWSZL69++vu+66S4GBgW32Wb9+vXuWoUOH\n",
              "KiEhQW+//XabtYvt379f/v7+Gj9+vCQpMzNT7777rmpra1utNVdXV6fq6uomrwstZ3gAANAFXLV7\n",
              "HJcvX660tDRJUmVlpRoaGhQaGuquDxkyRMXFxUa95s+fr6ioKE2ePFmbNm1Sjx6X/jOudI2O7lNc\n",
              "XKzBgwd73KOkpKTJ5evW+rRWczgc+sMf/tDicUFBQQoODlZJSUmrteZycnIUEhLS5HXo/9r8JwEA\n",
              "gJ+oqxIcFy9eLJfLpZycnA7pl5ubqxMnTmjDhg2aN2+e6uvrO6SvLwoLC5PT6bziPllZWXrxxRev\n",
              "fKCLPPPMM6qqqmryGnVthy4BAAB8iNeD45IlS7R582YVFBSoT58+kqQBAwbI399fpaWl7uOKiooU\n",
              "ERHhUe+kpCTV1NTo8OHD2rZtm+x2u+x2uxYtWtTuNTqqT3MRERE6efLkFfVoq4/pGs2Pq6mpUVVV\n",
              "lcLCwlqtNde7d28FBwc3efnxnD4AAF2WV3/NL126VOvWrdPWrVub3JsoSenp6XI4HJKkvXv36vTp\n",
              "0xo3blyr/RoaGpo8mb1nzx6Vl5crMjJSSUlJcjqdcjqdWrBgQbvX6Kg+zU2ZMkX5+fkqLS2VZVly\n",
              "OByaNm2aRz2az1JYWKiPP/5YkyZNarN2sVtvvVUNDQ366KOPJEkrV67UhAkTFBAQ0GoNAAB0bzbL\n",
              "sixvNP76668VHh6uyMhIBQUFSfrxDNXu3bslSWVlZZoxY4YKCwvVq1cvrVixwv1ARnZ2thwOhyoq\n",
              "KhQUFKSAgAAdOHBAgYGBSk5OVlVVlfz9/RUYGKiFCxcqMTGxxRnas8bAgQO91mfVqlXKzc2VJCUk\n",
              "JMjhcKhnz546e/asYmJiVFdXp6qqKg0aNEgzZsxQTk6OSkpKlJqa6r5c/cMPP+ixxx7Tvn375Ofn\n",
              "p+zsbD344INt1hwOh0pKStyXq3ft2qXMzEzV1tYqLCxMb731lsLDw9ustWXmCJueieO0IwAAHSnm\n",
              "Txc6ewRJXgyO6J4IjgAAdDxfCY78hgcAAIARgiMAAACMEBwBAABghOAIAAAAIwRHAAAAGCE4AgAA\n",
              "wAjBEQAAAEYIjgAAADBCcAQAAIARgiMAAACMEBwBAABghOAIAAAAIwRHAAAAGPHv7AHQtfT7+dOK\n",
              "Wbq0s8cAAABewBlHAAAAGCE4AgAAwAjBEQAAAEYIjgAAADBCcAQAAIARgiMAAACMEBwBAABghOAI\n",
              "AAAAIwRHAAAAGCE4AgAAwAjBEQAAAEYIjgAAADBCcAQAAIARgiMAAACMEBwBAABghOAIAAAAIwRH\n",
              "AAAAGCE4AgAAwAjBEQAAAEYIjgAAADBCcAQAAIARgiMAAACMEBwBAABghOAIAAAAIwRHAAAAGCE4\n",
              "AgAAwAjBEQAAAEYIjgAAADBCcAQAAIARgiMAAACMEBwBAABghOAIAAAAIwRHAAAAGCE4AgAAwAjB\n",
              "EQAAAEYIjgAAADBisyzL6uwh0HVc8/M49Z12T2ePAQBAl/L1zNzOHkESZxwBAABgiOAIAAAAIwRH\n",
              "AAAAGCE4AgAAwAjBEQAAAEYIjgAAADBCcAQAAIARgiMAAACMEBwBAABghOAIAAAAIwRHAAAAGCE4\n",
              "AgAAwAjBEQAAAEYIjgAAADDiteBYW1urSZMmKSYmRrGxsUpOTpbL5XLXy8vLlZKSoujoaI0cOVKf\n",
              "fPKJu7Z48WINHz5cPXr00JYtW5r0nTlzpkaNGiW73a64uDht3779sjO0dw1v9Vm9erWio6MVFRWl\n",
              "jIwMNTQ0GNUudvbsWT300EMaNmyYYmJitHHjRqNac7t371ZsbKxiYmKUmJio06dPG9UAAED35dUz\n",
              "jrNnz9bRo0d18OBBpaWladasWe7a/PnzFR8fr+PHj2vNmjWaPn26OywlJSWpoKBAY8eOvaTnsmXL\n",
              "dOjQITmdTuXl5Sk9PV2NjY0trt/eNbzRp7CwUM8995x27twpl8ulsrIy5eXltVlrbsmSJerdu7dc\n",
              "Lpc++OADPf7446qsrGyzdrHGxkY9/PDDeuWVV3Ts2DGlpqbqqaeearMGAAC6N68Fx4CAAKWmpspm\n",
              "s0mS4uPjVVRU5K5v2LBBWVlZkqS4uDiFhYVpx44dkqQxY8YoMjKyxb59+/Z1/7mqqqrVGdq7hjf6\n",
              "bNy4URMnTlRoaKhsNpuysrK0bt26NmvNrV+/3j3L0KFDlZCQoLfffrvN2sX2798vf39/jR8/XpKU\n",
              "mZmpd999V7W1ta3Wmqurq1N1dXWTly5Ybe4FAAD4abpq9zguX75caWlpkqTKyko1NDQoNDTUXR8y\n",
              "ZIiKi4uNes2fP19RUVGaPHmyNm3apB49Lv1nXOkaHd2nuLhYgwcPbrFHa7WSkhLZ7fYr7uNwOPSH\n",
              "P/yhxeOCgoIUHByskpKSVmvN5eTkKCQkpMmrvvCMR/sCAAB+Oq5KcFy8eLFcLpdycnI6pF9ubq5O\n",
              "nDihDRs2aN68eaqvr++Qvr4oLCxMTqfzivtkZWXpxRdfvPKBLvLMM8+oqqqqyavX0Os7dA0AAOA7\n",
              "vB4clyxZos2bN6ugoEB9+vSRJA0YMED+/v4qLS11H1dUVKSIiAiPeiclJammpkaHDx/Wtm3bZLfb\n",
              "ZbfbtWjRonav0VF9mouIiNDJkydb7NFazRt9mh9XU1OjqqoqhYWFtVprrnfv3goODm7ykp+tzb0A\n",
              "AAA/TV4NjkuXLtW6deu0devWJvcmSlJ6erocDockae/evTp9+rTGjRvXar+GhoYmT2bv2bNH5eXl\n",
              "ioyMVFJSkpxOp5xOpxYsWNDuNTqqT3NTpkxRfn6+SktLZVmWHA6Hpk2b1matuYtnKSws1Mcff6xJ\n",
              "kya1WbvYrbfeqoaGBn300UeSpJUrV2rChAkKCAhotQYAALo3m2VZXnma4euvv1Z4eLgiIyMVFBQk\n",
              "6cczVLt375YklZWVacaMGSosLFSvXr20YsUK9wMZ2dnZcjgcqqioUFBQkAICAnTgwAEFBgYqOTlZ\n",
              "VVVV8vf3V2BgoBYuXKjExMQWZ2jPGgMHDvRan1WrVik3N1eSlJCQIIfDoZ49e7ZaKykpUWpqqvty\n",
              "9Q8//KDHHntM+/btk5+fn7Kzs/Xggw+2WXM4HCopKXFfrt61a5cyMzNVW1ursLAwvfXWWwoPD2+z\n",
              "1pZrfh6nvtPuMToWAACY+XpmbmePIMmLwRHdE8ERAICO5yvBkW+OAQAAgBGCIwAAAIwQHAEAAGCE\n",
              "4AgAAAAjBEcAAAAYITgCAADACMERAAAARgiOAAAAMEJwBAAAgBGCIwAAAIwQHAEAAGCE4AgAAAAj\n",
              "BEcAAAAY8e/sAdC1zB5xt5bOzO3sMQAAgBdwxhEAAABGCI4AAAAwQnAEAACAEYIjAAAAjBAcAQAA\n",
              "YITgCAAAACMERwAAABghOAIAAMAIwREAAABGCI4AAAAwQnAEAACAEYIjAAAAjBAcAQAAYITgCAAA\n",
              "ACMERwAAABghOAIAAMAIwREAAABG/Dt7AHQddXV1Kigo0IULF+Tn59fZ43QrFy5c0J49ezRmzBj2\n",
              "/ipi3zsPe9852PfOcTX2ffDgwXryySfbPM5mWZbllQnQ7VRXVyskJERVVVUKDg7u7HG6Ffa+c7Dv\n",
              "nYe97xzse+fwpX3nUjUAAACMEBwBAABghOAIAAAAIwRHdJjevXvr+eefV+/evTt7lG6Hve8c7Hvn\n",
              "Ye87B/veOXxp33k4BgAAAEY44wgAAAAjBEcAAAAYITgCAADACMERHjt+/LjuuOMOxcTEKC4uTl9+\n",
              "+WWLx61evVrR0dGKiopSRkaGGhoarvKkXYvJvn/44YcaM2aMbr75Zo0YMULz5s1TY2NjJ0zbtZj+\n",
              "d16SLMtSYmKi+vbte/UG7KJM9/3w4cNKSEjQTTfdpJtuukmbN2++ypN2PSZ739jYqLlz5+rmm2/W\n",
              "qFGjNH78eLlcrk6YtuuYM2eOhgwZIpvNJqfTednjOvX3qwV4aPz48daaNWssy7Ksv/71r9Ztt912\n",
              "yTFfffWVdf3111tnzpyxGhsbrQkTJlgrVqy4ypN2LSb7/re//c06ceKEZVmWde7cOevOO+90/x20\n",
              "n8ne/92///u/W7NmzbJCQkKuznBdmMm+//DDD9bQoUOtnTt3WpZlWefPn7fKy8uv5phdksnev/32\n",
              "29aYMWOs+vp6y7Isa+HChVZ6evrVHLPL2bFjh3Xq1Clr8ODB1oEDB1o8prN/vxIc4ZGysjIrKCjI\n",
              "amhosCzLshobG63rrrvOOn78eJPjXnrpJSszM9P983vvvWfdeeedV3XWrsR035v7zW9+Yz3//PNX\n",
              "YcKuy5O9/+KLL6y7777bcrlcBMcrZLrvq1atsh566KHOGLHLMt37LVu2WLGxsVZ1dbXV2Nho/e53\n",
              "v7Oefvrpzhi5y2ktOHb271cuVcMjp06d0vXXXy9/f39Jks1mU0REhIqLi5scV1xcrMGDB7t/HjJk\n",
              "yCXHwJzpvl+stLRUGzdu1P3333+1xuySTPe+oaFBGRkZWrlypfz8/Dpj1C7FdN+PHDmi3r176/77\n",
              "75fdbtcvf/lLVVRUdMbIXYbp3k+YMEEJCQkKDQ3V9ddfr+3bt+vFF1/sjJG7lc7+/UpwBLqg6upq\n",
              "TZgwQfPmzdNtt93W2eN0Cy+88IImT56sm266qbNH6VbOnz+vbdu2aeXKlTpw4IBuuOEG/frXv+7s\n",
              "sbqFffv26YsvvtDp06dVUlKie+65R1lZWZ09FryM4AiPhIeH68yZMzp//rykHx8EKC4uVkRERJPj\n",
              "IiIidPLkSffPRUVFlxwDc6b7Lkk1NTVKSUlRWlqa5s6de7VH7XJM937Hjh169dVXNWTIEN11112q\n",
              "rq7WkCFDOPvVTp78b8348eN1ww03yGaz6ZFHHtHnn3/eGSN3GaZ7/+abb7ofBOvRo4d+9atf6aOP\n",
              "PuqMkbuVzv79SnCERwYNGqTRo0dr7dq1kqRNmzbpxhtv1LBhw5ocN2XKFOXn56u0tFSWZcnhcGja\n",
              "tGmdMXKXYLrv33//vVJSUpSSkqJnn322M0btckz3fufOnTp58qSKior06aefKjg4WEVFRRo4cGBn\n",
              "jP2TZ7rvDz74oPbu3avq6mpJ0vvvv6/Y2NirPm9XYrr3kZGR+vDDD1VfXy9J+u///m+NHDnyqs/b\n",
              "3XT679erdjcluoz//d//teLj463o6Gjr1ltvtQ4dOmRZlmX90z/9k/XOO++4j8vLy7MiIyOtyMhI\n",
              "67HHHnM/eYf2Mdn37Oxsy9/f34qNjXW/srOzO3PsLsH0v/N/V1hYyMMxHcB03998801rxIgR1i23\n",
              "3GKlpKRYxcXFnTVyl2Gy97W1tdasWbOsf/iHf7BuueUWKzk52f2pDmif2bNnWzfccIPl5+dnDRo0\n",
              "yIqKirIsy7d+v/Jd1QAAADDCpWoAAAAYITgCAADACMERAAAARgiOAAAAMEJwBAAAgBGCIwAAAIwQ\n",
              "HAEAAGCE4AgAAAAjBEcAAAAYITgCAADACMERAAAARv5/vCEREv4QiNAAAAAASUVORK5CYII=\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-e5449b56-a517-43f0-a5b8-7eb47f739535\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-e5449b56-a517-43f0-a5b8-7eb47f739535\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x78367c1785b0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">2-d distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_7.plot(kind='scatter', x='Amount (USD)', y='Money Laundering Risk Score', s=32, alpha=.8)\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-3adf73c9-28ab-4e7e-acd1-cb7213477054\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAiAAAAGnCAYAAACO1OzhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAs7klEQVR4nO3deXQUZaL+8afTHTqJSYggGq8IEUNEQEgEQoBRAQEF8USRRSQD\n",
              "uLGMXMTgGY1XnWEcjSjminJnAM11AeTACOKGjIgw4iBEhIAsggHCvskSEiBNutO/P/jZ1x6C0wWp\n",
              "aqj+fs7pY7q6Uu9T/Yd5qHqryuH3+/0CAACwUFS4AwAAgMhDAQEAAJajgAAAAMtRQAAAgOUoIAAA\n",
              "wHIUEAAAYDkKCAAAsBwFBAAAWI4CAgAALHfBFpCJEyeGOwIAADDJBVtAtm/fHu4IAADAJBdsAQEA\n",
              "APZFAQEAAJajgAAAAMtRQAAAgOUoIAAAwHIUEAAAYDkKCAAAsBwFBAAAWI4CAgAALEcBAQAgwlRW\n",
              "+fRThUeVVb6wZXCFbWQAAGCpg+UeTV9eqgXr9utklU+x0U71bJmsnA6NdVm829Isph4BWbBggdq2\n",
              "batWrVopKytLa9asMXM4AABwFgfLPfrPmas1Y8UOHfd45Ypy6LjHq+krtmvUe6v1U4XH0jymHQE5\n",
              "cuSIBg0apK+++kotWrTQ0qVLNWjQIK1bt86sIQEAwFlMX16qkgPlahDvlst5+vjDJW7J66tWyYFy\n",
              "Tf9mu8Z0T7Msj2lHQLZs2aL69eurRYsWkqSbbrpJO3bs0KpVq85Y1+Px6NixY0Evny9856UAALCT\n",
              "yiqfFqzbrxiXM1A+fuZyRsntcuqz9fssnRNiWgFp2rSpDh06pGXLlkmSPvroI5WXl6u0tPSMdfPz\n",
              "81W3bt2gV1FRkVnRAACIKBUer05W+VTHVfOffbcrSidP+VTh8VqWybQCUrduXb3//vvKy8tTmzZt\n",
              "9Pnnn6t58+Zyuc4865OXl6eysrKgV2ZmplnRAACIKPFul2KjnTrlra7xc4+3WrF1nIp3W3dtiqkj\n",
              "denSRV26dJF0+jRLcnKymjdvfsZ6brdbbnfw7Fun02lmNAAAIkZMtFO3t7xCM1bskNdXHXQaxuur\n",
              "lsfrU78WDRUTbd3fXlOvgtm7d2/g5+eee05du3ZVamqqmUMCAIAa/LZDilIvT9DBCo8OHz+l4x6v\n",
              "Dh8/pYMVHjW9PEE5HRpbmsfUAvLss8+qWbNmSk1N1fbt21VYWGjmcAAA4Cwui3dr0n0ZymnfWPEx\n",
              "Lnmr/YqPcSmnfWO9fl+G5fcBcfj9fr+lI4YoNzdXBQUF4Y4BAIDtVFadnnAa73ZZetrll7gTKgAA\n",
              "ESYm2hm24vEzngUDAAAsRwEBAACWo4AAAADLUUAAAIDlKCAAAMByFBAAAGA5CggAALAcBQQAAFiO\n",
              "AgIAACxHAQEAAJajgAAAAMtRQAAAgOUoIAAAwHIUEAAAYDkKCAAAsBwFBAAAWI4CAgAALEcBAQAA\n",
              "lqOAAAAAy1FAAACA5SggAADAchQQAABgOQoIAACwHAUEAABYjgICAAAsRwEBAACWo4AAAADLUUAA\n",
              "AIDlKCAAAMByFBAAAGA5CggAALAcBQQAAFjO1AIyf/583XjjjUpPT1fLli31zjvvmDkcAAC4SLjM\n",
              "2rDf71dOTo6WLFmiVq1aqbS0VM2aNVOfPn2UkJBg1rAAAOAiYFoBkSSHw6GjR49Kko4dO6b69evL\n",
              "7XafsZ7H45HH4wla5vP5zIwGAADCyLQC4nA4NGvWLPXp00eXXHKJjhw5orlz56pOnTpnrJufn69x\n",
              "48YFLcvKyjIrGgAACDPT5oB4vV79+c9/1ty5c7V9+3YtWrRIv/3tb/XTTz+dsW5eXp7KysqCXpmZ\n",
              "mWZFAwAAYWZaASkuLtaePXt08803S5LatWunhg0bavXq1Wes63a7lZiYGPRyOp1mRQMAAGFmWgG5\n",
              "+uqrtXfvXm3cuFGSVFJSoi1btui6664za0gAAHCRMG0OyBVXXKGpU6eqf//+ioqKUnV1tSZNmqRG\n",
              "jRqZNSQAALhImHoVzMCBAzVw4EAzhwAAABch7oQKAAAsRwEBAACWo4AAAADLUUAAAIDlKCAAAMBy\n",
              "FBAAAGA5CggAALAcBQQAAFiOAgIAACxHAQEAAJajgAAAAMtRQAAAgOUoIAAAwHIUEAAAYDkKCAAA\n",
              "sBwFBAAAWI4CAgAALEcBAQAAlqOAAAAAy1FAAACA5SggAADAchQQAABgOQoIAACwHAUEAABYjgIC\n",
              "AAAsRwEBAACWo4AAAADLUUAAAIDlKCAAAMByFBAAAGA5CggAALAcBcRElVU+/VThUWWVL9xRAAC4\n",
              "oLjCHcCODpZ7NH15qRas26+TVT7FRjvVs2Wycjo01mXx7nDHAwAg7EwrIIcOHdKtt94aeH/ixAlt\n",
              "3bpVBw4cUL169cwaNuwOlnv0nzNXq+RAuWJcTtVxRem4x6vpK7Zr+bbDmnRfBiUEABDxTCsg9evX\n",
              "V3FxceD9hAkT9I9//MPW5UOSpi8vVcmBcjWId8vlPH2G6xK35PVVq+RAuaZ/s11juqeFOSUAAOFl\n",
              "2RyQwsJCPfjggzV+5vF4dOzYsaCXz3fxzZuorPJpwbr9inE5A+XjZy5nlNwupz5bv485IQCAiGdJ\n",
              "AVm2bJmOHDmi3r171/h5fn6+6tatG/QqKiqyIlqtqvB4dbLKpzqumr9WtytKJ0/5VOHxWpwMAIAL\n",
              "iyUFpLCwUIMHD5bLVfMZn7y8PJWVlQW9MjMzrYhWq+LdLsVGO3XKW13j5x5vtWLrOBXvZu4vACCy\n",
              "mf6XsKKiQrNnz9a333571nXcbrfc7uCJmU6n0+xotS4m2qnbW16hGSt2yOurDjoN4/VVy+P1qV+L\n",
              "hoqJvvj2DQCA2mToCIjX69Urr7yikSNHSpK2bNmiL7/88ld/Z9asWWrdurWaNWt27ikvIr/tkKLU\n",
              "yxN0sMKjw8dP6bjHq8PHT+lghUdNL09QTofG4Y4IAEDYGToCMmrUKPl8Pn399deSTl/pMmDAAK1c\n",
              "ufKsv1NYWKiHH374/FJeRC6Ld2vSfRma/s12fbZ+n06e8ik+xqV+LRpyHxAAAP4/QwVk+fLlKi4u\n",
              "VkZGhiQpKSlJVVVVv/o7y5YtO/d0F6nL4t0a0z1NIzpfqwqPV/FuF6ddAAD4BUMFJCYmJui9z+dT\n",
              "dXXNEy5xek4IxQMAgDMZmgPSqlUrTZ8+XdXV1SopKdGIESPUuXNnk6IBAAC7MlRACgoKtHTpUu3b\n",
              "t0+dOnVSVFSUxo8fb1Y2AABgUyGfgvH5fHruuec0ZcoUTZkyxcxMAADA5kI+AuJ0OrV48WIzswAA\n",
              "gAhh6BRMr1699Pzzz2vPnj1Bz20BAAAwwuH3+/2hrhwVdWZfcTgcpjw4Ljc3VwUFBbW+XQAAEH6G\n",
              "LsPlklsAAFAbDD8LZufOnVq6dKkk6ZZbbtFVV11V66EAAIC9GZoD8uGHHyojI0OzZ8/W3/72N2Vk\n",
              "ZOjjjz82KxsAALApQ0dAxo0bp+XLlys1NVWSVFJSov79++vOO+80JRwAALAnQ0dAfD5foHxIUmpq\n",
              "KvNCAACAYYYKyOWXX64333xT1dXVqq6uVmFhoRo0aGBWNgAAYFOGCsjkyZP15ptvKjY2VrGxsXrz\n",
              "zTc1efJks7IBAACbMjQH5Nprr9Xy5ctVUVEhSYqPjzclFAAAsDdDR0CmTp2qw4cPKz4+XvHx8Tp0\n",
              "6JDeeOMNs7IBAACbMlRA/vKXv6hevXqB9/Xr19df/vKXWg8FAADszVABqemu7Wbchh0AANiboQJy\n",
              "5ZVXavbs2YH3s2bN0pVXXlnroQAAgL0ZmoT66quvKjs7W7///e8lSXFxcfrwww9NCQYAAOzLUAFp\n",
              "1qyZNmzYoE2bNkmSrrvuOjmdTlOCAQAA+zJ0CkaSnE6nYmJi9Pnnn+uzzz4zIxMAALC5kApIt27d\n",
              "VFxcLEnas2eP2rZtq7///e96/PHHNX78eDPzAQAAGwqpgOzevVvp6emSpPfee0+33HKLPvvsM33z\n",
              "zTeaMWOGmfkAAIANhVRAYmNjAz8vW7ZMvXr1kiRdeumlcrkMTSMBAAAIrYBERUVp165dqqio0D/+\n",
              "8Q/dcsstgc9OnDhhWjgAAGBPIR2+eOqpp5SRkSGXy6UuXbooLS1N0umjISkpKWbmAwAANhRSAenT\n",
              "p486duyo/fv3q1WrVoHlKSkpmjp1qmnhAACAPYU8gSM5OVnJyclBy/7jP/6j1gMBAAD7M3wfEAAA\n",
              "gPNFAQEAAJYzVEAqKyvPWHbgwIFaCwMAkaiyyqefKjyqrOLp4ogchm7ice+99+qDDz6Qw+GQJB0+\n",
              "fFi33XabVq9ebUo4ALCzg+UeTV9eqgXr9utklU+x0U71bJmsnA6NdVm8O9zxAFMZOgJy3XXXafTo\n",
              "0ZKk8vJy9erVS4888shZ1/d4PBo1apSaNm2qG264QTk5OeeXFgBs4mC5R/85c7VmrNih4x6vXFEO\n",
              "Hfd4NX3Fdo16b7V+qvCEOyJgKkNHQMaPH6+BAwfq+eef18KFCzVw4EA99NBDZ13/ySeflMPh0ObN\n",
              "m+VwOLRv377zDgwAdjB9ealKDpSrQbxbLufpfwte4pa8vmqVHCjX9G+2a0z3tDCnBMwTUgE5duxY\n",
              "4OeJEyeqd+/e6tq1q+6//34dO3ZMiYmJZ/zO8ePHVVhYqF27dgVO2fzrZbw/83g88niC277Px7lQ\n",
              "APZUWeXTgnX7FeNyBsrHz1zOKLldTn22fp9GdL5WMdHOMKUEzBXSKZikpCRdeumlSkpK0pVXXqmV\n",
              "K1fqpZdeCiyvyZYtW1SvXj298MILatu2rW666SYtWrSoxnXz8/NVt27doFdRUdG57xUAXMAqPF6d\n",
              "rPKpjqvm/wW7XVE6ecqnCo/X4mSAdUIqINXV1fL5fEH//fl1tiMVXq9X27dvV/PmzbVy5Uq99tpr\n",
              "GjBggPbv33/Gunl5eSorKwt6ZWZmnt+eAcAFKt7tUmy0U6e81TV+7vFWK7aOU/FuHvYJ+zI0CXXn\n",
              "zp06deqUJOmf//ynJk2apPLy8hrXbdSokaKiojRo0CBJUkZGhq655hp9//33Z6zrdruVmJgY9HI6\n",
              "OewIwJ5iop26veUVqvT65PUFlxCvr1oer089WyRz+gW2ZqiAZGdnq7q6Wrt379a9996rf/7zn3rg\n",
              "gQdqXPeyyy7Trbfeqr///e+SpG3btmnbtm26/vrrzz81AFzkftshRamXJ+hghUeHj5/ScY9Xh4+f\n",
              "0sEKj5penqCcDo3DHREwleE7ocbExOjTTz/V8OHDNXPmTG3evPms606ePFkvv/yybrjhBt11112a\n",
              "MmWKrrrqqvMKDAB2cFm8W5Puy1BO+8aKj3HJW+1XfIxLOe0b6/X7MrgPCGzP0AnGn69WWbhwocaM\n",
              "GfNv12/SpIkWL158rtkAwNYui3drTPc0jeh8rSo8XsW7XZx2QcQwVEAGDhyo5ORkpaWlqWPHjtq7\n",
              "d6/i4uLMygYAESEm2knxQMQxdArm6aef1rZt2/TNN9/I4XAoISFB77//vlnZAACATYV0BOTHH39U\n",
              "06ZNtXbt2ho/Z14HAAAwIqQC8thjj+mTTz5Rdnb2GZ85HA5t3bq11oMBAAD7CqmAfPLJJ5JOX0oL\n",
              "AABwvgxfhvtL+/fv16OPPlpbWQAAQIQIqYAcPHhQo0aN0p133qkZM2bo1KlTevLJJ5WamqqKigqz\n",
              "MwIAAJsJ6RTMww8/LLfbrTvuuEMzZ87UX//6V1VWVmrp0qVKT083OSIAALCbkArI5s2btWHDBknS\n",
              "/fffrwYNGmjHjh1KSkoyMxsAALCpkE7BxMTEBH52u91KTU2lfAAAgHMW0hGQXbt2KTc396zvCwoK\n",
              "aj8ZAACwrZAKyCOPPPKr7wEAAIwIqYD84Q9/MDsHAACIIOd1HxAAAIBzQQEBAACWo4AAAADLUUAA\n",
              "AIDlQpqE+rNrrrlGDocjaFlSUpI6dOig5557TvXq1avVcAAAwJ4MFZCcnBzt3r1bDz74oCTprbfe\n",
              "UlJSkvx+v0aMGKHZs2ebEhIAANiLoQLy+eefa8WKFYH3HTt2VPv27VVUVKTmzZvXejgAAGBPhuaA\n",
              "HD58WCdOnAi8P3HihI4ePSop+HbtAAAAv8bQEZD77rtPWVlZ6tevnyRpzpw5GjhwoCoqKpSSkmJG\n",
              "PgAAYEMOv9/vN/ILn376qb788ktJUteuXXXHHXeYEiw3N5dnzAAAYFOGjoBI0h133GFa6QAAAJHB\n",
              "UAEpLS3V+PHjtWXLFnm93sDyn4+IAAAAhMJQAenfv79uvfVWjRo1Sk6n06xMAADA5gwVkMrKSuXn\n",
              "55uVBQAARAhDl+G2bNlSO3bsMCsLAACIEIaOgBw8eFCtW7dWhw4dgu77MXfu3FoPBgAA7Mvwrdhz\n",
              "cnLMygIAACKEoQIyZMgQs3IAAIAIElIBeeWVVzR27Fg99thjZzwNVxI3DAMAAIaEVEDi4+MlSUlJ\n",
              "SWZmAQAAESKkAjJ8+HD5fD5deumlGj16dMgbT0lJkdvtVmxsrCQpLy9PAwYMOLekAADANkKeA+J0\n",
              "OvXuu+8aKiCSNGvWLKWnpxvNBQAAbMzQfUC6deumGTNm1HoIj8ejY8eOBb18Pl+tjwMAAC4Mhp6G\n",
              "e+mll6qsrEx16tRRXFyc/H6/HA6HDh8+XOP6KSkpSkxMlN/vV2Zmpl588UU1aNDgjPX++Mc/aty4\n",
              "cUHLsrKy9M033xjcHQAAcDEwVEC2b99e4/LGjRvXuHzHjh1q1KiRqqqq9PTTT+v777/X/Pnzz1jP\n",
              "4/HI4/EELXvmmWc0ceLEUKMBAICLiKH7gDRu3Fh79+7Vpk2b1LlzZ3m9XlVXV591/UaNGkmSoqOj\n",
              "NWbMGKWlpdW4ntvtltvtDlrGw+4AALAvQ3NA3n//fWVlZWno0KGSpPXr1+uuu+6qcd3jx4/r6NGj\n",
              "gfczZ85URkbGueYEAAA2YugISH5+vlatWqVu3bpJklq3bn3W0zL79+/XPffcI5/PJ7/fryZNmujd\n",
              "d989/8QAAOCiZ6iAOJ1O1a9fP2hZnTp1aly3SZMmWr169bknAwAAtmXoFExCQoL2798fuB37okWL\n",
              "VK9ePVOCAQAA+zJ0BGT8+PHq2bOntm7dqt/85jfatm2bPv30U7OyAQAAmzJUQNq2bavFixdr2bJl\n",
              "8vv96tixI8+HAQAAhhkqIJJUt25d9ezZ04wsAAAgQoRUQKKiogLzPmrCbdMBAIARIRWQ8vJy+f1+\n",
              "vfrqqzp58qRGjhwpSZo8eXLgSbcAAAChMnQr9jZt2ui77777t8tqQ25urgoKCmp9uwAAIPwMXYZb\n",
              "Xl6uAwcOBN4fOHBA5eXltR4KAADYm6FJqGPHjlXr1q3Vq1cvSdKCBQv0xz/+0YxcAADAxgwVkOHD\n",
              "h6tTp05avHixpNOnSVq0aGFKMAAAYF+GL8Nt2bKlWrZsaUYWAAAQIQwVkFWrVumpp57S1q1b5fV6\n",
              "A8u3bt1a68EAAIB9GSogQ4YM0ahRo9ShQwc5nU6zMgEAAJsz/DTc4cOHm5UFAABECEOX4Xbq1Ekr\n",
              "V640KwsAAIgQho6AfPXVV3rjjTeUmpqqmJiYwPJVq1bVejAAAGBfhgrIpEmTzMoBAAAiiKECcsst\n",
              "t5iVAwAARBBDBaRLly41PhX3yy+/rLVAAADA/gwVkMcffzzwc2Vlpd577z2lpaXVeigAAGBvhgrI\n",
              "HXfcEfQ+OztbXbt2rdVAAADA/gxdhvuvfD6f9uzZU1tZAABAhDB0BOTuu+8OzAHx+Xxau3Zt4Mm4\n",
              "AAAAoTJUQO66667/+0WXS0899ZTat29f25kAAIDNGX4WDAAAwPkyVEAkafbs2SouLlZlZWVgWUFB\n",
              "Qa2GAgAA9mZoEuro0aM1bdo0vf3223I4HHr//fdVVlZmVjYAAGBThgrI4sWL9eGHH6pBgwZ65ZVX\n",
              "VFRUpF27dpmVDQAA2JShAhITE6OoqCg5HA5VVVUpOTmZy3ABAIBhhuaAJCQk6MSJE/rNb36jnJwc\n",
              "JScnKy4uzqxsAADApgwdAZk5c6ZcLpdefvlltWrVStHR0ZozZ45Z2QAAgE0ZOgJyxRVXBH7+r//6\n",
              "L0mnnw8zYcKE2k0FAABs7bxuxS6dviwXAADAiPMuIH6//9+u89Zbb8nhcGjevHnnOxwAALCB8y4g\n",
              "Pz8b5mxKS0v1xhtvKCsr63yHAgAANhHSHJBfPoTul/x+vw4dOnTW36uurtZDDz2k119/XWPHjj3r\n",
              "eh6PRx6PJ2iZz+cLJRoAALgIhVRAfvkQOiOfFRQUqFOnTmrTps2vbj8/P1/jxo0LWsYREwAA7Cuk\n",
              "AnIuD6Fbt26d5syZo6+++urfrpuXl6fc3NygZc8884zhMQEAwMXB8MPoQrV06VKVlpaqadOmkqR9\n",
              "+/Zp2LBh2rt3r0aOHBm0rtvtltvtDlrmdDrNigYAAMLsvCehns3IkSO1d+9elZaWqrS0VFlZWZo6\n",
              "deoZ5QMAAEQe0woIAADA2RgqIFOmTNGJEyfOaaAlS5b86oRVAAAQOQwVkK+++kpNmjTRY489ppKS\n",
              "ErMyAQAAmzNUQGbMmKE1a9aofv36uvXWW9WzZ0/Nnz/frGwAAMCmDM8BueKKK/T000/rnXfe0fr1\n",
              "65WTk6NmzZpp0aJFZuQDAAA2ZOgy3MrKSk2fPl3/8z//o7i4OL388svq27evVq9erb59+6q0tNSk\n",
              "mAAAwE4MFZCUlBR1795dU6dOVbt27QLL27Ztq+7du9d6OAAAYE+GCsjq1at15ZVX1vjZG2+8USuB\n",
              "AACA/RmaAxIXF6dRo0bpzjvvlCRt2LBBM2fONCUYAACwL0MFZPjw4UpOTta2bdskSddcc43Gjx9v\n",
              "SjAAAGBfhgrI5s2b9fTTTys6OlqSFBsbK7/fb0owAABgX4YKSJ06dYLenzx5kgICAAAMM1RAunTp\n",
              "oueff16VlZX64osv1LdvX/Xp08esbAAAwKYMFZDnnntOUVFRSkxM1FNPPaVOnTrpmWeeMSsbAACw\n",
              "KYf/Aj2Hkpubq4KCgnDHAAAAJjB0HxCv16s5c+Zoy5Yt8nq9geXPPvtsrQcDAAD2ZaiA3Hvvvdq3\n",
              "b58yMzPldDrNygQAAGzOUAH5/vvv9cMPP8jhcJiVBwAARABDk1CvvvpqnTp1yqwsAAAgQhg6ApKa\n",
              "mqrOnTvr7rvvVkxMTGD56NGjaz0YAACwL0MFxOPxqFmzZtq4cWNgGadjAACAUYYKyFtvvWVWDgAA\n",
              "EEEMX4b73//931q4cKEk6bbbbtOjjz4ql8vQZgAAQIQz1Bxyc3O1ZcsW/e53v5PD4dCbb76p7du3\n",
              "67XXXjMrHwAAsCFDBWTJkiUqLi5WVNTpi2fuuOMO3XjjjaYEAwAA9mXoMly/36/q6uqg9xfondwB\n",
              "AMAFzNARkNtvv109evTQ0KFDJUnvvvuuevbsaUYuAABgY4YKyPjx4zVlyhR99NFHkqS+fftq2LBh\n",
              "pgQDAAD2ZaiAREVFaeTIkRo5cqRZeQAAQAQIqYDk5ub+6ucFBQW1EgYAAESGkArIq6++qnbt2un2\n",
              "228PXAEDAABwrkIqIIsWLdL//u//6r333lP//v31wAMP6NprrzU7GwAAsKmQDmd06dJF06ZN03ff\n",
              "fadGjRpp0KBB6tKli1asWGF2PgAAYEOGzqckJiYqOztb2dnZ+uGHH/TDDz+YlQsAANhYSAXE5/Pp\n",
              "gw8+UO/evdW9e3c5nU6tWrVKQ4YMMTsfAACwoZDmgFx11VVq1KiR7r//fnXq1EmSdPDgQR08eFCS\n",
              "1KpVqxp/r0ePHtq3b5+ioqKUkJCg1157TRkZGbUUHQAAXKxCKiAxMTE6ePCgXnrpJTkcjqDbrzsc\n",
              "Dm3durXG35s9e7aSkpIkSR988IGGDh2qNWvWnH9qAABwUQupgJSWlp7Txn8uH5JUVlYmh8NR43oe\n",
              "j0cejydomc/nO6cxAQDAhc/QnVDPxeDBg7V48WJJ0vz582tcJz8/X+PGjQtalpWVZXY0AAAQJg6/\n",
              "RY+zfeeddzRr1qwaS0hNR0CeeeYZTZw40YpoAADAYpbd1nTIkCFavHixDh06dMZnbrdbiYmJQS+n\n",
              "02lVNAAAYDHTCsjRo0e1Z8+ewPt58+apfv36qlevnllDAgCAi4Rpc0DKysrUr18/nTx5UlFRUWrQ\n",
              "oIE++eSTs05EBQAAkcO0AtK4cWMVFRWZtXkAAHAR49G2AADAchQQAABgOQoIAACwHAUEAABYjgIC\n",
              "AAAsRwEBAACWo4AAAADLUUAAAIDlKCAAAMByFBAAAGA5CggAALAcBQQAAFiOAgIAACxHAQEAAJaj\n",
              "gAAAAMtRQAAAgOUoIAAAwHIUEAAAYDkKCAAAsBwFBAAAWI4CAgAALEcBAQAAlqOAAAAAy1FAAACA\n",
              "5SggAADAchQQAABgOQoIAACwHAUEAABYjgICAAAsRwEBAACWo4AAAADLUUAAAIDlKCAAAMByphWQ\n",
              "yspK3XXXXUpLS1Pr1q3VvXt3lZSUmDUcAAC4iJh6BGTYsGHatGmT1qxZo+zsbD300ENmDgcAAC4S\n",
              "phWQmJgY9erVSw6HQ5KUlZWl0tLSGtf1eDw6duxY0Mvn85kVDQAAhJllc0AmTpyo7OzsGj/Lz89X\n",
              "3bp1g15FRUVWRQMAABazpIC88MILKikpUX5+fo2f5+XlqaysLOiVmZlpRTQAABAGLrMHmDBhgubO\n",
              "nasvvvhCcXFxNa7jdrvldruDljmdTrOjAQCAMDG1gBQUFGjmzJn64osvlJSUZOZQAADgImJaAdm1\n",
              "a5fGjh2rJk2aqEuXLpJOH+lYsWKFWUMCAICLhGkFpGHDhvL7/WZtHgAAXMS4EyoAALAcBQQAAFiO\n",
              "AgIAACxHAQEAAJajgAAAAMtRQAAAgOUoIAAAwHIUEAAAYDkKCAAAsBwFBAAAWI4CAgAALEcBAQAA\n",
              "lqOAAAAAy1FAAACA5SggAADAchQQAABgOQoIAACwHAUEAABYjgICAAAsRwEBAACWo4AAAADLUUAA\n",
              "AIDlKCAAAMByFBAAAGA5CggAALAcBQQAAFiOAgIAACxHAQEAAJajgAAAAMtRQAAAgOUoIAAAwHIR\n",
              "V0Aqq3z6qcKjyipfuKOcNzvtCwAgsrjCHcAqB8s9mr68VAvW7dfJKp9io53q2TJZOR0a67J4d7jj\n",
              "GWKnfQEARCZTj4CMHj1aKSkpcjgcKi4uNnOoX3Ww3KP/nLlaM1bs0HGPV64oh457vJq+YrtGvbda\n",
              "P1V4wpbNKDvtCwAgcplaQPr27auvv/5ajRs3NnOYf2v68lKVHChXg3i3Lr2kji5xu3TpJXXUIN6t\n",
              "kgPlmv7N9rDmM8JO+wIAiFymFpCbb75ZDRs2/LfreTweHTt2LOjl89XOvIbKKp8WrNuvGJdTLmfw\n",
              "7rqcUXK7nPps/b6LYh6FnfYFABDZLohJqPn5+apbt27Qq6ioqFa2XeHx6mSVT3VcNe+q2xWlk6d8\n",
              "qvB4a2U8M9lpXwAAke2CKCB5eXkqKysLemVmZtbKtuPdLsVGO3XKW13j5x5vtWLrOBXvvvDn49pp\n",
              "XwAAke2CKCBut1uJiYlBL6fTWSvbjol26vaWV6jS65PXF/yH2+urlsfrU88WyYqJrp3xzGSnfQEA\n",
              "RLaI+KfybzukaMW2Iyo5UC63yym3K0oe7+k/2E0vT1BOh/BOkjXCTvsCAIhcph4BGT58uBo2bKhd\n",
              "u3bptttuU2pqqpnDndVl8W5Nui9DOe0bKz7GJW+1X/ExLuW0b6zX78u4qO6dYad9AQBELoff7/eH\n",
              "O0RNcnNzVVBQUOvbraw6PUkz3u266E9V2GlfAACRJSJOwfxSTLTTNn+s7bQvAIDIckFMQgUAAJGF\n",
              "AgIAACxHAQEAAJajgAAAAMtRQAAAgOUoIAAAwHIUEAAAYDkKCAAAsBwFBAAAWI4CAgAALHfBPgum\n",
              "T58+SklJMfQ7Pp9PRUVFyszMlNPJLcqtwHduLb5v6/GdW4vv23q1/Z03btxYjz766L9d74ItIOfi\n",
              "2LFjqlu3rsrKypSYmBjuOBGB79xafN/W4zu3Ft+39cL1nXMKBgAAWI4CAgAALEcBAQAAlrNVAXG7\n",
              "3frDH/4gt9sd7igRg+/cWnzf1uM7txbft/XC9Z3bahIqAAC4ONjqCAgAALg4UEAAAIDlKCAAAMBy\n",
              "tikgP/74ozp27Ki0tDS1a9dO69evD3ckWxs9erRSUlLkcDhUXFwc7ji2V1lZqbvuuktpaWlq3bq1\n",
              "unfvrpKSknDHsr0ePXqoVatWSk9P10033aTVq1eHO1JEeOutt+RwODRv3rxwR7G9lJQUXXfddUpP\n",
              "T1d6erpmzZpl2di2KSDDhw/XsGHDtHnzZj3xxBMaOnRouCPZWt++ffX111+rcePG4Y4SMYYNG6ZN\n",
              "mzZpzZo1ys7O1kMPPRTuSLY3e/ZsrV27VsXFxcrNzeX/KxYoLS3VG2+8oaysrHBHiRizZs1ScXGx\n",
              "iouLNWDAAMvGtUUBOXDggFauXKmcnBxJ0j333KOdO3fyL0QT3XzzzWrYsGG4Y0SMmJgY9erVSw6H\n",
              "Q5KUlZWl0tLS8IaKAElJSYGfy8rKAt8/zFFdXa2HHnpIr7/+OpfhRgBXuAPUhp07d+rKK6+Uy3V6\n",
              "dxwOhxo1aqQdO3YoNTU1zOmA2jdx4kRlZ2eHO0ZEGDx4sBYvXixJmj9/fpjT2FtBQYE6deqkNm3a\n",
              "hDtKRBk8eLD8fr8yMzP14osvqkGDBpaMa4sjIEAkeeGFF1RSUqL8/PxwR4kI7777rnbu3Kk///nP\n",
              "euKJJ8Idx7bWrVunOXPm6Omnnw53lIjy1Vdfae3atVq1apUuu+wyDRkyxLKxbXEE5Oqrr9bevXvl\n",
              "9Xrlcrnk9/u1Y8cONWrUKNzRgFo1YcIEzZ07V1988YXi4uLCHSeiDBkyRCNGjNChQ4dUv379cMex\n",
              "naVLl6q0tFRNmzaVJO3bt0/Dhg3T3r17NXLkyDCns6+f/05GR0drzJgxSktLs2xsWxwBufzyy3Xj\n",
              "jTdq+vTpkqQ5c+aoYcOGnH6BrRQUFGjmzJlauHBh0NwEmOPo0aPas2dP4P28efNUv3591atXL4yp\n",
              "7GvkyJHau3evSktLVVpaqqysLE2dOpXyYaLjx4/r6NGjgfczZ85URkaGZePb4giIJE2ZMkVDhw7V\n",
              "Cy+8oMTERL311lvhjmRrw4cP16effqp9+/bptttuU0JCApN+TbRr1y6NHTtWTZo0UZcuXSSdfn7D\n",
              "ihUrwpzMvsrKytSvXz+dPHlSUVFRatCggT755BMmosI29u/fr3vuuUc+n09+v19NmjTRu+++a9n4\n",
              "PAsGAABYzhanYAAAwMWFAgIAACxHAQEAAJajgAA2Vl5ervj4eD344IPhjhJkyZIlWrBgwa+u88kn\n",
              "n2jEiBGB9dPT04M+Ly0tDboaaPLkyYHntjRr1kyDBg0KfPbz8y5at26t1NRUZWdna9myZUFjDRs2\n",
              "7Px3DEDIKCCAjc2aNUtt2rTR3LlzVVFREe44AaEUkLy8POXl5YW0vZUrV+qll17SkiVLVFxcrI0b\n",
              "N2rs2LFB68yaNUtr1qxRSUmJhgwZol69egWuIurdu7e+++47/fjjj+e2QwAMo4AANlZYWKgnnnhC\n",
              "N998c9BTLt9++21169ZNAwcOVPPmzdWxY0dt2LBBd999t66//nr16NEjUFgqKir0wAMPqGXLlmrZ\n",
              "sqXGjRsX2E7nzp2Dnljat29fvf3225KkoUOHavjw4br11luVlpamPn366NSpUyouLtbkyZM1Y8YM\n",
              "paen609/+tMZuZcuXaqkpKSQH3a4a9cuJSQkKCEhQdLpxzHceOONZ12/T58+GjFihCZMmBBY1r9/\n",
              "f7355pshjQfYzbk84fzIkSMaNGiQ0tLS1KJFCz355JOGxqSAADa1YcMG7dy5U7fddpsefPBBFRYW\n",
              "Bn3+7bffavz48dqwYYOuvfZa3XnnnZo8ebI2btyoOnXq6J133pEkPffcc/J4PFq7dq1WrFihefPm\n",
              "hfzI7uLiYn388cfauHGj9u/frzlz5ig9PV0jRozQoEGDVFxcrGefffaM31uyZInat28f8r726NFD\n",
              "CQkJatSokQYMGKBJkybpyJEjv/o77du31/r16wPvO3TooEWLFoU8JmAn5/KE8wceeEAZGRnavHmz\n",
              "1q9frzFjxhgakwIC2FRhYaEGDx4sp9OpXr16adu2bdq4cWPg8w4dOgRuw9y2bVu1a9dOV1xxhSSp\n",
              "Xbt2gdMRX3zxhR5++GFFRUXpkksu0eDBg7Vw4cKQMtx9992Ki4uT0+lUZmamtmzZEtLv7dq1K5BF\n",
              "0llv/vXz8ri4OC1dulTz589Xp06dNHfuXLVq1UqHDx8+6xj/eguk5ORk7dq1K6R8gN2c7Qnn3377\n",
              "rbp27aq2bdsqIyNDf/vb3yRJJSUlWrlypXJzcwPrJicnGxqTAgLYUFVVlaZNm6Z33nlHKSkpSk1N\n",
              "1YkTJ4KOgsTExAR+djqdZ7z3er01bvuXZcDlcsnn8wXeV1ZWBq0b6jb/VVxcXNC2GjRooEOHDgWt\n",
              "89NPP+nyyy8PypWRkaHRo0dr0aJFio+P15IlS846xrfffquWLVsGZY+NjQ0pHxAJjh49qmHDhmnG\n",
              "jBlauXKlFi5cqLFjx2r37t3asGGDGjZsqJEjR6pNmzbq0aOHVq9ebWj7FBDAhj766CM1adJEu3fv\n",
              "DjxbY/ny5Zo2bZqqqqoMbatbt24qLCyU3+/X8ePHNW3aNPXo0UOSlJqaGpjIuW3bNn399dchbTMx\n",
              "MVFlZWVn/bxVq1batGlT4H3Tpk0VHR2t+fPnS5Kqq6s1ZcqUQI4ffvhBa9euDay/c+dOHTx4UE2a\n",
              "NKlx+x9++KH++te/Bk1U3bhxo1q3bh1SfiASLFu2TFu3blXPnj2Vnp6ubt26SZI2bdokr9eroqIi\n",
              "3Xvvvfruu+/02GOPqXfv3ob+/2KbZ8EA+D+FhYVBl6FK0vXXX6+rrrpKH3/8saFtPfPMMxo9erRu\n",
              "uOEGSVK/fv3Uv39/SdLvf/97DRgwQDfccINatGgR8ryNu+++W9OmTVN6err69OlzxjyQ3r17609/\n",
              "+pN8Pp+cTqeio6P1wQcfKDc3V0899ZSqq6vVvn17Pf/885KkEydO6LHHHtO+ffsUGxsrv9+vF198\n",
              "MejS3QEDBigmJkbHjx9X8+bNNX/+/KC8CxYsUN++fQ19N4Cd+f1+tWjRIuiS9Z+tXLlSV111VeDZ\n",
              "VD179tSpU6e0ffv2kB8Ey7NgAFyQHnnkEXXu3Fn9+vUzfayffvpJXbt21cqVK1WnTh3TxwMuVCkp\n",
              "KZo3b57S09N15MgRNW/eXNOmTQsc/SguLlbz5s0VHR2tG264Qe+9955atWqloqIi9erVS7t375bb\n",
              "7Q5pLAoIgAvSoUOH9NlnnyknJ8f0sVasWCGfz6eOHTuaPhZwIfrlE87r168feML5qlWr9Pjjj+vQ\n",
              "oUOqqqpSo0aNNG/ePMXExOi7777T7373O508eVJut1sTJkzQLbfcEvKYFBAAAGA5JqECAADLUUAA\n",
              "AIDlKCAAAMByFBAAAGA5CggAALAcBQQAAFiOAgIAACxHAQEAAJajgAAAAMtRQAAAgOUoIAAAwHL/\n",
              "D7aoZIvSFIZnAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-3adf73c9-28ab-4e7e-acd1-cb7213477054\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-3adf73c9-28ab-4e7e-acd1-cb7213477054\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_8.plot(kind='scatter', x='Money Laundering Risk Score', y='Shell Companies Involved', s=32, alpha=.8)\n",
              "plt.gca().spines[['top', 'right',]].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-8294dd8f-f2b7-4cd1-933e-6d59f9b6e0f0\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAiAAAAGkCAYAAAAIQJ5PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAuIklEQVR4nO3deXQUZcL+/avTHTpkQVZBRRIRopCQhc1ERBZBNg8gCgoTtlEB\n",
              "56AwMM8g6ozLg2QYRlBABxAENYg+CqOMDCgiiCObEBbhMGggCSoQQBASTJpUp98/8rNfMyympKua\n",
              "dL6fc/qcpKrSdZUIfeWuu6ocPp/PJwAAABuFBTsAAACofiggAADAdhQQAABgOwoIAACwHQUEAADY\n",
              "jgICAABsRwEBAAC2o4AAAADbXbEF5MUXXwx2BAAAYJErtoDk5+cHOwIAALDIFVtAAABA6KKAAAAA\n",
              "21FAAACA7SggAADAdhQQAABgOwoIAACwHQUEAADYjgICAABsRwEBAAC2o4AAAADbVbsCUlLq1Yki\n",
              "j0pKvcGOAgBAteUKdgC7HC/0KGtznlbvKVBxqVc1w53qldhIGemxqh/tDnY8AACqlWoxAnK80KNH\n",
              "lu7Qki2HdNZjyBXm0FmPoawt+Rr75g6dKPIEOyIAANVKtSggWZvzlHOsUA2i3aoTVUNRbpfqRNVQ\n",
              "g2i3co4VKmsTT94FAMBOIV9ASkq9Wr2nQBEup1zOiofrcobJ7XJq1d6jzAkBAMBGIV9AijyGiku9\n",
              "quG68KG6XWEqPudVkcewORkAANVXyBeQaLdLNcOdOmeUXXC9xyhTzRpORburzXxcAACCLuQLSES4\n",
              "Uz0TG6rE8MrwViwhhrdMHsOrXgmNFBHuDFJCAACqn2rxa//Q9DhtyT2lnGOFcruccrvC5DHKy0fz\n",
              "q2OUkR4b7IgAAFQrIT8CIkn1o92aMyRVGbfEKjrCJaPMp+gIlzJuidXsIancBwQAAJtVixEQqbyE\n",
              "jO8erzGdb1SRx1C028VpFwAAgqTaFJCfRIQ7KR4AAARZtTgFAwAAriwUEAAAYDsKCAAAsB0FBAAA\n",
              "2I4CAgAAbEcBAQAAtqOAAAAA21FAAACA7SggAADAdhQQAABgOwoIAACwHQUEAADYjgICAABsRwEB\n",
              "AAC2o4AAAADbUUAAAIDtKCAAAMB2FBAAAGA7CggAALAdBQQAANiOAgIAAGxHAQEAALajgAAAANtR\n",
              "QAAAgO0oIAAAwHYUEAAAYDsKCAAAsJ2lBeRf//qXWrdurZSUFCUmJuq1116zcncAAKCKcFn1xj6f\n",
              "TxkZGVq/fr2SkpKUl5enm2++WQMGDFBMTIxVuwUAAFWAZQVEkhwOh3744QdJ0pkzZ1SvXj253e7z\n",
              "tvN4PPJ4PBWWeb1eK6MBAIAgsqyAOBwOvf322xowYICioqJ06tQpLV++XDVq1Dhv28zMTD3zzDMV\n",
              "lqWlpVkVDQAABJllc0AMw9CUKVO0fPly5efna+3atRo6dKhOnDhx3raTJ0/W6dOnK7zat29vVTQA\n",
              "ABBklhWQnTt36vDhw7r99tslSe3atVPjxo21Y8eO87Z1u92qVatWhZfT6bQqGgAACDLLCsj111+v\n",
              "I0eOaN++fZKknJwcHThwQDfddJNVuwQAAFWEZXNAGjZsqPnz52vQoEEKCwtTWVmZ5syZoyZNmli1\n",
              "SwAAUEVYehXM4MGDNXjwYCt3AQAAqiDuhAoAAGxHAQEAALajgAAAANtRQAAAgO0oIAAAwHYUEAAA\n",
              "YDsKCAAAsB0FBAAA2I4CAgAAbEcBAQAAtqOAAAAA21FAAACA7SggAADAdhQQAABgOwoIAACwHQUE\n",
              "AADYjgICAABsRwEBAAC2o4AAAADbUUAAAIDtKCAAAMB2FBAAAGA7CggAALAdBQQAANiOAgIAAGxH\n",
              "AQEAALajgAAAANtRQAAAgO0oIAAAwHYUEAAAYDsKCAAAsB0FBAAA2I4CAgAAbEcBAQAAtqOAAAAA\n",
              "21FAAACA7SggAADAdhQQAABgOwoIAACwHQUEAADYjgICAABsRwEBAAC2o4AAAADbUUAAAIDtKCAA\n",
              "AMB2FBAAAGA7CggAALAdBQQAANiOAgIAAGxHAQEAALajgAAAANtRQAAAgO0oIAAAwHYUEAAAYDsK\n",
              "CAAAsB0FBAAA2M5VmY3CwsLkcDguut7r9QYsEAAACH2VKiCFhYXy+Xx64YUXVFxcrIcffliSNHfu\n",
              "XNWsWdPSgAAAIPQ4fD6fr7Ibt2nTRtu3b//FZYEwYcIEzZgxI+DvCwAAgs/UHJDCwkIdO3bM//2x\n",
              "Y8dUWFgY8FAAACC0VeoUzE8mTpyo5ORk9e7dW5K0evVqPf3001bkAgAAIcxUARk9erQ6dOigdevW\n",
              "SSo/TZKQkGBJMAAAELpMFRBJqlevnlq1aqXOnTvLMAydO3dONWrUsCIbAAAIUabmgLz77rtKS0vT\n",
              "yJEjJUl79+5V//79rcgFAABCmKkCkpmZqezsbNWuXVuSlJycrPz8fCtyAQCAEGaqgDidTtWrV6/C\n",
              "Mk6/AAAAs0wVkJiYGBUUFPjvirp27VrVrVv3ott7PB6NHTtWzZs3V6tWrZSRkXF5aQEAQEgwNQl1\n",
              "2rRp6tWrlw4ePKjbbrtNubm5Wrly5UW3f+yxx+RwOPTVV1/J4XDo6NGjlx0YAABUfaYKSNu2bbVu\n",
              "3Tpt3LhRPp9Pt956q38+yH87e/asFi5cqG+//dY/YtKoUaMLbuvxeOTxeCos4/kyAACELlOnYGbP\n",
              "ni2fz6devXqpd+/eFy0fknTgwAHVrVtXU6dOVdu2bdWxY0etXbv2gttmZmbqqquuqvDaunWrqQMB\n",
              "AABVh6kCkp2drRtvvFGDBg3SqlWrdKnHyBiGofz8fLVs2VLbtm3TrFmzdN9996mgoOC8bSdPnqzT\n",
              "p09XeLVv39780QAAgCrBVAFZtGiRDh06pF69emnatGmKjY3VE088ccFtmzRporCwMP3mN7+RJKWm\n",
              "puqGG27Ql19+ed62brdbtWrVqvByOp2/4nAAAEBVYKqASFJUVJRGjhypt956S71799Zf/vKXC25X\n",
              "v3593XHHHfrwww8lSbm5ucrNzVWLFi0uLzEAAKjyTBUQwzC0fPly3XXXXUpOTpbL5dKWLVsuuv3c\n",
              "uXM1ffp0tWrVSv3799e8efN03XXXXXZoAABQtZm6Cua6665TamqqRowYoWXLlsntdl9y+6ZNm/of\n",
              "XAcAAPATUwVk+/btaty4sVVZAABANVGpArJixQr/19nZ2eet79u3b+ASAQCAkFepAjJz5syLrnM4\n",
              "HBQQAABgSqUKCPM4AABAIJmaAyJJ77zzjtasWSNJ6tGjh+65556AhwIAAKHN1GW4zz77rDIzM9Wy\n",
              "ZUslJCQoMzNTU6ZMsSobAAAIUaZGQN59911t3rxZkZGRkqQHH3xQ6enpevLJJy0JBwAAQpOpERCf\n",
              "z+cvH1L5XVEv9TwYAACACzE1AtK+fXsNHTpUDz30kCRp4cKFPDQOAACYZmoEZNasWbr22ms1YcIE\n",
              "TZgwQddcc41mzZplVTYAABCiTI2AREREaNq0aVZlAQAA1YSpEZDGjRvrj3/8o/bt22dVHgAAUA2Y\n",
              "KiCbNm1SZGSk+vTpo1tuuUXz5s3TmTNnrMoGAABClKkCEhcXp6effloHDx7UtGnTtG7dOl1zzTVW\n",
              "ZQMAACHKVAH5yfbt2/XOO+/ok08+UZcuXQKdCQAAhDhTk1Cff/55LV68WF6vVyNHjtSuXbsYAQEA\n",
              "AKaZKiD79+/X/PnzlZ6eblUeAABQDZgqIPPnz7cqBwAAqEZMFZDs7Gw9/vjjOnjwoAzD8C8/ePBg\n",
              "wIMBAIDQZaqADB8+XGPHjlV6erqcTqdVmQAAQIgzVUCcTqdGjx5tVRYAAFBNmLoMt0OHDtq2bZtV\n",
              "WQAAQDVhagRkw4YNeuWVV9SsWTNFRET4l2dnZwc8GAAACF2mCsicOXOsygEAAKoRUwWkU6dOVuUA\n",
              "AADVSKUKyN133y2Hw3HR9cuXLw9YIAAAEPoqVUD69+9vcQwAAFCdVKqADB8+3OocAACgGvlVT8MF\n",
              "AAC4HBQQAABgOwoIAACwnakC8s033+jcuXOSpM8//1xz5sxRYWGhJcEAAEDoMlVA+vXrp7KyMn33\n",
              "3Xe6//779fnnn+u3v/2tVdkAAECIMn0KJiIiQitXrtTo0aO1dOlSffXVV1bkAgAAIcxUAfF4PPJ4\n",
              "PFqzZo26dOliVSYAABDiTBWQwYMHq1GjRjp06JBuvfVWHTlyRJGRkVZlAwAAIcpUAXnyySeVm5ur\n",
              "TZs2yeFwKCYmRu+++65V2QAAQIgyVUAMw9DChQs1duxYSVJBQYH2799vSTAAABC6TD0Nd+zYsfJ6\n",
              "vfr3v/8tSapXr57uu+8+bdu2zZJwAAAgNJkqIJs3b9bOnTuVmpoqSapdu7ZKS0stCQYAAEKXqVMw\n",
              "ERERFb73er0qKysLaCAAABD6TBWQpKQkZWVlqaysTDk5ORozZow6d+5sUTQAABCqTBWQGTNm6LPP\n",
              "PtPRo0fVoUMHhYWF6S9/+YtV2QAAQIgyNQckOjpa8+bN07x586zKAwAAqoFKFZBPP/1UnTp10ooV\n",
              "Ky64vm/fvgENBQAAQlulCkhWVpY6deqkmTNnnrfO4XBQQAAAgCmVKiCvvPKKJGndunWWhgEAANWD\n",
              "qTkgknTkyBHl5ubKMAz/sttvvz2goQAAQGgzVUCee+45TZ8+XU2bNpXT6ZRUfgpm69atloQDAACh\n",
              "yVQBefXVV3XgwAHVq1fPqjwAAKAaMHUfkIYNG1I+AADAZTM1AtK9e3eNHz9eQ4YMqXBb9qSkpIAH\n",
              "AwAAoctUAXn99dclSe+//75/mcPh0MGDBwObCgAAhDRTBSQ3N9eqHAAAoBoxfRmuJHk8Hnk8Hv/3\n",
              "tWrVClggAAAQ+kxNQt28ebNatGihyMhI1alTx/8CAAAww9QIyLhx47R48WKNGTNGGzZs0KxZsypM\n",
              "RgUAAKgMUyMgpaWluuWWW2QYhmJiYvTEE0/orbfesiobAAAIUaYKSHh4uCSpXr16ys7O1vHjx3X8\n",
              "+HFLggEAgNBl6hTM/fffr++//16PP/64OnXqpNLSUk2ZMsWqbAAAIESZKiC///3vJUl33nmnTp48\n",
              "qZKSEsXExFgSDAAAhC7Tl+G+9dZb+vjjj+VwONS9e3cNGjTIilwAACCEmZoD8oc//EHPP/+8WrVq\n",
              "pcTERD3//PP64x//aFU2AAAQokyNgKxYsUK7du1SzZo1JUmjRo1ScnKy/vrXv1oSDgAAhCZTIyC1\n",
              "a9eW2+32fx8eHs6NyAAAgGmmCkhaWpp69OihN954Q2+88Yb69Omj9PR0rVixQitWrLjozy1atEgO\n",
              "h0Pvvffe5eYFAAAhwNQpmC+//FKS9Oqrr/qX7dq1S7t27ZLD4VDfvn3P+5m8vDy98sorSktLu8yo\n",
              "AAAgVJgqIOvWrTP15mVlZXrwwQc1e/ZsTZw40dTPAgCA0GX6Mtx9+/bp66+/lmEY/mUDBgy44LYz\n",
              "ZsxQhw4d1KZNm0u+538/XVeSvF6v2WgAAKCKMFVAJk6cqCVLlqhly5ZyOp2SJIfDccECsmfPHi1b\n",
              "tkwbNmz4xffNzMzUM888U2EZp2wAAAhdDp/P56vsxs2aNdPu3bsVGRn5i9v+/e9/17PPPuu/aubo\n",
              "0aOqVauWnnnmGT388MMVtr3QCMif/vQnvfjii5WNBgAAqhBTIyDXX3+9IiIiKrXtww8/XKFodO7c\n",
              "WePHj1f//v3P29btdle4vFeSf4QFAACEHlMF5K9//asGDhyoHj16VCgiw4YNC3gwAAAQukwVkLlz\n",
              "52r37t3y+XwV5oBUpoCsX7/+VwUEAAChx1QBWb9+vfbv3y+Xy/TFMwAAAH6m7oTatGlTmZizCgAA\n",
              "cEGmhjKaNm2qzp07q1+/fhXmgDz66KMBDwYAAEKXqQJy7tw5xcfHa9++ff5lDocj4KEAAEBoM1VA\n",
              "Fi1aZFUOAABQjZgqIIZhaObMmVqzZo0kqUePHho3bhyTUgEAgCmmmsOECRN04MAB/e53v5PD4dCC\n",
              "BQuUn5+vWbNmWZUPAACEINOX4e7cuVNhYeUXz/Tp00etW7e2JBgAAAhdpi7D9fl8Kisrq/A9l+UC\n",
              "AACzTI2A9OzZU3feeadGjBghSXr99dfVq1cvK3IBAIAQZqqATJs2TfPnz9eKFSskSffee69GjRpl\n",
              "STAAABC6KlVAvF6vPB6PIiMjNWbMGI0ZM0aS9OOPP1oaDgAAhKZKzQGZPHmysrKyzlu+ZMkSTZ48\n",
              "OeChAABAaKtUAVm7dq0eeOCB85aPHDlSK1euDHgoAAAQ2ipVQMrKyuR0Os9b7nK5/JfkAgAAVFal\n",
              "2kNRUZE8Hs95yz0eD/NAAACAaZUqIHfddZceffRRGYbhX2YYhn7/+9+rT58+loUDAAChqVJXwUyZ\n",
              "MkV9+vRR06ZN1aZNG0lSdna2brjhBuaAAAAA0ypVQKKiorR+/Xp98skn2r59uyTpkUceUdeuXS0N\n",
              "BwAAQpOpG5F17dqV0gEAAC4bl7AAAADbUUAAAIDtKCAAAMB2lZoD8tPD5y6mb9++AQkDAACqh0oV\n",
              "kJkzZ150ncPhoIAAAABTKlVA1q1bZ3UOAABQjVSqgOzevfuS65OSkgISBgAAVA+VKiD9+vW76DqH\n",
              "w6GDBw8GLJDVSkq9KvIYina7FBF+/gP2AACA9SpVQHJzc63OYbnjhR5lbc7T6j0FKi71qma4U70S\n",
              "GykjPVb1o93BjgcAQLVi+jLcZcuWaerUqZKkw4cP68svvwx4qEA7XujRI0t3aMmWQzrrMeQKc+is\n",
              "x1DWlnyNfXOHThSd/6RfAABgHVMF5M9//rMWLFigxYsXSyo//TJ69GgrcgVU1uY85RwrVINot+pE\n",
              "1VCU26U6UTXUINqtnGOFytqUH+yIAABUK6YKyPvvv68PPvhAUVFRkqRrrrlGRUVFlgQLlJJSr1bv\n",
              "KVCEyymXs+LhupxhcrucWrX3qEpKvUFKCABA9WOqgNSsWVNOZ8WJmz6fL6CBAq3IY6i41Ksargsf\n",
              "qtsVpuJz5RNTAQCAPUwVkNjYWH322WdyOBwqLS3VM888o5SUFIuiBUa026Wa4U6dM8ouuN5jlKlm\n",
              "Daei3aYeDAwAAC6DqQIya9YsPffcc/ryyy8VFRWljRs3XvIuqVeCiHCneiY2VInhleGtWEIMb5k8\n",
              "hle9EhpxSS4AADYy9Wt/w4YNtXr1av3444/y+Xz+uSBXuqHpcdqSe0o5xwrldjnldoXJY5SXj+ZX\n",
              "xygjPTbYEQEAqFZMn3c4cuSIcnNzZRj//5yJ22+/PaChAq1+tFtzhqQqa1O+Vu09quJzXkVHuDQw\n",
              "oTH3AQEAIAhMFZDnnntO06dPV9OmTf2TUR0Oh7Zu3WpJuECqH+3W+O7xGtP5Ru6ECgBAkJkqIK++\n",
              "+qoOHDigevXqWZXHchHhTooHAABBZmoSasOGDat0+QAAAFcGU0/D7d69u8aPH68hQ4YoIiLCv56n\n",
              "4QIAADN+1dNw33//ff/XVe1puAAAIPiqzdNwAQDAlcP003Al6eDBg3rhhRf0wQcfBDoPAACoBipV\n",
              "QLp166adO3dKkg4fPqy2bdvqww8/1B/+8AdNmzbNynwAACAEVaqAfPfdd/5nvrz55pvq1KmTVq1a\n",
              "pU2bNmnJkiVW5gMAACGoUgWkZs2a/q83btyo3r17S5Lq1Kkjl4uHuAEAAHMqVUDCwsL07bffqqio\n",
              "SJ9++qk6derkX/fjjz9aFg4AAISmSg1fPP7440pNTZXL5VKXLl0UHx8vqXw0JC4uzsp8AAAgBFWq\n",
              "gAwYMEC33nqrCgoKKtx0LC4uTvPnz7csHAAACE2VnsDRqFEjNWrUqMKya6+9NuCBAABA6PtV9wEB\n",
              "AAC4HBQQAABgOwoIAACwHQUEAADYjgICAABsRwEBAAC2o4AAAADbUUAAAIDtKCAAAMB2FBAAAGA7\n",
              "CggAALAdBQQAANiOAgIAAGxHAQEAmFJS6tWJIo9KSr3BjoIqzBXsAACAquF4oUdZm/O0ek+Biku9\n",
              "qhnuVK/ERspIj1X9aHew46GKsWwEpKSkRP3791d8fLySk5PVvXt35eTkWLU7AICFjhd69MjSHVqy\n",
              "5ZDOegy5whw66zGUtSVfY9/coRNFnmBHRBVj6SmYUaNGaf/+/dq1a5f69eunBx980MrdAQAskrU5\n",
              "TznHCtUg2q06UTUU5XapTlQNNYh2K+dYobI25Qc7IqoYywpIRESEevfuLYfDIUlKS0tTXl7eBbf1\n",
              "eDw6c+ZMhZfXy7lFALgSlJR6tXpPgSJcTrmcFT82XM4wuV1Ordp7lDkhMMW2Sagvvvii+vXrd8F1\n",
              "mZmZuuqqqyq8tm7dalc0AMAlFHkMFZd6VcN14Y8MtytMxee8KvIYNidDVWZLAZk6dapycnKUmZl5\n",
              "wfWTJ0/W6dOnK7zat29vRzQAwC+IdrtUM9ypc0bZBdd7jDLVrOFUtJvrGlB5lheQv/3tb1q+fLlW\n",
              "rVqlyMjIC27jdrtVq1atCi+n02l1NABAJUSEO9UzsaFKDK8Mb8USYnjL5DG86pXQSBHh/LuNyrO0\n",
              "rs6YMUNLly7Vxx9/rNq1a1u5KwCAhYamx2lL7inlHCuU2+WU2xUmj1FePppfHaOM9NhgR0QVY1kB\n",
              "+fbbbzVx4kQ1bdpUXbp0kVQ+0rFlyxardgkAsEj9aLfmDElV1qZ8rdp7VMXnvIqOcGlgQmPuA4Jf\n",
              "xbIC0rhxY/l8PqveHgBgs/rRbo3vHq8xnW9UkcdQtNvFaRf8aswYAgCYEhHupHjgsvEsGAAAYDsK\n",
              "CAAAsB0FBAAA2I4CAgAAbEcBAQAAtqOAAAAA21FAAACA7SggAADAdhQQAABgOwoIAACwHQUEAADY\n",
              "jgICAABsRwEBAAC2o4AAAADbUUAAAIDtKCAAAMB2FBAAAGA7CggAALAdBQQAANiOAgIAAGxHAQEA\n",
              "ALajgAAAANtRQAAAgO0oIAAAwHYUEAAAYDsKCAAAsB0FBAACrKTUqxNFHpWUeoMdBbhiuYIdAABC\n",
              "xfFCj7I252n1ngIVl3pVM9ypXomNlJEeq/rR7mDHA64ojIAAQAAcL/TokaU7tGTLIZ31GHKFOXTW\n",
              "YyhrS77GvrlDJ4o8wY4IXFEoIAAQAFmb85RzrFANot2qE1VDUW6X6kTVUINot3KOFSprU36wIwJX\n",
              "FAoIAFymklKvVu8pUITLKZez4j+rLmeY3C6nVu09ypwQ4GcoIABwmYo8hopLvarhuvA/qW5XmIrP\n",
              "eVXkMWxOBly5KCAAcJmi3S7VDHfqnFF2wfUeo0w1azgV7WbeP/ATCggAXKaIcKd6JjZUieGV4a1Y\n",
              "QgxvmTyGV70SGiki3BmkhMCVhzoOAAEwND1OW3JPKedYodwup9yuMHmM8vLR/OoYZaTHBjsicEVh\n",
              "BAQAAqB+tFtzhqQq45ZYRUe4ZJT5FB3hUsYtsZo9JJX7gAD/hREQAAiQ+tFuje8erzGdb1SRx1C0\n",
              "28VpF+AiKCAAEGAR4U6KB/ALOAUDAABsRwEBAAC2o4AAAADbUUAAAIDtKCAAAMB2FBAAAGA7CggA\n",
              "ALAdBQQAANiOAgIAAGxHAQEAALajgAAAANtRQAAAgO0oIAAAwHYUEAAAYDsKCAAAsB0FBAAA2I4C\n",
              "AgAAbEcBAQAAtqOAAAAA21FAAACA7SggAADAdhQQAABgOwoIAACwHQUEAADYjgICAABsRwEBAKCa\n",
              "KSn16kSRRyWl3qBlcAVtzwAAwFbHCz3K2pyn1XsKVFzqVc1wp3olNlJGeqzqR7ttzWLpCMjXX3+t\n",
              "W2+9VfHx8WrXrp327t1r5e4AAMBFHC/06JGlO7RkyyGd9RhyhTl01mMoa0u+xr65QyeKPLbmsbSA\n",
              "jB49WqNGjdJXX32lSZMmacSIEVbuDgAAXETW5jzlHCtUg2i36kTVUJTbpTpRNdQg2q2cY4XK2pRv\n",
              "ax7LCsixY8e0bds2ZWRkSJLuueceffPNN8rJyTlvW4/HozNnzlR4eb3BOy8FAEAoKSn1avWeAkW4\n",
              "nHI5K370u5xhcrucWrX3qK1zQiwrIN98842uueYauVzl00wcDoeaNGmiQ4cOnbdtZmamrrrqqgqv\n",
              "rVu3WhUNAIBqpchjqLjUqxquC3/su11hKj7nVZHHsC3TFXEVzOTJk3X69OkKr/bt2wc7FgAAISHa\n",
              "7VLNcKfOGWUXXO8xylSzhlPRbvuuTbGsgFx//fU6cuSIDKO8Tfl8Ph06dEhNmjQ5b1u3261atWpV\n",
              "eDmdTquiAQBQrUSEO9UzsaFKDK8Mb8USYnjL5DG86pXQSBHh9n32WlZArr76arVu3VpZWVmSpGXL\n",
              "lqlx48Zq1qyZVbsEAAAXMTQ9Ts2ujtHxIo9Onj2nsx5DJ8+e0/Eij5pfHaOM9Fhb81g61jJv3jyN\n",
              "GDFCU6dOVa1atbRo0SIrdwcAAC6ifrRbc4akKmtTvlbtParic15FR7g0MKFxUO4D4vD5fD5b91hJ\n",
              "EyZM0IwZM4IdAwCAkFNSWj7hNNrtsvW0y89xJ1QAAKqZiHBn0IrHT66Iq2AAAED1QgEBAAC2o4AA\n",
              "AADbUUAAAIDtKCAAAMB2FBAAAGA7CggAALAdBQQAANiOAgIAAGxHAQEAALa7Yp8FM2DAAMXFxQX8\n",
              "fb1er7Zu3ar27dvL6QzubWitEOrHJ4X+MXJ8VV+oHyPHV/VZeYyxsbEaN27cL253xRYQq5w5c0ZX\n",
              "XXWVTp8+rVq1agU7TsCF+vFJoX+MHF/VF+rHyPFVfVfCMXIKBgAA2I4CAgAAbEcBAQAAtqt2BcTt\n",
              "duupp56S2+0OdhRLhPrxSaF/jBxf1Rfqx8jxVX1XwjFWu0moAAAg+KrdCAgAAAg+CggAALAdBQQA\n",
              "ANiu2hSQRx99VHFxcXI4HNq5c2ew4wRcSUmJ+vfvr/j4eCUnJ6t79+7KyckJdqyAu/POO5WUlKSU\n",
              "lBR17NhRO3bsCHYkSyxatEgOh0PvvfdesKMEXFxcnG666SalpKQoJSVFb7/9drAjBZTH49HYsWPV\n",
              "vHlztWrVShkZGcGOFDDff/+9/88tJSVF8fHxcrlcOnnyZLCjBdS//vUvtW7dWikpKUpMTNRrr70W\n",
              "7EgBtXr1arVt21ZJSUlKS0vTrl27ghPEV018+umnvm+++cYXGxvr27FjR7DjBFxxcbFv5cqVvrKy\n",
              "Mp/P5/PNnj3b16lTp+CGssCpU6f8Xy9fvtyXlJQUvDAWyc3N9aWnp/vS0tJ8//jHP4IdJ+BC9e/g\n",
              "T8aPH+8bO3as/+/ikSNHgpzIOtOnT/fdddddwY4RUGVlZb46der4du3a5fP5yv8+ut1u35kzZ4Kc\n",
              "LDBOnjzpq1u3rm/Pnj0+n8/n27Bhgy8hISEoWarNCMjtt9+uxo0bBzuGZSIiItS7d285HA5JUlpa\n",
              "mvLy8oIbygK1a9f2f3369Gn/8YaKsrIyPfjgg5o9e3ZIXwIYqs6ePauFCxfqueee8/+/2ahRoyCn\n",
              "ss7ChQv1wAMPBDtGwDkcDv3www+Sym9ZXq9evZD5+3jgwAHVq1dPCQkJkqSOHTvq0KFDys7Otj1L\n",
              "tSkg1c2LL76ofv36BTuGJYYNG6brr79ef/rTn/TGG28EO05AzZgxQx06dFCbNm2CHcVSw4YNU6tW\n",
              "rfTAAw/o+PHjwY4TMAcOHFDdunU1depUtW3bVh07dtTatWuDHcsSGzdu1KlTp3TXXXcFO0pAORwO\n",
              "vf322xowYIBiY2N122236bXXXlONGjWCHS0gmjdvru+//14bN26UJK1YsUKFhYVB+YWVAhKCpk6d\n",
              "qpycHGVmZgY7iiVef/11ffPNN5oyZYomTZoU7DgBs2fPHi1btkxPPvlksKNYasOGDdq9e7eys7NV\n",
              "v359DR8+PNiRAsYwDOXn56tly5batm2bZs2apfvuu08FBQXBjhZwCxcu1LBhw+RyuYIdJaAMw9CU\n",
              "KVO0fPly5efna+3atRo6dKhOnDgR7GgBcdVVV+ndd9/V5MmT1aZNG3300Udq2bJlcP4cg3LiJ4hC\n",
              "/fzz9OnTfW3atKkwVyKURURE+E6cOBHsGAHx8ssv+xo1auSLjY31xcbG+txut69Bgwa+l19+OdjR\n",
              "LHP48GFfdHR0sGMEzPHjx31hYWE+wzD8y9q2betbs2ZNEFMFXmFhoS86Otq3b9++YEcJuC+++MLX\n",
              "vHnzCsvatm3r++ijj4KUyFolJSW+2rVr+77++mvb980ISAiZMWOGli5dqjVr1lSYKxEqfvjhBx0+\n",
              "fNj//Xvvvad69eqpbt26QUwVOA8//LCOHDmivLw85eXlKS0tTfPnz9fDDz8c7GgBc/bsWf+5dUla\n",
              "unSpUlNTgxcowOrXr6877rhDH374oSQpNzdXubm5atGiRZCTBdbbb7+t5ORk3XzzzcGOEnDXX3+9\n",
              "jhw5on379kmScnJydODAAd10001BThY4R44c8X/9v//7v+ratauaNWtme47QGju7hNGjR2vlypU6\n",
              "evSoevTooZiYmJC6TPXbb7/VxIkT1bRpU3Xp0kVS+b3+t2zZEuRkgXP69GkNHDhQxcXFCgsLU4MG\n",
              "DfTBBx+E3ETUUFZQUKB77rlHXq9XPp9PTZs21euvvx7sWAE1d+5cPfDAA5o0aZLCwsI0b948XXfd\n",
              "dcGOFVALFy7UQw89FOwYlmjYsKHmz5+vQYMGKSwsTGVlZZozZ46aNGkS7GgB8+c//1mfffaZDMNQ\n",
              "enq6Fi5cGJQcPAsGAADYjlMwAADAdhQQAABgOwoIAACwHQUEMCkuLk5XX321SktL/cvWrVsnh8Oh\n",
              "8ePHBy/Y/7N48WL1798/KPsuKir61ZOC586dq+nTpwc0z9NPP60GDRooJSVFLVq0UN++fSvck6N3\n",
              "797av3//Jd/j53fFvJTly5erTZs2SklJ0c0336yuXbuqrKzscg8BCFnV5ioYIJCaNGmiFStW6J57\n",
              "7pFUflVA27Ztg5yq6jIMQ2PGjLHkvX/zm9/ohRdeUFlZme6//34988wzevnllyWVP3QsEI4cOaJR\n",
              "o0Zp+/btio2NlSRlZ2cH7AotwzBC7oZfACMgwK8wcuRIvfrqq5LKLw/evHmzevbs6V/v9Xr1P//z\n",
              "P0pMTFRiYqIeeeQRnTt3TpI0YsQIjR49WnfccYfi4+M1YMAA/7rS0lI99thjat++vVJSUjRo0CCd\n",
              "OnVKhw8fVsOGDfXjjz/69zFkyBD9/e9/r3Tmo0ePqkuXLmrTpo0SEhI0duxY/2/o/z1q8sEHH6hz\n",
              "586SpPXr1ysxMVG/+93vlJycrISEBG3bts2/7bx589S8eXOlpqZq5syZFfb5xRdfqGvXrmrbtq1S\n",
              "U1P1zjvvSJLy8vJUu3ZtTZo0Sa1bt9acOXP09NNP+0eQFi9erG7dumnw4MFq1aqV2rZtq4MHD/rf\n",
              "96mnnlKzZs3Url07Pfnkk4qLi/vF4w8LC1OXLl2Un5/vXxYXF+d/OvaUKVPUokUL/5Nef76dJPl8\n",
              "Pk2aNEl9+/at8OcglV9e7HQ6K9yTpnXr1v4Csm/fPvXo0UNJSUlKSkrS3LlzJZXfY6Jbt27+Jzz/\n",
              "/OnHDodDTz31lNq1a6fJkyersLBQDz30kNq3b6+kpCSNGjXK//8NUBVRQIBfoUOHDsrLy9Phw4e1\n",
              "dOlSDRw4UE6n079+/vz5+uKLL7R9+3bt3LlTBw4cqPDhvHPnTv3zn//Uvn37VFBQoGXLlkmSpk+f\n",
              "rqioKG3dulU7d+5Uq1at9OSTT+raa69Vt27dlJWVJan8A+/jjz/W0KFDK525du3a+uc//6nt27dr\n",
              "9+7dysvL0//93/9V6mf/85//aPjw4dq1a5ceeeQRPfHEE5LKbx//1FNPacOGDdqxY4eKi4v9P/PD\n",
              "Dz9o1KhRWrJkibZt26Y1a9Zo4sSJ+u677ySVF7eEhARlZ2df8NTVF198oalTp+rLL79Ut27dNG3a\n",
              "NEnSypUrtWzZMu3YsUNbt271v98v8Xg8+uCDD3Tfffedt+7UqVP629/+puzsbO3cuVMbN25Uw4YN\n",
              "K/zs4MGDVVRUpH/84x+KjIys8PNJSUm67bbbFBsbq7vvvlvTp0/35zIMQ/369dOIESO0e/du7d69\n",
              "W/fee6+k8tGZgQMHavfu3XrnnXf0wAMPVCg+TqdTX3zxhaZPn66JEyeqY8eO2rp1q3bt2qWysjK9\n",
              "+OKLlTp24EpEAQF+paFDh2rx4sV69dVX9dvf/rbCuo8//lgjRoyQ2+2Wy+XSQw89pDVr1vjX3333\n",
              "3YqMjJTT6VT79u114MABSeV3d83KyvL/Fr506VLl5uZKksaNG6eXXnpJkvTKK69o8ODBio6OrnTe\n",
              "srIyTZo0ScnJyUpNTdW2bdv8v/3/kmbNmumWW26RJKWnp/vzfvLJJ+rVq5euueYaSapw19aNGzfq\n",
              "4MGD6tWrl1JSUtStWzdJ8s+5CA8PV0ZGxkX3mZ6erhtuuOG8fa5du1YDBw5UTEyMHA7HLz6NdcmS\n",
              "JUpJSVH9+vV16tQpDRo06LxtatWqpebNmysjI0Pz5s3TyZMnFRER4V/fp08fJSQk6KWXXqpQNH8S\n",
              "FhamZcuWaePGjerZs6c+//xzJSQkKCcnR/v371dJSYkGDx7s375+/foqLCxUdna2P3/z5s112223\n",
              "6bPPPvNv9/P/r9577z1Nnz5dKSkpSk1N1WeffRZSN1NE9cNJReBXGjZsmFq3bq34+Hg1b978ktv+\n",
              "91yAn3+4OZ1OGYYhqXyYf/bs2brzzjvPe4/27dsrMjJS69at0/z58/Xxxx+byjtjxgwdO3ZMW7Zs\n",
              "UUREhCZMmKCSkhJJksvlktfr9W/70/Jfynup4/T5fEpISPA/dfPn8vLyFBkZqbCwi/8O9Gv2eSE/\n",
              "zQE5efKkunfvrqeeeso/mvLz99+8ebM2btyo9evXKy0tTUuXLlXHjh0lSV27dtWaNWs0btw41apV\n",
              "66L7uvnmm3XzzTdr9OjR6tmzp1asWKEePXpcMt+ljuXnBdPn82nZsmWKj4+v9PsBVzJGQIBf6dpr\n",
              "r1VmZuZ5H2aS1K1bN73++us6d+6cDMPQggULLlgq/lv//v01c+ZM/xyDH3/8UXv37vWvHzdunIYN\n",
              "G6YWLVqY/iA6deqUGjVqpIiICB09etQ/H0MqH+HYvXu3iouLZRiG3nzzzUq9Z9euXbV69WodPXpU\n",
              "kvxzGyTp1ltvVW5uboWitHPnzsuet9C1a1ctW7ZMRUVF8vl8/rk4v6Ru3bpasGCBXnrppQrPwpCk\n",
              "wsJCFRQUqGPHjvrTn/6k2267TTt27PCvf/zxxzVgwAB169ZN33///Xnv/d133+nzzz/3f3/q1Cnl\n",
              "5ubqxhtv1E033aTIyEgtXbrUv/7EiROKiYlR69attWjRIknl80H+/e9/6/bbb79g/v79+2vatGn+\n",
              "Inbq1ClGQFClUUCAyzBy5Eilp6eft3zUqFFq3bq1WrdurZSUFMXFxVXqEt1JkyapXbt2uuWWW5SU\n",
              "lKS0tLQKp0nuvfdeFRUVaezYsZd8nw8//FCNGzf2vyZMmKBx48Zpy5YtSkhI0NChQ/2nRCQpLS1N\n",
              "vXv3VmJiojp37vyLIzo/SUxM1NNPP62OHTsqNTVVbrfbv65OnTpauXKlpk6dquTkZLVs2VKPPfbY\n",
              "ZV+aetddd6lfv35KSUlRu3btVLt27Uo/fDE1NVUDBw7U1KlTKyw/ffq0BgwYoFatWikpKUmlpaUa\n",
              "Pnx4hW3Gjx+vhx56SF27dvUXrp8YhqFnn31W8fHxSklJUceOHTV8+HD169dPLpdL77//vhYtWqRW\n",
              "rVopOTnZP+dnyZIl/ge73XvvvVqwYMFFnzkyc+ZM1axZUykpKUpKStIdd9yhvLy8yv1HA65APAsG\n",
              "qEK2bdumIUOG6D//+c8lT1+EusLCQsXExMjn82nixIkqLi42dUUQgOBjDghQRTz44IP66KOPtGDB\n",
              "gmpdPqTy+Td5eXkqKSlRQkJChVM/AKoGRkAAAIDtqvevUQAAICgoIAAAwHYUEAAAYDsKCAAAsB0F\n",
              "BAAA2I4CAgAAbEcBAQAAtqOAAAAA2/1/X0iItoDhv9sAAAAASUVORK5CYII=\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-8294dd8f-f2b7-4cd1-933e-6d59f9b6e0f0\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-8294dd8f-f2b7-4cd1-933e-6d59f9b6e0f0\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x78367ba9ffa0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Time series</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "def _plot_series(series, series_name, series_index=0):\n",
              "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
              "  xs = series['Date of Transaction']\n",
              "  ys = series['Amount (USD)']\n",
              "  \n",
              "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
              "\n",
              "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
              "df_sorted = _df_9.sort_values('Date of Transaction', ascending=True)\n",
              "for i, (series_name, series) in enumerate(df_sorted.groupby('Transaction ID')):\n",
              "  _plot_series(series, series_name, i)\n",
              "  fig.legend(title='Transaction ID', bbox_to_anchor=(1, 1), loc='upper left')\n",
              "sns.despine(fig=fig, ax=ax)\n",
              "plt.xlabel('Date of Transaction')\n",
              "_ = plt.ylabel('Amount (USD)')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-9741e7d6-c05d-4676-9709-10a13eb7cba6\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABHkAAAITCAYAAACT08DeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABBN0lEQVR4nO3de7hWZZ0//vdmb9gkCpoHEhQQN2cVUEBQ85SHPOQBj1OaSqh4\n",
              "alL7Oplmk6FYk5raqGRiOZaZqMBko5OHStMRSVFMRVEOiuIBERwUdLPX7w9/7glF2BwegcXrdV3P\n",
              "de217rXW/WGxb594d697VRVFUQQAAACAtVqz1V0AAAAAACtPyAMAAABQAkIeAAAAgBIQ8gAAAACU\n",
              "gJAHAAAAoASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASqBmdRewNN/85jczbty4TJ8+PY8/\n",
              "/nj69OmzzHPmzJmT008/PY8++miaN2+er3zlK7nkkksqXywAAAB8hj744IPMnTs377777uouhQrb\n",
              "aKONssEGGyzzuDU65Dn88MNzzjnnZJdddmnyOUOGDMnOO++cX//610mSWbNmVao8AAAAWC1mzZqV\n",
              "mTNnprq6OlVVVau7HCps9uzZadWqVbp27brU49bokGfXXXdd4v5HH300//Iv/5J58+Zl0aJF+e53\n",
              "v5sjjjgiU6ZMyYQJE3Lbbbc1HvuFL3zhsyoXAAAAKu7dd9/NK6+8klatWmWzzTZLixYtBD0lVhRF\n",
              "3njjjbz99tt55513ljqjZ40OeZbk7bffzkknnZQ//OEP2XzzzfPmm29m++23z0477ZSnn346W2yx\n",
              "RU455ZRMmDAhG2+8cX70ox+lb9++q7tsAAAAWCXefPPNVFdXp3379k16hIdymDdvXubMmVOukOeh\n",
              "hx7Kiy++mP3222+x/ZMnT059fX3Gjx+fiy++OCNHjsx//dd/5cADD8y0adPSvHnz1VQxAAAArHrN\n",
              "mnmX0rqiqTO11rqQpyiK9OrVKw899NAn2iZMmJD27dtnjz32SJLst99+ef/99zN9+vTU1dV91qUC\n",
              "AAAAfGbWuthvp512ytSpU3PPPfc07ps4cWLef//97LDDDmndunWefPLJJMn48eNTFEW23HLL1VUu\n",
              "AAAArFW6d++e7t27Z+utt051dXXj9v7777+6S/uE3//+97n11lsbt6dOnZoddthhlfZx5plnZsiQ\n",
              "IY39tWzZMj169EhdXV3q6uoydOjQvP7666u0zxW1Rs/kOfnkk3PnnXdm1qxZ2XfffbPBBhtkypQp\n",
              "ufPOO/Ptb387Z599dj744IN06NAhY8aMSVVVVX71q1/lxBNPzHvvvZfa2trcdtttqa2tXd1/FAAA\n",
              "AFgrPPvss0k+XBalX79+jdv/6P3330+LFi0+69I+4d57783cuXNzxBFHJEm22mqr/O1vf6ton506\n",
              "dcozzzyTJJkzZ06GDRuW3XffPU8++WRqalZvzLJGz+QZOXJkXn755dTX1+e1117LlClTkiTbb799\n",
              "7rvvvjzxxBN5+umnc9ddd6Vly5ZJkh122CGPPPJInnzyyTz66KPZbbfdVucfAQAAAEqhXbt2GTZs\n",
              "WLbddtscfvjhmT59enbcccf07NkzdXV1+frXv55FixYlSa644ooMGjQoBx54YLp06ZJevXrl6aef\n",
              "TpI8+eST6du3b7p165YuXbrkjDPOSJKMHTs2ffr0aZwlc/nllzf2PXv27Bx55JGpq6tLt27dcsQR\n",
              "R+Shhx7Kf/zHf+SOO+5I9+7dc/bZZ2fy5MmLLUw8evTo9OzZM127dk3//v0zYcKEJB/OyKmrq8sx\n",
              "xxyTbt26pa6uLn/+85+X+55stNFGufHGGzNnzpyMHj16he/tqrJGz+QBAAAA1hxvvfVWnnjiiTRr\n",
              "1izz58/P3XffnQ033DD19fXZa6+9cv311+ekk05KkkyaNCmPPvpoevTokVNOOSU//OEPc/PNN+fy\n",
              "yy/Pvvvum0suuSRJ8tprryVJBg0alAkTJqSmpiavvfZa+vbtm6985Supq6vLySefnJYtW2by5Mmp\n",
              "rq7OzJkz0759+xx77LGZO3duRo0aleTD2UcfefnllzN06NDcfffd2XHHHXP11VfnqKOOyvPPP58k\n",
              "mTZtWn7+85/npptuyo9+9KOcd955efDBB5f7ntTW1qZnz56ZNGlSjj766JW6vytrjZ7JAwAAAKw5\n",
              "hgwZ0vhWr4aGhpxxxhnp1q1bevbsmaeeeiqPP/5447F9+/ZNjx49kny4vu60adOSJLvuumtuuumm\n",
              "nH766bntttuyySabJElef/317L///qmrq8tuu+2Wt99+u/F69957b7773e+muro6SdK+fftl1vrn\n",
              "P/85Xbt2zY477pgkOfXUU/P666/nxRdfTJJsueWW2XPPPRtrmjFjxgrfl6IoVvjcVUnIAwAAADRJ\n",
              "69atG3++8MIL88Ybb+Sxxx7Lc889l0MOOSQLFy5sbP/H9XGrq6sbH+U64YQT8uCDD6Z79+7593//\n",
              "98Y3ZJ944okZNGhQnnvuuTz77LPp1KlTFixYULE/y8frq6+vX6HrLFy4MM8880y22267VVXaCltj\n",
              "Q54rrrhidZcAAAAAfIo5c+akbdu2adWqVaZPn57f//73TTrvySefzJZbbpnTTz89l156aZ544okk\n",
              "ydy5c9OpU6c0a9Ysf/jDHxZ79GqvvfbKxRdf3BgUzZw5M8mHodO8efOW2M/uu++e5557LuPHj0+S\n",
              "/PznP0/btm3TuXPnFf4zf9zbb7+dE044IRtttFEOO+ywVXbdFbXGrskzffr01V0CAAAA8CnOOeec\n",
              "HHbYYamrq8tmm22WXXbZpUnn/eY3v8no0aPTvHnzNDQ05NJLL02SDB8+PGeeeWZ+9KMfpWfPnovN\n",
              "jBk5cmROPvnkdOvWLTU1Nendu3duueWWHH300Rk8eHC6d++eAw44oHE9oOTDR7quu+66HH/88amv\n",
              "r0+bNm3y29/+tvFxsxU1bdq0dO/ePfX19SmKIrvvvnv+9Kc/rfY3ayVJVbGmPDj2MWeddVYuu+yy\n",
              "1V0GAAAArFFmzJiROXPmpK6uLq1atVrd5fAZmD9/fqZMmZKNNtooHTp0+NTj1tjHtQAAAABoOiEP\n",
              "AAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIAAAAAlICQBwAAAFgp3bt3T/fu3bP11lunurq6\n",
              "cXv//ffPvffem8033zxvvPFG4/Ff/vKX861vfStJsmjRohx33HHZcsst06FDh1x00UWNx5W97be/\n",
              "/W169uyZFi1aZMiQISt6+xvVrPQVAAAAgHXas88+mySZPHly+vXr17j9kUMOOSRDhw7N2LFjc/XV\n",
              "V2f69OkZO3ZskuSaa67J5MmTM3Xq1MyePTt9+/bNPvvsk/79+5e+rUePHrn++uvzm9/8JvPnz1/p\n",
              "vwczeQAAAGAt1tDQkHcWLqjop6GhYaVqvPzyy/P0009n+PDhueCCC3LDDTektrY2STJ69OgMGTIk\n",
              "NTU1adu2bQ466KDceOON60Rb7969M2jQoNTUrJo5OGbyAAAAwFps/gfvp8dv/rWifTzz1X/NBrUt\n",
              "V/j8li1b5pprrsnee++dM888MwMHDmxsmzlzZjp37ty43bFjx4wfP36daFvVzOQBAAAAKm706NFp\n",
              "27ZtJk2atLpLKa2KzuTp1KlTamtr87nPfS5Jcu655+aoo46qZJcAAACwTmnVvEWe+eq/VryPlXH3\n",
              "3Xdn3Lhxeeyxx/KlL30pV199dU499dQkSfv27fPiiy82Hjt9+vRsscUW60TbqlbxmTy33HJLJk6c\n",
              "mIkTJwp4AAAAYBVr1qxZNqhtWdFPs2YrHh/MmzcvJ510Uq688sq0a9cuo0aNygUXXJDp06cnSQ49\n",
              "9NCMGjUq9fX1ee211zJu3Lgcc8wx60TbqmZNHgAAAKBiTjvttAwaNCiHH354kmTQoEE57rjjMmTI\n",
              "kNx777057bTTMmHChHTq1ClVVVUZNmxYdtxxx8Zzy9w2duzYnHTSSZk/f36Kosidd96ZSy+9dIVD\n",
              "oKqiKIoVOrMJOnXqlNatW6coigwYMCCXXHJJNt10008ct3DhwixcuHCxfd/73vdyxRVXVKo0AAAA\n",
              "WCvNmDEjc+bMSV1dXVq1arW6y+EzMH/+/EyZMiUbbbRROnTo8KnHVfRxrb/85S958skn89hjj2WT\n",
              "TTbJcccdt8TjRowYkTZt2iz2qdRK0wAAAABlVNGQ56N0qXnz5vnWt76VBx54YInHnXvuuZk7d+5i\n",
              "nwEDBlSyNAAAAIBSqdiaPPPnz88HH3yQDTfcMEly8803p2/fvks8tra2NrW1tYvtq66urlRpAAAA\n",
              "AKVTsZDntddey2GHHZZFixalKIp07tw5N954Y6W6AwAAAFinVSzk6dy5cx5//PFKXR4AAADWSf/4\n",
              "OvMKvkuJNVBVVdVS2yu6Jg8AAACwav1jyMO6ZVlL2/jNAAAAACgBIQ8AAACspaqqqtaIT48ePdKj\n",
              "R4/U1dWlpqamcfuAAw7Ifffdl3bt2uXNN99sPH6//fbLmWeemaqqqjQ0NOT4449Phw4d0rFjx1x8\n",
              "8cWNx5W9bfjw4enSpUu6du2abt265Zprrlni/W2qiq3JAwAAAFReQ0NDioX/W9E+qmrXX+pjYs8+\n",
              "+2ySZPLkyenXr1/j9kcOOeSQDB06NGPHjs3VV1+d6dOnZ+zYsUmSa665JpMnT87UqVMze/bs9O3b\n",
              "N/vss0/69+9f+rbtttsujzzySDbeeOM8//zzGTBgQHbbbbf06tVrhf6ehDwAAACwFisW/m9eOGWj\n",
              "ivax9TVzks+1XuHzL7/88my77bYZPnx4fvrTn+b3v/99amtrkySjR4/OkCFDUlNTk7Zt2+aggw7K\n",
              "jTfemP79+5e+7eCDD268R126dMnGG2+cqVOnrnDI43EtAAAAoKJatmyZa665Jt/73vfy9a9/PQMH\n",
              "DmxsmzlzZjp37ty43bFjx7z88svrRNs/uuOOOzJv3rzsuuuuS7yHTWEmDwAAAKzFqmrX/3CmTYX7\n",
              "WFmjR49O27ZtM2nSpFVQUbk88sgjOeWUU3LjjTemdesVnzEl5AEAAIC1WLNmzVbqUarPwt13351x\n",
              "48blsccey5e+9KVcffXVOfXUU5Mk7du3z4svvth47PTp07PFFlusE21JMmHChBx66KG5+uqr8+Uv\n",
              "f3mZ93JpPK4FAAAAVMy8efNy0kkn5corr0y7du0yatSoXHDBBZk+fXqS5NBDD82oUaNSX1+f1157\n",
              "LePGjcsxxxyzTrT97W9/y0EHHZQrr7wygwcPXul7bSYPAAAAUDGnnXZaBg0alMMPPzxJMmjQoBx3\n",
              "3HEZMmRI7r333px22mmZMGFCOnXqlKqqqgwbNiw77rhj47llbjvjjDPyv//7vzn//PNz/vnnJ0mG\n",
              "Dx/eeK+WV1VRFMUKnVlhZ511Vi677LLVXQYAAACsUV555ZW88cYbqaurS6tWrVZ3OXwG5s+fnylT\n",
              "pmTTTTdNu3btPvU4j2sBAAAAlICQBwAAAKAEhDwAAAAAJSDkAQAAACgBb9cCAACAtUh1dXXjz2vo\n",
              "u5SokGbNlj5Xx0weAAAAWIss6x/6lNc/BnxL4jcDAAAAWCk9evRIjx49UldXl5qamsbtAw44IPfd\n",
              "d1/atWuXN998s/H4/fbbL2eeeWaSpKGhIccff3w6dOiQjh075uKLL248ruxtI0aMSNeuXdOjR490\n",
              "6dIlw4cPX+G/g8TjWgAAALDWqqqqWt0lJEmeffbZJMnkyZPTr1+/xu2PHHLIIRk6dGjGjh2bq6++\n",
              "OtOnT8/YsWNTVVWVa665JpMnT87UqVMze/bs9O3bN/vss0/69+9f+raTTz453/3ud5Mkb731Vnr1\n",
              "6pU999wzO++88wr9PZjJAwAAAGuxhoaGLHjv/Yp+GhoaVqrGyy+/PE8//XSGDx+eCy64IDfccENq\n",
              "a2uTJKNHj86QIUNSU1OTtm3b5qCDDsqNN964TrRtvPHGjfdo3rx5qa+vX6n7bCYPAAAArMXeX1if\n",
              "C7773xXt48KL90nLz7VY4fNbtmyZa665JnvvvXfOPPPMDBw4sLFt5syZ6dy5c+N2x44dM378+HWi\n",
              "LUlGjRqViy++ODNmzMh3vvOdFZ7Fk5jJAwAAAHwGRo8enbZt22bSpEmru5Q1ypAhQzJlypRMmjQp\n",
              "t956ayZOnLjC1zKTBwAAANZiLWprcuHF+1S8j5Xx3//93xk3blwee+yxfOlLX8rVV1+dU089NUnS\n",
              "vn37vPjii43HTp8+PVtsscU60faPunXrlr59++b2229Pnz59Pu1WLpWZPAAAALAWa9asWVp+rkVF\n",
              "Pyvz2vZ58+blxBNPzJVXXpl27dpl1KhRueCCCzJjxowkyaGHHppRo0alvr4+r732WsaNG5djjjlm\n",
              "nWibMGFC432aOXNm/vrXv65wwJOYyQMAAABU0GmnnZZBgwbl8MMPT5IMGjQoxx13XE444YTce++9\n",
              "Oe200zJhwoR06tQpVVVVGTZsWHbcccfGc8vcdumll2b8+PFp3rx5iqLIsGHDMnjw4BW+11VFURQr\n",
              "fHYFnXXWWbnssstWdxkAAACwRnnjjTfyyiuvpK6uLq1atVrd5fAZmD9/fqZMmZJ27dpl0003/dTj\n",
              "PK4FAAAAUAJCHgAAAIASEPIAAAAAlICQBwAAAKAEvF0LAAAA1iI1NR/+U74oiqyh71KiQqqrq5fa\n",
              "LuQBAACAtUizZs1SVVWVqqoqIc86oiiKVFVVCXkAAACgjD4Keii/Zs2attqONXkAAABgLfVR0LO6\n",
              "Pz169EiPHj1SV1eXmpqaxu0DDzww999/f9q1a5fZs2c3Hr///vvnrLPOSlVVVRoaGnLCCSekQ4cO\n",
              "6dixYy655JLG48re9tHnlVdeySabbJJ99tlnife3qczkAQAAAFbKs88+mySZPHly+vXr17j9kcGD\n",
              "B2fo0KEZM2ZMrr322kybNi1jxoxJklx77bWZPHlypk6dmrfeeit9+vTJPvvskx122KH0bR854YQT\n",
              "stdee+Wtt95aqb8HM3kAAABgLdbQ0JCGdxZW9tPQsFI1XnrppXn66adz0UUX5fzzz88vf/nL1NbW\n",
              "JkluvfXWfOMb30hNTU0222yzHHzwwbnxxhvXibYk+elPf5qOHTtml112Wal7nJjJAwAAAGu3+R/k\n",
              "9R2vrGgXmz3yzWSD2hU+v2XLlrnmmmuy11575eyzz86OO+7Y2PbKK69kq622atzu1KlTHnnkkXWi\n",
              "7dlnn80vfvGLPPLIIxk1atQy7+OymMkDAAAAVNzo0aPTtm3bPPnkk6u7lDVCQ0NDjjvuuFx11VVp\n",
              "1arVKrmmmTwAAACwNmvV/MOZNhXuY2X88Y9/zLhx4/L4449nzz33zLXXXpthw4YlSdq1a5epU6c2\n",
              "Hjtt2rRsueWWpW+bM2dOJk+enGOOOSZJ8u677+a9997LoEGD8vDDDzfpvn6cmTwAAACwFmvWrFma\n",
              "bVBb2U8TX+G9JPPmzcvQoUNz1VVXZfPNN88NN9yQ888/Py+99FKS5LDDDsv111+f+vr6vP766xk7\n",
              "dmy+9rWvlb5t4403zttvv52ZM2dm5syZufDCC/PFL35xhQOexEweAAAAoIJOP/307LTTThk8eHCS\n",
              "ZODAgTnhhBNywgkn5J577skpp5ySRx99NJ06dUpVVVVOPfXUDBgwIElK37aqVRVFUVTkyivprLPO\n",
              "ymWXXba6ywAAAIA1yty5czN9+vTU1dVlvfXWW93l8Bl49913M2XKlHTs2DFt2rT51OM8rgUAAABQ\n",
              "AkIeAAAAgBIQ8gAAAACUgJAHAAAA1kJr6BK7rEbergUAAABrkebNmydJ6uvrU19fv1KvN2ftUF9f\n",
              "nySpqVl6jCPkAQAAgLVIs2bNUlNTk6qqqnzwwQeruxw+Aw0NDamurk51dfVSjxPyAAAAwFqmqqoq\n",
              "zZs3T21t7eouhc9IU2ZsmdMFAAAAa6Gqqqo0a9Zsjfhsv/322X777bPNNtukefPmjdv/9E//lPHj\n",
              "x6dTp06ZN29e4/FHHXVUfvCDHzQGF//8z/+cLl26pGvXrrn66qsbjyt724UXXpi2bds23q9jjz12\n",
              "ife3qqqqSb8TZvIAAADAWqwoiry/6L2K9tGi+nNLDRomTpyYJJk2bVr69OnTuP2Rr371qzn99NNz\n",
              "00035de//nWmTJmS3/zmN0mSm266KU8//XSee+65zJ07N3379s0ee+yRXr16lb4tSb72ta/lpz/9\n",
              "6Sr5exLyAAAAwFrs/UXv5Zu39qhoH1ce8Uxqa9Zb4fMvvPDC7LDDDrnqqqty0UUX5b//+78bF5C+\n",
              "5ZZbcuKJJ6a6ujqf//znc9RRR+Xmm2/O8OHDS9+2qnlcCwAAAKioFi1a5Oc//3m++c1v5pRTTsl2\n",
              "223X2DZjxox07NixcbtTp06ZMWPGOtGWJLfeemt69+6dPffcM/fff/9S7+OymMkDAAAAa7EW1Z/L\n",
              "lUc8U/E+VtYdd9yRLbbY4hOPcq3Lhg0blvPOOy/NmzfPX//61xx66KF59NFHFwuFloeZPAAAALAW\n",
              "q6qqSm3NehX9NHXh30/z17/+Nb/73e/y+OOPZ9q0afn1r3/d2NahQ4dMnz69cXvatGnp0KHDOtH2\n",
              "hS98ofGxtZ133jl9+/bNhAkTln4zl0LIAwAAAFTM/Pnzc/zxx2fkyJHZZJNN8stf/jJnn312Zs2a\n",
              "lSQ54ogjct1112XRokV56623csstt+Soo45aJ9pefvnlxvv0/PPPZ+LEidl2221X+F57XAsAAACo\n",
              "mHPOOSd77LFH9t133yRJ7969c+qpp+bkk0/O2LFjc+yxx+bRRx9Nly5dUlVVlbPOOqsx6Ch723nn\n",
              "nZe//e1vqampSXV1df793/89Xbt2XeF7XVUURbHCZ1fQWWedlcsuu2x1lwEAAABrlAULFmTq1KnZ\n",
              "aqut0rJly9VdDp+Bpv6de1wLAAAAoASEPAAAAAAlIOQBAACAtVBDQ8PqLoHPSFNX2rHwMgAAAKxF\n",
              "WrRokWbNmuWVV17JpptumhYtWqz0K85ZcxVFkTfeeCNVVVWNr1v/NEIeAAAAWIs0a9YsW221VV59\n",
              "9dW88sorq7scPgNVVVXZYostUl1dvdTjhDwAAACwlmnRokU6dOiQ+vr6LFq0aHWXQ4U1b958mQFP\n",
              "IuQBAACAtdJHj+8s6xEe1h0WXgYAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIA\n",
              "AAAAlICQBwAAAKAEhDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ\n",
              "8gAAAACUwGcS8txwww2pqqrKmDFjPovuAAAAANY5FQ95pk2bluuuuy4DBw6sdFcAAAAA66yKhjwN\n",
              "DQ0ZOnRorrrqqtTW1n7qcQsXLsy8efMW+yxatKiSpQEAAACUSkVDnssuuyw777xzdthhh6UeN2LE\n",
              "iLRp02axz/jx4ytZGgAAAECpVCzkeeqpp3Lbbbfl/PPPX+ax5557bubOnbvYZ8CAAZUqDQAAAKB0\n",
              "aip14QceeCDTpk1Lly5dkiSzZs3KSSedlFdffTWnnHLKYsfW1tZ+4nGu6urqSpUGAAAAUDoVm8lz\n",
              "yimn5NVXX820adMybdq0DBw4MD//+c8/EfAAAAAAsPI+k1eoAwAAAFBZFXtc6+P+9Kc/fVZdAQAA\n",
              "AKxzzOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACAEhDyAAAAAJSAkAcAAACgBIQ8\n",
              "AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIAAAAAlICQBwAAAKAE\n",
              "hDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ8gAAAACUgJAHAAAA\n",
              "oASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACAEhDyAAAAAJSAkAcA\n",
              "AACgBIQ8AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIAAAAAlICQ\n",
              "BwAAAKAEhDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ8gAAAACU\n",
              "gJAHAAAAoASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACAEhDyAAAA\n",
              "AJSAkAcAAACgBIQ8AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIA\n",
              "AAAAlICQBwAAAKAEhDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ\n",
              "8gAAAACUgJAHAAAAoASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACA\n",
              "EhDyAAAAAJRATSUvvs8++2TWrFlp1qxZNthgg1x55ZXp27dvJbsEAAAAWCdVNOT53e9+lw033DBJ\n",
              "cscdd+T444/PE088UckuAQAAANZJFX1c66OAJ0nmzp2bqqqqSnYHAAAAsM6q6EyeJPn617+e+++/\n",
              "P0nyhz/8YYnHLFy4MAsXLlxs36JFiypdGgAAAEBpVHzh5RtvvDEvvfRShg8fnn/5l39Z4jEjRoxI\n",
              "mzZtFvuMHz++0qUBAAAAlEZVURTFZ9XZ5z73ubz88svZeOONF9u/pJk83/ve93LFFVd8VqUBAAAA\n",
              "rNUq9rjW22+/nXfffTft2rVLkowZMyYbb7xxPv/5z3/i2Nra2tTW1i62r7q6ulKlAQAAAJTOcoU8\n",
              "L7zwQq677rrcd999efnll/O5z30uvXv3zuGHH54jjzwyNTX/d7m5c+fmiCOOyHvvvZdmzZpl0003\n",
              "ze9//3uLLwMAAABUQJNDnpNPPjmPPfZYjjjiiFx66aX5whe+kAULFuSZZ57JXXfdlR//+Me59tpr\n",
              "M3DgwCRJx44drasDAAAA8Blpcshz0EEHZeTIkZ/Yv+222+bII4/M7Nmz88ILL6zS4gAAAABomiaH\n",
              "PAcccMBS2zfeeONPLKgMAAAAwGdjuV6h/uijj+aoo47KNttsk2222SZHH310JkyYUKnaAAAAAGii\n",
              "Joc8Dz/8cPbZZ5907tw5w4cPzw9/+MNstdVW2WefffLII49UskYAAAAAlqHJj2v9+Mc/zqhRo3Lo\n",
              "oYc27jv00EMzcODAjBgxImPGjKlEfQAAAAA0QZNn8vz9739fLOD5yMEHH5ynn356lRYFAAAAwPJp\n",
              "csiz3nrrfWpbq1atVkkxAAAAAKyYJj+utXDhwkyaNClFUXyibcGCBau0KAAAAACWT5NDnvfeey8H\n",
              "HXTQEtuqqqpWWUEAAAAALL8mhzzTpk2rYBkAAAAArIwmr8nzcW+//XbGjBmTSZMmrcp6AAAAAFgB\n",
              "TQ55jj322EycODHJhwFP7969893vfjdf+tKXcsMNN1SqPgAAAACaoMkhz9/+9rf06dMnSfLrX/86\n",
              "dXV1efrppzNhwoRceeWVlaoPAAAAgCZocsjTsmXLxp8feOCBHHrooUmSDh06rPqqAAAAAFguTQ55\n",
              "Fi1alLlz56a+vj4PPPBAvvjFLza2eYU6AAAAwOrV5LdrnXLKKdl+++3TunXrdO7cOb17906STJo0\n",
              "KW3btq1YgQAAAAAsW5NDnmHDhqVfv36ZOXNm9tlnn8b9LVq0yOWXX16R4gAAAABomiaHPEnSr1+/\n",
              "9OvXb7F93bp1W6UFAQAAALD8mhzybLTRRqmqqmrcrqqqymabbZZ99tknF110UdZff/2KFAgAAADA\n",
              "sjU55Jk4ceIn9r355psZOXJkzj777IwcOXJV1gUAAADAcmhyyNOxY8cl7rv22muz/fbbr9KiAAAA\n",
              "AFg+TX6F+qdeoFmzNGu20pcBAAAAYCWsdDpz11135fOf//yqqAUAAACAFdTkx7X69u272MLLSTJ7\n",
              "9uw0b948d9xxxyovDAAAAICma3LI89Of/nSx7aqqqmy66abp0qVLamqW603sAAAAAKxiTU5ndttt\n",
              "t0rWAQAAAMBKaPKaPKeeempefvnlJbYVRZHbbrstv/nNb1ZZYQAAAAA0XZNn8uy///7Zf//9s9FG\n",
              "G2XHHXdM27Zts2DBgkyePDl//etfs//+++fCCy+sZK0AAAAAfIomhzwHHnhgDjzwwDz44IP505/+\n",
              "lOeffz7rrbde9txzz1x22WXZZJNNKlknAAAAAEux3Csm77LLLtlll10qUQsAAAAAK6jJa/IAAAAA\n",
              "sOYS8gAAAACUgJAHAAAAoASWO+QZPnx4k/YBAAAA8NlZ7pDn9ttvb9I+AAAAAD47TX671t133527\n",
              "7rorM2fOzFlnndW4f+7cuRUpDAAAAICma3LI07Jly2y44YZp1qxZ2rRp07h/yy23zPe+972KFAcA\n",
              "AABA0zQ55Nltt92y22675ZBDDknv3r0rWRMAAAAAy6nJIc9HevXqlVtuuSUvvPBC6uvrG/dfcMEF\n",
              "q7QwAAAAAJpuuUOeo48+OrNmzcqAAQNSXV1diZoAAAAAWE7LHfJMmjQpzz77bKqqqipRDwAAAAAr\n",
              "YLlfob7lllvm/fffr0QtAAAAAKyg5Z7JU1dXl9133z2HHnpoWrZs2bj/m9/85iotDAAAAICmW+6Q\n",
              "Z+HChenevXueeeaZxn0e3QIAAABYvZY75LnhhhsqUQcAAAAAK2G5Q54bb7xxifu//vWvr3QxAAAA\n",
              "AKyY5Q55/vM//7Px5wULFuTBBx/MwIEDhTwAAAAAq9Fyhzy33nrrYttTp07Neeedt8oKAgAAAGD5\n",
              "Lfcr1D9uq622yt///vdVUQsAAAAAK2i5Z/KMGzeu8edFixblkUceSW1t7SotCgAAAIDls9whz+WX\n",
              "X/5/J9fUpK6uLrfccssqLQoAAACA5bPcIc/9999fiToAAAAAWAnLHfIkHy6+/Mc//jFJsu++++aw\n",
              "ww5bpUUBAAAAsHyWe+HlCy+8MCNGjEjPnj3Tq1evjBgxIsOHD69EbQAAAAA00XLP5Bk9enT+53/+\n",
              "J+utt16SZOjQoRk0aFDOP//8VV4cAAAAAE2z3DN5iqJoDHiSpFWrVimKYpUWBQAAAMDyWe6ZPAMG\n",
              "DMixxx6bE088MUly/fXXZ8CAAau8MAAAAACabrln8lx55ZVp165dzjrrrJx11lnZfPPNc+WVV1ai\n",
              "NgAAAACaaLln8rRq1So/+tGPKlELAAAAACtouUOed999N7/61a/y/PPPp76+vnG/2TwAAAAAq89y\n",
              "hzyDBw9O8+bN079//1RXV1eiJgAAAACW03KHPDNmzMjTTz9diVoAAAAAWEHLvfBy9+7d8+abb1ai\n",
              "FgAAAABW0HLP5LnooosyaNCg9O/fPy1btmzcP2rUqFVaGAAAAABNt9whz4knnphBgwalX79+1uQB\n",
              "AAAAWEMsd8gzZ86c3HjjjZWoBQAAAIAVtNxr8vTu3TszZ86sRC0AAAAArKDlnsnzxhtvZJtttsmg\n",
              "QYMWW5Pn9ttvX6WFAQAAANB0yx3yHHPMMTnmmGMqUQsAAAAAK2i5Q57jjjtuse1FixblP//zP1dZ\n",
              "QQAAAAAsv+Vek+cjkydPzjnnnJP27dtn+PDhq7ImAAAAAJbTcs3keffdd3PLLbfkF7/4RaZOnZr3\n",
              "3nsvDz/8cLp3716p+gAAAABogibP5DnxxBOz5ZZbZty4cfnOd76TGTNmZMMNNxTwAAAAAKwBmjyT\n",
              "57e//W369euXk08+Ofvuu2+qqqpSVVVVydoAAAAAaKImz+R59dVXc8wxx+TCCy9Mx44dc/755+eD\n",
              "Dz6oZG0AAAAANFGTQ571118/3/jGN/LQQw/lrrvuyoIFC/L+++9np512ytVXX13JGgEAAABYhhV6\n",
              "u1bPnj3zk5/8JDNnzszZZ5+dO++8c1XXBQAAAMByWOFXqCdJTU1NDjvsMCEPAAAAwGq2UiEPAAAA\n",
              "AGuGioU8CxYsyCGHHJKuXbumd+/e2XvvvTNlypRKdQcAAACwTqvoTJ6TTjopkydPzhNPPJGDDz44\n",
              "Q4cOrWR3AAAAAOusioU8LVu2zP7775+qqqokycCBAzNt2rQlHrtw4cLMmzdvsc+iRYsqVRoAAABA\n",
              "6Xxma/JcccUVOfjgg5fYNmLEiLRp02axz/jx4z+r0gAAAADWep9JyHPxxRdnypQpGTFixBLbzz33\n",
              "3MydO3exz4ABAz6L0gAAAABKoabSHfzkJz/J7bffnnvuuSfrrbfeEo+pra1NbW3tYvuqq6srXRoA\n",
              "AABAaVQ05Lnsssty880355577smGG25Yya4AAAAA1mkVC3lefvnlnH322encuXP22GOPJB/O2Hnk\n",
              "kUcq1SUAAADAOqtiIc8WW2yRoigqdXkAAAAA/sFn9nYtAAAAACpHyAMAAABQAkIeAAAAgBIQ8gAA\n",
              "AACUgJAHAAAAoASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACAEhDy\n",
              "AAAAAJSAkAcAAACgBIQ8AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIAS\n",
              "EPIAAAAAlICQBwAAAKAEhDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAA\n",
              "gBIQ8gAAAACUgJAHAAAAoASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4A\n",
              "AACAEhDyAAAAAJSAkAcAAACgBIQ8AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJC\n",
              "HgAAAIASEPIAAAAAlICQBwAAAKAEhDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQ\n",
              "AkIeAAAAgBIQ8gAAAACUgJAHAAAAoASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAA\n",
              "AFACQh4AAACAEhDyAAAAAJSAkAcAAACgBIQ8AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgD\n",
              "AAAAUAJCHgAAAIASEPIAAAAAlICQBwAAAKAEhDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpA\n",
              "yAMAAABQAkIeAAAAgBIQ8gAAAACUgJAHAAAAoASEPAAAAAAlUNGQ55vf/GY6deqUqqqqTJw4sZJd\n",
              "AQAAAKzTKhryHH744XnwwQfTsWPHSnYDAAAAsM6rqeTFd9111yYdt3DhwixcuHCxfYsWLapESQAA\n",
              "AACltEasyTNixIi0adNmsc/48eNXd1kAAAAAa401IuQ599xzM3fu3MU+AwYMWN1lAQAAAKw1Kvq4\n",
              "VlPV1tamtrZ2sX3V1dWrqRoAAACAtc8aMZMHAAAAgJVT0ZDn5JNPzhZbbJGXX345++67b+rq6irZ\n",
              "HQAAAMA6q6KPa40cObKSlwcAAADg/+dxLQAAAIASEPIAAAAAlICQBwAAAKAEhDwAAAAAJSDkAQAA\n",
              "ACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ8gAAAACUgJAHAAAAoASEPAAAAAAlIOQB\n",
              "AAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACAEhDyAAAAAJSAkAcAAACgBIQ8AAAAACUg\n",
              "5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIAAAAAlICQBwAAAKAEhDwAAAAA\n",
              "JSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ8gAAAACUgJAHAAAAoASEPAAA\n",
              "AAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACAEhDyAAAAAJSAkAcAAACgBIQ8\n",
              "AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIAAAAAlICQBwAAAKAE\n",
              "hDwAAAAAJSDkAQAAACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ8gAAAACUgJAHAAAA\n",
              "oASEPAAAAAAlIOQBAAAAKAEhDwAAAEAJCHkAAAAASkDIAwAAAFACQh4AAACAEhDyAAAAAJSAkAcA\n",
              "AACgBIQ8AAAAACUg5AEAAAAoASEPAAAAQAkIeQAAAABKQMgDAAAAUAJCHgAAAIASEPIAAAAAlICQ\n",
              "BwAAAKAEhDwAAAAAJSDkAQAAACgBIQ8AAABACVQ05Hn++eez0047pWvXrunfv3/+/ve/V7I7AAAA\n",
              "gHVWRUOek08+OSeddFKee+65/Mu//EuOP/74SnYHAAAAsM6qWMjz+uuvZ8KECTnmmGOSJIcddlhe\n",
              "eumlTJkypVJdAgAAAKyzaip14Zdeeimbb755amo+7KKqqiodOnTIjBkzUldXt9ixCxcuzMKFCxfb\n",
              "t2jRokqVBgAAAFA6a8TCyyNGjEibNm0W+4wfP351lwUAAACw1qhYyLPlllvm1VdfTX19fZKkKIrM\n",
              "mDEjHTp0+MSx5557bubOnbvYZ8CAAZUqDQAAAKB0KhbybLbZZtl+++1z0003JUluu+22bLHFFp94\n",
              "VCtJamtr07p168U+1dXVlSoNAAAAoHQqtiZPkowcOTLHH398Lr744rRu3To33HBDJbsDAAAAWGdV\n",
              "NOTp1q1bHn744Up2AQAAAEDWkIWXAQAAAFg5Qh4AAACAEhDyAAAAAJSAkAcAAACgBIQ8AAAAACUg\n",
              "5AEAAAAoASEPAAAAQAkIeQAAAABKoKooimJ1F7EkgwcPTqdOnVZ3Gcu0aNGijB8/PgMGDEh1dfXq\n",
              "LgdKxfiCyjC2oDKMLaictW18dezYMf/8z/+8ustgHbTGhjxri3nz5qVNmzaZO3duWrduvbrLgVIx\n",
              "vqAyjC2oDGMLKsf4gqbxuBYAAABACQh5AAAAAEpAyAMAAABQAkKelVRbW5vvf//7qa2tXd2lQOkY\n",
              "X1AZxhZUhrEFlWN8QdNYeBkAAACgBMzkAQAAACgBIQ8AAABACQh5AAAAAEqgoiHPggULcsghh6Rr\n",
              "167p3bt39t5770yZMqWx/fXXX8+Xv/zldOnSJdtss03+8pe/NLZdfPHF6datW5o1a5YxY8Ysdt0T\n",
              "Tjgh2223Xfr06ZP+/fvn3nvv/dQaVrSPSl3n+uuvT5cuXbL11lvnxBNPzAcffNCktn/07rvv5p/+\n",
              "6Z9SV1eXrl27ZvTo0U1q+7hHHnkkvXv3TteuXbPnnntm5syZTWpj9TO2PunTxs+0adOy++67p02b\n",
              "NunTp89Sr2FskRhfS/Jp4+u+++7LgAED0rNnz/Tq1SvnnHNOGhoalniNhoaGnHHGGdl6661TV1eX\n",
              "n/3sZ01q+7jnn38+O+20U7p27Zr+/fvn73//e5PaWP2MrU/6tLH18MMPp0+fPunTp0969eqVk08+\n",
              "OQsXLlziNXx3YWx90rL+XVUURfbcc89suOGGn3oN31ustYoKeu+994o777yzaGhoKIqiKK666qpi\n",
              "t912a2w/4YQTiu9///tFURTF+PHji/bt2xfvv/9+URRF8cgjjxQvvPBCsdtuuxV33HHHYtedM2dO\n",
              "48+PPfZYsdFGGxWLFi1aYg0r2kclrvPiiy8Wm2++efHqq68WDQ0NxVe+8pXiZz/72TLbPu4HP/hB\n",
              "cdxxxzWet+mmmxZvvvnmMtv+0aJFi4qtt966uO+++4qiKIp/+7d/Kw4//PBltrFmMLYWt7TxM3v2\n",
              "7OKBBx4ofv/73xe9e/deai3GFkVhfH3c0sbXY489VrzwwgtFUXx433beeefihhtuWOJ1fvWrXxV7\n",
              "7rlnUV9fX8yePbvo0KFD8dRTTy2z7eP22GOPxj5uvfXWol+/fk1qY/Uztha3tLE1f/78xustWrSo\n",
              "OOSQQ4rLLrtsidfx3YWxtbim/Lvq0ksvLYYOHVq0adPmU6/je4u1VUVDno979NFHi44dOzZut2rV\n",
              "qnj11Vcbt/v371/88Y9/XOycZQ3i+++/f6n/wVkVfayq6/z4xz8uTj755MbtO++8s9h5552X2fZx\n",
              "PXv2LB5++OHG7SOOOKK47rrrltn2j8aPH19069atcXvevHlFbW1t8d577y21jTWTsbXs8XP//fcv\n",
              "M+QxtlgS46vp30+nnXZa4/84/7j999+/uPnmmxu3/9//+3/Feeedt8y2f/Taa68VG2ywQfHBBx8U\n",
              "RVEUDQ0NRdu2bYvnn39+qW2smYytpo2t9957r9h3332Lyy+/fInX8d3FxxlbSx9bTz31VPHFL36x\n",
              "mDJlylJDHt9brK0+0zV5rrjiihx88MFJktmzZ+eDDz7IF77whcb2Tp06ZcaMGU261ne+851svfXW\n",
              "GTx4cG677bY0a/bJP8rK9rGqrzNjxox07NhxiddYWtsrr7yy2GMmK3qda6+9NhdccMESj9tggw3S\n",
              "unXrvPLKK0ttY81kbH367/3SGFs0hfHVtPE1a9asjB49OgceeGDjvj59+jT+fq/o+Bo3blyGDh2a\n",
              "JHnppZey+eabp6amJklSVVWVDh06ZMaMGUttY81kbC19bE2bNi29e/fOJptskjZt2uTUU09N4ruL\n",
              "ZTO2Pv33/oMPPsiJJ56YkSNHprq6+hPn+t6iDD6zkOfiiy/OlClTMmLEiFVyvUsuuSQvvPBCfve7\n",
              "3+Wcc87J+++/v0quuyZq165dJk6cuNLXGTZsWC688MKVL4g1irG14owtlsX4app58+blK1/5Ss45\n",
              "55z069evcf/EiRPTrl27lbr2QQcdlF/84hcrWyJrGGNr2Tp16pQnnngis2bNysKFC3P77bcn8d3F\n",
              "0hlbS/eDH/wggwcPTo8ePZbY7nuLMvhMQp6f/OQnuf322/Nf//VfWW+99ZIkG2+8cWpqajJr1qzG\n",
              "46ZNm5YOHTos17X32muvvPPOO5k0aVLuueeexkXqLrroohXuY1Vd5+M6dOiQ6dOnL/EaS2urxHU+\n",
              "ftw777yTuXPnpl27dkttY81ibH1oecbPil7H2Fr3GF8fWtbv/jvvvJMvf/nLOfjgg3PWWWet0HWa\n",
              "Or623HLLvPrqq6mvr0/y4cKZM2bMSIcOHZbaxprF2PpQU3/v119//Rx99NH59a9/vdzX8d21bjG2\n",
              "PrS03/s///nPueqqq9KpU6fssssumTdvXjp16pQ33nhjua7je4s1WqWfB7v00kuL7bffvnjrrbc+\n",
              "0XbccccttrBWu3btGhfW+sjHn7l8//33F3tO8ZFHHik22mijJV5/Rfuo1HVeeOGFTywCdtVVVy2z\n",
              "7eO+//3vf2IRvTfeeGOZbf9o0aJFRefOnRdbRO+www5bZhtrDmPr/zRl/DRlTR5ji48YX/9naePr\n",
              "nXfeKXbaaafiBz/4wVLrKIqiuOGGGz6xSOWTTz65zLaP22233RZbpHKHHXZoUhtrBmPr/yxtbD3/\n",
              "/PON11u4cGFx5JFHFt/97neXeB3fXRSFsfWPmvrvqqlTpy51TR7fW6ytKhryvPTSS0WSonPnzkXv\n",
              "3r2L3r17FwMGDGhsnzVrVrH33nsXdXV1Rc+ePRu/PIqiKH74wx8W7du3L1q0aFFsvPHGRfv27YvX\n",
              "X3+9mD9/frHTTjsVvXr1Knr37l3stNNOxb333vupNaxIH5W8zs9//vOic+fORefOnYshQ4Ys9h+t\n",
              "T2ubOXPmYv84/d///d/iyCOPLDp37lx06dKluOWWW5rUds011xTf+973GrcfeuihYtttty26dOlS\n",
              "7LbbbsWMGTOa1MbqZ2x90qeNn/nz5xft27cvNtlkk6J58+ZF+/bti+985ztFURhbLJnx9UmfNr6G\n",
              "Dx9e1NTUNN6n3r17F8OHD288r3fv3sXMmTOLoiiK+vr64tRTTy222mqronPnzsVPf/rTxuOW1jZ2\n",
              "7NjiG9/4RuP2s88+WwwcOLDo0qVLscMOOyz2P6qX1sbqZ2x90qeNrZEjRxa9evUqtttuu6Jnz57F\n",
              "GWec0bjQse8uPs7Y+qSl/ZvrI0sKeXxvUQZVRVEUq3UqEQAAAAAr7TN9uxYAAAAAlSHkAQAAACgB\n",
              "IQ8AAABACQh5AGAZOnXqlG7duqV3796pq6vLwQcfnIceeqhJ544ZMyb/8z//s8prGjlyZLp3754+\n",
              "ffpk9uzZjfsvuOCCxlfSrr/++tlqq60atydPnrzK61gZb7/9di655JLF9g0dOjT333//aqoIAGDt\n",
              "ZuFlAFiGTp06ZcyYMenTp0+S5Pbbb8+QIUNy9913Z8cdd1zquccff3z69OmTb33rW6u0ph49emTU\n",
              "qFEZNGjQpx6z++6751vf+lYOOeSQxfY3NDQkSZo1W73/X8+0adPSp0+fvP3226u1DgCAsjCTBwCW\n",
              "0+DBgzNs2LD85Cc/SZLce++9GTRoUPr27ZtevXrl+uuvT5L84Q9/yLhx4/Jv//Zv6dOnT37xi18k\n",
              "Sf7jP/4jO+64Y7bffvvsuuuueeKJJ5bYz4QJE7LTTjtlu+22y4ABA/LXv/41SXL44YfnhRdeyPHH\n",
              "H5/DDz+8STX/67/+aw477LDsu+++2WabbfLqq6/m29/+dvr3758+ffpk1113XWymT1VVVS6++OIM\n",
              "GDAgW221VW644YYkHwZEp59+enr06JHevXtnhx12yIIFC1JfX5999903/fr1S69evfLVr3418+fP\n",
              "b7zeDTfckD59+qR3797p169fpk2blmHDhuWdd95Jnz590q9fvyQfBlNjxoxJkrz++usZPHhwtt12\n",
              "22yzzTYZOXJk4/U6deqUCy64IIMGDcpWW22V4cOHN+k+AACU2up9gzsArPk6duxYPP7444vtu/32\n",
              "24sePXoURVEUb731VlFfX18URVHMnj276NChQ/HSSy8VRVEUxx13XHH55Zc3nvfggw8W++23X7Fg\n",
              "wYKiKIriL3/5S9GzZ89P9Llw4cJiyy23LO66666iKIrigQceKNq2bVu88847n1rTx+22227FHXfc\n",
              "URRFUXz/+98vNt9882LWrFmN7a+//nrjzzfffHOx7777Nm4nKX7yk58URVEUzzzzTLH++usXH3zw\n",
              "QfHYY48V3bt3LxYtWlQURVG8/fbbxaJFi4qGhobizTffLIqiKBoaGophw4YVI0aMKIqiKO6///6i\n",
              "U6dOxSuvvFIURVHMnz+/mD9/fjF16tSiTZs2n1rzkUceWXznO98piqIoXnvttWKLLbYoHn744cY/\n",
              "/xlnnFEURVG88cYbRevWrYuXX355qfcDAKDsalZ3yAQAa6PiH552nj17dr7xjW/kueeeS01NTWbP\n",
              "np2nnnoqW2yxxSfOGzt2bJ544onFHvN666238t577+Vzn/tc477JkyenWbNm2XfffZMku+yyS9q2\n",
              "bZuJEydml112WaGa999//7Rt27Zx+49//GOuuuqqvPPOO2loaMhbb7212PFf+9rXkiTdu3dPTU1N\n",
              "Zs2alc6dO6e+vj5DhgzJHnvskQMOOCDNmjVLQ0NDLr/88tx5552pr6/P3Llzs9NOOyVJ7rzzzhx7\n",
              "7LHZfPPNkyTrrbdek+q955578re//S1Jstlmm2Xw4MG55557MnDgwCTJV7/61STJJptsks6dO2fq\n",
              "1Klp3779Ct0bAIAy8LgWAKyARx99NNtss02SZNiwYdlll10yadKkTJw4MV27ds2CBQuWeF5RFDnu\n",
              "uOMyceLExs+rr766WMDzaaqqqlaq5vXXX7/x5xkzZuT000/PTTfdlKeeeiq//e1vP1Fzy5YtG3+u\n",
              "rq5OfX192rRpk6eeeipf/epX8+yzz2a77bbLlClT8pvf/Cb33Xdf/vznP2fSpEn59re//an3YEV9\n",
              "/M+/pPoAANZlQh4AWE5jx47NNddck7PPPjtJMmfOnHTs2DFVVVX5y1/+stgaO61bt87cuXMbtw86\n",
              "6KDcdNNNmTFjRpIP17iZMGHCJ/ro1q1bGhoa8sc//jFJ8tBDD2XWrFmNiz+vrLlz56Z58+bZfPPN\n",
              "UxRFfvaznzXpvDfeeCPz58/PPvvsk4svvjidOnXK008/nTlz5mSTTTZJ69at88477+SXv/xl4zlf\n",
              "+cpXctNNN+XVV19Nkrz77rt5991307p167z33nt5//33l9jXXnvtleuuu66x39tvvz177733yv3B\n",
              "AQBKzONaANAERx11VFq2bJn58+enZ8+e+cMf/tD4yNUll1ySU089NT/84Q/Tp0+fxR7FOvbYY3P8\n",
              "8cdnzJgxOe200zJ06ND8+Mc/zqGHHpr6+vq8//77OeCAAxoXHv5IixYtcvvtt+eb3/xmzj777LRs\n",
              "2TKjR49ebDbOyth2221z9NFHp1evXtl4440/8QauT/PSSy/lxBNPzAcffJBFixZl5513zn777Zd3\n",
              "3303Y8eOTbdu3bLpppvmi1/8YqZPn54k2XXXXfP9738/++67b6qqqtKiRYuMHj06HTt2zNe//vVs\n",
              "t912WX/99T8Rdl155ZU55ZRTsu2226Yoipx33nnLfJsZAMC6zCvUAQAAAErA41oAAAAAJSDkAQAA\n",
              "ACgBIQ8AAABACQh5AAAAAEpAyAMAAABQAkIeAAAAgBIQ8gAAAACUgJAHAAAAoASEPAAAAAAlIOQB\n",
              "AAAAKIH/D+Bk8fTHIaPtAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-9741e7d6-c05d-4676-9709-10a13eb7cba6\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-9741e7d6-c05d-4676-9709-10a13eb7cba6\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "def _plot_series(series, series_name, series_index=0):\n",
              "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
              "  xs = series['Date of Transaction']\n",
              "  ys = series['Amount (USD)']\n",
              "  \n",
              "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
              "\n",
              "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
              "df_sorted = _df_10.sort_values('Date of Transaction', ascending=True)\n",
              "for i, (series_name, series) in enumerate(df_sorted.groupby('Country')):\n",
              "  _plot_series(series, series_name, i)\n",
              "  fig.legend(title='Country', bbox_to_anchor=(1, 1), loc='upper left')\n",
              "sns.despine(fig=fig, ax=ax)\n",
              "plt.xlabel('Date of Transaction')\n",
              "_ = plt.ylabel('Amount (USD)')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-0d039a99-8556-4cd0-8f87-9da751f93cde\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABGkAAAITCAYAAAC0fUE2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABAIklEQVR4nO3deXhV1bk/8PckgSAgYBmUGWKAMJmgTKJV66+K2johDvVq9Trh\n",
              "fL3aax1arUNFvc7aKg7lOtThKoharVardSgCosUZFBRRARlEQOYk+/eHD7lGpkRyyI75fJ7nPD1n\n",
              "r33Wek/I4si3a6+dSZIkCQAAAABqVU5tFwAAAACAkAYAAAAgFYQ0AAAAACkgpAEAAABIASENAAAA\n",
              "QAoIaQAAAABSQEgDAAAAkAJCGgAAAIAUENIAAAAApEBebRewMWeeeWY8/vjj8cknn8S//vWvKCkp\n",
              "2eR7Fi1aFKeffnq89tpr0aBBg9h///3jyiuvzH6xAAAAsAWtWbMmFi9eHMuXL6/tUlKlQYMGkZeX\n",
              "nrijadOmsdVWW1Xp3PRUvR7Dhw+Pc889N3bdddcqv+e4446LXXbZJf785z9HRMTcuXOzVR4AAADU\n",
              "irlz58bnn38eubm5kclkarscNqK8vDxat24dHTp02OS5qQ5pdtttt/Uef+211+LXv/51LFmyJMrK\n",
              "yuKCCy6IQw89NKZPnx6TJ0+OMWPGVJy73XbbbalyAQAAIOuWL18es2fPjiZNmkSbNm2iYcOGgpqU\n",
              "SpIk5s+fH/Pnz4+WLVtuckVNqkOa9fnqq6/ipJNOiqeeeiratm0bCxYsiB133DGGDBkS7733XnTo\n",
              "0CFOOeWUmDx5crRs2TKuuuqq6NevX22XDQAAADViwYIFkZubG+3bt4+tt966tstJnSRJKp6nJbxa\n",
              "smRJfP311z+8kGb8+PHx0Ucfxb777lvp+LRp06K0tDQmTZoUV1xxRYwaNSr++te/xs9//vOYOXNm\n",
              "NGjQoJYqBgAAgJqXk+NeQHVBdYKiOhfSJEkSvXv3jvHjx6/TNnny5Gjfvn385Cc/iYiIfffdN1av\n",
              "Xh2ffPJJFBYWbulSAQAAAKqszsVuQ4YMiY8//jiee+65imNTpkyJ1atXx0477RTNmjWLt956KyIi\n",
              "Jk2aFEmSRMeOHWurXAAAAEiVJEkiSZIoLy+PsrKyKCsri/Ly8igvL690qVBVrF69Os4+++zo2rVr\n",
              "FBYWRlFRURxxxBExf/78dcbaXNOmTYurrrpqs/tJs1SvpBkxYkQ8+eSTMXfu3Bg6dGhsvfXWMX36\n",
              "9HjyySfjV7/6VZxzzjmxZs2a6NSpU4wbNy4ymUzcfffdceKJJ8aKFSsiPz8/xowZE/n5+bX9UQAA\n",
              "ACA11oYxm3vJ1BFHHBGLFi2KiRMnRps2baK8vDxGjx4d8+fPj9atW1caa2N1VOWSoA8//DBGjx4d\n",
              "v/71r9fbvnr16mjYsOH3+BTpkUmqG5MBAAAAtWbWrFmxaNGiKCwsjCZNmlTrvWsjgPLy8sjJyVkn\n",
              "HFm78iVJkoq2tf/77WAnSZJ45513YsCAAfHRRx9VurPy2vdedNFF8eCDD0Ymk4mePXvGXXfdFS1b\n",
              "toxzzjknvvrqq7jrrrsiImLkyJHx+uuvx5gxY+Kmm26KBx98MFq2bBnTpk2Lhg0bxsMPPxy9evWK\n",
              "rl27xpw5c6JLly7Rrl27eP7556Ndu3ZxwAEHxD//+c/o2rVrLFmyJI444ogYMWJERESMHTs2Lrnk\n",
              "koorbmrDsmXLYvr06dGuXbuK4GpD6tzlTgAAAMD3tzZE2VBAExGVApwNre2YOHFidO7cOdq2bVup\n",
              "35ycnHjkkUfi/vvvj1dffTWmTZsWjRs3jrPOOmudlTvfrmHtOG+//XZcffXV8cEHH8Ruu+0Wl112\n",
              "WURE3HzzzdGlS5eYOnVqPP/88xXv+/LLL+PNN9+Mxx9/PM4888wYNWpURdsf//jHisCmLhDSAAAA\n",
              "AJV8exVNVS/A+Xbw87e//S0OPPDAaNWqVUREnH766fHyyy9v8Pxv69evX/To0SMivtmXdubMmRsd\n",
              "97jjjqsIf4YNGxZLly6N8ePHxwcffBBvvfVWHHfccVWqPw1SvScNAAAAULMymUzFJsHVuT30dw0a\n",
              "NCg++eST+OKLL6JNmzabHHOtvLy8ShsJr1y5slJ7fn5+RYCTl5cXZWVlG+27WbNmlV6fdNJJccMN\n",
              "N0SbNm3i8MMPj6222qo6H6tWpXYlzY033ljbJQAAAMAPyndXyHz3sdba5+sLcta29e7dO/bZZ584\n",
              "6qijYsGCBRHxzV43//M//xMFBQXx+OOPx6JFiyKTycQf//jH2G233SIiolu3bvHmm29GaWlpLF26\n",
              "NJ544olKtX33+VotWrSIpUuXbvIznnLKKfHiiy/GQw89FP/xH/9R5Z9NGqR2Jc0nn3xS2yUAAADA\n",
              "D9LakObbK1rWrl5Ze/y7Gwevr+3BBx+M8847LwYOHFixQmbQoEFx0003xfLly2PgwIGVNg4uLy+P\n",
              "o48+OsaMGROFhYXRtm3b6NOnT6xYsWKd+r5r4MCB0a1btygsLIxOnTpV2pfm25o2bRr77rtvzJ07\n",
              "N7p161YjP68tJbV3dzr77LPjuuuuq+0yAAAAIFU25+5OafXdlTubcxnWmjVrom/fvnH99dfHvvvu\n",
              "W1Mlfm/u7gQAAADUCWsvtVq7qmdzApr77rsvOnfuHDvttFPss88+NVXiFpPay50AAACAH77NXTnz\n",
              "bUcddVT827/9W430VRuspAEAAABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJCGgAAAGCztWvXLrp0\n",
              "6RJFRUVRUFAQ+++/fyxZsqTG+i8qKopFixZVjDV+/Pga6zsthDQAAABAjbj//vtj6tSp8eGHH8bS\n",
              "pUvjlltuWeec0tLS79X31KlTY5ttttncElPNLbgBAACgDisvL49la1ZndYwmDRpGTk7V13msXLky\n",
              "VqxYEdtss03ceOONcf/990eLFi3io48+iltvvTVefvnlGDNmTJSWlkZeXl7ccMMN8dOf/jTGjx8f\n",
              "xx13XEU/n332WZx00klx3XXXRSaTiXnz5kXr1q2z8RFTQUgDAAAAddiyNauj5/2/y+oY7x/5u9g6\n",
              "v9EmzzvyyCOjUaNG8fnnn0fv3r3j+OOPj1tvvTXefPPNmDBhQpSUlERERM+ePeOSSy6JiIi///3v\n",
              "ccIJJ8THH38cQ4YMialTp0ZExBNPPBGnnXZanH766Vn7XGnjcicAAACgRqy93GnhwoXRsWPHOPXU\n",
              "UyMiol+/fhUBTUTEhAkTYsCAAVFYWBinnXZazJw5M77++uuK9kmTJsWIESPikUceiYKCgi39MWpN\n",
              "VlfSdOnSJfLz82OrrbaKiIjzzz8/Dj/88GwOCQAAAPVKkwYN4/0jf5f1MaqjYcOGMXz48Ljwwguj\n",
              "b9++0aRJk4q2lStXxtFHHx1PPfVU7LHHHvHll19Gy5YtY8WKFdG0adP4+OOPY/jw4TFq1KgYOHBg\n",
              "TX+UVMv65U4PPfRQpbQMAAAAqDk5OTlVuhRpS3vuuefWuwpm+fLlsWbNmth+++0jIuLKK6+saFu0\n",
              "aFHst99+cd5558X++++/xWpNC5c7AQAAADXiyCOPjKKioigsLIwPP/ww/vCHP6xzzo9+9KM499xz\n",
              "Y9CgQdGrV69o2PD/Vunce++98fHHH8dNN90URUVFUVRUFFddddWW/Ai1KpMkSZKtzrt06RLNmjWL\n",
              "JEli4MCBceWVV653F+ZVq1bFqlWrKh377W9/GzfeeGO2SgMAAIA6adasWbFo0aIoLCysdBkR3/h2\n",
              "zJHJZGqxkm8sW7Yspk+fHu3atdvknamyupLmpZdeirfeeiveeOONaNWqVRxzzDHrPW/kyJHRvHnz\n",
              "So9JkyZlszQAAACAVMlqSNOpU6eIiGjQoEGcddZZ8fLLL6/3vPPPPz8WL15c6VHfNgcCAAAA6res\n",
              "bRy8bNmyWLNmTbRo0SIiIh544IHo16/fes/Nz8+P/Pz8Ssdyc3OzVRoAAABA6mQtpPniiy/ikEMO\n",
              "ibKyskiSJAoKCuKee+7J1nAAAAAAdVrWQpqCgoL417/+la3uAQAAoF7Kyfm/nUuyeC8gaoFbcAMA\n",
              "AEAd8u2Qhh8Wf7IAAAAAKSCkAQAAgDoqk8mk6rFmzZo455xzoqCgILp16xY9e/aMX/ziF3HfffdF\n",
              "z5491/uemTNnRv/+/Wushrosa3vSAAAAANlXXl4eyaqvszpGJr9plS6zOuKII2LRokUxceLEaNOm\n",
              "TZSXl8fo0aNjwYIFG3xP165d4/XXX6/JcussIQ0AAADUYcmqr2PGKdtkdYztb10UsVWzjZ7zzjvv\n",
              "xFNPPRUfffRRtGnTJiK+2T/n+OOPj7/85S9RWloaRx11VLz22mtRVlYWd911V+y+++4xbdq06N+/\n",
              "fyxdujQivlkddO6558ZTTz0VX375ZfzXf/1XnHXWWRERceKJJ8arr74apaWl0bRp07jzzjujpKQk\n",
              "mx99i3K5EwAAALDZJkyYEJ07d4527dqtt33mzJlx3HHHxbRp0+LEE0+MCy+8cIN95efnx9tvvx1/\n",
              "+ctf4sILL4zVq1dHRMTvfve7eOedd2Lq1Klx0kknxRlnnJGVz1JbrKQBAACAOiyT3/SblS5ZHmNz\n",
              "dezYMfbcc8+IiNhtt93iD3/4wwbPPf744yMiol+/fpGbmxuzZs2KwsLCePzxx2PUqFHx9ddfR5Ik\n",
              "sXjx4s2uK02ENAAAAFCH5eTkbPJSpC1h8ODB8cknn8TcuXNju+22W6c9Pz+/4nlubm6UlpZusK/G\n",
              "jRtXOresrCw++OCDOO+882L8+PHRu3fvmDBhQuy11141+yFqmcudAAAAgM3Wp0+f2GeffeKoo46K\n",
              "+fPnR0RUbBz84Ycfbnb/ixYtiry8vOjUqVOUl5fHDTfcsNl9po2QBgAAAKgRDz30UPTp0ycGDBgQ\n",
              "hYWFUVhYGH/729+iVatWm933oEGD4oADDoiioqLo27dvdOzYsQYqTpdMkiRJbRexPmeffXZcd911\n",
              "tV0GAAAApMrs2bNj/vz5UVhYGE2aNKntclLn2zFHJpOpxUq+sWzZspg+fXq0a9cuWrduvdFzraQB\n",
              "AAAASAEhDQAAAEAKCGkAAAAAUkBIAwAAAJACebVdAAAAAFB1ubm5Fc9Tei8gvicraQAAAKAOycnx\n",
              "T/mqSMOdnarLnywAAACw2e6+++7o3bt39OzZMwoKCmLnnXeO8vLyzerz7LPPjuXLl1e8Hj58eFx6\n",
              "6aWbfN/agGbRokXRuHHjOPTQQyu1v/vuu9GrV6/o2bNn3HDDDeu8/+OPP46ddtpps2r/PlzuBAAA\n",
              "AHVUWlaLzJw5M/7zP/8zJkyYED169IiIiFdeeSUymcxm1XjDDTfEBRdcEE2aNKl0vKp9jh49Onr3\n",
              "7h3PPPNMfPXVV9GiRYuIiPjzn/8cxcXF8cADD6zzntWrV0fXrl3j9ddf/951f19W0gAAAEAdVl5e\n",
              "HitXrM7qY1MrYmbPnh05OTnRpk2bimO77rprxaVZL774YvTr1y+6d+8effv2jb/97W8RETFt2rTY\n",
              "euutK97z1VdfVQQwRx55ZEREDBkyJIqKiuKzzz6LiIipU6fG4MGDo3PnzrH33nvHypUrN1jXPffc\n",
              "E7/61a9iwIABMXr06IiIuOWWW+L222+Pp59+OoqKimLy5MkxcODAOPbYY6OkpCR22223dep67rnn\n",
              "Yscdd4wePXpE9+7d4957742IiJNOOin69OkTRUVF0b9//5gyZUqV/sw2xEoaAAAAqMNWryqNiy74\n",
              "W1bHuPSKvaPRVg032D5o0KDo379/dOnSJQYOHBiDBg2K4447LgoKCmLlypVxxBFHxM033xzDhw+P\n",
              "p59+Oo488siYPn36Rse8//7744EHHojx48dH69atK46/++678c9//jO22mqrGDBgQNx9990xYsSI\n",
              "dd4/efLkmDNnThxyyCGxZs2auPbaa+M///M/4/TTT48ZM2bE4sWL409/+lPF+TNmzIiJEydGfn5+\n",
              "TJs2reL4F198EYcffnjcd999se+++0ZZWVksWLAgIiIuvvjiaN++fURE3H777XHGGWfEyy+/XLUf\n",
              "6npYSQMAAABsltzc3HjmmWfiH//4R+y1114xYcKEKC4ujnfeeSfefPPNyGQyMXz48IiI2GeffaJl\n",
              "y5YxYcKE7zXWz372s9h6660jLy8vdtxxxw2GPbfddlsceuihkZeXF4cddlh89tln8cYbb2yw31/8\n",
              "4heRn5+/zvEXXnghunbtGvvuu2/FZ912220jIuKJJ56IkpKSKCwsjKuuuiref//97/WZ1rKSBgAA\n",
              "AOqwhvl5cekVe2d9jKro169f9OvXL84999z48Y9/HA8//HDst99+65y39pKmvLy8SpdSfXuT4A1p\n",
              "1KhRxfPc3NwoLS1d55xVq1bF2LFjIy8vL8aOHRsREStWrIjbbrstbr/99vX2++3Lm6rigw8+iF//\n",
              "+tcxfvz46N27d0yYMCH22muvavXxXVbSAAAAQB2Wk5MTjbZqmNXHpm77/dFHH1XsMxMRMW/evPj0\n",
              "00+jsLAwiouLI0mSirDk2WefjQULFsTgwYOjY8eOkSRJTJ48OSIiRo0aVanfJk2axJdfflntn8n9\n",
              "998fHTt2jHnz5sXs2bNj9uzZ8dJLL8WYMWNi1apV1eprzz33jE8++ST++te/RkREWVlZfPHFF7Fo\n",
              "0aLIy8uLTp06RXl5+XrvElVdQhoAAABgs6xZsyYuvfTS6Ny5cxQVFcWQIUPi8MMPj6OPPjoaNWoU\n",
              "Dz74YFx22WXRvXv3OPvss+O+++6LFi1aRMOGDeOKK66Igw46KPr06RNr1qyp1O9JJ50Ue++9d6WN\n",
              "g6ti9OjR69x2e8cdd4xtt912vXd02pg2bdrEgw8+GBdccEF07949evfuHc8991wMGjQoDjjggCgq\n",
              "Koq+fftGx44dq9Xv+mSSJEk2u5csOPvss+O6666r7TIAAAAgVebPnx+zZ8+OwsLCdW5NTfosW7Ys\n",
              "pk+fHu3atau0AfL6WEkDAAAAkAJCGgAAAIAUENIAAAAApICQBgAAACAFqnajcwAAACAV8vK++ad8\n",
              "kiSR0nsBsR65ubmbPEdIAwAAAHVITk5OZDKZyGQyQpo6IEmSyGQyQhoAAAD4oVob1JBuOTlV32nG\n",
              "njQAAABQR60NatLwyMnJiYULF1Y61qFDh5gwYUKllT8dOnSIIUOGVDrvgw8+iLy8vOjZs2fFo7i4\n",
              "uNY/U008qsNKGgAAAGCLeOKJJ6JZs2YxderUmDp1ahQVFVW0NW7cOKZOnVqL1dU+IQ0AAADUYeXl\n",
              "5RHL1mR3kCYNqnXZzobceeedceyxx8a0adPitttuixtuuGHza/sBEdIAAABAXbZsTcwbdFNWh2gz\n",
              "8cyIrfM3q4958+bFiy++GKNHj44ZM2bEgQceGNdee23FhrrLly+vtLKmR48e8dhjj23WmHWNkAYA\n",
              "AADImrX7stxxxx2xxx57RKtWrSoejz76aAwfPjwiXO4UIaQBAACAuq1Jg29WumR5jE3ZZpttYt68\n",
              "edGqVauKY4sWLYq2bdtGRMS9994bCxYsiPbt20dExLJly+LOO++sCGkQ0gAAAECdlpOTs9mXItWE\n",
              "3XffPW6++ea49dZbIyLiD3/4Q3To0CE6d+4cr7zySnz55ZfxxRdfVFzetGDBgujcuXPMnj27NstO\n",
              "FbfgBgAAADbbbbfdFnPnzo3u3btHUVFRPPjgg/HII49ERMSoUaPioIMOqghoIiJatWoVu+yyS9x+\n",
              "++0R8X970nz78dVXX9XGR6k1VtIAAAAAm23bbbeNRx99dL1t995773qP/+1vf6t4XlZWlpW66hIr\n",
              "aQAAAABSQEgDAAAAkAJCGgAAAIAUENIAAABAHZQkSW2XQA2zcTAAAADUIQ0aNIiIiNLS0igtLf3m\n",
              "FtykVmlpaURE5OVtOoIR0gAAAEAdkpOTE3l5eZHJZGLNmjW1XQ6bUF5eHrm5uZVuP74hQhoAAACo\n",
              "YzKZTDRo0CDy8/NruxSqoKqrnayJAgAAgDook8lETk5Oah65ubmxZMmSSscKCgrirbfeipycnNhz\n",
              "zz3j8ccfrwgsTjvttNhjjz1i6dKltV57Nh+ZTKbKf6ZW0gAAAEAdliRJrC5bkdUxGuZuVa2wYWPW\n",
              "rFkTv/zlL+Prr7+OZ555Jrbaaqsa6feHQEgDAAAAddjqshVx5sM9szrGTYe+H/l5jTe7nxUrVsRB\n",
              "Bx0U22yzTTz66KNV2ky3PnG5EwAAALBFnHHGGdGiRYu49957BTTr4ScCAAAAdVjD3K3ipkPfz/oY\n",
              "39e3L5MaOnRoPP/88/H222/HDjvsUBOl/aAIaQAAAKAOy2QyNXIp0uZq3bp1LFy4MFq0aFFxbMGC\n",
              "BdGmTZuK14ceemgceOCBsffee8fTTz8dJSUlW77QFHO5EwAAALDZhg4dGqNGjap4fc8990RBQUG0\n",
              "bdu20nmHHXZY3HLLLbHPPvvEv/71ry1dZqpZSQMAAABsthtuuCHOOuus2GGHHSInJye22267ePjh\n",
              "h9d77vDhwyMnJyf22WefeOqpp2KnnXbawtWmk5AGAAAA2GwtW7aMe++9d4Pt//jHPyq9HjZsWAwb\n",
              "NizLVdUtLncCAAAASAEhDQAAAEAKCGkAAACgDiovL6/tEqiCJEmqfK49aQAAAKAOadiwYeTk5MTs\n",
              "2bOjdevW0bBhw8hkMrVdFuuRJEnMnz8/MplMNGjQYJPnC2kAAACgDsnJyYmuXbvGnDlzYvbs2bVd\n",
              "DpuQyWSiQ4cOkZubu8lzhTQAAABQxzRs2DA6deoUpaWlUVZWVtvlsBENGjSoUkATIaQBAACAOmnt\n",
              "JTRVuYyGusHGwQAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAAUkBIAwAAAJACQhoAAACAFBDSAAAA\n",
              "AKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJCGgAAAIAU2CIh\n",
              "zejRoyOTycS4ceO2xHAAAAAAdU7WQ5qZM2fGHXfcEYMHD872UAAAAAB1VlZDmvLy8jjhhBPi5ptv\n",
              "jvz8/A2et2rVqliyZEmlR1lZWTZLAwAAAEiVrIY01113Xeyyyy6x0047bfS8kSNHRvPmzSs9Jk2a\n",
              "lM3SAAAAAFIlayHNO++8E2PGjInf/OY3mzz3/PPPj8WLF1d6DBw4MFulAQAAAKROXrY6fvnll2Pm\n",
              "zJnRrVu3iIiYO3dunHTSSTFnzpw45ZRTKp2bn5+/zuVQubm52SoNAAAAIHWytpLmlFNOiTlz5sTM\n",
              "mTNj5syZMXjw4Lj99tvXCWgAAAAA2EK34AYAAABg47J2udN3/eMf/9hSQwEAAADUOVbSAAAAAKSA\n",
              "kAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJCGgAAAIAUENIAAAAA\n",
              "pICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghpAAAAAFJASAMAAACQAkIaAAAAgBQQ0gAA\n",
              "AACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAAUkBIAwAAAJACQhoAAACAFBDS\n",
              "AAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJCGgAAAIAU\n",
              "ENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghpAAAAAFJASAMAAACQAkIaAAAA\n",
              "gBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAAUkBIAwAAAJACQhoA\n",
              "AACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJC\n",
              "GgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghpAAAAAFJASAMAAACQ\n",
              "AkIaAAAAgBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAAUkBIAwAA\n",
              "AJACQhoAAACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAAAABSIC+b\n",
              "ne+9994xd+7cyMnJia233jpuuumm6NevXzaHBAAAAKiTshrS/O///m+0aNEiIiIeffTROPbYY+PN\n",
              "N9/M5pAAAAAAdVJWL3daG9BERCxevDgymUw2hwMAAACos7K6kiYi4pe//GW88MILERHx1FNPrfec\n",
              "VatWxapVqyodKysry3ZpAAAAAKmR9Y2D77nnnvj000/j8ssvj1//+tfrPWfkyJHRvHnzSo9JkyZl\n",
              "uzQAAACA1MgkSZJsqcG22mqr+Oyzz6Jly5aVjq9vJc1vf/vbuPHGG7dUaQAAAAC1KmuXO3311Vex\n",
              "fPnyaNeuXUREjBs3Llq2bBk/+tGP1jk3Pz8/8vPzKx3Lzc3NVmkAAAAAqVOtkGbGjBlxxx13xPPP\n",
              "Px+fffZZbLXVVlFcXBzDhw+Pww47LPLy/q+7xYsXx6GHHhorVqyInJycaN26dfzlL3+xeTAAAADA\n",
              "elQ5pBkxYkS88cYbceihh8a1114b2223XaxcuTLef//9ePrpp+Pqq6+O2267LQYPHhwREZ07d7av\n",
              "DAAAAEAVVTmkOeCAA2LUqFHrHO/bt28cdthhsXDhwpgxY0aNFgcAAABQX1Q5pPnZz3620faWLVuu\n",
              "syEwAAAAAFVTrVtwv/baa3H44YdHnz59ok+fPnHEEUfE5MmTs1UbAAAAQL1R5ZDm1Vdfjb333jsK\n",
              "Cgri8ssvj8suuyy6du0ae++9d0ycODGbNQIAAAD84FX5cqerr746/vSnP8XBBx9ccezggw+OwYMH\n",
              "x8iRI2PcuHHZqA8AAACgXqjySpp33323UkCz1oEHHhjvvfdejRYFAAAAUN9UOaRp3LjxBtuaNGlS\n",
              "I8UAAAAA1FdVvtxp1apV8fbbb0eSJOu0rVy5skaLAgAAAKhvqhzSrFixIg444ID1tmUymRorCAAA\n",
              "AKA+qnJIM3PmzCyWAQAAAFC/VXlPmu/66quvYty4cfH222/XZD0AAAAA9VKVQ5qjjz46pkyZEhHf\n",
              "BDTFxcVxwQUXxP/7f/8vRo8ena36AAAAAOqFKoc0r7/+epSUlERExJ///OcoLCyM9957LyZPnhw3\n",
              "3XRTtuoDAAAAqBeqHNI0atSo4vnLL78cBx98cEREdOrUqearAgAAAKhnqhzSlJWVxeLFi6O0tDRe\n",
              "fvnl+PGPf1zR5hbcAAAAAJunynd3OuWUU2LHHXeMZs2aRUFBQRQXF0dExNtvvx3bbrtt1goEAAAA\n",
              "qA+qHNKcfPLJ0b9///j8889j7733rjjesGHDuP7667NSHAAAAEB9UeWQJiKif//+0b9//0rHevTo\n",
              "UaMFAQAAANRHVQ5pttlmm8hkMhWvM5lMtGnTJvbee+/4/e9/H02bNs1KgQAAAAD1QZVDmilTpqxz\n",
              "bMGCBTFq1Kg455xzYtSoUTVZFwAAAEC9UuWQpnPnzus9dtttt8WOO+5Yo0UBAAAA1DdVvgX3BjvI\n",
              "yYmcnM3uBgAAAKBe2+x05emnn44f/ehHNVELAAAAQL1V5cud+vXrV2nj4IiIhQsXRoMGDeLRRx+t\n",
              "8cIAAAAA6pMqhzQ33HBDpdeZTCZat24d3bp1i7y8at3JGwAAAIDvqHK6svvuu2ezDgAAAIB6rcp7\n",
              "0px66qnx2WefrbctSZIYM2ZM3H///TVWGAAAAEB9UuWVNPvtt1/st99+sc0228SgQYNi2223jZUr\n",
              "V8a0adPin//8Z+y3335x6aWXZrNWAAAAgB+sKoc0P//5z+PnP/95vPLKK/GPf/wjPvzww2jcuHHs\n",
              "ueeecd1110WrVq2yWScAAADAD1q1d/zdddddY9ddd81GLQAAAAD1VpX3pAEAAAAge4Q0AAAAACkg\n",
              "pAEAAABIgWqHNJdffnmVjgEAAABQddUOacaOHVulYwAAAABUXZXv7vTMM8/E008/HZ9//nmcffbZ\n",
              "FccXL16clcIAAAAA6pMqhzSNGjWKFi1aRE5OTjRv3rzieMeOHeO3v/1tVooDAAAAqC+qHNLsvvvu\n",
              "sfvuu8dBBx0UxcXF2awJAAAAoN6pckizVu/eveOhhx6KGTNmRGlpacXxiy66qEYLAwAAAKhPqh3S\n",
              "HHHEETF37twYOHBg5ObmZqMmAAAAgHqn2iHN22+/HVOnTo1MJpONegAAAADqpWrfgrtjx46xevXq\n",
              "bNQCAAAAUG9VeyVNYWFh7LHHHnHwwQdHo0aNKo6feeaZNVoYAAAAQH1S7ZBm1apVUVRUFO+//37F\n",
              "MZc+AQAAAGyeaoc0o0ePzkYdAAAAAPVatUOae+65Z73Hf/nLX252MQAAAAD1VbVDmieeeKLi+cqV\n",
              "K+OVV16JwYMHC2kAAAAANkO1Q5qHH3640uuPP/44LrzwwhorCAAAAKA+qvYtuL+ra9eu8e6779ZE\n",
              "LQAAAAD1VrVX0jz++OMVz8vKymLixImRn59fo0UBAAAA1DfVDmmuv/76/3tzXl4UFhbGQw89VKNF\n",
              "AQAAANQ31Q5pXnjhhWzUAQAAAFCvVTukifhm8+Bnn302IiKGDh0ahxxySI0WBQAAAFDfVHvj4Esv\n",
              "vTRGjhwZvXr1it69e8fIkSPj8ssvz0ZtAAAAAPVGtVfSPPLIIzFhwoRo3LhxRESccMIJsfPOO8dv\n",
              "fvObGi8OAAAAoL6o9kqaJEkqApqIiCZNmkSSJDVaFAAAAEB9U+2VNAMHDoyjjz46TjzxxIiIuOuu\n",
              "u2LgwIE1XhgAAABAfVLtlTQ33XRTtGvXLs4+++w4++yzo23btnHTTTdlozYAAACAeqPaK2maNGkS\n",
              "V111VTZqAQAAAKi3qh3SLF++PO6+++748MMPo7S0tOK41TQAAAAA31+1Q5phw4ZFgwYNYsCAAZGb\n",
              "m5uNmgAAAADqnWqHNLNmzYr33nsvG7UAAAAA1FvV3ji4qKgoFixYkI1aAAAAAOqtaq+k+f3vfx87\n",
              "77xzDBgwIBo1alRx/E9/+lONFgYAAABQn1Q7pDnxxBNj5513jv79+9uTBgAAAKCGVDukWbRoUdxz\n",
              "zz3ZqAUAAACg3qr2njTFxcXx+eefZ6MWAAAAgHqr2itp5s+fH3369Imdd9650p40Y8eOrdHCAAAA\n",
              "AOqTaoc0Rx11VBx11FHZqAUAAACg3qp2SHPMMcdUel1WVhZPPPFEjRUEAAAAUB9Ve0+ataZNmxbn\n",
              "nntutG/fPi6//PKarAkAAACg3qnWSprly5fHQw89FHfeeWd8/PHHsWLFinj11VejqKgoW/UBAAAA\n",
              "1AtVXklz4oknRseOHePxxx+P8847L2bNmhUtWrQQ0AAAAADUgCqvpHnwwQejf//+MWLEiBg6dGhk\n",
              "MpnIZDLZrA0AAACg3qjySpo5c+bEUUcdFZdeeml07tw5fvOb38SaNWuyWRsAAABAvVHlkKZp06Zx\n",
              "/PHHx/jx4+Ppp5+OlStXxurVq2PIkCHxxz/+MZs1AgAAAPzgfa+7O/Xq1Suuueaa+Pzzz+Occ86J\n",
              "J598sqbrAgAAAKhXvvctuCMi8vLy4pBDDhHSAAAAAGymzQppAAAAAKgZWQtpVq5cGQcddFB07949\n",
              "iouLY6+99orp06dnazgAAACAOi2rK2lOOumkmDZtWrz55ptx4IEHxgknnJDN4QAAAADqrKyFNI0a\n",
              "NYr99tsvMplMREQMHjw4Zs6cud5zV61aFUuWLKn0KCsry1ZpAAAAAKmzxfakufHGG+PAAw9cb9vI\n",
              "kSOjefPmlR6TJk3aUqUBAAAA1LotEtJcccUVMX369Bg5cuR6288///xYvHhxpcfAgQO3RGkAAAAA\n",
              "qZCX7QGuueaaGDt2bDz33HPRuHHj9Z6Tn58f+fn5lY7l5uZmuzQAAACA1MhqSHPdddfFAw88EM89\n",
              "91y0aNEim0MBAAAA1GlZC2k+++yzOOecc6KgoCB+8pOfRMQ3K2YmTpyYrSEBAAAA6qyshTQdOnSI\n",
              "JEmy1T0AAADAD8oWu7sTAAAAABsmpAEAAABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJCGgAAAIAU\n",
              "ENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghpAAAAAFJASAMAAACQAkIaAAAA\n",
              "gBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAAUkBIAwAAAJACQhoA\n",
              "AACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJC\n",
              "GgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghpAAAAAFJASAMAAACQ\n",
              "AkIaAAAAgBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAAUkBIAwAA\n",
              "AJACQhoAAACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAAAABSQEgD\n",
              "AAAAkAJCGgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghpAAAAAFJA\n",
              "SAMAAACQAkIaAAAAgBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAA\n",
              "UkBIAwAAAJACQhoAAACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoIaQAA\n",
              "AABSQEgDAAAAkAJCGgAAAIAUyGpIc+aZZ0aXLl0ik8nElClTsjkUAAAAQJ2W1ZBm+PDh8corr0Tn\n",
              "zp2zOQwAAABAnZeXzc532223Kp23atWqWLVqVaVjZWVl2SgJAAAAIJVSsSfNyJEjo3nz5pUekyZN\n",
              "qu2yAAAAALaYVIQ0559/fixevLjSY+DAgbVdFgAAAMAWk9XLnaoqPz8/8vPzKx3Lzc2tpWoAAAAA\n",
              "trxUrKQBAAAAqO+yGtKMGDEiOnToEJ999lkMHTo0CgsLszkcAAAAQJ2V1cudRo0alc3uAQAAAH4w\n",
              "XO4EAAAAkAJCGgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghpAAAA\n",
              "AFJASAMAAACQAkIaAAAAgBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAAAEAKCGkA\n",
              "AAAAUkBIAwAAAJACQhoAAACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASENAAAAQAoI\n",
              "aQAAAABSQEgDAAAAkAJCGgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABA\n",
              "CghpAAAAAFJASAMAAACQAkIaAAAAgBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAASAEhDQAA\n",
              "AEAKCGkAAAAAUkBIAwAAAJACQhoAAACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEAAABIASEN\n",
              "AAAAQAoIaQAAAABSQEgDAAAAkAJCGgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgB\n",
              "IQ0AAABACghpAAAAAFJASAMAAACQAkIaAAAAgBQQ0gAAAACkgJAGAAAAIAWENAAAAAApIKQBAAAA\n",
              "SAEhDQAAAEAKCGkAAAAAUkBIAwAAAJACQhoAAACAFBDSAAAAAKSAkAYAAAAgBYQ0AAAAACkgpAEA\n",
              "AABIASENAAAAQAoIaQAAAABSQEgDAAAAkAJCGgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCk\n",
              "AQAAAEiBrIY0H374YQwZMiS6d+8eAwYMiHfffTebwwEAAADUWVkNaUaMGBEnnXRSfPDBB/HrX/86\n",
              "jj322GwOBwAAAFBnZS2kmTdvXkyePDmOOuqoiIg45JBD4tNPP43p06dna0gAAACAOisvWx1/+umn\n",
              "0bZt28jL+2aITCYTnTp1ilmzZkVhYWGlc1etWhWrVq2qdKysrCxbpQEAAACkTio2Dh45cmQ0b968\n",
              "0mPSpEm1XRYAAADAFpO1kKZjx44xZ86cKC0tjYiIJEli1qxZ0alTp3XOPf/882Px4sWVHgMHDsxW\n",
              "aQAAAACpk7WQpk2bNrHjjjvGfffdFxERY8aMiQ4dOqxzqVNERH5+fjRr1qzSIzc3N1ulAQAAAKRO\n",
              "1vakiYgYNWpUHHvssXHFFVdEs2bNYvTo0dkcDgAAAKDOympI06NHj3j11VezOQQAAADAD0IqNg4G\n",
              "AAAAqO+ENAAAAAApIKQBAAAASAEhDQAAAEAKCGkAAAAAUkBIAwAAAJACQhoAAACAFBDSAAAAAKRA\n",
              "JkmSpLaLWJ9hw4ZFly5daruMTSorK4tJkybFwIEDIzc3t7bLgR8U8wuyw9yC7DC3IHvq2vzq3Llz\n",
              "/Md//Edtl0EdlNqQpq5YsmRJNG/ePBYvXhzNmjWr7XLgB8X8guwwtyA7zC3IHvOL+sLlTgAAAAAp\n",
              "IKQBAAAASAEhDQAAAEAKCGk2U35+flx88cWRn59f26XAD475BdlhbkF2mFuQPeYX9YWNgwEAAABS\n",
              "wEoaAAAAgBQQ0gAAAACkgJAGAAAAIAWyGtKsXLkyDjrooOjevXsUFxfHXnvtFdOnT69onzdvXuyz\n",
              "zz7RrVu36NOnT7z00ksVbVdccUX06NEjcnJyYty4cZX6/fd///fYYYcdoqSkJAYMGBB///vfN1jD\n",
              "9x0jW/3cdddd0a1bt9h+++3jxBNPjDVr1lSp7duWL18ev/jFL6KwsDC6d+8ejzzySJXavmvixIlR\n",
              "XFwc3bt3jz333DM+//zzKrVR+8ytdW1o/sycOTP22GOPaN68eZSUlGy0D3OLCPNrfTb1/ZQkSey5\n",
              "557RokWLDfZRXl4eZ5xxRmy//fZRWFgYt9xyS5XavuvDDz+MIUOGRPfu3WPAgAHx7rvvVqmN2mdu\n",
              "rWtDc+vVV1+NkpKSKCkpid69e8eIESNi1apV6+3Ddxfm1ro2NLeef/75GDhwYPTq1St69+4d5557\n",
              "bpSXl6+3D99b1Joki1asWJE8+eSTSXl5eZIkSXLzzTcnu+++e0X7v//7vycXX3xxkiRJMmnSpKR9\n",
              "+/bJ6tWrkyRJkokTJyYzZsxIdt999+TRRx+t1O+iRYsqnr/xxhvJNttsk5SVla23hu87Rjb6+eij\n",
              "j5K2bdsmc+bMScrLy5P9998/ueWWWzbZ9l2XXHJJcswxx1S8r3Xr1smCBQs22fZtZWVlyfbbb588\n",
              "//zzSZIkyX//938nw4cP32Qb6WBuVbax+bNw4cLk5ZdfTv7yl78kxcXFG63F3CJJzK/vqsr307XX\n",
              "XpuccMIJSfPmzTfYz913353sueeeSWlpabJw4cKkU6dOyTvvvLPJtu/6yU9+kowePTpJkiR5+OGH\n",
              "k/79+1epjdpnblW2sbm1bNmyiv7KysqSgw46KLnuuuvW24/vLsytyjY2t954441kxowZSZJ883Pb\n",
              "ZZddKr43vsv3FrUlqyHNd7322mtJ586dK143adIkmTNnTsXrAQMGJM8++2yl92xqEr7wwgsb/Quj\n",
              "JsaoqX6uvvrqZMSIERWvn3zyyWSXXXbZZNt39erVK3n11VcrXh966KHJHXfcscm2b5s0aVLSo0eP\n",
              "itdLlixJ8vPzkxUrVmy0jXQytzY9f1544YVNhjTmFutjfm18fr3zzjvJj3/842T69OkbDWn222+/\n",
              "5IEHHqh4/V//9V/JhRdeuMm2b/viiy+SrbfeOlmzZk2SJElSXl6ebLvttsmHH3640TbSydyq2n/7\n",
              "rVixIhk6dGhy/fXXr7cf3118l7lV9X9XnXbaaRWh0Hf53qK2bNE9aW688cY48MADIyJi4cKFsWbN\n",
              "mthuu+0q2rt06RKzZs2qUl/nnXdebL/99jFs2LAYM2ZM5OSs+1E2d4ya7mfWrFnRuXPn9faxsbbZ\n",
              "s2dXukzj+/Zz2223xUUXXbTe87beeuto1qxZzJ49e6NtpJO5teHf+40xt6gK82vDv/tr1qyJE088\n",
              "MUaNGhW5ubnrvLekpKTi9/v7zq/HH388TjjhhIiI+PTTT6Nt27aRl5cXERGZTCY6deoUs2bN2mgb\n",
              "6WRubfy7a+bMmVFcXBytWrWK5s2bx6mnnhoRvrvYNHOrav9dOHfu3HjkkUfi5z//ecUx31ukwRYL\n",
              "aa644oqYPn16jBw5skb6u/LKK2PGjBnxv//7v3HuuefG6tWra6TfNGrXrl1MmTJls/s5+eST49JL\n",
              "L938gkgVc+v7M7fYFPNr4y655JIYNmxY9OzZc73tU6ZMiXbt2m3WGAcccEDceeedm9UH6WNubVqX\n",
              "Ll3izTffjLlz58aqVati7NixEeG7i40zt6pmyZIlsf/++8e5554b/fv3rzjue4s02CIhzTXXXBNj\n",
              "x46Nv/71r9G4ceOIiGjZsmXk5eXF3LlzK86bOXNmdOrUqVp9//SnP42lS5fG22+/Hc8991zFJmu/\n",
              "//3vv/cYNdXPd3Xq1Ck++eST9faxsbZs9PPd85YuXRqLFy+Odu3abbSNdDG3vlGd+fN9+zG36h/z\n",
              "6xsb+91/8cUX4+abb44uXbrErrvuGkuWLIkuXbrE/Pnzq9VPVedXx44dY86cOVFaWhoR32xYPGvW\n",
              "rOjUqdNG20gXc+sbVf29b9q0aRxxxBHx5z//udr9+O6qX8ytb2zq937p0qWxzz77xIEHHhhnn332\n",
              "9+rH9xZZle3rqa699tpkxx13TL788st12o455phKG0O1a9euYmOotb57zeHq1asrXac3ceLEZJtt\n",
              "tllv/993jGz1M2PGjHU2sbr55ps32fZdF1988TqbwM2fP3+Tbd9WVlaWFBQUVNoE7pBDDtlkG+lh\n",
              "bv2fqsyfquxJY26xlvn1f6r6/fTxxx9vdE+a0aNHr7PJ4ltvvbXJtu/afffdK22yuNNOO1WpjXQw\n",
              "t/7PxubWhx9+WNHfqlWrksMOOyy54IIL1tuP7y6SxNz6to3NraVLlyZDhgxJLrnkko3WkSS+t6g9\n",
              "WQ1pPv300yQikoKCgqS4uDgpLi5OBg4cWNE+d+7cZK+99koKCwuTXr16VfzlnyRJctlllyXt27dP\n",
              "GjZsmLRs2TJp3759Mm/evGTZsmXJkCFDkt69eyfFxcXJkCFDkr///e8brOH7jJHNfm6//fakoKAg\n",
              "KSgoSI477rhKf+lsqO3zzz+v9I/Lr7/+OjnssMOSgoKCpFu3bslDDz1UpbZbb701+e1vf1vxevz4\n",
              "8Unfvn2Tbt26Jbvvvnsya9asKrVR+8ytdW1o/ixbtixp37590qpVq6RBgwZJ+/btk/POOy9JEnOL\n",
              "9TO/1rWx76611hfSFBcXJ59//nmSJElSWlqanHrqqUnXrl2TgoKC5IYbbqg4b2Ntjz32WHL88cdX\n",
              "vJ46dWoyePDgpFu3bslOO+1U6T+KN9ZG7TO31rWhuTVq1Kikd+/eyQ477JD06tUrOeOMMyo26vXd\n",
              "xXeZW+va0Ny6/PLLk7y8vIqfU3FxcXL55ZdXvM/3FmmQSZIkqdWlPAAAAABsuY2DAQAAANgwIQ0A\n",
              "AABACghpAAAAAFJASAMAm9ClS5fo0aNHFBcXR2FhYRx44IExfvz4Kr133LhxMWHChBqvadSoUVFU\n",
              "VBQlJSWxcOHCiuMXXXRRxS1NmzZtGl27dq14PW3atBqvY3N89dVXceWVV1Y6dsIJJ8QLL7xQSxUB\n",
              "ANQuGwcDwCZ06dIlxo0bFyUlJRERMXbs2DjuuOPimWeeiUGDBm30vccee2yUlJTEWWedVaM19ezZ\n",
              "M/70pz/FzjvvvMFz9thjjzjrrLPioIMOqnS8vLw8IiJycmr3/6uZOXNmlJSUxFdffVWrdQAApIWV\n",
              "NABQTcOGDYuTTz45rrnmmoiI+Pvf/x4777xz9OvXL3r37h133XVXREQ89dRT8fjjj8d///d/R0lJ\n",
              "Sdx5550REXHvvffGoEGDYscdd4zddtst3nzzzfWOM3ny5BgyZEjssMMOMXDgwPjnP/8ZERHDhw+P\n",
              "GTNmxLHHHhvDhw+vUs2/+93v4pBDDomhQ4dGnz59Ys6cOfGrX/0qBgwYECUlJbHbbrtVWmmTyWTi\n",
              "iiuuiIEDB0bXrl1j9OjREfFNwHP66adHz549o7i4OHbaaadYuXJllJaWxtChQ6N///7Ru3fvOPLI\n",
              "I2PZsmUV/Y0ePTpKSkqiuLg4+vfvHzNnzoyTTz45li5dGiUlJdG/f/+I+CZYGjduXEREzJs3L4YN\n",
              "GxZ9+/aNPn36xKhRoyr669KlS1x00UWx8847R9euXePyyy+v0s8BACDVavcO4ACQfp07d07+9a9/\n",
              "VTo2duzYpGfPnkmSJMmXX36ZlJaWJkmSJAsXLkw6deqUfPrpp0mSJMkxxxyTXH/99RXve+WVV5J9\n",
              "9903WblyZZIkSfLSSy8lvXr1WmfMVatWJR07dkyefvrpJEmS5OWXX0623XbbZOnSpRus6bt23333\n",
              "5NFHH02SJEkuvvjipG3btsncuXMr2ufNm1fx/IEHHkiGDh1a8ToikmuuuSZJkiR5//33k6ZNmyZr\n",
              "1qxJ3njjjaSoqCgpKytLkiRJvvrqq6SsrCwpLy9PFixYkCRJkpSXlycnn3xyMnLkyCRJkuSFF15I\n",
              "unTpksyePTtJkiRZtmxZsmzZsuTjjz9OmjdvvsGaDzvssOS8885LkiRJvvjii6RDhw7Jq6++WvH5\n",
              "zzjjjCRJkmT+/PlJs2bNks8++2yjPw8AgLTLq+2QCADqouRbVwsvXLgwjj/++Pjggw8iLy8vFi5c\n",
              "GO+880506NBhnfc99thj8eabb1a6TOrLL7+MFStWxFZbbVVxbNq0aZGTkxNDhw6NiIhdd901tt12\n",
              "25gyZUrsuuuu36vm/fbbL7bddtuK188++2zcfPPNsXTp0igvL48vv/yy0vn/9m//FhERRUVFkZeX\n",
              "F3Pnzo2CgoIoLS2N4447Ln7yk5/Ez372s8jJyYny8vK4/vrr48knn4zS0tJYvHhxDBkyJCIinnzy\n",
              "yTj66KOjbdu2ERHRuHHjKtX73HPPxeuvvx4REW3atIlhw4bFc889F4MHD46IiCOPPDIiIlq1ahUF\n",
              "BQXx8ccfR/v27b/XzwYAIA1c7gQA38Nrr70Wffr0iYiIk08+OXbdddd4++23Y8qUKdG9e/dYuXLl\n",
              "et+XJEkcc8wxMWXKlIrHnDlzKgU0G5LJZDar5qZNm1Y8nzVrVpx++ulx3333xTvvvBMPPvjgOjU3\n",
              "atSo4nlubm6UlpZG8+bN45133okjjzwypk6dGjvssENMnz497r///nj++efjxRdfjLfffjt+9atf\n",
              "bfBn8H199/Ovrz4AgLpMSAMA1fTYY4/FrbfeGuecc05ERCxatCg6d+4cmUwmXnrppUp7zDRr1iwW\n",
              "L15c8fqAAw6I++67L2bNmhUR3+zxMnny5HXG6NGjR5SXl8ezzz4bERHjx4+PuXPnVmxevLkWL14c\n",
              "DRo0iLZt20aSJHHLLbdU6X3z58+PZcuWxd577x1XXHFFdOnSJd57771YtGhRtGrVKpo1axZLly6N\n",
              "//mf/6l4z/777x/33XdfzJkzJyIili9fHsuXL49mzZrFihUrYvXq1esd66c//WnccccdFeOOHTs2\n",
              "9tprr8374AAAKeZyJwCogsMPPzwaNWoUy5Yti169esVTTz1VccnSlVdeGaeeempcdtllUVJSUulS\n",
              "pqOPPjqOPfbYGDduXJx22mlxwgknxNVXXx0HH3xwlJaWxurVq+NnP/tZxca5azVs2DDGjh0bZ555\n",
              "ZpxzzjnRqFGjeOSRRyqthtkcffv2jSOOOCJ69+4dLVu2XOcOUBvy6aefxoknnhhr1qyJsrKy2GWX\n",
              "XWLfffeN5cuXx2OPPRY9evSI1q1bx49//OP45JNPIiJit912i4svvjiGDh0amUwmGjZsGI888kh0\n",
              "7tw5fvnLX8YOO+wQTZs2XSesuummm+KUU06Jvn37RpIkceGFF27ybloAAHWZW3ADAAAApIDLnQAA\n",
              "AABSQEgDAAAAkAJCGgAAAIAUENIAAAAApICQBgAAACAFhDQAAAAAKSCkAQAAAEgBIQ0AAABACghp\n",
              "AAAAAFJASAMAAACQAv8fPBPZi3Pq98kAAAAASUVORK5CYII=\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-0d039a99-8556-4cd0-8f87-9da751f93cde\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-0d039a99-8556-4cd0-8f87-9da751f93cde\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "def _plot_series(series, series_name, series_index=0):\n",
              "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
              "  xs = series['Date of Transaction']\n",
              "  ys = series['Amount (USD)']\n",
              "  \n",
              "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
              "\n",
              "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
              "df_sorted = _df_11.sort_values('Date of Transaction', ascending=True)\n",
              "for i, (series_name, series) in enumerate(df_sorted.groupby('Transaction Type')):\n",
              "  _plot_series(series, series_name, i)\n",
              "  fig.legend(title='Transaction Type', bbox_to_anchor=(1, 1), loc='upper left')\n",
              "sns.despine(fig=fig, ax=ax)\n",
              "plt.xlabel('Date of Transaction')\n",
              "_ = plt.ylabel('Amount (USD)')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-b5c75205-c39b-472b-b1a6-779a104bf14d\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABIUAAAITCAYAAACZo6zJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABqnklEQVR4nO3dd3RUdf7G8WdK6gwJCEgnlBBCCCS0EEJXmg0borIIijRBLOBa\n",
              "f6uuq6CuawMVsLAKFlQQC4iIFekYQpGOhI70QCZ1Zu7vD2DWSEsgkztJ3q9z5hxm7p17n0QuxIfP\n",
              "fK/FMAxDAAAAAAAAKFesZgcAAAAAAABAyaMUAgAAAAAAKIcohQAAAAAAAMohSiEAAAAAAIByiFII\n",
              "AAAAAACgHKIUAgAAAAAAKIcohQAAAAAAAMohSiEAAAAAAIByiFIIAAAAAACgHLKbHeBc7rnnHn3x\n",
              "xRfavn27Vq5cqcTExPO+58iRI7r77ru1fPlyBQUF6ZprrtGzzz7r/7AAAAAAAJQgt9ut48ePKzs7\n",
              "2+woCAB2u11BQUEKDQ1VSEiIrNbzzwEFdCnUp08fPfjgg+rQoUOh3zNo0CC1b99e77//viRp3759\n",
              "/ooHAAAAAIApDh48qB07dshqtcpisZgdBwHGbrerXr16cjgc596vhPJckE6dOp3x9eXLl+uhhx7S\n",
              "sWPH5PF49Oijj+qmm27Sli1btGLFCs2YMcO3b/Xq1UsqLgAAAAAAfpeTk6MdO3YoPDxcl156qYKD\n",
              "gymGIEkyDEN5eXnav3+/tmzZoqZNm8puP3v1E9Cl0JkcPXpUQ4cO1Zw5c1SjRg0dPHhQLVu2VEpK\n",
              "itatW6fatWvrrrvu0ooVK1S5cmU999xzatGihdmxAQAAAAAoFocPH5bNZlOtWrVUoUIFs+MggBiG\n",
              "obCwMAUFBWnbtm1yuVyKjIw86/6lrhRatGiRfv/9d11xxRUFXt+4caPcbreWLVumsWPHatKkSfr6\n",
              "66919dVXKz09XUFBQSYlBgAAAACg+BVmzRiUT4X9vVHqSiHDMNS0aVMtWrTotG0rVqxQrVq11LVr\n",
              "V0nSFVdcoby8PG3fvl3R0dElHRUAAAAAACBglbpaMSUlRdu2bdP8+fN9r6WlpSkvL0+tWrVSRESE\n",
              "Vq9eLUlatmyZDMNQnTp1zIoLAAAAAECpkJ2drdjYWMXGxqphw4ay2Wy+51dddZXZ8Qpwu9366quv\n",
              "fGsKe71ebdq0Sa1bty6W4+fm5uree+/1ff3h4eGqVauW7/mp3qG0sxiGYZgd4myGDRum2bNna9++\n",
              "fapcubIqVKigLVu2KDU1VQ888IAOHTqk/Px81a1bV7NmzVJoaKh+/fVXjRgxQtnZ2QoJCdELL7yg\n",
              "zp07m/2lAAAAAABQLPbs2aMDBw4oOjr6vHeXuhBer1dr165V+/btdfz48QLbDMOQ2+02fYmWvLw8\n",
              "3w2o3n77bb+eKzs7W507d9Y999yj/v37+/VcF8swDHm9XuXk5Gjr1q2Kioo655pCAT0pNGnSJO3a\n",
              "tUtut1t//PGHtmzZIklq2bKlvv/+e61atUrr1q3T3LlzFRoaKklq1aqVli5dqtWrV2v58uUUQgAA\n",
              "AAAAXISsrCzVrFlTw4cPV/PmzdWnTx9t2bJFbdu2VVxcnKKjozVw4EB5PB7l5+fr5ZdfVrt27XT1\n",
              "1VerUaNGio+P1/r16+X1epWamqoWLVqocePGatSoke6991653W7NmDFDiYmJatKkiaKjo/Xyyy9L\n",
              "OlFy7N27V3379lV0dLQaN26sm266SYsXL9bUqVM1c+ZMxcbG6oEHHtDq1at9C2+73W5Nnz5dcXFx\n",
              "iomJUZs2bZSamiq3263PP/9c0dHR+tvf/qbGjRsrOjpaCxYsOO/3wTAMbdiwQZUrV/aVZTk5Obrm\n",
              "mmv0/PPPy+VyyWKx6O6771aTJk1Ur149TZw4UZLk8Xg0f/58JScnq2nTpmrSpImmTJnin/9gRVDq\n",
              "1hQCAAAAAAAl78iRI1q1apUsFouysrI0b948RUREyOVy6ZprrtGUKVM0cOBAGYahtWvXavny5apf\n",
              "v77uu+8+/etf/9KUKVM0fvx4XXHFFRo7dqy8Xq8OHjwoq9WqTp06acWKFbJardq5c6fatWun3r17\n",
              "q2bNmho1apTCwsK0ceNGWSwW7du3T1WqVNFtt93mmxQ6Nd0knZh02r59u4YNG6Z58+apRYsWmjx5\n",
              "sm666SZt2LBBhmEoPT1db775pv773//qP//5jx599NHzFkMWi0X16tVT+/btNXnyZN1///3atWuX\n",
              "Fi5cqGnTpvn2s9vt+u2337R69Wp17txZXbt2lcPh0KhRozR37lzVqVNHO3bsULt27dSlSxfVr1/f\n",
              "r//dziWgJ4UAAAAAAID5LBaL7rzzTt9drXJycjRy5EjFxsaqZcuWWrNmjVauXOnbNzExUbGxsbLZ\n",
              "bEpOTlZ6erqsVqvat2+v9957T6NGjdLnn3+uypUrS5J27dqlK6+8UjExMerZs6eOHj2qlStXyuv1\n",
              "6rvvvtOjjz4qm80mq9WqmjVrnjOr1+vVokWLFBMTo6SkJAUFBen222/X/v37lZ6eLkmqU6eOunbt\n",
              "KqvVqpSUFG3fvr1Q34egoCCNGjVKkydPltvt1qRJk3TdddcV+IjWiBEjZLVaFRsbqzZt2ujbb7/V\n",
              "Tz/9pJ07d6pnz56Ki4tTr169JMlXZJmFSSEAAAAAAHBepz6alZ+fr3HjxunQoUNauXKlbDabRo4c\n",
              "qZycHN++p5Z4kSSbzeZbh2jQoEHq2rWrZs+erfHjx+vll1/W3LlzNWLECPXs2VNz585Vbm6uWrVq\n",
              "pezsbL98HRaLRSEhIQXyeTyeQr3XZrOpQ4cOCgsL0+zZszVt2jTNmzfvvOeTpEaNGvmKs0ARsJNC\n",
              "r7zyitkRAAAAAADAXxiGoaNHj6patWoKDQ3Vjh079NVXX533fV6vV+vWrVO9evU0cuRIjRs3Tmlp\n",
              "aZKkY8eOqX79+vJ6vfr222+1ceNGSZLValW3bt00duxYeTweeb1e7dmzR9KJkiojI+O085yaSNq0\n",
              "aZNWrFih/Px8TZ06VdWqVVO9evUu+uu32+0aNmyY7rrrLkVHR6tZs2YFtk+aNMl3N7QVK1aoW7du\n",
              "6tixo3bt2qXPP/9c0ok1hhYvXlygSDNDwE4KFXZ0CwAAAAAAlJygoCDdf//9uuWWWxQTE6NLL71U\n",
              "HTp0OO/73G63pk6dqhkzZigoKEher1cvvfSSgoKC9OSTT+qBBx7QuHHjFBcXp+bNm0uSQkJCNH78\n",
              "eI0aNUqNGzeW3W5XYmKiPvjgA91444365JNPFBsbq6uvvloDBgyQdKIUqlu3riZOnKgBAwbI7XYr\n",
              "MjJSH3/8sW9q52LY7Xb169dPDz30kEaMGHHa9vz8fDVt2lTZ2dl69tln1bhxY3k8Hn3yySd6+OGH\n",
              "9eCDD8rtdqtmzZr65ptvLjrPxQjYW9KPHj1aL774otkxAAAAAAAIKP6+JT3OzePx6IcfftDgwYO1\n",
              "detW2Ww2SZLL5ZLT6dSBAwdUpUoVU7IV9Zb0ATspBAAAAAAAEEhycnI0YMAALViwQK+//rqvECqt\n",
              "KIUAAAAAAAAKITQ0VB9//PEZtzkcDgXoh7HOKmAXmgYAAAAAAID/UAoBAAAAAACUQ5RCAAAAAAAA\n",
              "5RClEAAAAAAAQDlEKQQAAAAAAC5aXl6eRo8erfr16ys6OlqxsbG65ZZbdODAgQs63g033KB//vOf\n",
              "591v9OjR6tu3r+/53LlzZbFY9NVXX/le69evn+655x799NNPuvLKKyVJBw4c0COPPFLgWElJSZo6\n",
              "dWqh8g0ZMkT3339/ofYtbhaL5YK/r39GKQQAAAAAAC7aLbfcopUrV2rp0qXasmWL1q1bp+7duxdL\n",
              "eXEu3bp105IlS3zP58+fr+bNm+u7777zvbZo0SJ1795dnTt31pw5cyRJhw8f1oQJE4o9j8fjkcfj\n",
              "Kfbj+gOlEAAAAAAApZjX69Xx3By/Prxe7zkzrF27VnPmzNH777+vSy+9VJJktVp15513Ki4uTtu3\n",
              "b1fbtm0VFxen6OhoDRgwwFecfPfdd4qLi1NsbKyio6P17LPP+o67YcMGJScnKyoqSj169FBOTs5p\n",
              "577sssu0f/9+bdmyRZL0yy+/6NFHH9XChQslSenp6dq3b58uu+wyffXVV4qNjZUkDR48WFlZWYqN\n",
              "jVXTpk19x/v555/VqlUr1alTR/369fO9np6erg4dOqhhw4Zq166d9uzZ49t2//33q2fPnurQoYNi\n",
              "YmK0fft2DRkyRPHx8YqNjVXr1q2VlpYmSfr3v/+tW265RZK0YsUKWSwWzZgxQ5I0ZswYjRkzRpLO\n",
              "+v7iZC/2IwIAAAAAgBLjys9Tkw+e9Os51vd7UhVCQs+6fcmSJYqKilLNmjXPuL1KlSr65ptvVLFi\n",
              "RbndbnXr1k1vv/22hg4dqrFjx+ree+/VsGHDJEn79+/3ve+3337TwoULFRYWpjZt2ujdd9/17XdK\n",
              "aGioWrRooW+++Ua1atXSrl27dPPNN+vvf/+7XC6Xvv76ayUmJsrhcBR431tvvaXWrVtrw4YNBV5P\n",
              "T0/X4sWLlZeXp8aNG2v+/Pnq1q2bhg8frpYtW+rVV1/V77//rlatWikmJsb3vtTUVKWmpqpOnTqS\n",
              "pCeffFK1atWSJE2ePFmjRo3SggULdNVVV+mVV16RJM2ZM0eJiYn69ttvdeONN+rHH3/0lWJne39x\n",
              "ohQCAAAAAAB+5fV6NWrUKC1btkyGYejw4cNauXKlJKlTp056/vnntWnTJnXv3l29evXyve+qq65S\n",
              "hQoVJEktW7b0TQP9VceOHfXjjz+qfv36SkhIkCS1aNFC33//vX766Sd16NCh0FlvuukmBQcHKzg4\n",
              "WHFxcdq0aZO6deumxYsX+8qcBg0aqFu3bgXe17VrV18hJElffPGFJk2apMzMTBmGoYyMDElSXFyc\n",
              "JGndunX64Ycf9PTTT+vBBx/U0aNHtXXrVnXu3Pmc7y9Ofi2F6tWrp5CQEIWFhUmSHnnkEd18883+\n",
              "PCUAAAAAAOWKIyhY6/s96fdznEtycrK2b9+uffv2qXr16qdtf+qpp3TgwAGlpqbK4XBo8ODBys3N\n",
              "lSQ98cQT6tu3r2bPnq3/+7//07Rp0zRt2jRJJ6aATrFarXK73Wc8f/fu3XXHHXeoTp066tSpk6QT\n",
              "RdH8+fO1ePFiTZ48udBf66kOQ5JsNpvy8/PPuJ/FYinw3Ol0+n69adMmPfzww1q0aJGaNm2qJUuW\n",
              "qHv37r7tHTp00KxZs5Senq6rrrpKf//73/Xee+8pMTFRwcHB531/cfH7mkLTp09XWlqa0tLSKIQA\n",
              "AAAAAChmVqtVFUJC/fqwWs9dH8THx6tXr17q37+/b2Fpr9erKVOmaN26dTpy5IiqVasmh8Oh7du3\n",
              "F7gzWFpampo0aaIHHnhADz30kFJTU4v8PejUqZMOHz6sGTNmqEePHpJOFEWzZs3SgQMH1KVLl9Pe\n",
              "U7FiReXm5p5xnaIzad++vd544w1JJz5iNn/+/LPue+TIEdntdtWtW1der1cvv/xyge3dunXThAkT\n",
              "lJiYKOlEgfXss8+qa9euhXp/cWGhaQAAAAAAcNGmT5+u+Ph4tWnTRtHR0YqOjta8efNUtWpVPfjg\n",
              "g0pNTVV0dLRuvfXWAh/neumllxQdHa0mTZroySef1HPPPVfkc4eEhKhVq1bKyspSixYtJEkJCQnK\n",
              "yspSq1atFBISctp7qlWrpuuvv15NmjQpsND02bz++utasWKFGjZsqH79+qldu3Zn3bdt27bq3bu3\n",
              "YmNj1axZswIfK5Okq6++Wvv27dPll18uSerRo4f27t3r++jc+d5fXCyGYRh+ObJOfHwsIiJChmEo\n",
              "KSlJzz77rKpWrXrafrm5ub6xsVP+8Y9/+D6rBwAAAAAATtizZ48OHDig6Ojo0xZPRvlmGIa8Xq9y\n",
              "cnK0detWRUVFKTIy8qz7+3VS6Oeff9bq1auVmpqqKlWqaODAgWfcb9y4cYqMjCzwWLZsmT+jAQAA\n",
              "AAAAlGt+LYXq1q0rSQoKCtJ999131lunPfLII8rIyCjwSEpK8mc0AAAAoFgZeR5lfbJKXlee2VEA\n",
              "ACgUv919zOVyKT8/XxUrVpQkffjhh77P9f1VSEjIaZ/vs9ls/ooGAAAAFLsjwz9V3pId8mbkyDm4\n",
              "rdlxAAA4L7+VQn/88YduvPFGeTweGYahBg0a6L333vPX6QAAAABThV3bVHlLdsj19jKF39pCVse5\n",
              "b98MAIDZ/FYKNWjQQCtXrvTX4QEAAICAEnpVnDInLZEn/YiypqXKOSzZ7EgAyqg/f7LGj/eOQjnA\n",
              "LekBAACAYmCxW+W8K0WS5JqyXN7jued5BwBcGKuV/5XH2VkslkLvy+8kAAAAoJiEXhkrW4NLZBzL\n",
              "Uda0X82OAwDAOVEKAQAAAMXEYrPKOeLktNB/V8h7LMfkRADKOovFElCP/Px8jRkzRg0aNFCjRo3U\n",
              "pEkT3XrrrTp48GCxnWP06NHKysoy/WsNxEdRUQoBAAAAxSi0Z2PZo6vIOJ4r13tMCwHwP6/XK0/2\n",
              "Mb8+vF5vobLccsstWrlypZYuXaotW7Zo3bp16t69uw4cOFBgv7y8vAv+el9++WVlZWVd8PuL4kw5\n",
              "LyZ7oKEUAgAAAIrRn6eFst5bIW8G00IA/MvIzdTWuyr59WHkZp43x9q1azVnzhy9//77uvTSSyWd\n",
              "WP/ozjvv1O+//67o6GjddNNNio2N1aRJk1S5cmUdP37c9/5rrrlGzz33nKQTE1CjRo1SkyZNFBUV\n",
              "pddff12S1K9fP0lSSkqKYmNjtWvXLu3atUs9evRQo0aNFB0dreeff953zNTUVHXo0EExMTGKiYnx\n",
              "HT8pKUlTp0717derVy+98sorkqQbb7xRN910k1q3bq3GjRvrq6++KpB96tSpWrNmjbp06aL4+HjF\n",
              "xMTomWee8R3LYrHooYceUrNmzVSrVi29/PLL58zz008/qX79+gWKtxYtWujjjz8u3G+Ai0ApBAAA\n",
              "ABSzkB4xssdUkZGZJ9e7K8yOAwAlYsmSJYqKilLNmjXPuP3333/X7bffrg0bNmjUqFFq3769Jk6c\n",
              "KEnauXOnFi5cqGHDhvn2t1gsWr9+vb7++ms98sgj2rhxoz744ANJ0qJFi7RhwwbVrl1bQ4cOVaNG\n",
              "jbR582b9+OOPevHFF/Xdd98pLy9P119/vW677TZt2rRJmzZt0m233Vaor2X16tX69ttvtW3bttOy\n",
              "Dxw4ULfccoteeuklrV27Vr/++qveffdd/fjjj773h4SEaM2aNfrqq6/02GOPKS8v76x5OnfurIoV\n",
              "K+qzzz6TJC1cuFCHDx9Wnz59ivzfoKj8dkt6AAAAoLyyWC1yjmyvo/d+rqz3VsgxoJWsFcPMjgWg\n",
              "jLKEONXwjSN+P8fFql27tq666irf8/vvv1/Dhw/X3//+d7366qu69tprVbFiRd/2kSNHSpLi4uLU\n",
              "pk0bffvtt2rcuPFpx120aJFeeukl3zmuuOIKzZ07VxEREcrLyytQNJ2tsPqr3r17q1KlSmfMnpaW\n",
              "pq1bt+rWW2/1bXe5XL7pIUkaPHiwpBMTPzabTTt37tThw4fPmmfkyJF67bXXdOONN+qVV17RHXfc\n",
              "USJ3maMUAgAAAPwg5PJGssdeKveG/XJNWa4K93cyOxKAMspqtUphEWbHUHJysrZv3659+/apevXq\n",
              "p20PDw8v8Lxr164KCwvTl19+qWnTpmnu3LnnPH5hF1IuzH42m01ut9v3PCen4Ed9nc6CJdifsxuG\n",
              "oYiICG3YsOGsxw8L+98/BPz1XGcyYMAAPf7441q4cKHmz5+v11577bxfQ3Hg42MAAACAH5yYFjq5\n",
              "ttC0VHkPl8yiqABglvj4ePXq1Uv9+/f3LSzt9Xo1ZcoUbd68+YzvGTFihIYMGaKGDRsqISGhwLZT\n",
              "Hy3buHGjVqxYoW7dukmSHA6HDh8+7NsvJSVF48ePlyTt3r1bX3/9ta644golJCQoNDRUkyZN8u27\n",
              "Z88eSVL9+vW1dOlSSdL69ev166+FvzFAQkKCnE5ngbWC1q5dqz/++OO87ztbnuDgYA0cOFA33nij\n",
              "evbsqapVqxY6z8WgFAIAAAD8JOSyaNnjqsnIzpdrynKz4wCA302fPl3x8fFq06aNoqOjFR0drXnz\n",
              "5qlKlSpn3P/2229XVlaW7rrrrtO2eTweNWnSRD169NDYsWN9Hx0bOnSoevTo4VtoetKkSdq0aZMa\n",
              "NWqkzp076/7779dll12m4OBgffbZZ3rvvffUqFEjNW7cWNOmTZMk/d///Z9++eUXNWrUSA888MBp\n",
              "hdS5BAcH68svv9Tnn3+umJgYRUdH64477pDL5Trv+86WR5Luvvtu7d+/X/fdd1+hs1wsi2EYRomd\n",
              "rQhGjx6tF1980ewYAAAAwEXJ+XGrjo6YKUtYkKrMGyJbZYfZkQCUcgcOHNCePXsUHR0th6N0/5ny\n",
              "008/acCAAfr9999ls9l8r1ssFu3fv7/EJmYCwTvvvKPJkydryZIlF3wMwzBkGIays7O1detWRUVF\n",
              "KTIy8qz7s6YQAAAA4EchnRsoqFl15a/ZJ9fbyxTxYFezIwFAQOjbt68WLFigCRMmFCiEyqOOHTtq\n",
              "27Zt+uSTT0r0vJRCAAAAgB9ZLCfuRHZk+AxlfZQmxx1tZKt68XfxAYDS7uOPPz7rtgD9UJPfLFiw\n",
              "wJTzsqYQAAAA4GfBHesrKKGGlOOW6+1lZscBAEASpRAAAADgdxaLRc6720uSsj5Kk2d/psmJAADg\n",
              "42MAAABAiQhOqaeglrWUn7pbrjeXKuKxy82OBKCUsttP/K/8qUWFgVNO/X6wWCyF2p9SCAAAACgB\n",
              "vrWF7vxYWR+vkuPOJNmqVzA7FoBSyGq1ymKxyGKxUArhNIUthCQ+PgYAAACUmODkugpqXVvK98j1\n",
              "5oXfchgAJPmKoUB55OXl6e6771a9evUUHR2t2NhYvf766wX2GTNmjBo2bKjExMQzPj/T44EHHtCQ\n",
              "IUNK7Ovo2rWr4uLiFBcXJ5vNptjYWMXFxSkpKcnv57711lvVqFEj9erV66KOU1hMCgEAAAAl5NTa\n",
              "Qkdun66sT1bLcWdb2WpGmB0LQClWlALA3/r27avc3FytXbtWERER2rhxo6644gq53W7dd999kqQ3\n",
              "3nhDGzduVFRU1Bmfn8mpr/Fivtb8/HwFBQUVat+ffvqpwLkXLVqkKlWqXPDxCmvXrl364osvdPz4\n",
              "cd9HBAvD4/FIkmw2W5HPyaQQAAAAUIJCkuoquG1dye1V5mSmhQBcPK/Xq5zsfL8+vF7vOTOsXbtW\n",
              "8+bN03vvvaeIiBNld+PGjfXcc8/pueeekyS1aNFCubm56t69u+64447Tnq9du1YtW7ZU48aNFRMT\n",
              "o3vvvdd3/H379umyyy5Tw4YNlZycrP3790uS3G63hg8frkaNGqlRo0a6/fbblZOTI0nq06eP+vbt\n",
              "q9atWysmJkbSiRKqefPmiouLU+vWrbVkSeH/HE5KStIdd9yhxMREdezYUfn5+erYsaPi4+MVHR2t\n",
              "3r1769ixY5KkOXPmqFGjRrrtttvUuHFjRUdH+247v2fPHnXo0EExMTGKiYlRnz59dPDgQXXu3Fl5\n",
              "eXmKj4/Xo48+Kkl64oknfHk7duyoTZs2SZLGjBmjnj17qmPHjoqJidGOHTsK/XX8GZNCAAAAQAlz\n",
              "jmyvw0t3KHvmGjmGtJW9VqTZkQCUYnm5Hj3+6Dd+PcdTY3sqNOzscyXLli1TVFSUqlWrVuD1Ll26\n",
              "aN++fdqzZ49Wrlx52uTNn58PGjRIvXr10tixYyXJV/xIUlpamlJTU1WtWjVdffXVevnllzV27Fi9\n",
              "+OKLSk1N1erVq2W329WtWzf961//0jPPPCNJWrVqlZYuXaqKFSvq22+/1UcffaSlS5cqLCxM33zz\n",
              "jfr3768tW7YU+vuwZcsWLV26VCEhIfJ6vfr0009VrVo1eb1eDRgwQM8++6wv/7Zt2zR58mRNnTpV\n",
              "//73v/Xoo49qwYIFeuutt1S3bl398ssvvq+zSpUqmjNnjlq3bq0NGzZIkiZNmqSNGzcqNTVVdrtd\n",
              "b7zxhoYOHaoff/xRkrRy5Uqlpqaqdu3ahc7/V5RCAAAAQAkLbl1bwe2ilLd4u1yTlijyqZ5mRwIA\n",
              "03Xu3FmPPfaYMjMz1bVrV/Xu3du3rUuXLr7CKTk5WWvWrJEkfffdd7rtttsUFhYmSRo8eLDeeOMN\n",
              "3/uuvfZaVaxYUZI0c+ZMrV+/Xi1atPBtz8jIkMvlksPhKFTGfv36KSQkRNKJO32NHTtW8+bNk8fj\n",
              "0fHjx9WqVSvfvnXq1FHXrl0lSR07dtT48eMlSe3bt/cVPF26dNH1119/xnN9/vnnWrVqleLj4yXp\n",
              "tGmtrl27XlQhJFEKAQAAAKZw3t1ehxdvV/ZnJ6eF6lQ0OxKAUio4xKanxvq3XA4OOfd6NUlJSdq+\n",
              "fbv++OOPAtNCP/74o6pXr66aNWue9xwDBw5Uly5d9NVXX2nChAl6+eWXfev7hIaG+vaz2Wy+dXT+\n",
              "6q/rDlWo8L+7PBqGob59+2rChAnnzXI2fz7e5MmT9fPPP2vRokWqVKmSnnnmGd8UjyRfeSRJdrvd\n",
              "l/nyyy/XqlWr9NVXX2nGjBl68skntW7dutPOZRiGRo8erTFjxpwxi9PpvOCv4xTWFAIAAABMENyi\n",
              "loI71JM8hlwTF5sdB0ApZrVaFRoW5NeH1Xru+iA+Pl6XX365BgwYoOPHj0uSNm7cqIceekh///vf\n",
              "C/V1rF27VrVr19bIkSP1n//8R2lpaed9z+WXX673339fOTk5ys/P19tvv63LL7/8jPtef/31mjFj\n",
              "hjZv3izpxALNp9b5uRCHDx/WJZdcokqVKuno0aN6//33C/W+DRs2KCIiQoMGDdJbb72l7du3KyMj\n",
              "47T9rr32Wr399tu+j9Hl5uZq0aJFF5z3TJgUAgAAAEziHNleh39JV/YXv8kxNFn2qEpmRwKAC/bp\n",
              "p5/q/vvvV1xcnIKCgmSz2XTvvfcWWDD6XD744AN9/PHHCgoKktfr1UsvvXTe94wePVpbt25Vs2bN\n",
              "JEkpKSl67LHHzrhvz5499a9//UvXXXed3G638vPz1b17d3Xs2LHwX+SfDB8+XLNnz1b9+vV1ySWX\n",
              "KDk5WTt37jzv++bNm6cJEybIarXK4/Ho8ccfV+XKlXXw4MHTjn/o0CFfPo/Ho/79+yslJeWC8p6J\n",
              "xTAMo9iOVoxGjx6tF1980ewYAAAAgF8dGT5DuT//rtBrm6riuCvNjgOgFMjIyND27dsVHR2t8PBw\n",
              "s+MgAGVlZWnLli2KiopSZOTZb2bAx8cAAAAAEzlHnvgX35wv18mdftjkNACA8oRSCAAAADBRULMa\n",
              "CunaUPIayny9eNeKAADgXCiFAAAAAJM5R7aXJOXM2SD31kMmpwEAlBeUQgAAAIDJguKqKeTy6BPT\n",
              "Qm8wLQSgcAJ0iWAEgML+3uDuYwAAAEAAcI5sr9zvtijn6w3KH9ZOQY2qmB0JQIAKCgqSJLndbrnd\n",
              "7vPeLh7li9frldvtlvS/3ytnQykEAAAABICg2EsV0iNGufM2yfX6IlV8qbfZkQAEKKvVKrvdLovF\n",
              "ovz8fLPjIABZLBbZ7fbzFoaUQgAAAECAcI5IUe68Tcr5ZqPyNx1QUExVsyMBCFAWi0VBQUEKCQkx\n",
              "OwoClMViOe8+zJgBAAAAASIopqpCezWWJGW+ttDkNAACncVikdVqDZjHrFmz1KZNG7Vs2VJxcXHq\n",
              "1q2bpBOTTa+++qr2799/UccfNGiQXn311Qt679VXX62WLVuqZcuWstlsSkhIUMuWLdW5c2e/f1+G\n",
              "DBmi+Ph43XjjjSX236IwhZDEpBAAAAAQUJwjUpTzzUblfrtZ+ev/UFCTamZHAoDz2rt3r4YOHapf\n",
              "f/1VUVFRkqTU1FRfOfHyyy+rS5cuql69uin55syZ4/u1xWLRggULVLFixQL7uN1u2e3FW5P88ccf\n",
              "+uijj3Ts2DHZbLZCv8/r9UqS39eLYlIIAAAACCD26CoKvbKJJCnzNe5EBuD8DMOQNyvPr4/z3c3q\n",
              "jz/+kM1m0yWXXOJ7rWXLlrJYLHrqqae0Z88e3XzzzUpMTFRaWpoyMzM1aNAgxcfHKz4+Xv/85z99\n",
              "79u9e7f69OmjZs2aqXnz5vrHP/5x2vkWLFiguLg4rVixQgcOHFCPHj18+99xxx2F/t516dJF99xz\n",
              "j9q1a6cePXrI7XarZ8+eat26tZo2bap+/frJ5XJJkn788UfFx8drxIgRSkhIUNOmTbVixQpJOmOG\n",
              "o0ePqmvXrsrJyVGrVq307LPPSpJeeOEFJSUlqWXLlurVq5e2b98uSXryySd14403qmfPnoqPj9fe\n",
              "vXsL/XVcKCaFAAAAgADjvKudcr7eoNzvtyj/t30KamrOv6wDKB2M7Hztb/2KX89x6Yp7ZQkPPuv2\n",
              "5s2bq0OHDoqKilLnzp2VkpKifv36qVatWnr88cf1zjvvaPr06UpMTJQkPfTQQ8rNzdXq1auVnZ2t\n",
              "Dh06KDY2VjfffLP69++vHj166NNPP5V0onD5s+nTp2vcuHGaPXu26tevr5deekn169fXvHnzJEmH\n",
              "Dx8u0te2adMm/fzzzwoKCpJhGPrggw9UuXJlGYahESNGaPz48Xr44YclSRs2bNDbb7+t119/XRMn\n",
              "TtRjjz2mb775RtOmTTstQ8WKFTVnzhxfESZJH3zwgTZu3KjFixfLZrNp6tSpGjFihGbPni1JWrx4\n",
              "sVauXKlq1UpmSpRJIQAAACDA2BtUVuhVTAsBKD2sVqtmzJihRYsWqVevXlq4cKGaNm2qLVu2nHH/\n",
              "+fPna8iQIbJarXI4HBowYIC+/fZbZWZm6pdfftGYMWN8+1at+r9F96dOnar//Oc/+uGHH1S/fn1J\n",
              "UnJysr7++muNGTNGn3/+uRwOR5Gy9+/f33frdsMw9NJLL6lFixZq3ry5Zs+e7St0JCk6Olpt27aV\n",
              "JLVr105bt24tUoZZs2Zp/vz5atWqlRITE/X8889rx44dvu1XXnlliRVCEpNCAAAAQEBy3tVOObPX\n",
              "K/fHrcpfs1dBzWqYHQlAgLKEBenSFff6/RyFERsbq9jYWA0bNky9evXSF198odGjR5//+IVcGLl5\n",
              "8+ZasGCB1qxZo06dOkk6Uc6kpaVp/vz5mjlzpv7xj39o5cqVhV7Dx+l0+n79wQcf6Pvvv9dPP/2k\n",
              "iIgIvfrqq/r+++9920NDQ32/ttlscrvd58zwV4Zh6JFHHtHQoUPPm6UkMCkEAAAABCB7vUsU2jtO\n",
              "knR8AnciA3B2FotF1vBgvz7OV9rs3r1bCxf+78+qI0eOaNu2bWrYsKEkKSIiQhkZGb7t3bp109tv\n",
              "vy3DMORyuTR16lT16NFDTqdTnTp10n/+8x/fvn/++FhCQoK+/PJLDRo0SHPnzpUkbdu2TU6nU337\n",
              "9tX48eO1adMmZWZmXtD38siRI6pSpYoiIiJ0/Phx/fe//y3U+wqb4brrrtPEiRN9H3HLz88/Y3lU\n",
              "UiiFAAAAgADlHN5OslmUt2Cb8lbtMTsOAJyV2+3WU089pZiYGCUmJqpjx44aOHCgrr32WknSPffc\n",
              "oyFDhvjW1/nHP/6hoKAgNWvWTG3btlXv3r3Vt29fSSc+IrZixQo1bdpUiYmJmjBhQoFzNWnSRN98\n",
              "843uvfdezZgxQz/++KPv41gpKSn697//rcjIyAv6OgYMGKCsrCw1btxYV1xxhTp27Fio9xU2w9/+\n",
              "9jfdfvvt6tq1qxISEpSYmFhgEqmkWYzzLSFuktGjR+vFF180OwYAAABgqozHvlb2Z2sV3L6eLnnz\n",
              "JrPjAAgAOTk52rZtm+rXr1/g40zAKYX9PcKkEAAAABDAHMPbSXar8hamKy91l9lxAABlCKUQAAAA\n",
              "EMDsdSoq7Lp4SVImawsBAIoRpRAAAAAQ4BzDkk9MCy3ZobwVO82OAwAoIyiFAAAAgABnrxWpsBua\n",
              "SWJaCMD/eL1esyMgQBX294bdzzkAAAAAFAPn0GRlf7ZWect2KnfpDoW0rWt2JAAmCQ4OltVq1Z49\n",
              "e1S1alUFB5//lvEoHwzDUF5eng4cOCCr1arg4OBz7k8pBAAAAJQCtpoRCu/TTFkfpilzwkIFJ9Xh\n",
              "fwKBcspqtap+/frau3ev9uzZY3YcBKDw8HDVrVtXVuu5PyBGKQQAAACUEo4hycqasUb5v+5S3pId\n",
              "CmkXZXYkACYJDg5W3bp15Xa75fF4zI6DAGKz2WS32wv1DweUQgAAAEApYateQeE3JSjr/VRlTvhF\n",
              "wcl1mRYCyjGLxaKgoCAFBQWZHQWlFAtNAwAAAKWIY0hbKcSu/JV7lLco3ew4AIBSjFIIAAAAKEVs\n",
              "lzoVfnOCJClz/EIZhmFyIgBAaUUpBAAAAJQyjjuTpFC78lfvVd6CbWbHAQCUUpRCAAAAQCljq+pU\n",
              "+C2JkqTjE5gWAgBcGEohAAAAoBRy3JkkS1iQ3Gv3Kfen382OAwAohSiFAAAAgFLIVtmh8H4tJEmZ\n",
              "TAsBAC4ApRAAAABQSjkGtZElPEjudX8o9/stZscBAJQylEIAAABAKWWtFK7wv7WUdHJayMu0EACg\n",
              "8CiFAAAAgFLMcUcbWRzBcm88oNzvNpsdBwBQilAKAQAAAKWYtWKYwm9rJYlpIQBA0VAKAQAAAKWc\n",
              "Y2BrWZzBcm8+qNx5m8yOAwAoJSiFAAAAgFLOGhmq8AGtJUmZry2U4fGanAgAUBpQCgEAAABlgGNA\n",
              "K1kiQuTeekg532w0Ow4AoBSgFAIAAADKAGtEqBwDT04Lvb6IaSEAwHlRCgEAAABlRPhtrWSJCJXn\n",
              "98PKmbPB7DgAgABHKQQAAACUEVZniBx3tJEkZb6xSIabaSEAwNlRCgEAAABlSHj/lrJUDJMn/Yhy\n",
              "Zq8zOw4AIIBRCgEAAABliNURLMegk9NCry9mWggAcFaUQgAAAEAZE35rC1kvCZdn51Flf/Gb2XEA\n",
              "AAGKUggAAAAoY/48LeSauFhGvsfkRACAQEQpBAAAAJRB4be2kLVyuDy7MpT9OdNCAIDTUQoBAAAA\n",
              "ZZAlLEiOwW0lSZkTF8vIY1oIAFAQpRAAAABQRoXfnCBrFYe8e44p+7M1ZscBAAQYSiEAAACgjLKE\n",
              "Bskx5OS00KQlMvLcJicCAAQSSiEAAACgDAvvmyDrpU559x1X9gymhQAA/0MpBAAAAJRhlhC7HENP\n",
              "TgtNXiIjl2khAMAJlEIAAABAGRfep7ms1SvI+0emsj5ZbXYcAECAoBQCAAAAyjhLsF3OocmSJNeb\n",
              "S2Tk5JucCAAQCCiFAAAAgHIg7IZmstaIkPeAS1kfrzI7DgAgAJRIKTRlyhRZLBbNmjWrJE4HAAAA\n",
              "4C8swTY5h5+aFloqI5tpIQAo7/xeCqWnp+vNN99UcnKyv08FAAAA4BzCrouXrXakvIeylPVRmtlx\n",
              "AAAm82sp5PV6NXjwYI0fP14hISFn3S83N1fHjh0r8PB4PP6MBgAAAJQ7liCbHMPaSZJcby+TNyvP\n",
              "5EQAADP5tRR68cUX1b59e7Vq1eqc+40bN06RkZEFHsuWLfNnNAAAAKBcCusdJ1udSHkPZynrw5Vm\n",
              "xwEAmMhvpdDatWs1Y8YM/d///d95933kkUeUkZFR4JGUlOSvaAAAAEC5ZQmyyXlXiiTJ9fZyeV1M\n",
              "CwFAeeW3UmjBggVKT09Xo0aNVK9ePS1ZskRDhw7VG2+8cdq+ISEhioiIKPCw2Wz+igYAAACUa6FX\n",
              "x8kWVUnG0WxlvZ9qdhwAgEn8Vgrddddd2rt3r9LT05Wenq7k5GRNnjxZd911l79OCQAAAKAQLHar\n",
              "nHedXFtoynJ5M3NNTgQAMEOJ3JIeAAAAQGAJvaqJbPUvkZGRo6xpTAsBQHlUYqXQjz/+qOuuu66k\n",
              "TgcAAADgHCw2q5wjTq4t9N/l8h5nWggAyhsmhQAAAIByKrRXY9kaVJZxLFdZ760wOw4AoIRRCgEA\n",
              "AADllMVmlXPkyWmh936VNyPH5EQAgJJEKQQAAACUY6E9G8veqIqM47lyMS0EAOUKpRAAAABQjlms\n",
              "Ft+0UNZ7v8p7NNvkRACAkkIpBAAAAJRzId1iZG9cVYYrT67/Mi0EAOUFpRAAAABQzp2YFmovScqa\n",
              "9qu8R7JMTgQAKAmUQgAAAAAUcnm07E0ulZGVL9eU5WbHAQCUAEohAAAAALJYLHLefXJa6P2V8hxy\n",
              "mZwIAOBvlEIAAAAAJEkhXRrKHl9dRna+XO8wLQQAZR2lEAAAAABJJ6aFKpxaW+jDlfIcZFoIAMoy\n",
              "SiEAAAAAPsGd6iuoeQ0pxy3X28vMjgMA8CNKIQAAAAA+BdYW+ihNngOZJicCAPgLpRAAAACAAoLb\n",
              "11NQYk0p1y3XW0wLAUBZRSkEAAAAoIAC00LT0+T547jJiQAA/kApBAAAAOA0we2iFNSylpTnkevN\n",
              "pWbHAQD4AaUQAAAAgNNYLBY5R3WQJGV9slqevcdMTgQAKG6UQgAAAADOKKRtXQW1qSPle5TJtBAA\n",
              "lDmUQgAAAADOqsLJtYWyP10tz+4Mk9MAAIoTpRAAAACAswpuU0fBbetKbq8yJy8xOw4AoBhRCgEA\n",
              "AAA4p1N3Isv+bK3cu46aGwYAUGwohQAAAACcU3Cr2gpOiZLcXrkmMS0EAGUFpRAAAACA8/JNC81a\n",
              "K/eOIyanAQAUB0ohAAAAAOcVnFhLwR3qSx5DrolMCwFAWUApBAAAAKBQfNNCX/wmdzrTQgBQ2lEK\n",
              "AQAAACiU4OY1FNK5geQ1lDlxkdlxAAAXiVIIAAAAQKGdmhbK+Wq93NsOm5wGAHAxKIUAAAAAFFpQ\n",
              "0+oK6Rp9YlrodaaFAKA0oxQCAAAAUCTOkSmSpJw56+XectDkNACAC0UpBAAAAKBIguKqKaRbI8mQ\n",
              "Mt9YbHYcAMAFohQCAAAAUGS+aaG5G5S/+YDJaQAAF4JSCAAAAECRBTW+VCE9Yk5MC73G2kIAUBpR\n",
              "CgEAAAC4IM6RKZJFyp23Sfkb9psdBwBQRJRCAAAAAC5IUKOqCu0VK0nciQwASiFKIQAAAAAXzDni\n",
              "5LTQ/M3KX/eH2XEAAEVAKQQAAADggtkbVlbolU0ksbYQAJQ2lEIAAAAALopzRIpktSj3hy3KX7vP\n",
              "7DgAgEKiFAIAAABwUez1L1Ho1aemhRaanAYAUFiUQgAAAAAumnN4imSzKPen35W3ao/ZcQAAhUAp\n",
              "BAAAAOCi2etVUtg1TSWxthAAlBaUQgAAAACKhWN4smSzKO+XbcpL2212HADAeVAKAQAAACgW9rqV\n",
              "FHZdvCQpcwJrCwFAoKMUAgAAAFBsHMPbSXar8hZtV96vu8yOAwA4B0ohAAAAAMXGXitSYdczLQQA\n",
              "pQGlEAAAAIBi5Rx2clpo6Q7lLd9pdhwAwFlQCgEAAAAoVraaEQrr01ySdJxpIQAIWJRCAAAAAIqd\n",
              "c2iyFGRT/vKdyl26w+w4AIAzoBQCAAAAUOxs1Sso/KYT00KZ43+RYRgmJwIA/BWlEAAAAAC/cAxN\n",
              "loJtyk/drbzF282OAwD4C0ohAAAAAH5hu9Sp8JsTJZ24ExnTQgAQWCiFAAAAAPiNY3CSFGJXftoe\n",
              "5S1MNzsOAOBPKIUAAAAA+I2tqlPhtyRKkjLHMy0EAIGEUggAAACAXznuTJJC7cpfs1d5P28zOw4A\n",
              "4CRKIQAAAAB+ZavikKNfC0nS8deYFgKAQEEpBAAAAMDvHIOSZAkLknvtPuX+uNXsOAAAUQoBAAAA\n",
              "KAHWS8IV/reWkrgTGQAECkohAAAAACXCMaiNLOFBcq/fr9zvtpgdBwDKPUohAAAAACXCWjFM4be1\n",
              "kiRlvrZQhpdpIQAwE6UQAAAAgBLjGNhaFmew3BsPKHf+JrPjAEC5RikEAAAAoMQUnBZaxLQQAJiI\n",
              "UggAAABAiXIMbC1LhRC5Nx9UzjcbzY4DAOUWpRAAAACAEmWNCJVjYGtJJ6eFPF6TEwFA+UQpBAAA\n",
              "AKDEhd/WSpaIEHl+P6ScuUwLAYAZKIUAAAAAlDhrhRA5bm8jScp8nWkhADADpRAAAAAAU4T3bylL\n",
              "ZKg82w4rZ/Z6s+MAQLlDKQQAAADAFFZniByDkiSdnBZyMy0EACWJUggAAACAacL7tZClUpg8O44q\n",
              "56t1ZscBgHKFUggAAACAaayOYDlPTQu9sUhGvsfkRABQflAKAQAAADBV2K2JslYOl2dnhrK/YFoI\n",
              "AEoKpRAAAAAAU1nDg+W488S0kGviYhl5TAsBQEmgFAIAAABguvCbT04L7c5Q9udrzY4DAOUCpRAA\n",
              "AAAA01nCguQYmixJypy4hGkhACgBlEIAAAAAAkL4Tc1lreqQd+8xZc9cY3YcACjzKIUAAAAABARL\n",
              "6J+mhSYvkZHnNjkRAJRtlEIAAAAAAkZ4n+ayVnPKu++4sj5dbXYcACjTKIUAAAAABAxLiF3OYSem\n",
              "hVyTl8rIZVoIAPyFUggAAABAQAm7oZms1SvIuz9TWR+vMjsOAJRZlEIAAAAAAool2C7n8HaSJNeb\n",
              "S2Xk5JucCADKJkohAAAAAAEn7Lp4WWtGyHvQpazpTAsBgD/4tRTq0aOHmjdvrsTERHXs2FErV670\n",
              "5+kAAAAAlBGWYJucd52cFnprqbxZeSYnAoCyx6+l0Mcff6zVq1crLS1No0eP1u233+7P0wEAAAAo\n",
              "Q8J6N5WtTqS8h7KU/VGa2XEAoMzxaylUsWJF368zMjJksVj8eToAAAAAZYglyCbHsBPTQplvL5PX\n",
              "xbQQABQnu79PMGDAAP3www+SpDlz5pxxn9zcXOXm5hZ4zePx+DsaAAAAgAAX1rupXJOWyLPzqLI+\n",
              "XCnn4LZmRwKAMsPvC02/99572rlzp55++mk99NBDZ9xn3LhxioyMLPBYtmyZv6MBAAAACHAWu1XO\n",
              "ESfXFnpnOdNCAFCMLIZhGCV1srCwMO3atUuVK1cu8PqZJoX+8Y9/6JVXXimpaAAAAAAClOH26mDv\n",
              "d+RJPyLnvR3lHJZsdiQAKBP8Nil09OhR7dmzx/d81qxZqly5si655JLT9g0JCVFERESBh81m81c0\n",
              "AAAAAKWIxW6V864USZJrynJ5M3PP8w4AQGEUaU2hrVu36s0339T333+vXbt2KSwsTAkJCerTp4/6\n",
              "9u0ru/1/h8vIyNBNN92k7OxsWa1WVa1aVV999RWLTQMAAAAostArY5U5abE8vx9W1tRffSURAODC\n",
              "FfrjY8OGDVNqaqpuuukmtWvXTtWrV1dOTo7Wr1+vuXPnKjU1VRMnTlRycvGMco4ePVovvvhisRwL\n",
              "AAAAQOmXPWe9Mh74SpaIEFWdN1TWiFCzIwFAqVboSaHevXtr0qRJp73erFkz9e3bV4cOHdLWrVuL\n",
              "NRwAAAAAnBLas7FcE5fIveWgXO/9qgp3tzc7EgCUaoVeU+iqq6465/bKlSsrKSnpogMBAAAAwJlY\n",
              "bFY5R5z42FjWeyvkzcgxOREAlG5FWmh6+fLluvnmmxUfH6/4+HjdcsstWrFihb+yAQAAAEABIT1i\n",
              "ZI+pIiMzT653+X8RALgYhS6FFi9erB49eqhBgwZ6+umn9a9//Uv169dXjx49tHTpUn9mBAAAAABJ\n",
              "ksVqkXPkiY+NZU39Vd6j2SYnAoDSq9BrCj3//PN65513dP311/teu/7665WcnKxx48Zp1qxZ/sgH\n",
              "AAAAAAWEXN5I9thL5d6wX64py1Xh/k5mRwKAUqnQk0K//fZbgULolGuvvVbr1q0r1lAAAAAAcDYn\n",
              "poVOri00LVXeI1kmJwKA0qnQpVB4ePhZtzkcjmIJAwAAAACFEXJZtOxx1WRk58v1znKz4wBAqVTo\n",
              "j4/l5uZqzZo1MgzjtG05Oaz6DwAAAKDkWCwWOe9ur6MjZirrg5UKv721bJX5x2oAKIpCl0LZ2dnq\n",
              "3bv3GbdZLJZiCwQAAAAAhRHSuYGCmlVX/pp9cr29TBEPdjU7EgCUKoUuhdLT0/0YAwAAAACKxmI5\n",
              "cSeyI8NnKOujNDnuaCNbVafZsQCg1Cj0mkJ/dfToUc2aNUtr1qwpzjwAAAAAUGjBHesrKKGGlOOW\n",
              "6+1lZscBgFKl0KXQbbfdprS0NEknCqGEhAQ9+uijuvzyyzVlyhR/5QMAAACAszq1tpAkZU1fJc/+\n",
              "TJMTAUDpUehS6Ndff1ViYqIk6f3331d0dLTWrVunFStW6NVXX/VXPgAAAAA4p+CUegpqWUvKdcv1\n",
              "5lKz4wBAqVHoUig0NNT36wULFuj666+XJNWtW7f4UwEAAABAIZ1aW0iSsj5ZJc++4yYnAoDSodCl\n",
              "kMfjUUZGhtxutxYsWKCOHTv6tnFLegAAAABmCk6uq6DWtaU8j1xvLjE7DgCUCoUuhe666y61bNlS\n",
              "bdq0UYMGDZSQkCBJWrNmjapVq+a3gAAAAABwPgXWFvp0jTx7jpmcCAACX6FvST98+HC1bt1au3fv\n",
              "Vo8ePXyvBwcH66WXXvJLOAAAAAAorJCkugpuW1d5S3coc/ISRT7Z4/xvAoByrEi3pG/durWuvfZa\n",
              "hYWF+V5r3LixWrRoUezBAAAAAKCoTq0tlD1zjdy7M0xOAwCBrdClUKVKlXTJJZf4HpUrV1aTJk10\n",
              "7733KjOT2z4CAAAAMF9w69oKbhclub1yTWJtIQA4l0KXQmlpaVq5cqXvkZqaqmnTpik7O1tjxozx\n",
              "Z0YAAAAAKLRTawtlz1or986j5oYBgABW6DWFoqKizvjaxIkT1bJly2INBQAAAAAXKrhFLQV3qKe8\n",
              "X9LlmrhYkc9cYXYkAAhIRVpT6IwHsFpltV70YQAAAACg2PjWFvriN7m3HzE5DQAEpotuc+bOnatL\n",
              "LrmkOLIAAAAAQLEITqipkE4NJI+hzImLzY4DAAGp0B8fa9GihSwWS4HXDh06pKCgIH322WfFHgwA\n",
              "AAAALoZzZIpyf/5dOV+uk3tYsuz1+MdsAPizQpdCL7/8coHnFotFVatWVaNGjWS3F/owAAAAAFAi\n",
              "gprVUEjXhsr9YasyX1+kis9fbXYkAAgohW5zOnfu7M8cAAAAAFDsnCPbK/eHrcqZs0HuYe1kb1jZ\n",
              "7EgAEDAKvabQiBEjtGvXrjNuMwxDM2bM0AcffFBswQAAAADgYgXFVVPI5dGS11DmG4vMjgMAAaXQ\n",
              "k0JXXnmlrrzySlWqVElt27ZVtWrVlJOTo40bN2rhwoW68sor9dRTT/kzKwAAAAAUmXNke+V+t0U5\n",
              "X29Q/rB2CmpUxexIABAQCj0pdPXVV2v16tV65plnFBERoc2bN+vQoUO67LLLtHTpUo0fP16VKlXy\n",
              "Z1YAAAAAKLKg2EsV0iNGMiTX60wLAcApRV4hukOHDurQoYM/sgAAAACAXzhHpCh33iblfLNR+ZsO\n",
              "KCimqtmRAMB0hZ4UAgAAAIDSKiimqkJ7NZYkZTItBACSKIUAAAAAlBPOESmSRcqdt0n56/8wOw4A\n",
              "mI5SCAAAAEC5YI+uotArm0hiWggApAsohZ5++ulCvQYAAAAAgcZ5VzvJalHud1uUv45pIQDlW5FL\n",
              "oZkzZxbqNQAAAAAINPYGlRV61clpodcWmpwGAMxV6LuPffPNN5o7d652796t0aNH+17PyMjwSzAA\n",
              "AAAA8AfnXe2UM3u9cn/Yqvw1exXUrIbZkQDAFIWeFAoNDVXFihVltVoVGRnpe8THxzMpBAAAAKDU\n",
              "sNe7RKHXxEmSMl9jbSEA5VehJ4U6d+6szp0767rrrlNCQoI/MwEAAACAXzmHt1POV+uU+/Pvylu1\n",
              "R8EJNc2OBAAlrtCl0ClNmzbV9OnTtXXrVrndbt/rjz/+eLEGAwAAAAB/sUdVUljvpsr+bK0yX1uo\n",
              "SybfZHYkAChxRS6FbrnlFu3bt09JSUmy2Wz+yAQAAAAAfucY3k7ZX65T3i/pylu5W8EtapkdCQBK\n",
              "VJFLoTVr1mjDhg2yWCz+yAMAAAAAJcJep6LCrotX9qerlTlhoS55u6/ZkQCgRBX5lvR16tRRXl6e\n",
              "P7IAAAAAQIlyDEuW7FblLd6uvBW7zI4DACWqyJNC0dHR6tKli66//nqFhob6Xr/nnnuKNRgAAAAA\n",
              "+Ju9VqTCbmim7I9XnVhbaMrNZkcCgBJT5FIoNzdXsbGxWr9+ve81PkoGAAAAoLRyDk1W9mdrlbd0\n",
              "h3KX7VBIUl2zIwFAiShyKTRlyhR/5AAAAAAAU9hqRii8TzNlfZimzAkLFfxuHf7hG0C5UORS6L33\n",
              "3jvj6wMGDLjoMAAAAABgBseQZGXNWKP8FbuUt2SHQtpFmR0JAPyuyKXQl19+6ft1Tk6OfvnlFyUn\n",
              "J1MKAQAAACi1bNUrKPymBGW9n6rM1xYqOLku00IAyrwil0KffPJJgefbtm3TY489VmyBAAAAAMAM\n",
              "jiFtlfXpauWn7lbeonSFtK9vdiQA8Ksi35L+r+rXr6/ffvutOLIAAAAAgGlslzoVfnOCJClzwkIZ\n",
              "hmFyIgDwryJPCn3xxRe+X3s8Hi1dulQhISHFGgoAAAAAzOC4M0lZH69S/qq9yluwTSGdGpgdCQD8\n",
              "psil0EsvvfS/N9vtio6O1vTp04s1FAAAAACYwVbVqfBbEpX13xUn1hbqWJ+1hQCUWUUuhX744Qd/\n",
              "5AAAAACAgOC4M0nZ01cpf80+5f70u0K7NDQ7EgD4xQWtKfTJJ59o6NChGjp0qGbMmFHcmQAAAADA\n",
              "NLbKDoX3ayGJtYUAlG1FLoWeeuopjRs3TnFxcWratKnGjRunp59+2h/ZAAAAAMAUjkFtZAkPknvd\n",
              "H8r9fovZcQDAL4pcCn366af65ZdfdN999+nee+/VTz/9pI8//tgf2QAAAADAFNZK4Qr/W0tJJ6eF\n",
              "vEwLASh7ilwKGYah8PBw33OHw8E4JQAAAIAyx3FHG1kcwXJvPKDc7zabHQcAil2RS6GkpCTddttt\n",
              "+vnnn/Xzzz9r4MCBSkpK8kc2AAAAADCNtWKYwm9rJUnKfI1pIQBlT5FLoVdffVU1a9bU6NGjNXr0\n",
              "aNWoUUOvvvqqP7IBAAAAgKkcA1vL4gyWe9NB5c7bZHYcAChWRb4lvcPh0HPPPeePLAAAAAAQUKyR\n",
              "oQof0Fqu1xcp87WFCuneSBbbBd3EGQACTpFLoaysLL377rvavHmz3G6373WmhQAAAACURY4BrZQ1\n",
              "7Ve5tx5SzjcbFXZlE7MjAUCxKHLFfcMNN2jOnDmqWLGiqlWr5nsAAAAAQFlkjQiVY2BrSVLm64tk\n",
              "eLwmJwKA4lHkSaEdO3Zo3bp1/sgCAAAAAAEp/LZWcr37qzy/H1bOnA0KuybO7EgAcNGKPCkUGxur\n",
              "gwcP+iMLAAAAAAQkqzNEjjvaSJIy31gkw820EIDSr8iTQs8884zatWunNm3aKDQ01Pf6O++8U6zB\n",
              "AAAAACCQhPdvKde7K+RJP6Kc2esUdm282ZEA4KIUuRQaMmSI2rVrp9atW8tms/kjEwAAAAAEHKsj\n",
              "WI5BbZT54s/KfGOxQq+Kk8XOncgAlF5FLoWOHDmi9957zx9ZAAAAACCghd/aQln/XSHPjqPK/nKd\n",
              "wq9nWghA6VXkWjshIUG7d+/2RxYAAAAACGinpoUkyfXGIhn5HpMTAcCFK/Kk0IEDBxQfH6927doV\n",
              "WFNo5syZxRoMAAAAAAJR+K0t5JqyXJ5dGcr+4jeF39jc7EgAcEGKXAr1799f/fv390cWAAAAAAh4\n",
              "lrAgOQa31fHnflDmG4sVdk1TWYJZbxVA6VPkUmjgwIEFnns8Hn355ZfFFggAAAAAAl34zQlyvb1M\n",
              "3j3HlD1rrcL7JpgdCQCK7IKXyt+4caMefPBB1apVS08//XRxZgIAAACAgGYJDZJjSFtJUubExTLy\n",
              "3CYnAoCiK9KkUFZWlqZPn6633npL27ZtU3Z2thYvXqzY2Fh/5QMAAACAgBTe9+S00L7jyp65RuG3\n",
              "tDA7EgAUSaEnhYYMGaI6deroiy++0MMPP6wdO3aoYsWKFEIAAAAAyiVLiF2OoSenhSYtkZHLtBCA\n",
              "0qXQpdBHH32k5s2ba9iwYbr66qtlt9tlsVj8mQ0AAAAAAlp4n+ayVq8g7x+Zyvp0tdlxAKBICl0K\n",
              "7d27V/3799dTTz2lqKgo/d///Z/y8/P9mQ0AAAAAApol2C7n0GRJkmvyEhk5/D8SgNKj0KWQ0+nU\n",
              "nXfeqUWLFmnu3LnKyclRXl6eUlJS9Prrr/szIwAAAAAErLAbmslaI0LeAy5lfcK0EIDS44LuPhYX\n",
              "F6cXXnhBu3fv1pgxYzR79uzizgUAAAAApYIl2Cbn8D9NC2UzLQSgdLjgW9JLkt1u14033kgpBAAA\n",
              "AKBcC7suXrbakfIeylLW9DSz4wBAoVxUKQQAAAAAkCxBNjmGtZMkud5aJm9WnsmJAOD8/FYK5eTk\n",
              "6LrrrlNMTIwSEhLUvXt3bdmyxV+nAwAAAABThfWOk61OpLyHs5T9YZrZcQDgvPw6KTR06FBt3LhR\n",
              "q1at0rXXXqvBgwf783QAAAAAYBpLkE3Ou1IkSZnvLJPXxbQQgMDmt1IoNDRUV155pSwWiyQpOTlZ\n",
              "6enpZ9w3NzdXx44dK/DweDz+igYAAAAAfhF6dZxsUZVkHMlW1gcrzY4DAOdUYmsKvfLKK7r22mvP\n",
              "uG3cuHGKjIws8Fi2bFlJRQMAAACAYmGxW+W86+TaQu8skzcz1+REAHB2JVIKjR07Vlu2bNG4cePO\n",
              "uP2RRx5RRkZGgUdSUlJJRAMAAACAYhV6VRPZ6l8iIyNHWdNSzY4DAGfl91LohRde0MyZM/X1118r\n",
              "PDz8jPuEhIQoIiKiwMNms/k7GgAAAAAUO4vNKueIE2sLuf67XN7jTAsBCEx+LYVefPFFffjhh/r2\n",
              "229VsWJFf54KAAAAAAJGaK/GsjWoLONYrrKm/mp2HAA4I7+VQrt27dKYMWN09OhRde3aVYmJiWrb\n",
              "tq2/TgcAAAAAAcNis8o58uS00Lsr5D2WY3IiADid3V8Hrl27tgzD8NfhAQAAACCghfZsLNfExXJv\n",
              "PijXuytUYVQHsyMBQAEldvcxAAAAAChPLFaLb1oo671f5T2abXIiACiIUggAAAAA/CSkW4zsjavK\n",
              "cOXJ9e4Ks+MAQAGUQgAAAADgJyemhdpLkrKm/irvkSyTEwHA/1AKAQAAAIAfhVweLXuTS2Vk5cs1\n",
              "hWkhAIGDUggAAAAA/Mhisch598lpofdT5T3MtBCAwEApBAAAAAB+FtKloezx1WVk58v1zjKz4wCA\n",
              "JEohAAAAAPA7i8WiCifXFnJ9sFKegy6TEwEApRAAAAAAlIjgTvUV1LyGlOOW622mhQCYj1IIAAAA\n",
              "AEpAgbWFPkqT50CmyYkAlHeUQgAAAABQQoLb11NQYk0p1y3XW0wLATAXpRAAAAAAlJAC00LT0+T5\n",
              "47jJiQCUZ5RCAAAAAFCCgttFKahlLSnPI9ebS82OA6AcoxQCAAAAgBJksVjkHNVBkpT1yWp59h4z\n",
              "ORGA8opSCAAAAABKWEjbugpqU0fK9yiTaSEAJqEUAgAAAAATVDi5tlD2p6vl2cO0EICSRykEAAAA\n",
              "ACYIblNHwW3rSm6vMictNjsOgHKIUggAAAAATHLqTmTZn62Ve3eGyWkAlDeUQgAAAABgkuBWtRWc\n",
              "EiW5vXJNZFoIQMmiFAIAAAAAE/mmhWatlXvHEZPTAChPKIUAAAAAwETBibUU3KG+5DHkmrjE7DgA\n",
              "yhFKIQAAAAAwmW9a6Mvf5E5nWghAyaAUAgAAAACTBTevoZDODSSPocyJi8yOA6CcoBQCAAAAgABw\n",
              "aloo56v1cm87bHIaAOUBpRAAAAAABICgptUV0jVa8hrKfJ1pIQD+RykEAAAAAAHCOTJFkpQzZ73c\n",
              "Ww+ZnAZAWUcpBAAAAAABIiiumkK6NZIMMS0EwO8ohQAAAAAggPimheZuUP7mAyanAVCWUQoBAAAA\n",
              "QAAJanypQnrEnJgWeo1pIQD+QykEAAAAAAHGOTJFski58zYpf+N+s+MAKKMohQAAAAAgwAQ1qqrQ\n",
              "XrGSmBYC4D+UQgAAAAAQgJwjTk4Lzd+s/HV/mB0HQBlEKQQAAAAAAcjesLJCr2wiiWkhAP5BKQQA\n",
              "AAAAAco5IkWyWpT7wxbl/7bP7DgAyhhKIQAAAAAIUPb6lyj06pPTQhMWmpwGQFlDKQQAAAAAAcw5\n",
              "PEWyWZT70+/KW73X7DgAyhBKIQAAAAAIYPZ6lRR2TVNJTAsBKF6UQgAAAAAQ4BzDkyWbRXm/bFNe\n",
              "2m6z4wAoIyiFAAAAACDA2etWUth18ZKYFgJQfCiFAAAAAKAUcAxvJ9mtylu0XXm/7jI7DoAygFII\n",
              "AAAAAEoBe61IhV3PtBCA4kMpBAAAAAClhHPYyWmhpTuUt3yn2XEAlHKUQgAAAABQSthqRiisT3NJ\n",
              "0nGmhQBcJEohAAAAAChFnEOTpSCb8pfvVO7SHWbHAVCKUQoBAAAAQCliq15B4TedmBbKHP+LDMMw\n",
              "ORGA0opSCAAAAABKGcfQZCnYpvzU3cpbvN3sOABKKUohAAAAAChlbJc6FX5zoqQTdyJjWgjAhaAU\n",
              "AgAAAIBSyDE4SQqxKz9tj/IWppsdB0ApRCkEAAAAAKWQrapT4bckSpIyxzMtBKDoKIUAAAAAoJRy\n",
              "3JkkhdqVv2av8n7eZnYcAKUMpRAAAAAAlFK2Kg45+rWQJB1/jWkhAEVDKQQAAAAApZhjUJIsYUFy\n",
              "r92n3B+3mh0HQClCKQQAAAAApZj1knCF/62lJO5EBqBoKIUAAAAAoJRzDGojS3iQ3Ov3K/e7LWbH\n",
              "AVBKUAoBAAAAQClnrRim8NtaSZIyX1sow8u0EIDzoxQCAAAAgDLAMbC1LM5guTceUO78TWbHAVAK\n",
              "UAoBAAAAQBlQcFpoEdNCAM6LUggAAAAAygjHwNayVAiRe/NB5Xyz0ew4AAIcpRAAAAAAlBHWiFA5\n",
              "BraWJLleXyTD4zU5EYBARikEAAAAAGVI+G2tZIkIkXvrIeXMZVoIwNlRCgEAAABAGWKtECLH7W0k\n",
              "SZlMCwE4B0ohAAAAAChjwvu3lCUyVJ5th5Uze73ZcQAEKEohAAAAAChjrM4QOQYlSZIy31gsw820\n",
              "EIDTUQoBAAAAQBkU3q+FLJXC5Nl+RDlfrTM7DoAARCkEAAAAAGWQ1REs55+nhfI9JicCEGgohQAA\n",
              "AACgjAq7NVHWyuHy7Dyq7C+YFgJQEKUQAAAAAJRR1vBgOe48MS3kmsi0EICCKIUAAAAAoAwLv/nk\n",
              "tNDuDGXPWmt2HAABhFIIAAAAAMowS1iQHEOTJUmZE5fIyGNaCMAJlEIAAAAAUMaF39Rc1qoOefce\n",
              "U/bMNWbHARAgKIUAAAAAoIyzhP5pWmjyEhl5bpMTAQgElEIAAAAAUA6E92kuazWnvPuOK+vT1WbH\n",
              "ARAAKIUAAAAAoBywhNjlHHZiWsg1eamMXKaFgPKOUggAAAAAyomwG5rJWr2CvPszlfXxKrPjADAZ\n",
              "pRAAAAAAlBOWYLucw9tJklxvLpWRk29yIgBmohQCAAAAgHIk7Lp4WWtGyHvQpazpTAsB5RmlEAAA\n",
              "AACUI5Zgm5x3nZwWemupvFl5JicCYBZKIQAAAAAoZ8J6N5WtTqS8h7KU/VGa2XEAmIRSCAAAAADK\n",
              "GUuQTY5hJ6eF3lkur4tpIaA88mspdM8996hevXqyWCxKS0vz56kAAAAAAEVwYlqooryHs5T14Uqz\n",
              "4wAwgV9LoT59+uiXX35RVFSUP08DAAAAACgii90q5wimhYDyzK+lUKdOnVS7du3z7pebm6tjx44V\n",
              "eHg8Hn9GAwAAAIByL/SqONnqVZJxNFtZ01LNjgOghAXEmkLjxo1TZGRkgceyZcvMjgUAAAAAZZrF\n",
              "bpXzrhRJkmvKcnkzc01OBKAkBUQp9MgjjygjI6PAIykpyexYAAAAAFDmhV4ZK1uDS2Qcy1HW1F/N\n",
              "jgOgBAVEKRQSEqKIiIgCD5vNZnYsAAAAACjzLDarnCNOTgu9u0LeYzkmJwJQUgKiFAIAAAAAmCe0\n",
              "Z2PZo6vIOJbLtBBQjvi1FBo2bJhq166tXbt2qWfPnoqOjvbn6QAAAAAAF+C0aaEMpoWA8sDuz4NP\n",
              "mjTJn4cHAAAAABSTkB4xssdUkXvTQbneW6EKozqYHQmAn/HxMQAAAACALFaLnCPbS5Ky3vtV3qPZ\n",
              "JicC4G+UQgAAAAAASVLI5Y1kj71UhitPrv+uMDsOAD+jFAIAAAAASDo1LXRibaGsab/KeyTL5EQA\n",
              "/IlSCAAAAADgE3JZtOxx1WRk5cs1ZbnZcQD4EaUQAAAAAMDHYrHIeffJtYXeXynPIZfJiQD4C6UQ\n",
              "AAAAAKCAkM4NFNSsuozsfLneYVoIKKsohQAAAAAABVgsf7oT2Ycr5TmQaXIiAP5AKQQAAAAAOE1w\n",
              "x/oKSqgh5bjlemeZ2XEA+AGlEAAAAADgNAXWFvpoFdNCQBlEKQQAAAAAOKPglHoKallLynXL9eZS\n",
              "s+MAKGaUQgAAAACAMyqwttDHq+T547jJiQAUJ0ohAAAAAMBZBSfXVVDr2lKeh2khoIyhFAIAAAAA\n",
              "nFWBtYU+WS3P3mMmJwJQXCiFAAAAAADnFJJUV8Ft60r5HmVOXmJ2HADFhFIIAAAAAHBep9YWyp6x\n",
              "Rp7dGSanAVAcKIUAAAAAAOcV3Lq2gttFSW4v00JAGUEpBAAAAAAolFNrC2V/tlbuXUfNDQPgolEK\n",
              "AQAAAAAKJbhFLQV3qCe5vXJNZFoIKO0ohQAAAAAAheZbW+jztXJvP2JyGgAXg1IIAAAAAFBowQk1\n",
              "FdKpgeQxlDlpsdlxAFwESiEAAAAAQJE4R6ZIknK+WCd3+mGT0wC4UJRCAAAAAIAiCWpWQyFdG0pe\n",
              "Q5lvMC0ElFaUQgAAAACAIju1tlDO7PVy/37I5DQALgSlEAAAAACgyILiqink8mimhYBSjFIIAAAA\n",
              "AHBBfNNCc9Yrf/NBk9MAKCpKIQAAAADABQmKvVQhPWIkQ3K9scjsOACKiFIIAAAAAHDBnCNO3ols\n",
              "7kblbzpgchoARUEpBAAAAAC4YEExVRXaq7EkKfN1poWA0oRSCAAAAABwUZwjUiSLlDtvk/LX/2F2\n",
              "HACFRCkEAAAAALgo9ugqCr2yiSSmhYDShFIIAAAAAHDRnHe1k6wW5X63RfnrmBYCSgNKIQAAAADA\n",
              "RbM3qKzQq05OC7220OQ0AAqDUggAAAAAUCx800I/bFX+mr1mxwFwHpRCAAAAAIBiYa93iUKviZMk\n",
              "Zb7G2kJAoKMUAgAAAAAUG+fwdpLNotyff1feqj1mxwFwDpRCAAAAAIBiY4+qpLDeTSWxthAQ6CiF\n",
              "AAAAAADFyjG8nWS3Ku+XdOWt3G12HABnQSkEAAAAAChW9joVFXZdvCQpcwLTQkCgohQCAAAAABQ7\n",
              "x7DkE9NCi7crb8Uus+MAOANKIQAAAABAsbPXilTYDc0ksbYQEKgohQAAAAAAfuEcmiwF2ZS3dIdy\n",
              "l+0wOw6Av6AUAgAAAAD4ha1mhML7nJwWmrBQhmGYnAjAn1EKAQAAAAD8xjEkWQq2KX/FLuUtZVoI\n",
              "CCSUQgAAAAAAv7FVr6DwmxIkMS0EBBpKIQAAAACAXzmGtJVC7MpP3a28xdvNjgPgJEohAAAAAIBf\n",
              "2S51Kvzmk9NC439hWggIEJRCAAAAAAC/c9yZJIXalb9qr/J+STc7DgBRCgEAAAAASoCtqlPhtyRK\n",
              "kjInMC0EBAJKIQAAAABAiXDcmSRLWJDy1+xT7s+/mx0HKPcohQAAAAAAJcJW2aHwfi0kcScyIBBQ\n",
              "CgEAAAAASoxjUBtZwoPk/u0P5f6w1ew4QLlGKQQAAAAAKDHWSuEK/1tLSUwLAWajFAIAAAAAlCjH\n",
              "HW1kcQTLvWG/cr/bbHYcoNyiFAIAAAAAlChrxTCF39ZK0slpIS/TQoAZKIUAAAAAACXOMbC1LM5g\n",
              "uTcdVO63m8yOA5RLlEIAAAAAgBJnjQxV+IDWkqTM1xYxLQSYgFIIAAAAAGAKx4BWskSEyL3loHK+\n",
              "2Wh2HKDcoRQCAAAAAJjCGhEqx8BT00ILZXi8JicCyhdKIQAAAACAacJvayVLRKg8vx9WztcbzI4D\n",
              "lCuUQgAAAAAA01idIXLc0UaSlPn6IhlupoWAkkIpBAAAAAAwVXj/lrJUDJMn/Yhy5qw3Ow5QblAK\n",
              "AQAAAABMZXUEyzGIaSGgpFEKAQAAAABMF35rC1kvCZdnx1Flf7nO7DhAuUApBAAAAAAw3Z+nhVxv\n",
              "LJKR7zE5EVD2UQoBAAAAAAJC+K0tZK0cLs+uDGV/8ZvZcYAyj1IIAAAAABAQLGFBcgxuK0nKfGOx\n",
              "jDymhQB/ohQCAAAAAASM8JsTZK3ikHfPMWXPWmt2HKBMoxQCAAAAAAQMS2iQHENOTgtNXCwjz21y\n",
              "IqDsohQCAAAAAASU8L4Jsl7qlHffcWXPXGN2HKDMohQCAAAAAAQUS4hdjqEnp4UmL5WRy7QQ4A+U\n",
              "QgAAAACAgBPep7ms1SvIu++4sj5dbXYcoEyiFAIAAAAABBxLsF3OocmSJNebS2Xk5JucCCh7KIUA\n",
              "AAAAAAEp7IZmstaIkHd/prI+YVoIKG6UQgAAAACAgGQJtsk5/E/TQtlMCwHFiVIIAAAAABCwwq6L\n",
              "V1DLWnIMS5ZsFrPjAGWK3ewAAAAAAACcjSXIpsrT+pkdAyiTmBQCAAAAAAAohyiFAAAAAAAAyiG/\n",
              "lkKbN29WSkqKYmJi1KZNG/3222/+PB0AAAAAAAAKya+l0LBhwzR06FBt2rRJDz30kG6//XZ/ng4A\n",
              "AAAAAACF5LdSaP/+/VqxYoX69+8vSbrxxhu1c+dObdmyxV+nBAAAAAAAQCH57e5jO3fuVI0aNWS3\n",
              "nziFxWJR3bp1tWPHDkVHRxfYNzc3V7m5uQVe83g8/ooGAAAAAABQ7gXEQtPjxo1TZGRkgceyZcvM\n",
              "jgUAAAAAAFBm+a0UqlOnjvbu3Su32y1JMgxDO3bsUN26dU/b95FHHlFGRkaBR1JSkr+iAQAAAAAA\n",
              "lHt+K4UuvfRStWzZUtOmTZMkzZgxQ7Vr1z7to2OSFBISooiIiAIPm83mr2gAAAAAAADlnt/WFJKk\n",
              "SZMm6fbbb9fYsWMVERGhKVOm+PN0AAAAAAAAKCS/lkKNGzfW4sWL/XkKAAAAAAAAXICAWGgaAAAA\n",
              "AAAAJYtSCAAAAAAAoByiFAIAAAAAACiHKIUAAAAAAADKIUohAAAAAACAcohSCAAAAAAAoByiFAIA\n",
              "AAAAACiHKIUAAAAAAADKIYthGIbZIc7khhtuUL169cyOcV4ej0fLli1TUlKSbDab2XGAMoXrC/AP\n",
              "ri3AP7i2AP8pbddXVFSU7r33XrNjAOcVsKVQaXHs2DFFRkYqIyNDERERZscByhSuL8A/uLYA/+Da\n",
              "AvyH6wvwDz4+BgAAAAAAUA5RCgEAAAAAAJRDlEIAAAAAAADlEKXQRQoJCdETTzyhkJAQs6MAZQ7X\n",
              "F+AfXFuAf3BtAf7D9QX4BwtNAwAAAAAAlENMCgEAAAAAAJRDlEIAAAAAAADlEKUQAAAAAABAOeTX\n",
              "UignJ0fXXXedYmJilJCQoO7du2vLli2+7fv371evXr3UqFEjxcfH6+eff/ZtGzt2rBo3biyr1apZ\n",
              "s2YVOO4dd9yh5s2bKzExUW3atNF333131gwXeg5/Heftt99Wo0aN1LBhQw0ZMkT5+fmSpMWLFysx\n",
              "MVGJiYlq2rSphg0bptzc3DMeIysrS7feequio6MVExOjTz/9tFDb/mrp0qVKSEhQTEyMLrvsMu3e\n",
              "vbtQ22A+rq3Tne3aOsUwDF122WWqWLHiWY/h9Xo1atQoNWzYUNHR0ZowYUKhtv3V5s2blZKSopiY\n",
              "GLVp00a//fZbobYhMHB9ne5c19f5rr1T+LsLXFunO9v1k56eri5duigyMlKJiYnnPAbXFri2Tne2\n",
              "a+v7779XUlKS4uLi1LRpUz344IPyer1nPAY/F6LcMPwoOzvbmD17tuH1eg3DMIzx48cbnTt39m2/\n",
              "4447jCeeeMIwDMNYtmyZUatWLSMvL88wDMNYunSpsXXrVqNz587GZ599VuC4R44c8f06NTXVqFSp\n",
              "kuHxeM6Y4ULP4Y/j/P7770aNGjWMvXv3Gl6v17jmmmuMCRMmGIZhGC6Xy3c8j8djXHfddcaLL754\n",
              "xuP885//NAYOHOg7ZtWqVY2DBw+ed9ufeTweo2HDhsb3339vGIZh/Pvf/zb69Olz3m0IDFxbBZ3r\n",
              "2jrlP//5jzF48GAjMjLyrMd59913jcsuu8xwu93GoUOHjLp16xpr164977a/6tq1qzFlyhTDMAzj\n",
              "k08+MVq3bl2obQgMXF8Fnev6Ksy1dwp/d4Frq6BzXT+HDh0yFixYYHz11VdGQkLCObNwbYFrq6Bz\n",
              "XVupqanG1q1bDcM48X1r37697+eyv+LnQpQXfi2F/mr58uVGVFSU77nD4TD27t3re96mTRvj22+/\n",
              "LfCe8130P/zwwzn/gCqOcxTXcZ5//nlj2LBhvuezZ8822rdvf9p+2dnZRs+ePY2XXnrpjMeJi4sz\n",
              "Fi9e7Ht+0003GW+++eZ5t/3ZsmXLjMaNG/ueHzt2zAgJCTGys7PPuQ2BiWvr3NfW2rVrjY4dOxpb\n",
              "tmw5Zyl05ZVXGh9++KHv+d///nfjscceO++2P/vjjz+MChUqGPn5+YZhGIbX6zWqVatmbN68+Zzb\n",
              "ELi4vs5+fRX27zXD4O8unI5r6/zXzw8//HDeUohrC3/FtVX4v5tGjhzpK6H+ip8LUV6U6JpCr7zy\n",
              "iq699lpJ0qFDh5Sfn6/q1av7tterV087duwo1LEefvhhNWzYUDfccINmzJghq/X0L+Viz1Hcx9mx\n",
              "Y4eioqLOeoz09HQlJCSoSpUqioyM1IgRIyRJe/bsKTA6fK7jnGvbxIkT9fjjj59xvwoVKigiIkJ7\n",
              "9uw55zYEJq6ts/++z8/P15AhQzRp0iTZbLbT3puYmOj7vX2h19YXX3yhwYMHS5J27typGjVqyG63\n",
              "S5IsFovq1q2rHTt2nHMbAhfX14VdF/zdhfPh2jr3z4Vnw7WF8+HaKty1tW/fPn366ae6+uqrfa/x\n",
              "cyHKoxIrhcaOHastW7Zo3LhxxXK8Z599Vlu3btXHH3+sBx98UHl5ecVyXDPVq1dPq1at0r59+5Sb\n",
              "m6uZM2dKkmrWrKm0tLSLPv7w4cP11FNPXfRxEFi4ts7tn//8p2644QY1adLkjNvT0tJUs2bNizpH\n",
              "79699dZbb13UMRCYuL4uHH934Vy4ti4c1xbOhWurcI4dO6ZrrrlGDz74oFq3bu17nZ8LUR6VSCn0\n",
              "wgsvaObMmfr6668VHh4uSapcubLsdrv27dvn2y89PV1169Yt0rG7deum48ePa82aNZo/f75vseZn\n",
              "nnnmgs9RXMf5q7p162r79u3nPYbT6dQtt9yi999/v8jHKew5/rrf8ePHlZGRoZo1a55zGwIL19YJ\n",
              "5/p9/9NPP2n8+PGqV6+eOnTooGPHjqlevXo6cOBAkY5T2GurTp062rt3r9xut6QTC1zv2LFDdevW\n",
              "Pec2BB6urxOK47ooruPwd1fZwLV1QlGunws9DtdW+cK1dcL5ft8fP35cvXr10rXXXqvRo0df0HH4\n",
              "uRBlir8/n/af//zHaNmypXH48OHTtg0cOLDAQmI1a9b0LSR2yl8/M5qXl1fgc5ZLly41KlWqdMbj\n",
              "X+g5/HWcrVu3nrbo2fjx4w3DMIzNmzf7jpebm2v07dvXePTRR894nCeeeOK0RQMPHDhw3m1/5vF4\n",
              "jAYNGhRYNPDGG2887zYEDq6t/znXtfVn27ZtO+eaQlOmTDlt0cDVq1efd9tfde7cucCiga1atSrU\n",
              "NgQOrq//Odf1VdhrzzD4uwsncG39T2Gun8KsKcS1BcPg2vqzc11bx48fN1JSUox//vOf58xhGPxc\n",
              "iPLDr6XQzp07DUlGgwYNjISEBCMhIcFISkrybd+3b5/RvXt3Izo62oiLi/P9ZWMYhvGvf/3LqFWr\n",
              "lhEcHGxUrlzZqFWrlrF//37D5XIZKSkpRtOmTY2EhAQjJSXF+O67786a4ULO4c/jTJ482WjQoIHR\n",
              "oEEDY9CgQb4/5CZNmmQ0bdrUaN68uREXF2eMGjXKt4Df7t27C/xAkJmZafTt29do0KCB0ahRI2P6\n",
              "9OmF2vbGG28Y//jHP3zPFy1aZDRr1sxo1KiR0blzZ2PHjh2F2gbzcW2d7mzX1p+dqRRKSEgwdu/e\n",
              "bRiGYbjdbmPEiBFG/fr1jQYNGhgvv/yyb79zbfv888+NO++80/d8w4YNRnJystGoUSOjVatWBX5I\n",
              "ONc2BAaur9Od6/o62zb+7sJfcW2d7mzXj8vlMmrVqmVUqVLFCAoKMmrVqmU8/PDDhmFwbeF0XFun\n",
              "O9u19fTTTxt2u933fUpISDCefvpp3/v4uRDlkcUwDMPUUSUAAAAAAACUuBK9+xgAAAAAAAACA6UQ\n",
              "AAAAAABAOUQpBAAAAAAAUA5RCgEAcB716tVT48aNlZCQoOjoaF177bVatGhRod47a9YsLVmypNgz\n",
              "TZo0SbGxsUpMTNShQ4d8rz/++OO+W/w6nU7Vr1/f93zjxo3FnuNiHD16VM8++2yB1wYPHqwffvjB\n",
              "pEQAAADlCwtNAwBwHvXq1dOsWbOUmJgoSZo5c6YGDRqkb775Rm3btj3ne2+//XYlJibqvvvuK9ZM\n",
              "TZo00TvvvKN27dqddZ8uXbrovvvu03XXXVfgda/XK0myWs39t6H09HQlJibq6NGjpuYAAAAor5gU\n",
              "AgCgiG644QYNHz5cL7zwgiTpu+++U7t27dSiRQs1bdpUb7/9tiRpzpw5+uKLL/Tvf/9biYmJeuut\n",
              "tyRJU6dOVdu2bdWyZUt16tRJq1atOuN5VqxYoZSUFDVv3lxJSUlauHChJKlPnz7aunWrbr/9dvXp\n",
              "06dQmZ988kndeOON6tmzp+Lj47V371498MADatOmjRITE9WpU6cCk0QWi0Vjx45VUlKS6tevrylT\n",
              "pkg6USjdfffdatKkiRISEtSqVSvl5OTI7XarZ8+eat26tZo2bap+/frJ5XL5jjdlyhQlJiYqISFB\n",
              "rVu3Vnp6uoYPH67jx48rMTFRrVu3lnSiyJo1a5Ykaf/+/brhhhvUrFkzxcfHa9KkSb7j1atXT48/\n",
              "/rjatWun+vXr6+mnny7U9wEAAAB/UiI3vgcAoBSLiooyVq5cWeC1mTNnGk2aNDEMwzAOHz5suN1u\n",
              "wzAM49ChQ0bdunWNnTt3GoZhGAMHDjReeukl3/t++eUX44orrjBycnIMwzCMn3/+2YiLizvtnLm5\n",
              "uUadOnWMuXPnGoZhGAsWLDCqVatmHD9+/KyZ/qpz587GZ599ZhiGYTzxxBNGjRo1jH379vm279+/\n",
              "3/frDz/80OjZs6fvuSTjhRdeMAzDMNavX284nU4jPz/fSE1NNWJjYw2Px2MYhmEcPXrU8Hg8htfr\n",
              "NQ4ePGgYhmF4vV5j+PDhxrhx4wzDMIwffvjBqFevnrFnzx7DMAzD5XIZLpfL2LZtmxEZGXnWzH37\n",
              "9jUefvhhwzAM448//jBq165tLF682Pf1jxo1yjAMwzhw4IARERFh7Nq165zfDwAAABRkN7uUAgCg\n",
              "NDL+9OnrQ4cO6c4779SmTZtkt9t16NAhrV27VrVr1z7tfZ9//rlWrVpV4GNnhw8fVnZ2tsLCwnyv\n",
              "bdy4UVarVT179pQkdejQQdWqVVNaWpo6dOhwQZmvvPJKVatWzff822+/1fjx43X8+HF5vV4dPny4\n",
              "wP5/+9vfJEmxsbGy2+3at2+fGjRoILfbrUGDBqlr16666qqrZLVa5fV69dJLL2n27Nlyu93KyMhQ\n",
              "SkqKJGn27Nm67bbbVKNGDUlSeHh4ofLOnz9fv/76qyTp0ksv1Q033KD58+crOTlZktSvXz9JUpUq\n",
              "VdSgQQNt27ZNtWrVuqDvDQAAQHnEx8cAALgAy5cvV3x8vCRp+PDh6tChg9asWaO0tDTFxMQoJyfn\n",
              "jO8zDEMDBw5UWlqa77F3794ChdDZWCyWi8rsdDp9v96xY4fuvvtuTZs2TWvXrtVHH310WubQ0FDf\n",
              "r202m9xutyIjI7V27Vr169dPGzZsUPPmzbVlyxZ98MEH+v777/XTTz9pzZo1euCBB876PbhQf/36\n",
              "z5QPAAAAhUcpBABAEX3++ed64403NGbMGEnSkSNHFBUVJYvFop9//rnAGkERERHKyMjwPe/du7em\n",
              "TZumHTt2SDqxRs+KFStOO0fjxo3l9Xr17bffSpIWLVqkffv2+Ra7vlgZGRkKCgpSjRo1ZBiGJkyY\n",
              "UKj3HThwQC6XSz169NDYsWNVr149rVu3TkeOHFGVKlUUERGh48eP67///a/vPddcc42mTZumvXv3\n",
              "SpKysrKUlZWliIgIZWdnKy8v74zn6tatm958803feWfOnKnu3btf3BcOAAAAHz4+BgBAIdx8880K\n",
              "DQ2Vy+VSXFyc5syZ4/sI2LPPPqsRI0boX//6lxITEwt8NOy2227T7bffrlmzZmnkyJEaPHiwnn/+\n",
              "eV1//fVyu93Ky8vTVVdd5Vto+ZTg4GDNnDlT99xzj8aMGaPQ0FB9+umnBaZ9LkazZs10yy23qGnT\n",
              "pqpcufJpdyg7m507d2rIkCHKz8+Xx+NR+/btdcUVVygrK0uff/65GjdurKpVq6pjx47avn27JKlT\n",
              "p0564okn1LNnT1ksFgUHB+vTTz9VVFSUBgwYoObNm8vpdJ5Wjr366qu666671KxZMxmGoccee+y8\n",
              "d3sDAABA4XFLegAAAAAAgHKIj48BAAAAAACUQ5RCAAAAAAAA5RClEAAAAAAAQDlEKQQAAAAAAFAO\n",
              "UQoBAAAAAACUQ5RCAAAAAAAA5RClEAAAAAAAQDlEKQQAAAAAAFAOUQoBAAAAAACUQ5RCAAAAAAAA\n",
              "5dD/A+XgVJYKMhAZAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-b5c75205-c39b-472b-b1a6-779a104bf14d\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-b5c75205-c39b-472b-b1a6-779a104bf14d\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "def _plot_series(series, series_name, series_index=0):\n",
              "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
              "  xs = series['Date of Transaction']\n",
              "  ys = series['Amount (USD)']\n",
              "  \n",
              "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
              "\n",
              "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
              "df_sorted = _df_12.sort_values('Date of Transaction', ascending=True)\n",
              "for i, (series_name, series) in enumerate(df_sorted.groupby('Date of Transaction')):\n",
              "  _plot_series(series, series_name, i)\n",
              "  fig.legend(title='Date of Transaction', bbox_to_anchor=(1, 1), loc='upper left')\n",
              "sns.despine(fig=fig, ax=ax)\n",
              "plt.xlabel('Date of Transaction')\n",
              "_ = plt.ylabel('Amount (USD)')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-c98b054d-5b2c-42a1-8983-efc681cec8b2\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABJoAAAITCAYAAABPBnasAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABS0klEQVR4nO3deViVZf7H8c8BFJNUzNRUOBwWBbdAQ3NL03Itd832rDTTpkVb\n",
              "1GYaqzE1NbVtjDZnSmtsXHFttKxsRadw0gIF2QV3QVFQ4P794eX5iSIc8DkB9X5d17kunuf7nPv+\n",
              "wvDEz8/vfu5jM8YYAQAAAAAAAJfJo7IbAAAAAAAAwO8DQRMAAAAAAAAsQdAEAAAAAAAASxA0AQAA\n",
              "AAAAwBIETQAAAAAAALAEQRMAAAAAAAAsQdAEAAAAAAAASxA0AQAAAAAAwBIETQAAAAAAALCEV2U3\n",
              "UJrHHntM0dHRSklJ0U8//aSIiIgy33P06FH96U9/0rZt21SjRg0NHDhQs2bNcn+zAAAAAAD8hs6c\n",
              "OaPs7GydPHmyslvB71z9+vVVp04dl66t0kHTiBEj9Mwzz6hbt24uv+eBBx5Q165dtWTJEklSVlaW\n",
              "u9oDAAAAAKBSZGVlKSMjQ56enrLZbJXdDn7nDh8+LB8fH7Vo0aLMa6t00NS9e/cSz2/btk2TJ09W\n",
              "Tk6OCgsL9eyzz2rkyJFKSEjQ9u3btXz5cue111xzzW/VLgAAAAAAbnfy5Ent27dPPj4+atSokWrW\n",
              "rEnYBLcxxujgwYM6duyYjh8/XubKpiodNJXk2LFjeuihh7R+/Xo1adJEhw4dUvv27dWlSxf98ssv\n",
              "8vPz0/jx47V9+3Y1aNBAL7/8stq1a1fZbQMAAAAAYIlDhw7J09NTzZo1c/lxJuBy5eTk6OjRo7+/\n",
              "oOnbb7/V3r171b9//2Ln4+PjVVBQoJiYGM2YMUNRUVHasGGDbr31ViUnJ6tGjRqV1DEAAAAAANbz\n",
              "8ODzvfDbKM+KuWoXNBlj1Lp1a3377bcX1bZv365mzZqpZ8+ekqT+/fvr9OnTSklJUUhIyG/dKgAA\n",
              "AAAAwB9KtYs/u3TpoqSkJG3evNl5LjY2VqdPn9Z1112nunXr6n//+58kKSYmRsYY+fv7V1a7AAAA\n",
              "AABUW02bNpXD4VBoaKjsdrtuuukmbdq0yaX3fvjhh/rss88s72n27NkKDAxUWFhYsQ8Ae/zxxxUW\n",
              "FqawsDDVrl1bzZo1cx7HxsZa3sflOHjwoKZOnVrs3G233aY1a9ZUUkfWsRljTGU3cSnjxo3TunXr\n",
              "lJWVpQYNGqhOnTpKSEjQjz/+qKeeekqHDx/WmTNnZLfbtWrVKtWqVUv//e9/NWHCBJ06dUre3t6a\n",
              "O3euevToUdnfCgAAAAAAlkhNTdXRo0cVEhIiHx8ft87VtGlTLVu2TF26dJEk/eMf/9Cjjz6q1atX\n",
              "q1evXqW+d9iwYQoPD9e0adMs7SkoKEhvv/22br755kte07FjRz366KO65557ip0vLCyUJHl6elra\n",
              "U3nFx8crMjJSx48fr9Q+XJWbm6uEhATVr19fdru91Gur9IqmqKgopaenq6CgQPv371dCQoIkqX37\n",
              "9vr888+1Y8cO/fLLL9q4caNq1aolSbruuuv0ww8/6H//+5+2bdtGyAQAAAAAgEVGjx6tu+++W7Nn\n",
              "z5YkrV69WhEREWrZsqVCQkI0f/58SdLSpUv12Wef6c0331RYWJheeeUVSdKbb76pa6+9Vq1atVJk\n",
              "ZKS+++67Euf58ssv1a5dO7Vo0UJt27bVp59+Kknq16+f0tLSNGbMGPXr18+lnidOnKi+ffuqW7du\n",
              "atGihVJSUjR27Fi1adNGYWFhioyMLLbiyWazafLkyWrbtq2aNWumBQsWSDobUt17770KCgpSaGio\n",
              "WrVqpdzcXJ0+fVrdunVT69atFRISooEDByonJ8c53oIFCxQWFqbQ0FC1bt1a8fHxGjNmjE6ePKmw\n",
              "sDC1bt1a0tlw7MMPP5Qkpaenq0+fPmrevLlCQkKcP2/pbPj3+OOPKyIiQs2aNdPTTz/t0s/ht1Lt\n",
              "9mgCAAAAAACVp1OnTvrPf/4jSercubO2b98uLy8v7d+/X+3atdPAgQM1atQoLV26tNiKpk8//VRL\n",
              "ly7VDz/8oCuuuEIbNmzQPffc41xUck5eXp5uv/12vf766xoxYoQ2btyou+66SwkJCdq4caOaNm2q\n",
              "jz76yLnKyhU//vijfvzxR+fWOs8//7yaNWsmSXr77bf16KOPauvWrc7rvb299fPPP+unn35St27d\n",
              "NGHCBG3btk1ff/219uzZI09PTx0+fFi1atWSzWbTsmXLdM0116ioqEj33HOPZs6cqZkzZ2rt2rWa\n",
              "M2eOvvnmGzkcDucKpnfffVeRkZGKi4srsd+HHnpIISEh+s9//qP09HRFRkbquuuu00033SRJys7O\n",
              "VmxsrPbt26cWLVpo/PjxCgoKcvnn4U4ETQAAAAAAwGXn78Bz4MAB3X333dq7d6+8vLx07Ngx/fTT\n",
              "TyV+INfKlSsVFxendu3aOc9lZ2frxIkTuvLKK53nduzYIZvNphEjRkg6u4qpQYMG+v77711exXSh\n",
              "nj17Ftu/OTo6WlFRUTpx4oSMMcrOzi52/YMPPihJateunTw9PZWamqqwsDAVFhZq5MiRuvHGGzVi\n",
              "xAh5enqqsLBQL730kjZv3qyCggKdOHFC7du3lyStWbNGI0aMkMPhkCTVqVPHpX6//fZb5+owPz8/\n",
              "9e/fXxs3bnQGTeceCWzatKn8/f21e/fuKhM0VelH5wAAAAAAQNUSExOj0NBQSdLYsWPVuXNn7d69\n",
              "W3FxcXI4HMrLyyvxfcYY3XbbbYqLi3O+Dh48WCxkuhSbzXZZPZ8/x+7duzVlyhQtWbJECQkJWrJk\n",
              "ifLz84tdX7t2befX58KkBg0aKC4uTnfddZfi4uIUHh6unTt3KioqSl9//bW++eYb7dmzRxMmTLho\n",
              "vMt14fd/xRVXOL/28PBQQUGBpfNdjiobNL366quV3QIAAAAAADjPhx9+qA8//FBPPfWUpLMrkhwO\n",
              "hzw8PLR+/XrFx8c7r61bt26xlUJDhgzR8uXLtXv3bkln9zz68ssvL5ojPDxcxhitWLFCkrRp0yYd\n",
              "OnRInTp1suR7OHr0qLy8vGS321VUVOTcg6ksGRkZysnJ0fDhw/XGG2/Iz89PsbGxOnLkiOrXr6+r\n",
              "rrpKR48e1ccff+x8z+DBg7V8+XIlJydLko4fP67jx4/L19dX+fn5lwzlunTpotdff90574YNG9S/\n",
              "f//L+r5/K1X20bmUlJTKbgEAAAAAgD+8O++8U97e3jp16pRCQkK0YsUK5yfOTZ8+XRMnTtTLL7+s\n",
              "Vq1a6dprr3W+b/To0RozZow2bNigMWPG6Mknn9QLL7ygIUOGqLCwUGfOnNFNN9100Yd41apVS//6\n",
              "17/0xBNPaMqUKfL29tbixYvl6+tryfdz/fXXa9CgQQoLC5Ovr68GDBjg0vv27t2rcePGqaCgQEVF\n",
              "RYqMjNTIkSN14sQJrVu3Tg6HQ1dddZWuv/56paWlSZIGDBigZ555Rn369JHNZlONGjW0fPlyhYaG\n",
              "aujQoWrZsqVq166tXbt2FZsrKipKDz74oJo3by5jjCZOnFjmp/xVFTZz/sOVVcikSZM0b968ym4D\n",
              "AAAAAIAqJTU1VUePHlVISIh8fHwqux38AeTm5iohIUH169eX3W4v9doq++gcAAAAAAAAqheCJgAA\n",
              "AAAAAFiCoAkAAAAAAACWIGgCAAAAAACAJQiaAAAAAAAAYAmCJgAAAAAAAFiCoAkAAAAAAFyW3Nxc\n",
              "3XzzzQoICFBoaKi6dOminTt3Ouvp6em64YYbFBAQoJCQEK1fv95Zmzx5shwOhzw8PPThhx8WG3fE\n",
              "iBFq0aKFwsLC1KZNG61evfqSPVR0DneNM3/+fAUEBMjf31+jRo1Sfn6+S7XzHT9+XLfeeqvsdrsC\n",
              "AgL0/vvvu1S70Oeff67Q0FAFBASoU6dO2rt3r0u1iiBoAgAAAAAAl23s2LFKSkpSfHy8brnlFj3w\n",
              "wAPO2sSJExUZGamUlBS9++67uv/++53hSr9+/bR+/XpFRkZeNObChQu1e/duxcXFKSoqSvfee68K\n",
              "CwtLnL+ic7hjnF9//VUzZszQV199pZSUFB08eFCvvPJKmbULPf/886pZs6ZSU1O1ceNGPfXUU8rK\n",
              "yiqzdr7CwkKNHj1ac+fOVUpKinr37q0JEyaUWasogiYAAAAAAKqxoqIiHc/Pc+urqKio1B58fHw0\n",
              "atQoeXicjRm6deum9PR0Z33dunV64oknJEk33nijGjVq5Fwp1LNnT7Vq1arEcRs2bOj8+ujRo6X2\n",
              "UNE53DHOkiVL1Lt3bwUEBMjDw0Pjxo3TsmXLyqxdaPXq1XrkkUckSS1btlSnTp20ZMmSMmvn27p1\n",
              "qzw9PTVw4EBJ0qRJk7Rlyxbl5uaWWqsorwq/EwAAAAAAVLrcM6fV8qPn3TrHr3c+rzretVy+fv78\n",
              "+erbt68kKSsrSwUFBQoICHDW/f39lZyc7NJY48eP19q1a5WTk6PFixfL09Pzomsudw6rx0lLS5Pd\n",
              "bnceBwcHKzMzs8xaUlKS+vfvr7i4OElSZmamgoODndfa7XalpqaWWXv55Ze1b98+vfrqq0pKSlKz\n",
              "Zs2c19WvX18+Pj5KSUkpteZqMHchVjQBAAAAAADLTJ48WcnJyXrttdcsGW/hwoVKS0vTP/7xD02d\n",
              "OlV5eXmWjFsVBQYGOkOmyzF58mS9+uqrFnRUfm5d0eRwOOTt7a0rrrhCkjR16lSNGjXKnVMCAAAA\n",
              "APCH4lOjpn6983m3z+GK5557TmvXrtUXX3yhOnXqSJKuueYaeXp6KiUlxblSKC0tTQ6Ho1w9DB06\n",
              "VJMmTVJMTIwOHz6sqVOnSpIGDRqk2bNnV2iOlStXWjLOhfz9/Yttqp2YmKgmTZqUWbtQkyZNlJiY\n",
              "6Jw/NTVVN910U5m18wUGBiojI8N5fPToUZ04cUIBAQE6cODAJWsV5fYVTUuXLlVsbKxiY2MJmQAA\n",
              "AAAAsJiHh4fqeNdy6+vc3kulmTZtmpYvX64tW7YU21tJkvr3768FCxZIkr744gvt379fAwYMKHW8\n",
              "/Pz8Yp9ct2XLFh05ckRhYWEaOnSo4uLiFBcXp9mzZ1d4DqvGudCdd96pTZs2KSUlRUVFRYqKitLw\n",
              "4cPLrF1o4MCBevPNNyWd3UT8+++/1x133FFm7Xw33HCDCgoKtGbNGknSvHnz1KtXL/n4+JRaqzDj\n",
              "RgEBAeann36q0HsnTpxobTMAAAAAAPwOpKSkmNjYWHPixInKbsVpz549RpLx8/MzoaGhJjQ01LRt\n",
              "29ZZT01NNV26dDF2u90EBweb6OhoZ+2pp54yjRo1MjVq1DC+vr6mUaNGJj093eTk5JiIiAgTHBxs\n",
              "QkNDTUREhFm9evUle6jIHO4cZ86cOcbPz8/4+fmZESNGmLy8vDJre/fuNaGhoc7rsrOzTf/+/Y2f\n",
              "n5+x2+3mnXfecak2a9Ys89hjjzmPN23aZJo3b27sdrvp0KGD2b17t0u1c06cOGFiY2NNSkpKid/r\n",
              "+WzGGFPxmKp0DodDdevWlTFGHTt21KxZsy5KNaWzKeW5jwo857nnnqu05wkBAAAAAKiqUlNTdfTo\n",
              "UYWEhFzeyhPARbm5uUpISFD9+vWLbWReErc+OvfVV1/pf//7n3788UddffXVuu+++0q8bubMmapX\n",
              "r16xV0xMjDtbAwAAAAAAgMXcGjSdS7lq1KihJ554Qlu3bi3xuqlTpyo7O7vYq2PHju5sDQAAAAAA\n",
              "ABZz26fO5ebm6syZM/L19ZUkffzxx2rXrl2J13p7e8vb27vYOU9PT3e1BgAAAAAAADdwW9C0f/9+\n",
              "DR8+XIWFhTLGKCgoSB988IG7pgMAAAAAAEAlc1vQFBQUpJ9++sldwwMAAAAA8Ifk4fH/u+C48fO9\n",
              "gIvYbLYyr3HrHk0AAAAAAMBa5wdNwG/JlW2O+O0EAAAAAACAJQiaAAAAAACopmw2W5V4nTx5Ur17\n",
              "95bD4VBYWJi6du2qXbt2OesZGRnq3r27HA6Hmjdvrg0bNjhrU6ZMUWBgoDw9PbV48eJi444cOVKh\n",
              "oaFq2bKl2rZtq+jo6Ev2UNE53DXOggUL5HA4ZLfbdfvtt+v06dOy2WzavXu3rr/+etWpU0ctW7Ys\n",
              "dYwTJ05o4MCBCggIkMPh0KJFi1yqXfjasmWLwsLC5HA41LlzZyUlJblUO/cqD4ImAAAAAACqsaKi\n",
              "IhWeynHrq6ioqMw+xo4dq6SkJMXHx+uWW27RAw884KxNnDhRkZGRSklJ0bvvvqv7779f+fn5kqR+\n",
              "/fpp/fr1ioyMvGjMhQsXavfu3YqLi1NUVJTuvfdeFRYWljh/Redwxzi//vqrZsyYoa+++kopKSk6\n",
              "ePCgXnnlFUmSr6+vXnzxRb377rtl9vL888+rZs2aSk1N1caNG/XUU08pKyurzNr5CgsLNXr0aM2d\n",
              "O1cpKSnq3bu3JkyYUGatoty2GTgAAAAAAHA/k39CiePru3WO4IVHpSvqXrLu4+OjUaNGOY+7deum\n",
              "N99803m8bt06/frrr5KkG2+8UY0aNdL69es1dOhQ9ezZ85LjNmzY0Pn10aNHS+2xonO4Y5wlS5ao\n",
              "d+/eCggIkCSNGzdOL7/8sp599lk1btxY/fr109q1a8scZ/Xq1YqKipIktWzZUp06ddKSJUv05JNP\n",
              "llo739atW+Xp6amBAwdKkiZNmqRrrrlGubm52rZt2yVrPj4+Ln2vF2JFEwAAAAAAsNT8+fPVt29f\n",
              "SVJWVpYKCgqcoYsk+fv7Kzk52aWxxo8fL39/f911111avHhxiRtSX+4cVo+TlpYmu93uPA4ODlZm\n",
              "ZmaZ70tKSlJYWJjzODMzU8HBwc5ju92u1NTUMmsvv/yyHn/8ceeYzZo1c15Xv359+fj4KCUlpdRa\n",
              "RbGiCQAAAACAaszmfeXZFUdunsNVkydPVnJysj788ENL5l64cKEWLlyolStXaurUqerdu7dq1apl\n",
              "ydhVTWBgoOLi4i57nMmTJ1vQTcWwogkAAAAAgGrMw8NDnlfUdevLw8O1+OC5557T2rVrtWnTJtWp\n",
              "U0eSdM0118jT07PYKpm0tDQ5HI5yfZ9Dhw5Vbm6uYmJitHLlSoWFhSksLEzPPPNMheewapwL+fv7\n",
              "O1cXSVJiYqKaNGlSrjEkqUmTJkpMTHQep6amOldKlVY7X2BgoDIyMpzHR48e1YkTJxQQEFBqraII\n",
              "mgAAAAAAwGWbNm2ali9fri1bthTbW0mS+vfvrwULFkiSvvjiC+3fv18DBgwodbz8/Hzt3LnTebxl\n",
              "yxYdOXJEYWFhGjp0qOLi4hQXF6fZs2dXeA6rxrnQnXfeqU2bNiklJUVFRUWKiorS8OHDyzWGJA0c\n",
              "ONC519Wvv/6q77//XnfccUeZtfPdcMMNKigo0Jo1ayRJ8+bNU69eveTj41NqrcJMFTVx4sTKbgEA\n",
              "AAAAgConIyPDxMbGmhMnTlR2K0579uwxkoyfn58JDQ01oaGhpm3bts56amqq6dKli7Hb7SY4ONhE\n",
              "R0c7a0899ZRp1KiRqVGjhvH19TWNGjUy6enpJicnx0RERJjg4GATGhpqIiIizOrVqy/ZQ0XmcOc4\n",
              "c+bMMX5+fsbPz8+MGDHC5OXlGWOMycnJMY0aNTK+vr7Gy8vLNGrUyIwfP94YY8zevXtNaGioc4zs\n",
              "7GzTv39/4+fnZ+x2u3nnnXdcqs2aNcs89thjzuNNmzaZ5s2bG7vdbjp06GB2797tUu2cEydOmNjY\n",
              "WJORkVHyD/88NmOMqXhM5T6TJk3SvHnzKrsNAAAAAACqlH379ungwYMKCQm5vJUngItyc3OVkJCg\n",
              "hg0bqmnTpqVey6NzAAAAAAAAsARBEwAAAAAAACxB0AQAAAAAAABLEDQBAAAAAADAEl6V3QAAAAAA\n",
              "AHCdp6en8+sq+vle+J3y8Ch7vRIrmgAAAAAAqEZc+cc+4A7nh5yXwm8nAAAAAAC4LCdPnlTv3r3l\n",
              "cDgUFhamrl27ateuXc56RkaGunfvLofDoebNm2vDhg3O2pQpUxQYGChPT08tXry42LgjR45UaGio\n",
              "WrZsqbZt2yo6OvqSPVR0DneNs2DBAjkcDtntdt1+++06ffq0JGnNmjW69tprFRISoubNm2v8+PEq\n",
              "KioqcYyioiKNHj1adrtdAQEBmjFjhku1C+3cuVPt27eXw+FQ27ZttX37dpdqFUHQBAAAAABANWWz\n",
              "2arES5LGjh2rpKQkxcfH65ZbbtEDDzzgrE+cOFGRkZFKSUnRu+++q/vvv1+nT5+WzWZTv379tH79\n",
              "ekVGRl70PS1cuFC7d+9WXFycoqKidO+996qoqKjEHio6hzvGiYuL04wZM7R161alpKTo4MGDeuWV\n",
              "V2Sz2dSgQQMtXbpUiYmJ2rFjh2JiYvTGG2+UOM7ChQsVHx+vpKQkbdu2TW+++aa2b99eZu3C19ix\n",
              "YzV69GilpKRo0qRJuv/++12qnf+/r6sImgAAAAAAqMaKioqUd+q0W1+XWnFzjo+Pj0aNGuV8rK9b\n",
              "t25KT0931tetW6eJEydKkm688UY1atTIuVKoZ8+eatWqVYnjNmzY0Pn10aNHS+2honO4Y5wlS5ao\n",
              "d+/estvt8vDw0Lhx47Rs2TJJUteuXZ1j+Pj4qE2bNkpOTi5xnGXLlumBBx6Ql5eXGjdurEGDBumD\n",
              "Dz4os3a+9PR07dy5U+PHj5ckjR49WllZWdq5c2eptYpiM3AAAAAAAKqx0/kF+uuz/3HrHC/O6KNa\n",
              "V9R0+fr58+erb9++kqSsrCwVFBTIbrc76/7+/kpKSnJprPHjx2vt2rXKycnR4sWLS9wn6HLnsHqc\n",
              "tLS0YmMEBwcrMzPzoutSU1O1fv16LV++3HkuLCxMGzZsUGBgoDIyMhQUFOSsBQQEKCYmRpJKrS1e\n",
              "vFjR0dH65JNPtHfvXl199dWqWfPs/34eHh5q0qSJ9u7dK19f30vW2rRpU67v+RxWNAEAAAAAAMtM\n",
              "njxZycnJeu211ywZb+HChUpLS9M///lPTZ06VXl5eZaMW9mOHDmiAQMG6JFHHlGPHj2c5+Pi4hQY\n",
              "GHhZY99999365JNPLrfFCmFFEwAAAAAA1VhNby+9OKOP2+dwxXPPPae1a9fqiy++UJ06dSRJ11xz\n",
              "jTw9PZWamupc5ZOWllbuMGXIkCGaOHGiYmJidOTIEU2ZMkWSNGjQIM2ePbtCc6xatcqScS7k7++v\n",
              "vXv3Oo8TExPVpEkT5/HRo0d10003qX///nrhhRcuOU6zZs2KjZOSkiI/P78ya+cLCgrSoUOHdPr0\n",
              "adWsWVNFRUXKzMxUUFCQfH19L1mrKFY0AQAAAABQjXl4eKjWFTXd+jq391Jppk2bpuXLl2vLli3F\n",
              "9laSpAEDBmj+/PmSpC+++EL79+9X//79Sx0vPz+/2F5BW7Zs0ZEjRxQWFqYhQ4YoLi5OcXFxmj17\n",
              "doXnsGqcC915553atGmTUlNTVVRUpKioKA0fPlySdOzYMfXq1Uu9evXSnDlzSh1n6NChev/991VQ\n",
              "UKD9+/crOjpad999d5m18/n5+alVq1ZauHChJOkf//iHGjdurDZt2pRaqzBTRU2cOLGyWwAAAAAA\n",
              "oMo5cOCAiY2NNSdOnKjsVpwSEhKMJOPn52dCQ0NNaGioadu2rbOemppqunTpYux2uwkODjbR0dHO\n",
              "2lNPPWUaNWpkatSoYXx9fU2jRo1Menq6ycnJMRERESY4ONiEhoaaiIgIs3r16kv2UJE53DnOnDlz\n",
              "jJ+fn/Hz8zMjRowweXl5xhhjnn76aePp6en8OYWGhpqnn37a+b7Q0FCzd+9eY4wxZ86cMXfffbdp\n",
              "1qyZ8fPzMy+88ILzutJqH374oRk5cqTz+KeffjLh4eHGbrebVq1ame+//96l2jknTpwwsbGx5sCB\n",
              "AyX/8M9jM8aYisdU7jNp0iTNmzevstsAAAAAAKBKOXjwoPbt26eQkBD5+PhUdjv4A8jNzVVCQoKa\n",
              "Nm160Wq1C/HoHAAAAAAAACxB0AQAAAAAAABLEDQBAAAAAADAEgRNAAAAAAAAsIRXZTcAAAAAAABc\n",
              "5+V19p/yxhhV0c/3wu+Up6dnmdcQNAEAAAAAUI14eHjIZrPJZrMRNOE3YYyRzWYjaAIAAAAA4Pfq\n",
              "XNgEuJuHh+s7L7FHEwAAAAAA1dS5sKmyX6dOnVKfPn0UGBiosLAwdevWTb/88ouzvm/fPnXv3l0O\n",
              "h0MtWrTQp59+6qw9++yzCgoKkqenp5YsWVJs3JEjRyo0NFQtW7ZU27ZttWbNmkv2UNE53DXOq6++\n",
              "KofDIbvdrjvuuEOnT5+WzWbT559/rpYtW6ply5Zq3ry57rrrLuXl5ZU4xokTJzRo0CAFBAQoMDBQ\n",
              "//znP12qXfj64osvFBYWpsDAQHXu3FnJycku1SoSZhI0AQAAAACAy/bQQw9p7969io+P16233qr7\n",
              "77/fWZs4caI6duyolJQUvffee7rvvvuUn58vSerXr5/Wr1+vyMjIi8Z86623tHv3bsXFxentt9/W\n",
              "Pffco8LCwhLnr+gc7hgnLi5OM2bM0Ndff62UlBQdPHhQ8+fPlyR17NhRO3bsUFxcnOLj43Xo0CHN\n",
              "mTOnxHFefPFFeXt7KzU1VRs3btSkSZO0f//+MmvnKyws1H333adXXnlFycnJ6tu3ryZMmFBmraII\n",
              "mgAAAAAAqMaKiopUdDzfva+iolJ7qF27tkaOHOl8xKpbt27KyMhw1teuXasnnnhCktS9e3c1btxY\n",
              "GzdulCT16NFDLVu2LHHcq6++2vn1sWPHSu2honO4Y5yPPvpIvXv3lr+/vzw8PPTwww/r3//+tySp\n",
              "Tp068vb2liTl5+fr1KlTl1w1tHLlSmfwExYWpk6dOumjjz4qs3a+b775Rp6enrr11lslnQ3SPv/8\n",
              "c508ebLUWkWxRxMAAAAAANVZ7hkduP41t07R6IfHpDreLl8/b9489e3bV5K0f/9+FRQUyN/f31n3\n",
              "8/NTcnKyS2M98sgjio6OVk5OjpYsWVLihtSXO4fV46SmpiogIMB5HBwcrH379jmP4+PjNWjQIKWl\n",
              "palnz556+umnJUnJycnq16+f4uLiJEmZmZkKDg52vi8gIEApKSll1ubMmaOMjAwtWLBASUlJ8vPz\n",
              "c17n6+srHx8fpaSklFpzNZi7ECuaAAAAAACAZaZOnaqkpCS9+uqrloz35ptvKi0tTR988IGmTJmi\n",
              "vLw8S8atTKGhoYqPj1dmZqZOnz6txYsXS5IcDoczZLocTz/9tBYsWHDZ41QEK5oAAAAAAKjOfGqc\n",
              "XXHk5jlcMW3aNK1Zs0ZffPGF6tSpI0lq3LixPD09lZaW5lwplJ6eLofDUa4WBg8erCeeeELbt2/X\n",
              "4cOHNXnyZEnSkCFDNGvWrArNsXr1akvGuZDdbldiYqLzODExUU2bNr3ounr16mnUqFH66KOPNGbM\n",
              "mIvqTZo0UWJionN1VEpKinr37l1m7XyBgYFKT093Hh87dkwnTpxQQECADh48eMlaRbGiCQAAAACA\n",
              "aszDw0Medbzd+3Lh4+2ff/55LVu2TFu2bCm2t5Ik3XLLLc4VNl999ZWysrLUr1+/UsfLz8/Xrl27\n",
              "nMdffvmljhw5otDQUA0ePFhxcXGKi4vTrFmzKjyHVeNc6I477tCmTZuUlpamoqIivfXWWxoxYoQk\n",
              "adeuXc7NxfPy8rRy5Uq1adPmkv39/e9/l3R2g/Hvv/9et99+e5m183Xt2lUFBQVau3atJGn+/Pnq\n",
              "1auXateuXWqtwkwVNXHixMpuAQAAAACAKufYsWNmx44dJjc3t7JbcUpMTDSSjJ+fnwkNDTWhoaGm\n",
              "bdu2znpaWprp2rWrsdvtJjg42KxZs8ZZe/rpp02jRo1MjRo1jK+vr2nUqJHJyMgwOTk5pl27diYk\n",
              "JMSEhoaadu3amejo6Ev2UJE53DnOK6+8Yvz8/Iyfn5+57bbbTF5enjHGmLlz55qQkBDTokULExwc\n",
              "bEaPHu383zIpKcmEhoY6x8jOzjYDBgwwfn5+JiAgwLz33nsu1WbPnm0ef/xx5/HmzZtN8+bNTUBA\n",
              "gOnQoYNJSEhwqXZObm6u2bFjhzl27FiJ3+v5bMYYU/GYyn0mTZqkefPmVXYbAAAAAABUKdnZ2UpJ\n",
              "SVFISMjlrTwBXHTy5EklJCQoICBA9erVK/VaHp0DAAAAAACAJQiaAAAAAAAAYAmCJgAAAAAAAFiC\n",
              "oAkAAAAAgGqoim65jD84r8puAAAAAAAAuK5GjRqSpIKCAhUUFMjDgzUkcK+CggJJkpdX2TESQRMA\n",
              "AAAAANWIh4eHvLy8ZLPZdObMmcpuB38ARUVF8vT0lKenZ5nXEjQBAAAAAFDN2Gw21ahRQ97e3pXd\n",
              "Cv4gXF05x/o6AAAAAACqIZvNJg8PjyrxOn36tIYNG6awsDC1a9dOffv21d69e531Q4cOacCAAQoN\n",
              "DdW1116rr7/+2lmbNWuWWrZsKS8vL0VHRxcb98EHH1RERITat2+v66+/Xlu2bLlkDxWdw13jLFq0\n",
              "SKGhoWrevLnGjRunwsLCYnWbzaabb75ZV1111SXHkKTHH39czZs3V4sWLfT3v//dpdqFr8TERHXr\n",
              "1k1hYWG6/vrr9euvv7pUO79XVxE0AQAAAABQjRljlF9w0q0vVzYef+ihhxQfH68dO3Zo8ODBGjNm\n",
              "jLM2ZcoUderUSXv27NGiRYt05513Oh/7u/nmm7VhwwZ17979ojHnz5+v//3vf4qNjdXbb7+tkSNH\n",
              "qqioqMT5KzqHO8ZJSkrSc889p61btyohIUH79+/X22+/fdH3FhwcXOo4ixcv1i+//KLdu3crJiZG\n",
              "c+bM0a5du8qsXWjcuHF66KGHtHv3bk2ePFmjR492qVYRPDoHAAAAAEA1drrwlB77d0u3zvHayF/l\n",
              "7VX7kvVatWppwIABzuNOnTpp7ty5zuNPPvlECQkJkqQOHTqoadOm+vLLL3XzzTerY8eOlxzX19fX\n",
              "+XV2dnapPVZ0DneMs2zZMg0aNEjXXHONJOnhhx/WjBkz9Mgjj0iSdu3apVWrVmnRokX697//fclx\n",
              "li5dqrFjx8rT01NXXXWVRo0apY8//ljTp08vtXa+AwcOaPv27frPf/4jSRo+fLj+9Kc/KSEhQXXr\n",
              "1r1kLSQkxOWf2flY0QQAAAAAACz16quvavDgwZKkw4cP68yZM87QRZIcDodSU1NdGmvKlCkKDg7W\n",
              "sGHDtHz58hL3CrrcOaweJzU1VQEBASWOcebMGY0dO1ZRUVElbq4dERGhffv2lTlOabXo6GjnirK0\n",
              "tDQ1adLE+YlxNptNdrtdqamppdYqihVNAAAAAABUYzU9r9BrI391+xyumjFjhhISEvTZZ59ZMves\n",
              "WbM0a9Ysbd68Wc8884y++eYb1axZ05KxK8MLL7ygYcOGqWXLlkpOTr6oHhsbe9lzDBo0SIMGDbrs\n",
              "cSqCFU0AAAAAAFRjNptN3l613fpydTPouXPnasWKFdqwYYNq1z77qF2DBg3k5eWlrKws53XJycmy\n",
              "2+3l+j5vvvlmHT9+XD///LM2b96siIgIRURE6KWXXqrwHFaNcyG73a6UlJQSx/jyyy/1+uuvy+Fw\n",
              "qFu3bsrJyZHD4dDBgwfLNU5ptfP5+/srMzNTBQUFks7u6ZWamiq73V5qraIImgAAAAAAwGWbN2+e\n",
              "Pv74Y23atKnY3kqSNHLkSL311luSpG3btikjI0M9evQodbwzZ84490qSpJiYGB04cEBBQUG6+eab\n",
              "FRsbq9jYWP35z3+u8BxWjXOh4cOHKzo6WllZWTLG6K233tLtt98uSdq6datSUlKUnJysr7/+WnXr\n",
              "1lVycrIaNmx40TgjR47UO++8o8LCQh05ckRLly7VqFGjyqydr1GjRmrfvr0WL14sSVq+fLn8/PwU\n",
              "EhJSaq2ibMaVreMrwaRJkzRv3rzKbgMAAAAAgColLy9PSUlJCgwMVK1atSq7HUlSenq6/P39FRQU\n",
              "pDp16kiSvL299cMPP0iS9u/fr3vuuUdJSUmqWbOm3njjDfXs2VOSNH36dL311ls6ePCg6tSpo1q1\n",
              "aumnn36Sj4+PevfurezsbHl5ecnHx0d/+9vf1KtXrxJ7qMgcJYU7Vo3zzjvvaNasWZKkG2+8UW+9\n",
              "9ZZq1KhR7Jrk5GRFRETo2LFjznMRERFav369mjZtqsLCQj322GPasGGDbDabHnvsMT3++OOSVGot\n",
              "Ojpa0dHRevfddyVJ8fHxGj16tA4fPqy6detq0aJFatu2bZm1c8rzO0fQBAAAAABANVIVgyb8vpXn\n",
              "d45H5wAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAIBqqKioqLJbwB9EeXZd8nJjHwAAAAAAwGI1a9aU\n",
              "h4eH9u3bp4YNG6pmzZqy2WyV3RZ+p4wxOnjwoGw220WbmZeEoAkAAAAAgGrEw8NDgYGByszM1L59\n",
              "+yq7HfwB2Gw2+fn5ydPTs8xrCZoAAAAAAKhmatasKbvdroKCAhUWFlZ2O/idq1Gjhkshk0TQBAAA\n",
              "AABAtXTuUSZXHmcCfitsBg4AAAAAAABLEDQBAAAAAADAEgRNAAAAAAAAsARBEwAAAAAAACxB0AQA\n",
              "AAAAAABLEDQBAAAAAADAEgRNAAAAAAAAsARBEwAAAAAAACxB0AQAAAAAAABLEDQBAAAAAADAEgRN\n",
              "AAAAAAAAsARBEwAAAAAAACxB0AQAAAAAAABLEDQBAAAAAADAEr9J0LRo0SLZbDatWrXqt5gOAAAA\n",
              "AAAAlcDtQVNycrLeeecdderUyd1TAQAAAAAAoBK5NWgqKirSmDFj9Prrr8vb2/uS1+Xn5ysnJ6fY\n",
              "q7Cw0J2tAQAAAAAAwGJuDZrmzZunrl276rrrriv1upkzZ6pevXrFXjExMe5sDQAAAAAAABZzW9C0\n",
              "c+dOLV++XH/5y1/KvHbq1KnKzs4u9urYsaO7WgMAAAAAAIAbeLlr4K1btyo5OVnNmzeXJGVlZemh\n",
              "hx5SZmamxo8fX+xab2/vix6t8/T0dFdrAAAAAAAAcAO3rWgaP368MjMzlZycrOTkZHXq1Elvv/32\n",
              "RSETAAAAAAAAfh/c/qlzAAAAAAAA+GNw26NzF/riiy9+q6kAAAAAAABQCVjRBAAAAAAAAEsQNAEA\n",
              "AAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEET\n",
              "AAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQ\n",
              "NAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACw\n",
              "BEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAA\n",
              "AEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAA\n",
              "AACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAA\n",
              "AAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0A\n",
              "AAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQ\n",
              "BAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMAS\n",
              "BE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAA\n",
              "LEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAA\n",
              "AMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAA\n",
              "AAAALEHQBAAAAAAAAEsQNAEAAAAAAMASBE0AAAAAAACwBEETAAAAAAAALEHQBAAAAAAAAEsQNAEA\n",
              "AAAAAMASXu4cvE+fPsrKypKHh4fq1Kmj1157Te3atXPnlAAAAAAAAKgkbg2aPvnkE/n6+kqSVq5c\n",
              "qdGjR2vHjh3unBIAAAAAAACVxK2Pzp0LmSQpOztbNpvNndMBAAAAAACgErl1RZMk3XvvvdqyZYsk\n",
              "af369SVek5+fr/z8/GLnCgsL3d0aAAAAAAAALOT2zcA/+OADpaWlafr06Zo8eXKJ18ycOVP16tUr\n",
              "9oqJiXF3awAAAAAAALCQzRhjfqvJrrjiCqWnp6tBgwbFzpe0oum5557Tq6+++lu1BgAAAAAAgMvk\n",
              "tkfnjh07ppMnT6pp06aSpFWrVqlBgwa66qqrLrrW29tb3t7exc55enq6qzUAAAAAAAC4QbmCpsTE\n",
              "RL3zzjv6/PPPlZ6eriuuuELh4eEaMWKEbrvtNnl5/f9w2dnZGjlypE6dOiUPDw81bNhQa9euZUNw\n",
              "AAAAAACA3ymXg6Zx48bpxx9/1MiRI/XKK6/ommuuUV5enn799Vdt3LhRs2fP1ltvvaVOnTpJkgIC\n",
              "AthnCQAAAAAA4A/E5aBp0KBBioqKuuh827Ztddttt+nw4cNKTEy0tDkAAAAAAABUHy4HTbfcckup\n",
              "9QYNGly0yTcAAAAAAAD+ODzKc/G2bds0atQotWnTRm3atNHtt9+u7du3u6s3AAAAAAAAVCMuB03f\n",
              "ffed+vTpo6CgIE2fPl1/+9vfFBgYqD59+uiHH35wZ48AAAAAAACoBlx+dG727Nl6//33NXToUOe5\n",
              "oUOHqlOnTpo5c6ZWrVrljv4AAAAAAABQTbi8omnXrl3FQqZzBg8erF9++cXSpgAAAAAAAFD9uBw0\n",
              "1a5d+5I1Hx8fS5oBAAAAAABA9eXyo3P5+fn6+eefZYy5qJaXl2dpUwAAAAAAAKh+XA6aTp06pUGD\n",
              "BpVYs9lsljUEAAAAAACA6snloCk5OdmNbQAAAAAAAKC6c3mPpgsdO3ZMq1at0s8//2xlPwAAAAAA\n",
              "AKimXA6a7rnnHsXGxko6GzKFh4fr2Wef1U033aRFixa5qz8AAAAAAABUEy4HTf/9738VEREhSVqy\n",
              "ZIlCQkL0yy+/aPv27Xrttdfc1R8AAAAAAACqCZeDplq1ajm/3rp1q4YOHSpJstvt1ncFAAAAAACA\n",
              "asfloKmwsFDZ2dkqKCjQ1q1bdcMNNzhreXl5bmkOAAAAAAAA1YfLnzo3fvx4tW/fXnXr1lVQUJDC\n",
              "w8MlST///LMaN27stgYBAAAAAABQPbgcND388MOKjIxURkaG+vTp4zxfs2ZNzZ8/3y3NAQAAAAAA\n",
              "oPpwOWiSpMjISEVGRhY7FxoaamlDAAAAAAAAqJ5cDprq168vm83mPLbZbGrUqJH69Omjl156SVde\n",
              "eaVbGgQAAAAAAED14HLQFBsbe9G5Q4cOKSoqSk8++aSioqKs7AsAAAAAAADVjMtBU0BAQInn3nrr\n",
              "LbVv397SpgAAAAAAAFD9eFz2AB4e8vC47GEAAAAAAABQzV12QrRx40ZdddVVVvQCAAAAAACAaszl\n",
              "R+fatWtXbDNwSTp8+LBq1KihlStXWt4YAAAAAAAAqheXg6YFCxYUO7bZbGrYsKGaN28uLy+XhwEA\n",
              "AAAAAMDvlMsJUY8ePdzZBwAAAAAAAKo5l/domjBhgtLT00usGWO0fPlyffTRR5Y1BgAAAAAAgOrF\n",
              "5RVNAwYM0IABA1S/fn1df/31aty4sfLy8hQfH69vvvlGAwYM0IsvvujOXgEAAAAAAFCFuRw03Xrr\n",
              "rbr11lv19ddf64svvtCePXtUu3Zt9erVS/PmzdPVV1/tzj4BAAAAAABQxZV7F+9u3bqpW7du7ugF\n",
              "AAAAAAAA1ZjLezQBAAAAAAAApSFoAgAAAAAAgCUImgAAAAAAAGCJcgdN06dPd+kcAAAAAAAA/ljK\n",
              "HTStWLHCpXMAAAAAAAD4Y3H5U+c+/fRTbdy4URkZGZo0aZLzfHZ2tlsaAwAAAAAAQPXictBUq1Yt\n",
              "+fr6ysPDQ/Xq1XOe9/f313PPPeeW5gAAAAAAAFB9uBw09ejRQz169NCQIUMUHh7uzp4AAAAAAABQ\n",
              "DbkcNJ3TunVrLV26VImJiSooKHCe/+tf/2ppYwAAAAAAAKheyh003X777crKylLHjh3l6enpjp4A\n",
              "AAAAAABQDZU7aPr5558VFxcnm83mjn4AAAAAAABQTXmU9w3+/v46ffq0O3oBAAAAAABANVbuFU0h\n",
              "ISG68cYbNXToUNWqVct5/rHHHrO0MQAAAAAAAFQv5Q6a8vPzFRYWpl9//dV5jsfoAAAAAAAAUO6g\n",
              "adGiRe7oAwAAAAAAANVcuYOmDz74oMTz995772U3AwAAAAAAgOqr3EHTmjVrnF/n5eXp66+/VqdO\n",
              "nQiaAAAAAAAA/uDKHTT9+9//LnaclJSkP//5z5Y1BAAAAAAAgOrJ43IHCAwM1K5du6zoBQAAAAAA\n",
              "ANVYuVc0RUdHO78uLCzUDz/8IG9vb0ubAgAAAAAAQPVT7qBp/vz5//9mLy+FhIRo6dKlljYFAAAA\n",
              "AACA6qfcQdOWLVvc0QcAAAAAAACquXIHTdLZDcE3bdokSerbt6+GDx9uaVMAAAAAAACofsq9GfiL\n",
              "L76omTNnqlWrVmrdurVmzpyp6dOnu6M3AAAAAAAAVCPlXtG0bNkyff/996pdu7YkacyYMercubP+\n",
              "8pe/WN4cAAAAAAAAqo9yr2gyxjhDJkny8fGRMcbSpgAAAAAAAFD9lHtFU8eOHXXPPfdo7NixkqT3\n",
              "3ntPHTt2tLwxAAAAAAAAVC/lXtH02muvqWnTppo0aZImTZqkJk2a6LXXXnNHbwAAAAAAAKhGyr2i\n",
              "ycfHRy+//LI7egEAAAAAAEA1Vu6g6eTJk/rnP/+pPXv2qKCgwHmeVU0AAAAAAAB/bOUOmoYNG6Ya\n",
              "NWqoQ4cO8vT0dEdPAAAAAAAAqIbKHTSlpqbql19+cUcvAAAAAAAAqMbKvRl4WFiYDh065I5eAAAA\n",
              "AAAAUI2Ve0XTSy+9pM6dO6tDhw6qVauW8/z7779vaWMAAAAAAACoXsodNI0dO1adO3dWZGQkezQB\n",
              "AAAAAADAqdxB09GjR/XBBx+4oxcAAAAAAABUY+Xeoyk8PFwZGRnu6AUAAAAAAADVWLlXNB08eFBt\n",
              "2rRR586di+3RtGLFCksbAwAAAAAAQPVS7qDp7rvv1t133+2OXgAAAAAAAFCNlTtouu+++4odFxYW\n",
              "as2aNZY1BAAAAAAAgOqp3Hs0nRMfH69nnnlGzZo10/Tp063sCQAAAAAAANVQuVY0nTx5UkuXLtW7\n",
              "776rpKQknTp1St99953CwsLc1R8AAAAAAACqCZdXNI0dO1b+/v6Kjo7WlClTlJqaKl9fX0ImAAAA\n",
              "AAAASCrHiqZ//etfioyM1Lhx49S3b1/ZbDbZbDZ39gYAAAAAAIBqxOUVTZmZmbr77rv14osvKiAg\n",
              "QH/5y1905swZd/YGAAAAAACAasTloOnKK6/Ugw8+qG+//VYbN25UXl6eTp8+rS5duujvf/+7O3sE\n",
              "AAAAAABANVChT51r1aqV5s6dq4yMDD355JNat26d1X0BAAAAAACgmqlQ0HSOl5eXhg8fTtAEAAAA\n",
              "AACAywuaAAAAAAAAgHPcFjTl5eVpyJAhatGihcLDw9W7d28lJCS4azoAAAAAAABUMreuaHrooYcU\n",
              "Hx+vHTt2aPDgwRozZow7pwMAAAAAAEAlclvQVKtWLQ0YMEA2m02S1KlTJyUnJ5d4bX5+vnJycoq9\n",
              "CgsL3dUaAAAAAAAA3OA326Pp1Vdf1eDBg0uszZw5U/Xq1Sv2iomJ+a1aAwAAAAAAgAV+k6BpxowZ\n",
              "SkhI0MyZM0usT506VdnZ2cVeHTt2/C1aAwAAAAAAgEW83D3B3LlztWLFCm3evFm1a9cu8Rpvb295\n",
              "e3sXO+fp6enu1gAAAAAAAGAhtwZN8+bN08cff6zNmzfL19fXnVMBAAAAAACgkrktaEpPT9eTTz6p\n",
              "oKAg9ezZU9LZlUs//PCDu6YEAAAAAABAJXJb0OTn5ydjjLuGBwAAAAAAQBXzm33qHAAAAAAAAH7f\n",
              "CJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAA\n",
              "WIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAA\n",
              "AIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAA\n",
              "AAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIA\n",
              "AAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYIm\n",
              "AAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYg\n",
              "aAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABg\n",
              "CYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAA\n",
              "AJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAA\n",
              "AABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAA\n",
              "AAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoA\n",
              "AAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKg\n",
              "CQAAAAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCYImAAAAAAAAWIKgCQAAAAAAAJYgaAIAAAAAAIAl\n",
              "CJoAAAAAAABgCbcGTY899pgcDodsNptiY2PdORUAAAAAAAAqmVuDphEjRujrr79WQECAO6cBAAAA\n",
              "AABAFeDlzsG7d+/u0nX5+fnKz88vdq6wsNAdLQEAAAAAAMBNqsQeTTNnzlS9evWKvWJiYiq7LQAA\n",
              "AAAAAJRDlQiapk6dquzs7GKvjh07VnZbAAAAAAAAKAe3PjrnKm9vb3l7exc75+npWUndAAAAAAAA\n",
              "oCKqxIomAAAAAAAAVH9uDZrGjRsnPz8/paenq2/fvgoJCXHndAAAAAAAAKhEbn10Lioqyp3DAwAA\n",
              "AAAAoArh0TkAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJ\n",
              "AAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUI\n",
              "mgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABY\n",
              "gqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAA\n",
              "gCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAA\n",
              "AABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAA\n",
              "AAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYA\n",
              "AAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBo\n",
              "AgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJ\n",
              "giYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAA\n",
              "liBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAA\n",
              "AGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAA\n",
              "AAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUImgAA\n",
              "AAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAlnBr0LRnzx516dJFLVq0UIcOHbRr1y53TgcAAAAAAIBK\n",
              "5Nagady4cXrooYe0e/duTZ48WaNHj3bndAAAAAAAAKhEbguaDhw4oO3bt+vuu++WJA0fPlxpaWlK\n",
              "SEhw15QAAAAAAACoRF7uGjgtLU1NmjSRl9fZKWw2m+x2u1JTUxUSElLs2vz8fOXn5xc7V1hY6K7W\n",
              "AAAAAAAA4AZVYjPwmTNnql69esVeMTExld0WAAAAAAAAysFtQZO/v78yMzNVUFAgSTLGKDU1VXa7\n",
              "/aJrp06dquzs7GKvjh07uqs1AAAAAAAAuIHbgqZGjRqpffv2Wrx4sSRp+fLl8vPzu+ixOUny9vZW\n",
              "3bp1i708PT3d1RoAAAAAAADcwG17NElSVFSURo8erRkzZqhu3bpatGiRO6cDAAAAAABAJXJr0BQa\n",
              "GqrvvvvOnVMAAAAAAACgiqgSm4EDAAAAAACg+iNoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABY\n",
              "gqAJAAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYwmaMMZXdREmGDRsmh8NR2W2U\n",
              "qbCwUDExMerYsaM8PT0rux3gd4X7C3AP7i3APbi3APepbvdXQECAHn/88cpuA6gUVTZoqi5ycnJU\n",
              "r149ZWdnq27dupXdDvC7wv0FuAf3FuAe3FuA+3B/AdUHj84BAAAAAADAEgRNAAAAAAAAsARBEwAA\n",
              "AAAAACxB0HSZvL29NW3aNHl7e1d2K8DvDvcX4B7cW4B7cG8B7sP9BVQfbAYOAAAAAAAAS7CiCQAA\n",
              "AAAAAJYgaAIAAAAAAIAlCJoAAAAAAABgCbcGTXl5eRoyZIhatGih8PBw9e7dWwkJCc76gQMH1K9f\n",
              "PzVv3lxt2rTRV1995azNmDFDoaGh8vDw0KpVq4qNe//99+vaa69VRESEOnTooM8+++ySPVR0DneN\n",
              "895776l58+YKDg7W2LFjdebMGZdq5zt58qTuuOMOhYSEqEWLFlq2bJlLtQv98MMPCg8PV4sWLdSr\n",
              "Vy9lZGS4VEPl49662KXun+TkZN14442qV6+eIiIiSh2DewsS91dJLnV/ff755+rYsaNatWql1q1b\n",
              "65lnnlFRUVGJYxQVFenRRx9VcHCwQkJC9MYbb7hUu9CePXvUpUsXtWjRQh06dNCuXbtcqqHycW9d\n",
              "7FL31nfffaeIiAhFRESodevWGjdunPLz80scg79d4N66WFn/rjLGqFevXvL19b3kGPzdAi6DcaNT\n",
              "p06ZdevWmaKiImOMMa+//rrp0aOHs37//febadOmGWOMiYmJMc2aNTOnT582xhjzww8/mMTERNOj\n",
              "Rw+zcuXKYuMePXrU+fWPP/5o6tevbwoLC0vsoaJzuGOcvXv3miZNmpjMzExTVFRkBg4caN54440y\n",
              "axd64YUXzH333ed8X8OGDc2hQ4fKrJ2vsLDQBAcHm88//9wYY8ycOXPMiBEjyqyhauDeKq60++fw\n",
              "4cNm69atZu3atSY8PLzUXri3YAz314VKu79+/PFHk5iYaIw5+3Pr2rWrWbRoUYnj/POf/zS9evUy\n",
              "BQUF5vDhw8Zut5udO3eWWbtQz549nXP8+9//NpGRkS7VUPm4t4or7d7Kzc11jldYWGiGDBli5s2b\n",
              "V+I4/O0C91Zxrvy76pVXXjFjxowx9erVu+Q4/N0CKs6tQdOFtm3bZgICApzHPj4+JjMz03ncoUMH\n",
              "s2nTpmLvKes/JFu2bCn1P3pWzGHVOLNnzzbjxo1zHq9bt8507dq1zNqFWrVqZb777jvn8ciRI807\n",
              "77xTZu18MTExJjQ01Hmck5NjvL29zalTp0qtoWri3ir7/tmyZUuZQRP3FkrC/eX636dHHnnE+Q+E\n",
              "Cw0YMMB8/PHHzuOnn37a/PnPfy6zdr79+/ebOnXqmDNnzhhjjCkqKjKNGzc2e/bsKbWGqol7y7V7\n",
              "69SpU6Zv375m/vz5JY7D3y5ciHur9Htr586d5oYbbjAJCQmlBk383QIq7jfdo+nVV1/V4MGDJUmH\n",
              "Dx/WmTNndM011zjrDodDqampLo01ZcoUBQcHa9iwYVq+fLk8PC7+Vi53DqvHSU1NVUBAQIljlFbb\n",
              "t29fsUd+KjrOW2+9pb/+9a8lXlenTh3VrVtX+/btK7WGqol769K/96Xh3oIruL9cu7+ysrK0bNky\n",
              "3Xrrrc5zERERzt/vit5f0dHRGjNmjCQpLS1NTZo0kZeXlyTJZrPJbrcrNTW11BqqJu6t0u+t5ORk\n",
              "hYeH6+qrr1a9evU0YcIESfztQtm4ty79e3/mzBmNHTtWUVFR8vT0vOi9/N0CrPGbBU0zZsxQQkKC\n",
              "Zs6cacl4s2bNUmJioj755BM988wzOn36tCXjVkVNmzZVbGzsZY/z8MMP68UXX7z8hlClcG9VHPcW\n",
              "ysL95ZqcnBwNHDhQzzzzjCIjI53nY2Nj1bRp08sae9CgQXr33Xcvt0VUMdxbZXM4HNqxY4eysrKU\n",
              "n5+vFStWSOJvF0rHvVW6F154QcOGDVPLli1LrPN3C7DGbxI0zZ07VytWrNCGDRtUu3ZtSVKDBg3k\n",
              "5eWlrKws53XJycmy2+3lGvvmm2/W8ePH9fPPP2vz5s3OjRNfeumlCs9h1TgXstvtSklJKXGM0mru\n",
              "GOfC644fP67s7Gw1bdq01BqqFu6ts8pz/1R0HO6tPx7ur7PK+t0/fvy4+vXrp8GDB2vSpEkVGsfV\n",
              "+8vf31+ZmZkqKCiQdHYz19TUVNnt9lJrqFq4t85y9ff+yiuv1O23364lS5aUexz+dv2xcG+dVdrv\n",
              "/ZdffqnXX39dDodD3bp1U05OjhwOhw4ePFiucfi7BZTB3c/mvfLKK6Z9+/bmyJEjF9Xuu+++Ypu9\n",
              "NW3a1LnZ2zkXPoN7+vTpYs+t/vDDD6Z+/foljl/ROdw1TmJi4kUb073++utl1i40bdq0izZ2PHjw\n",
              "YJm18xUWFpqgoKBiGzsOHz68zBqqDu6t/+fK/ePKHk3cWziH++v/lXZ/HT9+3HTp0sW88MILpfZh\n",
              "jDGLFi26aOPU//3vf2XWLtSjR49iG6ded911LtVQNXBv/b/S7q09e/Y4x8vPzze33XabefbZZ0sc\n",
              "h79dMIZ763yu/rsqKSmp1D2a+LsFVJxbg6a0tDQjyQQFBZnw8HATHh5uOnbs6KxnZWWZ3r17m5CQ\n",
              "ENOqVSvnHzBjjPnb3/5mmjVrZmrWrGkaNGhgmjVrZg4cOGByc3NNly5dTOvWrU14eLjp0qWL+eyz\n",
              "zy7ZQ0XmcOc4b7/9tgkKCjJBQUHmgQceKPYfzkvVMjIyiv0D+cSJE+a2224zQUFBpnnz5mbp0qUu\n",
              "1RYuXGiee+455/G3335r2rZta5o3b2569OhhUlNTXaqh8nFvXexS909ubq5p1qyZufrqq02NGjVM\n",
              "s2bNzJQpU4wx3FsoGffXxS51f02fPt14eXk5f07h4eFm+vTpzveFh4ebjIwMY4wxBQUFZsKECSYw\n",
              "MNAEBQWZBQsWOK8rrbZ69Wrz4IMPOo/j4uJMp06dTPPmzc11111X7P+wL62Gyse9dbFL3VtRUVGm\n",
              "devW5tprrzWtWrUyjz76qHPzbf524ULcWxcr7d9c55QUNPF3C7CGzRhjKnVJFQAAAAAAAH4XftNP\n",
              "nQMAAAAAAMDvF0ETAAAAAAAALEHQBAAAAAAAAEsQNAEAUAaHw6HQ0FCFh4crJCREgwcP1rfffuvS\n",
              "e1etWqXvv//e8p6ioqIUFhamiIgIHT582Hn+r3/9q/Pjoq+88koFBgY6j+Pj4y3v43IcO3ZMs2bN\n",
              "KnZuzJgx2rJlSyV1BAAAgMvFZuAAAJTB4XBo1apVioiIkCStWLFCDzzwgD799FNdf/31pb539OjR\n",
              "ioiI0BNPPGFpTy1bttT777+vzp07X/KaG2+8UU888YSGDBlS7HxRUZEkycOjcv//TcnJyYqIiNCx\n",
              "Y8cqtQ8AAABYhxVNAACU07Bhw/Twww9r7ty5kqTPPvtMnTt3Vrt27dS6dWu99957kqT169crOjpa\n",
              "c+bMUUREhN59911J0ocffqjrr79e7du3V/fu3bVjx44S59m+fbu6dOmia6+9Vh07dtQ333wjSRox\n",
              "YoQSExM1evRojRgxwqWen3/+eQ0fPlx9+/ZVmzZtlJmZqaeeekodOnRQRESEunfvXmzFk81m04wZ\n",
              "M9SxY0cFBgZq0aJFks6GVH/605/UsmVLhYeH67rrrlNeXp4KCgrUt29fRUZGqnXr1rrzzjuVm5vr\n",
              "HG/RokWKiIhQeHi4IiMjlZycrIcffljHjx9XRESEIiMjJZ0Nx1atWiVJOnDggIYNG6a2bduqTZs2\n",
              "ioqKco7ncDj017/+VZ07d1ZgYKCmT5/u0s8BAAAAbmYAAECpAgICzE8//VTs3IoVK0zLli2NMcYc\n",
              "OXLEFBQUGGOMOXz4sLHb7SYtLc0YY8x9991n5s+f73zf119/bfr372/y8vKMMcZ89dVXplWrVhfN\n",
              "mZ+fb/z9/c3GjRuNMcZs3brVNG7c2Bw/fvySPV2oR48eZuXKlcYYY6ZNm2aaNGlisrKynPUDBw44\n",
              "v/74449N3759nceSzNy5c40xxvz666/myiuvNGfOnDE//vijCQsLM4WFhcYYY44dO2YKCwtNUVGR\n",
              "OXTokDHGmKKiIvPwww+bmTNnGmOM2bJli3E4HGbfvn3GGGNyc3NNbm6uSUpKMvXq1btkz7fddpuZ\n",
              "MmWKMcaY/fv3Gz8/P/Pdd985v/9HH33UGGPMwYMHTd26dU16enqpPw8AAAC4n1dlB10AAFRH5rwn\n",
              "zw8fPqwHH3xQu3fvlpeXlw4fPqydO3fKz8/vovetXr1aO3bsKPbI3ZEjR3Tq1CldccUVznPx8fHy\n",
              "8PBQ3759JUndunVT48aNFRsbq27dulWo5wEDBqhx48bO402bNun111/X8ePHVVRUpCNHjhS7/q67\n",
              "7pIkhYWFycvLS1lZWQoKClJBQYEeeOAB9ezZU7fccos8PDxUVFSk+fPna926dSooKFB2dra6dOki\n",
              "SVq3bp3uueceNWnSRJJUu3Ztl/rdvHmz/vvf/0qSGjVqpGHDhmnz5s3q1KmTJOnOO++UJF199dUK\n",
              "CgpSUlKSmjVrVqGfDQAAAKzBo3MAAFTAtm3b1KZNG0nSww8/rG7duunnn39WbGysWrRooby8vBLf\n",
              "Z4zRfffdp9jYWOcrMzOzWMh0KTab7bJ6vvLKK51fp6am6k9/+pMWL16snTt36l//+tdFPdeqVcv5\n",
              "taenpwoKClSvXj3t3LlTd955p+Li4nTttdcqISFBH330kT7//HN9+eWX+vnnn/XUU09d8mdQURd+\n",
              "/yX1BwAAgMpF0AQAQDmtXr1aCxcu1JNPPilJOnr0qAICAmSz2fTVV18V23Opbt26ys7Odh4PGjRI\n",
              "ixcvVmpqqqSzex5t3779ojlCQ0NVVFSkTZs2SZK+/fZbZWVlOTckv1zZ2dmqUaOGmjRpImOM3njj\n",
              "DZfed/DgQeXm5qpPnz6aMWOGHA6HfvnlFx09elRXX3216tatq+PHj+sf//iH8z0DBw7U4sWLlZmZ\n",
              "KUk6efKkTp48qbp16+rUqVM6ffp0iXPdfPPNeuedd5zzrlixQr179768bxwAAABuxaNzAAC4YNSo\n",
              "UapVq5Zyc3PVqlUrrV+/3vn426xZszRhwgT97W9/U0RERLHH4u655x6NHj1aq1at0iOPPKIxY8Zo\n",
              "9uzZGjp0qAoKCnT69Gndcsstzs2wz6lZs6ZWrFihxx57TE8++aRq1aqlZcuWFVuVdDnatm2r22+/\n",
              "Xa1bt1aDBg0u+mS6S0lLS9PYsWN15swZFRYWqmvXrurfv79Onjyp1atXKzQ0VA0bNtQNN9yglJQU\n",
              "SVL37t01bdo09e3bVzabTTVr1tSyZcsUEBCge++9V9dee62uvPLKiwK31157TePHj1fbtm1ljNGf\n",
              "//znMj/lDwAAAJXLZs7fZAIAAAAAAACoIB6dAwAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJ\n",
              "AAAAAAAAliBoAgAAAAAAgCUImgAAAAAAAGAJgiYAAAAAAABYgqAJAAAAAAAAliBoAgAAAAAAgCUI\n",
              "mgAAAAAAAGCJ/wM6SQc7ctHFvQAAAABJRU5ErkJggg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-c98b054d-5b2c-42a1-8983-efc681cec8b2\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-c98b054d-5b2c-42a1-8983-efc681cec8b2\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x78367ba9f0d0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Values</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_13['Amount (USD)'].plot(kind='line', figsize=(8, 4), title='Amount (USD)')\n",
              "plt.gca().spines[['top', 'right']].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-79232c39-892a-4722-8010-d987a37c786f\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApAAAAFuCAYAAAA2xSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAA9n0lEQVR4nO3deXxU9b3/8fdkkkwWkglJCFtIQjZUZIewb4ILuICidS0gIggq\n",
              "oLW19Fdve739yb2/ei2IguICotaqFdGqaEVBQMCwiuACCQTCToCsJJNkcn5/TBISEyQTksz2ej4e\n",
              "8yCZOTPz4TzG4e3nfM/nmAzDMAQAAAA0kJ+rCwAAAIBnIUACAADAKQRIAAAAOIUACQAAAKcQIAEA\n",
              "AOAUAiQAAACcQoAEAACAUwiQAAAAcAoBEgAAAE4hQAJAC8nMzFTnzp1VWFjokvcvKipSfHy89u7d\n",
              "65L3B+A9CJAA3MbSpUtlMpn0+9//3tWlNNjatWtlMplUXl5+0W1/+9vfavbs2WrVqpUk6c9//rOG\n",
              "DBlSZ7tly5YpNja2+vcDBw7ozjvvVIcOHdSqVSt16NBBY8eO1bFjx2rV0KpVK4WFhal169bq3bu3\n",
              "/vCHPygnJ6f6dUJDQ/Xoo4/qN7/5zaX+tQH4OAIkALexaNEiRUVF6dVXX5XNZnN1OU0qMzNTn376\n",
              "qSZNmuT0c8eOHauwsDDt3r1bhYWF2rFjh26//XaZTKZa2+Xm5qqgoECnTp3S4sWLtX37dvXo0UMH\n",
              "Dx6s3mbixIlavXo1XUgAl4QACcAtbNmyRVu3btUbb7yhvLw8vfvuu7Uenzx5sm6//XbNmDFDUVFR\n",
              "io6O1vz585Wdna1rr71WYWFhuuKKK7Rp06bq59jtdv31r39VamqqrFar+vbtq1WrVlU//vNOn1S3\n",
              "KzhixAjNnj1bd911l6xWqzp16qTFixdLkg4dOqQxY8ZIkiIiItSqVSs99dRT9f793n//ffXp00et\n",
              "W7d2ar+cPn1aP/74ox544AFFRkZKktq2batJkyapXbt29T7H399f/fv31/vvv6/Q0FA98cQT1Y+1\n",
              "bt1a/fr10/vvv+9UHQBQEwESgFtYtGiRevbsqeuuu04333yzFi1aVGeblStXatSoUTp58qRefvll\n",
              "Pfroo5o4caKefvpp5ebm6uqrr9bkyZOrt58/f74WLFigf/zjHzp9+rQee+wxjRs3Ttu3b3eqtmXL\n",
              "lmnq1Kk6e/as5s+fr4ceekgZGRmKi4urDqS5ubkqLCzUH/7wh3pfY+vWrbryyiudel9JioqKUrdu\n",
              "3TR9+nQtXbpUu3btUkVFRYOeGxwcrAkTJujzzz+vdX/37t21ZcsWp2sBgCoESAAud/bsWb399tua\n",
              "Nm2aJGnatGnatGmTvv3221rbDRkyRLfeeqvMZrPGjx8vq9Wqa665Rt26dZPZbNbEiRO1d+9e5eXl\n",
              "SZKWLFmi3/72t+rdu7f8/f11xx13aMyYMVqyZIlT9U2YMEFXXXWV/Pz8NGHCBEVGRmrbtm1OvcaZ\n",
              "M2dktVqdek6VNWvWaMyYMVq8eLHS0tIUHR2txx57rEGH+ePi4nT69Ola91mtVp05c6ZRtQCA5IIA\n",
              "OWvWLCUkJMhkMmnnzp0Nes7Zs2d19913KzU1VV27dvWoBfYALq7q5Jm7775bkjRy5EglJyfX6UK2\n",
              "b9++1u+hoaG17gsNDZUkFRQUSJKys7OVlJRU6znJyck6dOiQU/V16NChzvtWvUdDRUZGVgfbKgEB\n",
              "ASorK6uzbVlZmQICAqp/j4qK0pNPPqn09HTl5eXp1Vdf1UsvvaR58+Zd9H0PHTqkqKioWvfl5eVV\n",
              "Hw4HgMZo8QB56623asOGDYqPj2/wc6ZMmaJevXpp79692rNnj+bMmdN8BQJoUYZh6IUXXlBpaalS\n",
              "U1PVrl07tW/fXocPH9abb76p/Pz8Rr92p06dlJmZWeu+zMxMxcXFSZLCwsJUVFRU6/GjR4869R5+\n",
              "fg37Gu3Tp4/27NlT677ExEQdOHBAdru91v379u2rE3yrWCwWjR8/XqNHj77oofiSkhKtWLFCV199\n",
              "da37v/vuO/Xt27dBdQNAfVo8QA4bNqzOonXJsYD+qquuUt++fdWrV6/qBfQZGRnaunWrHn300ept\n",
              "L7RwHIDn+fzzz7Vv3z79+9//1s6dO6tvu3btkiS99tprjX7tqVOn6umnn9bOnTtVXl6ud955R598\n",
              "8ommTp0qSerVq5cKCgr09ttvq6KiQmvXrq1z8s7FVH0f/fTTT7+43fjx47V161bl5uZW3zdmzBj5\n",
              "+/tr7ty5ys/Pl91u15o1a/Tyyy9rypQpkhxHYH7/+99r165dstlsstvt+uKLL7RmzRoNGzas3vcq\n",
              "Ly/Xli1bNGHCBOXn5+vJJ5+sfiw3N1fp6ekaP368U39PAKjJ39UFSI4vtGnTpumTTz5R+/btlZOT\n",
              "o969e2vQoEH6/vvvFRsbqxkzZmjr1q2KiorS//zP/6hXr16uLhtAE1i8eLFGjx6tkSNH1rq/Xbt2\n",
              "mjp1qhYvXqyHH364Ua/96KOPym6369Zbb9XJkyeVkpKiFStWVHffEhMT9dxzz+mxxx7T/fffr+uv\n",
              "v1733nuv0tPTG/weqampevjhhzVy5EiVlpbq97//fb3LbFJSUnTNNddo2bJl1UdRIiIitHr1as2d\n",
              "O1ddunRRcXGxEhIS9PTTT+uuu+6SJAUGBionJ0e33Xabjh49KrPZrNjYWD3++ON15jlGRETIZDLJ\n",
              "bDarc+fOuvbaa7V06VLFxMRUb7N8+XKNGjVKl112mbO7EwCqmQzDMFzxxgkJCVq5cqV69uypTz75\n",
              "RHfeeac6d+5c/fiZM2e0bNky5ebm6rbbbtPq1as1cuRIrVq1SlOnTlVWVlatNUIA4O4yMzM1atQo\n",
              "7d69u3qYeEsqKipS165d9dlnn6lLly4t/v4AvIdbdCANw1DXrl21cePGOo9t3bpVHTt2rO5OjBkz\n",
              "RqWlpTp48KCSk5NbulQAaLSkpCRlZWW57P1DQ0Nd+v4AvIdbjPEZNGiQDhw4oNWrV1fft3PnTpWW\n",
              "lqpPnz4KDw+vXg+Vnp4uwzDUqVMnV5ULAADg01r8EPb06dP18ccf6/jx44qKilJYWJgyMjK0fft2\n",
              "PfbYYzp9+rTKysoUFxenlStXKigoSNu2bdPMmTNVXFwsi8Wip59+WsOHD2/JsgEAAFDJZWsgAQAA\n",
              "4Jnc4hA2AAAAPAcBEgAAAE4hQAIAAMApLRogFyxY0JJvBwAAgGbQogHy4MGDLfl2AAAAaAYcwgYA\n",
              "AIBTCJAAAABwCgESAAAATiFAAgAAwCkESAAAADiFAAkAAACnOB0gExIS1KVLF/Xs2VM9e/bU22+/\n",
              "3Rx1AQAAwE35N+ZJb7/9tnr27NnEpQAAAMATcAgbAAAATmlUB3LixIkyDENpaWn67//+b7Vp06bO\n",
              "NjabTTabrdZ9dru9cVUCAADAbTjdgVy3bp127dql7du3Kzo6WpMmTap3u3nz5slqtda6paenX3LB\n",
              "AAAAcC2TYRhGY5987NgxpaamqqCgoM5j9XUgn3jiCS1YsKCxbwc0m5xCm579Yp86RATrgeFJri4H\n",
              "AAC35tQh7KKiIpWVlSkiIkKS9NZbb6lXr171bmuxWGSxWGrdZzabG1cl0Iz+vee45q74TqeLSiVJ\n",
              "aZ0j1TuutYurAgDAfTkVIE+cOKEJEybIbrfLMAwlJiZq+fLlzVUb0KwKbeV68l979M7Ww5KkQH8/\n",
              "lZZXaP7qfVo+Jc3F1QEA4L6cCpCJiYnasWNHc9UCtJj0A2f0m3d3KvtMsUwmadrQRN3WN1bXzV+v\n",
              "dXtPadvBs+oTTxcSAID6MMYHPsVWbte8VT/o9iWblH2mWB0jgvWP+wdo7tjLlRwTpgm9YyVJ81fv\n",
              "dXGlAAC4LwIkfMaPx/M17rmv9eJX+2UY0m19YvXpnKHqnxhVvc1DVyXL38+k9ftytO3gGRdWCwCA\n",
              "+yJAwuvZKwy9+FWmblr4tX48XqDI0EC9+Os++uttPRQWFFBr206RIbqtr6ML+bfP97miXAAA3B4B\n",
              "El4t+8w53blks+at+lGl9gqNvjxGn80Zpmu7trvgc2aOcHQhN2TkaEsWXUgAAH6OAAmvZBiG3tma\n",
              "rTEL1is964xCA83671u66aWJfdUmzPKLz3V0ITtJkv72OWshAQD4OQIkvE5OoU3TXt+m3/1zlwpt\n",
              "5eob31qrZg/THWlxMplMDXqNh65KVoDZpI2Zp/XN/tPNXDEAAJ6FAAmvsvr7E7pu/jp9/v0JBZhN\n",
              "+t11XfT29IGKiwpx6nU6RgTrV5VdyPmrWQsJAEBNBEh4hUJbuX7/3i5NXb5VOYWl6tI2TCsfHKyZ\n",
              "I5Jl9mtY1/HnHhyZrECznzbtP63NdCEBAKhGgITH25J1RmMWrNM/tmTLZJLuH9pZHzw0WF07WC/p\n",
              "dTtEBOv2fqyFBADg5wiQ8Fi2crv+e9WP+tWL54eCv3X/AP2f669QUEDTXHd95sgkBZr99M2BM9qY\n",
              "mdMkrwkAgKcjQMIj/XS8QOOf36gXvsqUYUi3Vg4FH1BjKHhTaG8N1h1plWshP98nwzCa9PUBAPBE\n",
              "BEh4FHuFoSXrMnXjwg364Vi+IkMD9cI9ffR0PUPBm8rMEckK9PdTetYZbcxkLSQAAARIeIzDZ8/p\n",
              "rpc266lPHEPBR10Wo0/nDNV1V154KHhTaGcN0l1pcZIc18imCwkA8HUESLg9wzD07tZsXTd/vb45\n",
              "cEYhgWbNu6WbXp7UVzFhQS1Sw4wRSbL4+2lL1ll9nUEXEgDg2wiQcGunC2164I1t+m3lUPA+8a21\n",
              "avZQ3enEUPCm0DY8SHf1d3Qh/0YXEgDg4wiQcFtf/HBC185fp8/2OIaC//baLnpn+kDFR4W6pJ4Z\n",
              "wx1dyG0Hz2r9Ps7IBgD4LgIk3E7VUPD7XnMMBU9t20orHxysB0c2fih4U4gJD9Ld/eMl0YUEAPg2\n",
              "AiTcytasMxq7YH31UPCpQzrrw4eGXPJQ8KbywIhEBQX4acehXH2195SrywEAwCUIkHALpeUV+n+f\n",
              "OoaCHzpzTh0jgvX3qQP0xxuabih4U4gJC9I9lV3I+auZCwkA8E0ESLicYyj411q0NlMVhnRL745a\n",
              "NWeoBiY17VDwpjJ9eJKCAvy0MztXa+lCAgB8EAESLlNRYejl9ft143Mb9P2xfLUOCdDiu3vrmV/1\n",
              "VHgzDQVvCm3CLJo4MEGSNP9z1kICAHwPARIucfjsOd318mb95eMfVFpeoZFd2uizR4ZpTLf2ri6t\n",
              "QaYNS1RwgFnfHs7Tmp9OurocAABaFAESLcowDL237bDGzF+vzfsdQ8GfurmbXp3cr8WGgjeF6FYW\n",
              "TRzIWkgAgG8iQKLFnCkq1Yw3tus3736rAlu5esdF6JNZQ3VX/5YdCt5Upg1LVEigWbsO5+mLH+hC\n",
              "AgB8BwESLeLLH0/omr+t06d7jsvf7/xQ8IRo1wwFbwpRrWqshfyCtZAAAN9BgESzKrKVa+6K7zRl\n",
              "2VblFNqUEnN+KLi/2fM/ftOGJSo00KzdR/K1mi4kAMBHeP6/4HBb2w6e0dhn1+ut9EOSpPuGdNa/\n",
              "Hh6iKzu6x1DwphAZGqhJgxIkSfO5Og0AwEcQINHkSssr9NfPftRtL2zSwdPn1MEapL9P7a8n3Gwo\n",
              "eFO5f6ijC7nnaL7+/f0JV5cDAECzI0CiSe09UaCbF32t59dUDgXv1VGr5gzToORoV5fWbFqHBmry\n",
              "4ARJjjOyKyroQgIAvBsBEk2iaij4DQs3aM/RfEWEBGjR3b31zO09ZQ1236HgTeX+oYlqZfHXD8fy\n",
              "9e/vj7u6HAAAmhUBEpfsSG6x7n75m+qh4CO6tNG/5wzTWA8ZCt4UIkICdS9dSACAjyBAotEMw9CK\n",
              "7Yd13d/WadP+0woOMOsv46/U0sn9FBPuOUPBm8rUIYkKs/jrx+MF+mwPXUgAgPciQKJRzhSVauab\n",
              "2/XoO46h4L3iIvTJ7KG6Z0C8Rw4FbwrWkADdO6SzJLqQAADvRoCE09b8dFLXzl+nVbsdQ8EfuyZV\n",
              "704fqM4ePBS8qdw3uLPCgvz104kCrdpNFxIA4J0IkGiwIlu5/vD+d7p36RadKrApOaaV3p85WA9d\n",
              "leIVQ8GbgjUkQFMGO7qQC77YSxcSAOCV+FcfDbLt4FmNfXa9/v6NYyj4lMGd9dHDQ9Qt1nuGgjeV\n",
              "KUMcXci9Jwr18XfHXF0OAABNjgCJX1RaXqGnP/tJt72wUQdPn1N7a5DenNpf/3Gjdw4FbwrW4ABN\n",
              "HZIoSXr2i32y04UEAHgZAiQuaN+JAt2y+Gs9tyZDFYZ0c6+O+nTOMA324qHgTeXeIQkKD/LXvpN0\n",
              "IQEA3ocAiToqKgy9suGArl+4QbuPOIaCP39Xb/3NR4aCN4XwoADdP9TRhVywei9dSACAVyFAopYj\n",
              "ucW655Vv9F8ffa/S8goNT22jz+YM0/XdfWcoeFOZPDhB1uAAZZ4q0ke7jrq6HAAAmgwBEpIcQ8Hf\n",
              "33FY181fp42Z54eCL7u3n9r64FDwphAWFKD7h1adkc1aSACA9yBAQmeLSvXQ33fokbe/VUFJuXp2\n",
              "Yih4U5k0KEERIQHaf6pIH357xNXlAADQJAiQPq5qKPjH3x2Tv59Jj16dqn8+wFDwphJWYy3kwi8y\n",
              "VG6vcHFFAABcOgKkjzpXWq7/UzkU/GSBTUltQrVi5iDNGsVQ8KY2aVCCWocEaH9OkT78lrWQAADP\n",
              "R1LwQdsPndX1z27Qm5VDwScPStDHs4aqe2yEawvzUq0s/po2LEmSYy4kXUgAgKcjQPqQMnuFnvn3\n",
              "T7p18UYdyClSe2uQ3rivv/58U1eGgjeziQPjFRkaqKzT57RyJ11IAIBnI0D6iIyTBbpl0UY9+6Vj\n",
              "KPi4nh306exhGpLCUPCWEGrx17RhlWshv6QLCQDwbARIL1dRYWjp1wd0/bMb9N2RPFmDA/TcXb20\n",
              "4I5esoYwFLwlTRwYr6jQQB08fU4rdnBGNgDAcxEgvdjR3GL9+tVv9J//+l628goNqxwKfkP3Dq4u\n",
              "zSeFBPpr+nBHF/K5LzNURhcSAOChGh0gly5dKpPJpJUrVzZhOWgKhmFo5Y4junb+On2dcVpBAX76\n",
              "r3Fd9dq9/dTOylBwV7pnQLyiWwXq0Jlzen87XUgAgGdqVIDMysrSSy+9pAEDBjR1PbhEuedK9dBb\n",
              "OzTn7Z0qKClXj04R+njWUP16YAJDwd1ASKC/HhjuOCN74Zp9dCEBAB7J6QBZUVGhqVOnauHChbJY\n",
              "LM1RExpp7U8ndc3f1unjXcdk9jPpkdGpeu+BgUpq08rVpaGGu/vHK7qVRdlnivXetsOuLgcAAKc5\n",
              "HSCfeeYZDR48WH369PnF7Ww2m/Lz82vd7HZ7owvFhZ0rLdcTK3drcuVQ8MQ2oVoxY5Bmj2YouDsK\n",
              "DjTrgeFVZ2RnqLScLiQAwLM4lS52796t9957T3/84x8vuu28efNktVpr3dLT0xtdKOq3o3Io+Oub\n",
              "D0qqHAr+8FD16BTh2sLwi+4ZEK82YRYdyS3WP+lCAgA8jFMBcv369crKylJKSooSEhK0efNmTZs2\n",
              "TYsXL66z7dy5c5WXl1frlpaW1mSF+7rqoeAvbNKBnCK1Cw/S6/el6c83dVVwIEPB3V1QgFkzKtdC\n",
              "Pr+GLiQAwLOYDMMwGvvkESNGaM6cORo/fnyDtn/00Uf1zDPPNPbtUCnjZKEeeXunvjuSJ0m6qUcH\n",
              "/de4K5nr6GFKyuwa9v/W6GSBTf/35it1d/94V5cEAECDsEDOg5wfCr5e3x3JU3iQv569s5eevZOh\n",
              "4J4oKMCsmSMqu5BfZshWzhphAIBn8L+UJ69du7aJysDFHMsr1m/f3aUNGTmSpKEp0frrrT2Y6+jh\n",
              "7kiL0+KvMnU0r0TvbD2sXw+gCwkAcH90ID3ABzuP6Nq/rdOGjBwFBfjpyXFdtXxKGuHRCzi6kMmS\n",
              "pEVr6EICADwDAdKN5Z4r1UN/367Z/9ip/JJy9Yi16uNZQzWRoeBe5fZ+ndQuPEjH8kr09pZsV5cD\n",
              "AMBFESDd1Fd7T+na+ev0UeVQ8DmjU/TPGYMYCu6FggLMenCkYy3kojWZKimjCwkAcG8ESDdTXGrX\n",
              "f3ywW5NeTdeJfJsSox1DweeMTlUAQ8G91q/6dVIHa5CO59OFBAC4PxKJG9mZnavrn12v5ZscQ8En\n",
              "DYzXx7MYCu4LLP5mzRxZuRZybQZdSACAWyNAuoEye4X+9vleTVi8UftzitQ23KLlU9L0n+OuZCi4\n",
              "D7mtb6w6WIN0It+mt9IPubocAAAuiADpYpmnCjVh8UYt+GKf7BWGbuzRQZ/NGaZhqW1cXRpamMXf\n",
              "rAevqupCshYSAOC+CJAuUlFh6LWNWbr+2fXaddgxFHzBHT218M5eiggJdHV5cJHb+nRSx4hgnSqw\n",
              "6c1v6EICANwTAdIFjueVaNLSdP3pwz0qKavQ0JRoffbIMI3r2dHVpcHFAv399FBlF/KFr+hCAgDc\n",
              "EwGyhX347VFdO3+d1u/LkcXfT3++8Qq9dm+a2luDXV0a3MStfWIV29rRhXxj80FXlwMAQB0EyBaS\n",
              "e65UD7+1Q7Pe2qG84jJ16+gYCj55cGf5+TEUHOcFmP30cHUXcr+KS+lCAgDcCwGyBazf5xgK/q9v\n",
              "j8rsZ9KsUSlaMXOQkmMYCo763dI7Vp0ig5VTSBcSAOB+CJDNqLjUrj99sFu/fsUxFLxzdKjemzFI\n",
              "j17NUHD8sgCznx4emSLJsRbyXGm5iysCAOA8Ukwz+TY7V9cvXK/XKoeC/3pAvD6eNUQ9GQqOBrq5\n",
              "d0fFRYbodFGpXt9EFxIA4D4IkE2szF6h+av36pbFG7X/VJFiwix6bUqa/mv8lQoJ9Hd1efAgNddC\n",
              "vrhuP11IAIDbIEA2of2nCnXrC5s0f7VjKPj13dvrsznDNJyh4Gikm3t1VEJUiM4UlVZf4hIAAFcj\n",
              "QDYBwzC0fFOWxj67Xt9m5yqscij4c3f2UutQhoKj8fzNfnr4KsdayCXr9qvIRhcSAOB6BMhLdDyv\n",
              "RBNfTdd/fOAYCj44OUqfzXEMBTeZGM+DSzeuZwd1jg7VmaJSvbYpy9XlAABAgLwU//rZUPA/3XiF\n",
              "Xp/SXx0iGAqOpuNfYy3kknX7VUgXEgDgYgTIRsg7V6ZZb+3Qw7WGgg/RvQwFRzO5qUcHJUaHKvdc\n",
              "mV7bmOXqcgAAPo4A6aQN+3J07fx1+rBqKPhVyZVDwcNcXRq8mL/ZT7NGOdZCvrR+vwpKylxcEQDA\n",
              "lxEgG6i41K4/f7hH97zyjY7nlyghKkTvPjBQj17ThaHgaBE39uigpDZ0IQEArkfyaYBdh3N1w8L1\n",
              "Wlb5j/Y9A+L0yeyh6h3X2rWFwadUXQZTkl5af0D5dCEBAC5CgPwF5fYKPfvFPt2yaKMyTxWpTZhF\n",
              "S+/tp7+M78ZQcLjEDd07KDmmlfKKy7Ts6yxXlwMA8FEEyAuoGgr+zOd7VV5h6Ppu7fXvOcM0skuM\n",
              "q0uDD6vZhXx5/X7lFdOFBAC0PALkzxiGodc3H9TYZ9drZ+VQ8Pm399RzdzEUHO7h+m7tlRLTSvkl\n",
              "5Vr69QFXlwMA8EEEyBpO5Jdo8tItemLlbpWUVWhQkmMo+PheDAWH+zD7mTR7tKML+cqGA3QhAQAt\n",
              "jgBZ6eNdx3Tt/HX6au8pWfz99B83XKE37mMoONzT2Cvbq0vbMBWUlOvVDXQhAQAty+cDZF5xmeb8\n",
              "Y4ce/Pt25Z4r05Udw/XRw0M0ZQhDweG+/Gp0IV/dcEB55+hCAgBajk8HyK8zcnTd/HVaufOo/EzS\n",
              "w1cla8WMwUppy1BwuL/rurbTZe3CVGAr1ysb9ru6HACAD/HJAFlSZtd//muP7n75Gx3LqxoKPki/\n",
              "uaaLAv19cpfAA/n5mTS78ozsV7/OUu65UhdXBADwFT6Xlr47nKcbFm7Q0soZenf3j9PHs4aqTzxD\n",
              "weF5rq3sQhbayvXyetZCAgBahs8EyHJ7hRZ+sU83L/paGScLHUPBJ/fT/725m0ItDAWHZ/LzM2nO\n",
              "6FRJ0rKNWTpbRBcSAND8fCJAHsgp0m0vbtL/Vg4FH3NlO302Z5hGXsZQcHi+a7u21RXtwx1dSNZC\n",
              "AgBagFcHSMMw9Mbmgxq7YL12HMpVmMVfz/yqhxbd3VuRDAWHlzCZTJpTeUb2sq+zdIYuJACgmXlt\n",
              "gDyZX6J7l23RH1fuVnGZXQMTo/TpI8N0S+9YhoLD61x9RVt17RCuolK7XlpPFxIA0Ly8MkD+eDxf\n",
              "18xfp7U/nVKgv5/+eP3lenNqf3VkKDi8lKML6VgL+drGLJ0utLm4IgCAN/PKAJnUppU6tQ5R1w6O\n",
              "oeBThyYyFBxeb/TlMerW0apzpXYtoQsJAGhGXhkgA8x+enlSX70/c7BSGQoOH1FzLeTyjQfpQgIA\n",
              "mo1XBkhJahsexFBw+JyrLotR91irisvsWrKOLiQAoHmQsAAvUqsLuemgcuhCAgCaAQES8DIju8So\n",
              "R6cIFZfZ9eJXma4uBwDghQiQgJep2YV8ffNBnSwocXFFAABvQ4AEvNCI1Dbq2SlCJWUVevEr1kIC\n",
              "AJoWARLwQiaTSY9c7ZgL+QZdSABAEyNAAl5qWEq0esdFyFZeoRfW0oUEADQdAiTgpWpenebNbw7q\n",
              "ZD5dSABA0yBAAl5saEq0+sS3lq28QovWckY2AKBpOB0gr7nmGnXv3l09e/bU0KFDtWPHjuaoC0AT\n",
              "MJlMeqSyC/n39EM6nkcXEgBw6ZwOkO+884527dqlnTt36tFHH9XkyZOboSwATWVwcpT6JbRWaXmF\n",
              "Fq/NcHU5AAAv4HSAjIiIqP45Ly9PJpOpKesB0MRqdiHfSs+mCwkAuGT+jXnSxIkTtWbNGknSJ598\n",
              "Uu82NptNNlvty6jZ7fbGvB2ASzQwKUppnSOVfuCMFq3N0JPjrnR1SQAAD9aok2iWL1+u7Oxs/eUv\n",
              "f9Hjjz9e7zbz5s2T1WqtdUtPT7+kYgE0Ts2r0/wjPVtHc4tdXBEAwJNd0lnYkyZN0po1a3T69Ok6\n",
              "j82dO1d5eXm1bmlpaZfydgAuwaCkaPXvHKlSe4UWsRYSAHAJnAqQubm5Onr0aPXvK1euVFRUlCIj\n",
              "I+tsa7FYFB4eXutmNpsvvWIAjVZ1dZq3t2TrCF1IAEAjObUGMi8vT7fddpuKi4vl5+enNm3a6KOP\n",
              "PuJEGsBDDEiM0sDEKG3af1rPr8nQUzd3c3VJAAAP5FSAjI+PZx0j4OEeuTpVm17cpHe3ZmvmiCTF\n",
              "tg5xdUkAAA/DlWgAH5PWOVKDk6NUZjf0/BquTgMAcB4BEvBBVdfIfndrtrLPnHNxNQAAT0OABHxQ\n",
              "v4RIDUmOVnmFoefXcEY2AMA5BEjARz1ytWMu5D+3HaYLCQBwCgES8FF94iM1NMXRhVz45T5XlwMA\n",
              "8CAESMCHVc2FfG/7ER06TRcSANAwBEjAh/WOa63hqW1kpwsJAHACARLwcVXXyF6x44iycopcXA0A\n",
              "wBMQIAEf1yuutUZ0qepCckY2AODiCJAAqudCvr/jsA7QhQQAXAQBEoB6dorQVZfFqMKQFn7BWkgA\n",
              "wC8jQAKQdH4t5MqdR7T/VKGLqwEAuDMCJABJUvfYCI2+vLILyVpIAMAvIEACqDZ7lGMt5Ac7jyiT\n",
              "LiQA4AIIkACqdYu1avTlbVVhSM+yFhIAcAEESAC1VK2F/PDbo8o4WeDiagAA7ogACaCWKztadc0V\n",
              "bWUY0oIvWAsJAKiLAAmgjqq5kB/tOqp9J+hCAgBqI0ACqOOKDuG6rmu7yi4kayEBALURIAHUa3bl\n",
              "WsiPvzumvXQhAQA1ECAB1Ovy9uEac2VlF3I1XUgAwHkESAAXVLML+ePxfBdXAwBwFwRIABd0Wbtw\n",
              "Xd+tvSS6kACA8wiQAH7R7NEpMpmkVbuP64djdCEBAARIABeR2jaMLiQAoBYCJICLmj3K0YX8dM9x\n",
              "7Tma5+pyAAAuRoAEcFEpbcN0Q/cOkuhCAgAIkAAaaPaoZJlM0r+/P6HdR+hCAoAvI0ACaJDkmDDd\n",
              "1MPRhZxPFxIAfBoBEkCDzRqVIj+TtPoHupAA4MsIkAAaLKlNK43r2VGSNH/1XhdXAwBwFQIkAKc8\n",
              "fFVyZRfypHYdznV1OQAAFyBAAnBKYptWGl/dhWQtJAD4IgIkAKc9PCpFZj+TvvzxpHZm57q6HABA\n",
              "CyNAAnBa5+jQGl1I1kICgK8hQAJolFmjkmX2M2ntT6e049BZV5cDAGhBBEgAjRIfFapberEWEgB8\n",
              "EQESQKM9dJWjC/nV3lPadpAuJAD4CgIkgEaLjwrVhN6shQQAX0OABHBJHr4qRf5+Jq3fl6NtB8+4\n",
              "uhwAQAsgQAK4JJ0iQ3Rrn1hJrIUEAF9BgARwyR4cmVzdhdyaRRcSALwdARLAJesUGaLb+naSJP2N\n",
              "tZAA4PUIkACaxIMjkxRgNunrjNNKP0AXEgC8GQESQJOIbV2jC/k5XUgA8GYESABN5sGRyQowm7Rp\n",
              "/2lt3n/a1eUAAJoJARJAk+kYEazb+zm6kMyFBADvRYAE0KQeHJmsQLOfNu8/o02ZdCEBwBsRIAE0\n",
              "qfbWYN2Rdv6MbMMwXFwRAKCpORUgS0pKNH78eKWmpqpHjx66+uqrlZGR0Vy1AfBQM0YkKdDsp/QD\n",
              "dCEBwBs53YGcNm2afvrpJ3377bcaN26cpk6d2hx1AfBg7a3BupMuJAB4LacCZFBQkMaOHSuTySRJ\n",
              "GjBggLKyspqjLgAebubIZAX6+2lL1ll9nUEXEgC8ySWtgVywYIHGjRtX72M2m035+fm1bna7/VLe\n",
              "DoAHaRsepLvS4iQ5zsimCwkA3qPRAfKpp55SRkaG5s2bV+/j8+bNk9VqrXVLT09vdKEAPM/MEUmy\n",
              "+Ptp68Gz2pCR4+pyAABNpFEB8umnn9aKFSu0atUqhYSE1LvN3LlzlZeXV+uWlpZ2ScUC8Cwx4UG6\n",
              "u3+8JMfVaehCAoB3cDpAPvPMM3rrrbf0+eefKyIi4oLbWSwWhYeH17qZzeZLqRWAB3pgeKIs/n7a\n",
              "fihX6/bRhQQAb+BUgDx8+LB+85vfKDc3VyNHjlTPnj3Vv3//5qoNgBeICQ/SPQPoQgKAN/F3ZuPY\n",
              "2Fi+/AE47YHhSXrzm4PamZ2rtXtPaWSXGFeXBAC4BFyJBkCzaxNm0a8ru5DzV+/jf0QBwMMRIAG0\n",
              "iOnDkxQcYNa32bla+9MpV5cDALgEBEgALSK6lUUTB1auhWQuJAB4NAIkgBZz/7BEBQeYtetwnr78\n",
              "8aSrywEANBIBEkCLiW5l0cRBrIUEAE9HgATQoqYPS1JIoFnfHcnT6h/oQgKAJyJAAmhRkaGBmjQo\n",
              "QRLXyAYAT0WABNDipg1NVGigWXuO5uvz70+4uhwAgJMIkABaXOvQQE0enCCJtZAA4IkIkABcYuqQ\n",
              "RLWy+Ov7Y/n6bA9dSADwJARIAC7ROjRQk2ushayooAsJAJ6CAAnAZaYO7awwi79+PF6gz/Ycd3U5\n",
              "AIAGIkACcJmIkEDdW7kWcsEX++hCAoCHIEACcKn7hiQqLMjRhfyULiQAeAQCJACXsoYEaMrgzpKk\n",
              "BavpQgKAJyBAAnC5KUM6KyzIXz+dKNAnu4+5uhwAwEUQIAG4nDU4QPcNOd+FtNOFBAC3RoAE4Bam\n",
              "DOms8CB/7TtZqI+/owsJAO6MAAnALYQHBWjq0ERJ0rNf0IUEAHdGgATgNu4dnCBrcIAyThbqo11H\n",
              "XV0OAOACCJAA3EZYUIDuH+pYC0kXEgDcFwESgFuZNChBESEByjxVpH99SxcSANwRARKAW3F0Ic+v\n",
              "hSy3V7i4IgDAzxEgAbidSYMS1DokQPtzivQhXUgAcDsESABup5XFX/cPc3QhF36ZQRcSANwMARKA\n",
              "W5o0MEGRoYE6kFOkD3bShQQAd0KABOCWQi3+mlbdhWQtJAC4EwIkALf16wHxigwNVNbpc3p/xxFX\n",
              "lwMAqESABOC2Qi3+ml5jLWQZXUgAcAsESABu7dcD4xXdKlCHzpzT+9vpQgKAOyBAAnBrIYH+mj4s\n",
              "SZK0cM0+upAA4AYIkADc3j0DHF3I7DPFWrH9sKvLAQCfR4AE4PaCA816YHhlF/LLDJWW04UEAFci\n",
              "QALwCHf3j1d0K4sOny3We3QhAcClCJAAPEJwoFkzRji6kM/RhQQAlyJAAvAYd/ePU0yYRUdyi/Xu\n",
              "tmxXlwMAPosACcBjBAWc70I+TxcSAFyGAAnAo9yZ5uhCHs0r0Ttb6UICgCsQIAF4lKAAs2ZWdSHX\n",
              "ZMhWbndxRQDgewiQADzOHWlxahcepGN5JXpnC11IAGhpBEgAHicowKyZI6u6kJkqKaMLCQAtiQAJ\n",
              "wCPd3q+T2luDdDy/RG/ThQSAFkWABOCRLP5mzRyZLElatDaDLiQAtCACJACP9au+sepgDdKJfJv+\n",
              "kX7I1eUAgM8gQALwWLW7kKyFBICWQoAE4NF+1beTOkYE62SBTX//hi4kALQEAiQAjxbo76cHK7uQ\n",
              "i7+iCwkALYEACcDj3donVh0jgnWqwKY3Nh90dTkA4PWcCpCzZs1SQkKCTCaTdu7c2UwlAYBzAv39\n",
              "9PBVji7kC1/tV3EpXUgAaE5OBchbb71VGzZsUHx8fHPVAwCNMqFPrGJbByun0KY3v6ELCQDNyakA\n",
              "OWzYMMXGxjZXLQDQaAHmml3ITJ0rLXdxRQDgvZptDaTNZlN+fn6tm93OYSUAzeeW3rGKiwxRTmEp\n",
              "ayEBoBk1W4CcN2+erFZrrVt6enpzvR0AKMDsp4cqu5AvfrWfLiQANJNmC5Bz585VXl5erVtaWlpz\n",
              "vR0ASJJu6dVR8VEhOl1UquWb6EICQHNotgBpsVgUHh5e62Y2m5vr7QBAkuRv9tPDV6VIkpas268i\n",
              "G11IAGhqTgXI6dOnKzY2VocPH9a1116r5OTk5qoLABptfM8OSogK0Rm6kADQLJwKkC+++KIOHz6s\n",
              "8vJynThxQhkZGc1VFwA0Wu0uZKYK6UICQJPiSjQAvNK4nh3UOTpUZ8+V6bWNWa4uBwC8CgESgFfy\n",
              "N/tp1ijHMpuX1u9XQUmZiysCAO9BgATgtW7q0VGJbUKVSxcSAJoUARKA1zL7mTR7lGMt5EvrD9CF\n",
              "BIAmQoAE4NVu6N5BSW1ClVdcpmVfZ7m6HADwCgRIAF7N7GfSrOou5H7l04UEgEtGgATg9W7o3kHJ\n",
              "Ma2UX1KupRuyXF0OAHg8AiQAr1dzLeTLG/Yrr5guJABcCgIkAJ9wfbf2Sm3bSgUl5Xp1wwFXlwMA\n",
              "Ho0ACcAn+PmZNHtUqiTp1a8P0IUEgEtAgATgM8Zc2U5d2oapoKRcr9CFBIBGI0AC8Bl+fibNHu1Y\n",
              "C7l0wwHlnaMLCQCNQYAE4FOu69pOl7ULU4GtXC9v2O/qcgDAIxEgAfgUPz+T5lR1Ib/OUu65UhdX\n",
              "BACehwAJwOdcc0U7Xd4+XIW2cr20ni4kAPdSZq/Q0dxibT90Vqu+O6ZNmaddXVId/q4uAABaWlUX\n",
              "cvrr27Ts6yxNHZKo1qGBri4LgJczDEP5xeU6UVCi43klOp5fohNVf+ZX/WlTTqFNhnH+eaMvb6uB\n",
              "SVGuK7weBEgAPumaK9rqivbh+v5Yvl5av1+/u+4yV5cEwIOVllfoZEFlEMyz6Xh+iU5WhsLjeecD\n",
              "YklZRYNez9/PpJgwi9pag5Qc06qZq3ceARKATzKZHF3Iaa9v02sbszR1aKIi6UIC+BnDMJR7rqy6\n",
              "S1gzIJ6oDIcnC0qUU9jw9dTW4AC1Cw9SW2uQ2oZZ1M4apLbhQWoXHlT9c1RooPz8TM34N7s0BEgA\n",
              "PuvqK9rqyo7h2n0kX0vW7dfvx9CFBHyJrdyuk/m22l3CvBKdKLDVOrRsK29Y1zDAbFJMmCMEtguv\n",
              "DIVWi9qGnw+IbcODFBxobua/WfMjQALwWSaTSXNGpWrq8q1avilL9w/trKhWFleXBeASGYahM0Wl\n",
              "NbqGtlqHkat+PuvELNjWIQGVgfB8EKwZENuFB6l1iHt3DZsSARKATxt1eYy6x1q163Celqzbr7lj\n",
              "L3d1SQB+QUmZvbpTeLzGIeUTNU5EOZlvU6m9YV3DQH8/x6Hj8CDFhFtqHUauCottwiwKCvD8rmFT\n",
              "IkAC8GlVayGnLNuq5ZsO6v5hiYqmCwm0uIoKQ6eLSmuFw+qTUPLPH1J25jr2UaGB1UHw/BpDi2Kq\n",
              "fg4PUkRIgEwm3+gaNiUCJACfN7JLjHrEWvVtZRfyD3QhgSZ1rrS8Rii01Tkz+WS+TScLSlRmNy7+\n",
              "YpKCAvxqrDEMqrXGsOqQckxYkAL9GXfdXAiQAHyeowuZqnuXbalcC5moNmF0IYGLsVcYOl1YNxAe\n",
              "z3MEwqrQWFBS3qDXM5mk6FaWynB4fm1h25onpYQHKTzYn66hixEgAUDSiC5t1LNThHZm5+rFrzL1\n",
              "xxuucHVJgEsV2sqrQ2H1kOu82oeUTxXaZK9oWNcwJNBcp2vYrjIktq2x1jDATNfQExAgAUDn10JO\n",
              "XrpFb3xzUNOGJyomLMjVZQFNrtxeoVOFtjpnJp/IK6m+QsqJfJsKbQ3rGvqZpDZhlp+dmVz3kHIr\n",
              "C11Db0KABIBKw1PbqFdchHYcytULa/frP26kCwnPYRiGCmzl57uEeTVG2NQYep1TaFMDm4YKs/g7\n",
              "zkyuZ9B11e/RrQLlT9fQ5xAgAaCSyWTSI6NTNfHVdL35zUE9MDxRMeF0IeF6ZfYKnSyoHFVTfRi5\n",
              "5nWUHY+dK7U36PXMVZfJqxMKLbXWHIZaiAmoH58MAKhhaEq0+sS31raDZ7X4q0z96cauri4JXsww\n",
              "DOUXl9cTCGufkHK6yCajgV3D8CD/Ol3CqkDo+NmiqFCLzD4y8BrNgwAJADVUrYX89SvpevObQ3pg\n",
              "eJLa0oVEIxiGofySch05W6yjucU6kuv489jPQmJJmXOXyWtbzyHlqsvntQ23KCSQf9rR/PiUAcDP\n",
              "DEmOVt/41tp68KwWr83Un2+iC4m67BWGThaU6MhZRzisCoiOwFiiI7nFDT4RJSIkoNaYmrbhltqj\n",
              "a6xBivShy+TB/REgAeBnTCaTHrk6VXe//I3+nu7oQraz0oX0NcWl9nqCYbEOV/5+PK9E5Q04GyUy\n",
              "NFAdI4LVISJIHSKC1cEaXCMcOtYhcpk8eBoCJADUY1BSlNISIpWedUaL1mboyXFXurokNCHDcFw2\n",
              "ryoY1gqKuY4O4pmi0ou+jr+fSe2sQeoYEey4tQ5Wh8qfq/4MDiQcwvsQIAGgHiaTSXOuTtFdL32j\n",
              "f6Rna8aIJLW3Bru6LDRQaXmFTuSX6HCN9YdHzhbraN75wGgrv/jaw1YW/xrBMEgdI0LUISJIsZVB\n",
              "MSYsiJNR4JMIkABwAYOSotW/c6S+OXBGi9Zk6r/G04V0F/klZfUcVi7RkbPndDTXMRC7IWctx4RZ\n",
              "1LF1cO0OovV8J9EaHND8fxnAAxEgAeAXzBmdqjtf2qy3tzi6kB0i6EI2t4oKQycLbPWuP6y6ryHX\n",
              "Vg7096sOhjW7h1WBsZ01SBZ/Di8DjUGABIBfMDApSgMSI7V5/xk9vyZD//fmbq4uyeOVlNkveGLK\n",
              "kcqTU8rsF28ftg4JqLXeMPZn6w+jWwVy6TygmRAgAeAiHhmdqtuXbNY7Wx1dyNjWIa4uyW0ZhqGz\n",
              "5xyHlw//bP5h1TrE0w04OcXsZ1K78KALrj9sbw3mKimAC/FfHwBcRP/EKA1KitLGzNN6fk2m5t3i\n",
              "u13IMnuFjueV1A2GNdYfFpdd/HJ6IYHmOmctV/3eMSJYMWEWrq8MuDECJAA0wCNXp2pj5ia9uzVb\n",
              "D4703i5koa287mHlGp3EE/klasDoQ7UJszgOK1evP6wMipUB0RocwOFlwIMRIAGgAfolRGpIcrQ2\n",
              "ZOTo+TUZmndLd1eX5LSKCkM5hbZ6g2FVBzG/ISenmP2qh2L/PBhWnZzCYGzAuxEgAaCB5oxO0YaM\n",
              "HL279bBmjkhWp0j36kKWlNl1rOrwco3h2FXzD4/llqjUfvHZh9bggJ+dmFL7DOboUAuX1AN8HAES\n",
              "ABqob0KkhqZEa/2+HD33ZYb+59aW60IahqG84rL6T0zJdVyPOafQdtHX8TNJ7cKDqruGP19/2CEi\n",
              "WK04OQXARfAtAQBOmDM6Vev35eif2w/rwZHJiotqmi5kub1CJwpsdeYd1vz9XOnFT04JDjD/LBgG\n",
              "1RqO3TY8SAGcnALgEhEgAcAJfeJba1hqG63be0oLv9ynv97Wo0HPK7KVX/DElKO5JTqeXyJ7A85O\n",
              "iW4VWOs6yz9ffxgRwskpAJofARIAnPTI6BSt23tKK3Yc0UNXJSsuMkQ5haW1hmMf+dlh5txzZRd9\n",
              "3QCzSe2t59ccVncPa4RFTk4B4A4IkADgpF5xrTWiSxut/emUbnruaxWX2VVafvGTU8KC/OusN6x5\n",
              "skp0K4vMnJwCwAMQIAGgER4Znap1e08pr9jRWTSZpLZhQfWvP6wMieFBAS6uGgCaBgESABqhR6cI\n",
              "/evhIcovLlds5ckpgf6cnALANzj9bbdv3z4NGjRIqamp6tevn/bs2dMcdQGA2+vawaqBSVHqFBlC\n",
              "eATgU5z+xps+fbqmTZumvXv36vHHH9fkyZOboSwAAAC4K6cC5MmTJ7V161bdc889kqQJEyYoOztb\n",
              "GRkZzVIcAAAA3I9TayCzs7PVvn17+fs7nmYymRQXF6dDhw4pOTm51rY2m002W+2rItjtFx+CCwAA\n",
              "APfWbIt25s2bJ6vVWuuWnp7eXG8HAACAFuJUgOzUqZOOHTum8vJySY5rsx46dEhxcXF1tp07d67y\n",
              "8vJq3dLS0pqmagAAALiMUwEyJiZGvXv31htvvCFJeu+99xQbG1vn8LUkWSwWhYeH17qZzVxBAQAA\n",
              "wNM5PQfyxRdf1OTJk/XUU08pPDxcS5cubY66AAAA4KacDpBdunTRpk2bmqMWAAAAeAAm3wIAAMAp\n",
              "BEgAAAA4hQAJAAAAp5gMwzBa6s1uueUWJSQktMh72e12paenKy0tjbO/a2C/XBj7pn7slwtj39SP\n",
              "/XJh7Jv6sV8uzBX7Jj4+XrNnz/7FbVo0QLak/Px8Wa1W5eXlKTw83NXluA32y4Wxb+rHfrkw9k39\n",
              "2C8Xxr6pH/vlwtx133AIGwAAAE4hQAIAAMApBEgAAAA4xWsDpMVi0Z/+9CdZLBZXl+JW2C8Xxr6p\n",
              "H/vlwtg39WO/XBj7pn7slwtz133jtSfRAAAAoHl4bQcSAAAAzYMACQAAAKcQIAEAAOAUjw+Q+/bt\n",
              "06BBg5Samqp+/fppz5499W73yiuvKCUlRUlJSbr//vtVVlbWwpW2rIbsl7Vr1yo4OFg9e/asvhUX\n",
              "F7ug2pYza9YsJSQkyGQyaefOnRfcztc+L1LD9o0vfmZKSko0fvx4paamqkePHrr66quVkZFR77Yf\n",
              "ffSRLrvsMqWkpOiWW25Rfn5+C1fbchq6X7KysmQ2m2t9ZjIzM11Qccu65ppr1L17d/Xs2VNDhw7V\n",
              "jh076t3O175rGrJffPF7psrSpUtlMpm0cuXKeh93q+8Yw8ONHDnSWLp0qWEYhvHuu+8affv2rbPN\n",
              "/v37jfbt2xvHjh0zKioqjBtvvNF47rnnWrjSltWQ/bJmzRqjR48eLVuYi3311VdGdna2ER8fb+zY\n",
              "saPebXzx82IYDds3vviZKS4uNj7++GOjoqLCMAzDWLhwoTF8+PA62xUUFBgxMTHGDz/8YBiGYTz4\n",
              "4IPGY4891pKltqiG7pcDBw4YVqu1ZYtzA2fPnq3+ecWKFUb37t3rbOOL3zUN2S+++D1jGI7/VgYO\n",
              "HGgMGDDAeP/99+s87m7fMR7dgTx58qS2bt2qe+65R5I0YcIEZWdn1/m/4H/+85+66aab1K5dO5lM\n",
              "Jj3wwAN66623XFFyi2jofvFFw4YNU2xs7C9u42uflyoN2Te+KCgoSGPHjpXJZJIkDRgwQFlZWXW2\n",
              "W7VqlXr16qXLLrtMkjRz5kyv/tw0dL/4qoiIiOqf8/LyqvdTTb74XdOQ/eKLKioqNHXqVC1cuPCC\n",
              "43rc7TvGowNkdna22rdvL39/f0mSyWRSXFycDh06VGu7Q4cOKT4+vvr3hISEOtt4k4buF0nKzMxU\n",
              "79691a9fPy1atKilS3VLvvZ5cZavf2YWLFigcePG1bm/vs/NsWPHVF5e3pLlucyF9oskFRUVqV+/\n",
              "furdu7eefPJJ2e32Fq7ONSZOnKhOnTrpiSee0Ouvv17ncV/9rrnYfpF873vmmWee0eDBg9WnT58L\n",
              "buNu3zH+LnlXuIXevXvr8OHDslqtOnz4sMaOHavo6Gj96le/cnVpcFO+/pl56qmnlJGRoS+++MLV\n",
              "pbiVX9ov7du315EjRxQTE6MzZ87o9ttv1//+7//qd7/7nQsqbVnLly+XJL322mt6/PHH9cknn7i4\n",
              "Ivdwsf3ia98zu3fv1nvvvad169a5uhSneHQHslOnTrXSt2EYOnTokOLi4mptFxcXp4MHD1b/npWV\n",
              "VWcbb9LQ/RIeHi6r1SpJio2N1Z133qn169e3eL3uxtc+L87w5c/M008/rRUrVmjVqlUKCQmp83h9\n",
              "n5uaRwK81cX2i8ViUUxMjCQpMjJSU6ZM8ZnPTJVJkyZpzZo1On36dK37ff275kL7xde+Z9avX6+s\n",
              "rCylpKQoISFBmzdv1rRp07R48eJa27nbd4xHB8iYmBj17t1bb7zxhiTpvffeU2xsrJKTk2ttN2HC\n",
              "BH344Yc6fvy4DMPQCy+8oDvuuMMVJbeIhu6XY8eOqaKiQpJUUFCgjz76SL169Wrxet2Nr31enOGr\n",
              "n5lnnnlGb731lj7//PNaa7hquu6667R9+3b9+OOPkqRFixZ5/eemIfvl5MmT1WcW22w2rVixwus/\n",
              "M7m5uTp69Gj17ytXrlRUVJQiIyNrbedr3zUN3S++9j0zY8YMHTt2TFlZWcrKytKAAQO0ZMkSzZgx\n",
              "o9Z2bvcd47LTd5rIjz/+aAwYMMBISUkx+vTpY+zatcswDMO47777jA8++KB6uyVLlhiJiYlGYmKi\n",
              "MWXKFKO0tNRVJbeIhuyXhQsXGldccYXRvXt344orrjD+9Kc/VZ9R6a2mTZtmdOzY0TCbzUZMTIyR\n",
              "lJRkGAafF8No2L7xxc9Mdna2IclITEw0evToYfTo0cNIS0szDMMwnnjiCWPx4sXV237wwQdGly5d\n",
              "jKSkJGPcuHFGbm6uq8pudg3dL++9957RtWvX6s/MQw89ZJSUlLiy9GaXlZVl9OvXz7jyyiuN7t27\n",
              "G6NGjaqebODL3zUN3S+++D1T0/Dhw6vPwnbn7xiuhQ0AAACnePQhbAAAALQ8AiQAAACcQoAEAACA\n",
              "UwiQAAAAcAoBEgAAAE4hQAIAAMApBEgAAAA4hQAJAAAApxAgAQAA4BQCJAAAAJxCgAQAAIBT/j9o\n",
              "IpXPXMzu+AAAAABJRU5ErkJggg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-79232c39-892a-4722-8010-d987a37c786f\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-79232c39-892a-4722-8010-d987a37c786f\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_14['Money Laundering Risk Score'].plot(kind='line', figsize=(8, 4), title='Money Laundering Risk Score')\n",
              "plt.gca().spines[['top', 'right']].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-ecbef7ca-7db7-43e2-ba0d-5158540ccaa9\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApAAAAFuCAYAAAA2xSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABc9ElEQVR4nO3dd1xV9f8H8NcdcNmXDbJlOJDlwomzMk3TXGWWNsw0V99fllnZ\n",
              "sq9WjlJbambWt+EemVZqDhyJE8TJEAEBZW8ucO/5/XGRJDG5CJw7Xs/H4z4eyT3c8+J0vbx9vz/n\n",
              "HIkgCAKIiIiIiBpIKnYAIiIiIjIsLCCJiIiISCcsIImIiIhIJywgiYiIiEgnLCCJiIiISCcsIImI\n",
              "iIhIJywgiYiIiEgnLCCJiIiISCcsIImIiIhIJywgiYhu07t3b7z77rv39RqpqamwsbFBcnJy04S6\n",
              "DzY2Njhw4MA9t/v222/h5eXV/IGIyCiwgCQyUP369YNEIsHKlSvrfL24uBi2traQSCRITEwUKd29\n",
              "paSk6H3GxvLx8UFJSQn8/f2bbR+3jp+1tTVsbGzg7OyMQYMGIS4urs52JSUl6NevX7NkWLVqFcLC\n",
              "wqBUKmFvb4+wsDCsWLGiWfZFRPqFBSSRAQsODr6jgPz+++/h6+srUiKqrKxs0f3FxsaipKQESUlJ\n",
              "sLe3x/Dhw1tkv+vXr8cbb7yBzz//HPn5+bhx4wbWrFkDT0/PZtmfIAiorq5ultcmIt2xgCQyYMOG\n",
              "DcONGzdw/Pjx2q99+eWXePHFF+/Y9tdff0Xnzp2hVCrRpk0bLF68GBqNpvZ5iUSCFStWoFevXrCx\n",
              "sUFoaCgOHz5c5zW+++47hIeHQ6lUokOHDvj5558BABqNBn5+fli3bl2d7RcvXoyOHTs26mf7/PPP\n",
              "ERISAjs7O7i7u+Ppp59GTk5O7fPPPPMMnnrqqTrf069fP7z11lsN/pmqq6vx2muvwd3dHS4uLpg7\n",
              "d+4dOa5fv44nn3wSnp6ecHV1xbhx45CdnV1nn9OnT8cTTzwBBwcHzJw5847u6q3x8MqVK+Hn5wel\n",
              "UokxY8agqKio9nUSEhLQv39/2NnZoX379li9ejUkEglSUlIadLyUSiWefvpppKSk1DlOEokEe/fu\n",
              "BaAdrQ8ZMgSOjo5QKpUICQlBdHR0va8XExMDb29vfPrpp/U+f/jwYfTs2RNRUVGQSqVQKBTo2rUr\n",
              "Ro4cWbtNeXk53nrrLbRp0wa2trbw9/ev8x759ttva/8fh4SE1Hnu1jFcs2YNwsPDYWVlhZMnT6Ki\n",
              "ogJvvPEGAgIC4ODggD59+uDMmTMNOkZE1HRYQBIZMLlcjkmTJuGrr74CoP2lXlRUhEceeaTOdidO\n",
              "nMBjjz2GOXPmIDc3Fz/99BOWLl2K5cuX19nu66+/xrp161BQUICBAwdi/Pjxtc99++23eOutt7Bm\n",
              "zRrk5+dj5cqVmDx5Mg4fPgypVIrJkydj1apVtdsLgoBVq1bVW8w2hLu7O7Zs2YKCggIcP34cV65c\n",
              "wYwZM3R+nX/7mT7++GNs2LABf/75J9LT0yGXy+sU4yqVCgMHDoSHhweuXLmC5ORkyOVyPPnkk3X2\n",
              "sXbtWkyYMAG5ublYunRpvTmysrJw6dIlXLx4EZcuXcLZs2exZMkSANpCdujQoQgKCkJWVhb27t2L\n",
              "b775RqefMy8vD99++y3c3d3h4OBQ7zZz586Fp6cnMjIykJ+fj82bN9e77nH9+vUYNmwYvvrqK7z8\n",
              "8sv1vla/fv3w22+/Yc6cOfjjjz/qFK23vPDCC/jjjz+wfft2FBUV4fDhwwgNDQUAbN68GTNnzsSy\n",
              "ZcuQn5+PTz/9FNOmTcO2bdvqvMY333yDnTt3oqSkBB07dsSUKVMQExODgwcPIjs7G2PHjsWgQYNQ\n",
              "UFCg0/EiovskEJFB6tu3r/Dmm28KaWlpgq2trZCfny88+eSTwvz584WrV68KAISEhARBEARh8uTJ\n",
              "wogRI+p8/9KlS4W2bdvW/hmAsG7duto/x8fHCwCErKwsQRAEITQ0VPjqq6/qvMakSZOE559/XhAE\n",
              "QcjKyhLMzc2F+Ph4QRAEYd++fYKNjY1QVFRUb/5/ZryXLVu2CI6OjrV/njhxojB+/Ph6j0lDf6bA\n",
              "wEBh+fLltc9XV1cLLi4uwjvvvCMIgiBs3rxZ8PDwEDQaTe026enpAgAhLS2tdp9PPPHEv/5sa9eu\n",
              "FRQKhVBZWVm7zezZs4WHH35YEARBiI6OFqRSaZ1j9csvvwgAhKtXr9Z7PG7tw9bWVrC1tRUACP7+\n",
              "/kJMTEyd7QAIe/bsEQRBEJ555hlh6NChQnx8fJ2f6VZGT09P4d133xV8fX2F2NjYevd7u927dwtj\n",
              "xowRPDw8BIlEIkRGRgpHjhwRBEEQsrOzBQDCiRMn6v3ehx56SHj55ZfrfG3mzJnCoEGD6vx8v/32\n",
              "W+3zOTk5AgDh0qVLdb4vMDBQ+P777++Zl4iaDjuQRAbOy8sL/fv3x+LFi7F9+3Y8//zzd2yTlpaG\n",
              "gICAOl8LDAxEampqna95eHjU/re1tTUA7Uk5gHbE+sorr8De3r728dNPPyEjIwMA4ObmhpEjR9au\n",
              "yVy5ciWefPJJ2NraNurn2rJlC3r27AlXV1fY2dnh6aefRl5eHtRqtU6v828/U3p6Olq3bl37vEwm\n",
              "g4+PT+2fExIScOPGDTg4ONT+zB06dIBCoahz7G5/jbtxdnaGmZlZnSy3cly/fh2Ojo51jpWfn1+D\n",
              "fr7Tp0+jqKgI58+fBwCcO3furtsuXrwYgYGBGDlyJNzc3PDss8/ixo0btc/n5+dj6dKlmDp1KsLC\n",
              "wu6574cffhgbNmzA9evXcfXqVfj5+WHIkCEoLCzE1atXAQBt27at93sb+p68/djeWhLQrVu3Ou/D\n",
              "69evIz09/Z55iajpsIAkMgJTp07FggULMHjwYLRq1eqO5729vZGUlFTna0lJSXWKpXtxd3fHF198\n",
              "gYKCgtpHSUkJdu3aVSfH999/j2vXrmHr1q2YMmVKo36e9PR0jBkzBjNmzEBqaiqKiorw/fffA9CO\n",
              "xgHA1tYWpaWldb7vVjHbUF5eXnXWGKrVaqSlpdX+2d3dHb6+vnV+5oKCAlRUVKBnz56120ml9/dR\n",
              "6unpiby8vNqCEgCuXbum02sEBwfjq6++wsyZM+96HJycnPDJJ5/g8uXLOHPmDFJSUvB///d/tc87\n",
              "ODhg//79WLp0KT7++GOd9u/r64u33noLhYWFSEpKqi2Ar1y5Uu/2DX1P3n5s3d3dAQBxcXF1/n+U\n",
              "lZXh9ddf1ykvEd0fFpBERmDQoEHYs2cPPvnkk3qff+655/Drr79i8+bNUKvVOHPmDBYtWoTJkyc3\n",
              "eB8vv/wy5s+fjxMnTkCj0UClUuHEiRM4depU7TZ9+vSBp6cnRo4ciYiIiAadQFNZWYmKiorah0ql\n",
              "QklJCTQaDZydnWFhYYGEhAQsXLiwzvd16dIF+/fvx6VLl1BVVYVPP/20tuvVUBMnTsSSJUtw6dIl\n",
              "qFQqvP/++8jLy6t9fuTIkaiqqsK8efNQWFgIALh58ybWr1+v037upXv37ggICMBrr72GsrIyZGRk\n",
              "YMGCBTq/zoMPPoguXbrgnXfeqff5n3/+GUlJSdBoNLC1tYVCoYBcLq+zTadOnXD48GF88cUXeO21\n",
              "1+66r2+++Qbr16/HzZs3AQDZ2dlYunQpXFxc0L59e7i4uGDcuHGYNm0aLl++DADIzMzE6dOnAQCT\n",
              "Jk3CN998gwMHDkCtVuPPP//EmjVr/vU96evrixEjRmDatGm1BXZxcTF2796NzMzMhh8oIrpvLCCJ\n",
              "jIBEIsHAgQPveiHobt26YdOmTfjvf/8LBwcHjBkzBjNnzsSsWbMavI9Zs2bh3XffxZQpU+Do6AhP\n",
              "T0+8+uqrd3QBp06ditOnTze4+9ihQwdYWlrWPgICAtCuXTssXLgQEyZMgK2tLSZOnHjHGdfjx4/H\n",
              "E088gZ49e8Lb2xsFBQXo1atXg38eAJgzZw5GjhyJvn37wsvLC5WVlejWrVvt87a2tjh27BhSU1MR\n",
              "GhoKOzs79OzZE4cOHdJpP/cil8vxyy+/4OLFi3Bzc8PAgQNrT9SxsLDQ6bXmz5+PtWvX4uLFi3c8\n",
              "FxsbiwEDBsDW1hYBAQGwt7fH4sWL79guKCgIR44cwa5du/D888/Xu2zA0dERq1atQkhICKytrREW\n",
              "FoaCggLs3bsXlpaWAIDVq1ejb9++GDx4MGxsbNCrV6/aUfuYMWOwZMkSvPTSS7C3t8eMGTOwbNmy\n",
              "Omdx1+fHH39E586d8eCDD8LW1hZt27bF6tWrazvTRNQyJAL/1hFRE9q9ezfGjRuHjIwMWFlZiR3H\n",
              "YG3btg1PPPEEysvLIZFIxI5DRFQHO5BE1GTKysrw8ccf48UXX2TxqKNjx47hypUrEAQBly9fxttv\n",
              "v40nn3ySxSMR6SUWkETUJL788ks4OzsDAN58802R0xiezMxMPPTQQ7C2tsbAgQPRvXv3u65pJSIS\n",
              "G0fYRERERKQTdiCJiIiISCcsIImIiIhIJywgiYiIiEgnLCCJiIiISCctWkAuW7asJXdHRERERM2g\n",
              "RQtIXe/tSkRERET6hyNsIiIiItIJC0giIiIi0gkLSCIiIiLSCQtIIiIiItIJC0giIiIi0gkLSCIi\n",
              "IiLSCQtIIiIiItIJC0giIiIi0onOBeRvv/2GLl26ICwsDN27d0dsbGxz5CIiIiIiPSXXZeP8/HyM\n",
              "Hz8ehw4dQocOHRAdHY3x48cjPj6+ufIRERERkZ7RqQOZlJQEJycndOjQAQAQFRWF1NRUnD59ulnC\n",
              "EREREZH+0amADAoKQm5uLo4ePQoA2LFjB4qLi5GSknLHtiqVCkVFRXUearW6SUITNbWCskq8u+M8\n",
              "Vh5MEjsKERGR3tNphK1UKrFp0ybMnTsXJSUl6NGjB4KDgyGX3/kyCxcuxHvvvVfna927d7+/tETN\n",
              "4NS1PMz48QwyCisAAF1bO6KTj4PIqYiIiPSXRBAEobHfrFKp4O7ujhMnTiAwMPCO51QqVZ2vzZs3\n",
              "D8uWLWvs7oialEYjYOWhZCz+4zLUGgEyqQRqjYA+bVzw3XORYscjIiLSWzqfhZ2ZmVn73/Pnz8eA\n",
              "AQPuKB4BQKFQwM7Ors5DJpPdX1qiJpJbosJz607go98uQa0R8Gi4B36Z3htyqQSHrmTj1LV8sSMS\n",
              "ERHpLZ0LyLfffhvt2rVDYGAgrl27hjVr1jRHLqJmczw5F0OWR+PA5Wwo5FIsHBmKZU9EINjDDqM6\n",
              "eQEAPt17ReSURERE+kunNZAAsHr16ubIQdTs1BoBX+xPxCd7r0AjAAEu1vh8fCe0c7er3Wb6gEBs\n",
              "Pp2O6IQcnLqWh86+jiImJiIi0k+8Ew2ZhOxiFSZ+E4Mle7TF48hOntgxvXed4hEAvB2tMKaLtgv5\n",
              "yZ4EMaISERHpPRaQZPSOJuZg8LJoHE7MgaWZDItGh2Hp2AhYK+pvwL/ULxByqQSHE3NwIiWvhdMS\n",
              "ERHpPxaQZLTUGgFL91zB+DXHkVOiQhs3G+yY3gtjunj/6/dpu5DabT7Zw7WQRERE/8QCkozSjaIK\n",
              "jP/6LyzflwBBAB7v4o3t03ojyM22Qd8/fUAgzGQSHE3KxfHk3GZOS0REZFhYQJLROXQlG0OWReOv\n",
              "5DxYmcvw6eMR+Gh0GCzNG34ZKU97S4yt6UJ+updrIYmIiG7HApKMRrVag49/u4QJ38Qgt7QS7VvZ\n",
              "YeeM3hjR0bNRrzetfyDMZVIcS87FX+xCEhER1WIBSUYhs7Ac41b/hS8OaO9lPb6bD7a+1BP+LjaN\n",
              "fk0Pe0s83pVrIYmIiP6JBSQZvP2XbmLIsmicSMmHjUKOz57siP8+FgoLs/u/89FL/QNgLpPi+NU8\n",
              "HE3KaYK0REREho8FJBmsKrUGC3ddxLPfnkB+WRVCPLUj66FhHk22j1ZKSzwRWbMWck8C7uPW8URE\n",
              "REaDBSQZpPT8MoxdeQwrDyUDAJ7p6YfNU3vCz9m6yff1Ur9AmMuliEnJw9EkroUkIiJiAUkG54/z\n",
              "WRiyLBpnUgtgayHHV091wruPdoBCfv8j6/q4Ky3wZKQPAO09stmFJCIiU8cCkgxGZbUG7/1yHpO/\n",
              "P4WiimqEe9tj18woPBzSqtn3PbVfABRyKU6k5ONIIruQRERk2lhAkkFIzS3D6K+OYu2RFADApN6t\n",
              "sfHFHvB2tGqR/bvZWeDJbtou5CfsQhIRkYljAUl6b9e5TDyyPBpx6YVQWprh6wld8NbQYJjLW/bt\n",
              "O7Wvtgt56lo+ohN4RjYREZkuFpCktyqq1Ji3LR4v/XAaxapqdPZ1wK5ZUXgg2E2UPK52FhjfzRcA\n",
              "u5BERGTaWECSXrqaU4pRXx7F939dAwBM6RuAnyd3h6e9pai5pvTzh4WZFGdSC3DwSraoWYiIiMTC\n",
              "ApL0zo7YDAxdHo3zGUVwtDbH2me74vXB7WAmE//t6mprgadqupCf7uV1IYmIyDSJ/xuZqEZFlRpz\n",
              "t5zDzJ/OoLRSjUg/R+yaGYX+bV3FjlbHi30DYGEmxdm0AhxgF5KIiEwQC0jSC4k3SzDi8yP4KSYV\n",
              "EgkwY0AgfnyhG9yVFmJHu4OLrQITevgBAD7dw7WQRERkelhAkui2nE7Ho58dxqWsYjjbmOO75yLx\n",
              "ykNtIdeDkfXdTO7jD0szGWLTC7H/8k2x4xAREbUo/f0NTUavrLIaszfG4v82xKKsUo2eAU7YNTMK\n",
              "UUEuYke7J2cbBSb04FpIIiIyTSwgSRRXbhRj+GdHsOlUOqQS4D8PtMH3z3eDq53+jazvZnIff1iZ\n",
              "yxCXXoh9F9mFJCIi08ECklqUIAjYcCINj352GAk3S+Bqq8APk7pj1gNBkEklYsfTiZPNbWsh93Et\n",
              "JBERmQ4WkNRiSlXV+M/6s3htcxwqqjSICnLGrllR6BHgJHa0Rpvcxx/W5jLEXy/CXnYhiYjIRLCA\n",
              "pBZxIaMIw1YcxrazGZBJJXh1UFusezYSzjYKsaPdF0drc0zs6QcA+JR3pyEiIhPBApKalSAI+OH4\n",
              "NYz44giSc0rhbmeBnyd3x7T+gZAa2Mj6bl6I0nYhz2cU4Y8LN8SOQ0RE1Ox0LiB37dqFTp06ISIi\n",
              "AiEhIVi3bl1z5CIjUFxRhRk/ncGbW+NRWa1B/7Yu2DUrCl39HMWO1qQcrM3xTC8/ANozsjUadiGJ\n",
              "iMi4yXXZWBAEPPXUUzhw4ADCwsKQkpKCdu3aYeTIkbC1tW2ujGSA4q8XYtqPp3EttwxyqQSvPdwW\n",
              "k3r7G03X8Z9eiPLHuqPXcDGzCH9cyMLDIa3EjkRERNRsdO5ASiQSFBQUAACKiorg5OQEhcKw17FR\n",
              "0xEEAeuOpmDkF0dxLbcMnvaWWP9iD0zuE2C0xSMA2FuZ41l2IYmIyETo1IGUSCRYv349Ro4cCWtr\n",
              "a+Tn52PLli0wNze/Y1uVSgWVSlXna2q1+v7Skl4rLK/CnE1x+O18FgDgwWA3LBodBnurO98fxmhS\n",
              "b398eyQFl7KK8fv5LAwOZReSiIiMk04dyOrqanzwwQfYsmULrl27hn379uHpp59GTk7OHdsuXLgQ\n",
              "SqWyziMmJqbJgpN+OZtWgEeWR+O381kwk0nw9tBgrHq6s8kUjwCgtDLDs71bA2AXkoiIjJtOBeTZ\n",
              "s2eRkZGBPn36AAC6du0KLy8vnDlz5o5t586di8LCwjqPyMjIpklNekMQBHwdnYwxXx1Fen45vB0t\n",
              "sWlKTzzXuzUkEuMdWd/N871aw9ZCjss3irE7PkvsOERERM1CpwLS29sbmZmZuHjxIgAgMTERSUlJ\n",
              "aNu27R3bKhQK2NnZ1XnIZLKmSU16oaCsEi98dxIf/HoRVWoBg0PcsXNGFMK97cWOJhqllRme66Xt\n",
              "Qi7bd4VdSCIiMko6rYF0c3PDqlWrMHbsWEilUmg0Gnz22Wfw8fFprnykp05dy8OMH88go7AC5jIp\n",
              "5g1tj6e6+5pk1/GfnuvdGt8cuYorN0rw67lMDAv3EDsSERFRk9KpgASAcePGYdy4cc2RhQyARiNg\n",
              "VXQyFv1+GWqNAD8nK3z2ZCeEeCrFjqY3lJZmmNTbH5/svYLl+xIwJLSVwd3nm4iI6N/wTjTUYLkl\n",
              "Kjy37gQ+3H0Jao2AYeEe+GVGbxaP9Xi2tx/sLORIuKntQhIRERkTFpDUIDFX8zBkeTQOXM6GQi7F\n",
              "wpGhWP5EBGwtzMSOppfsLMzwQpQ/AGDZ3itQcy0kEREZERaQ9K80GgGf/ZmAJ1Ydw40iFfxdrLFt\n",
              "Wi+Mi/Thesd7eKaXH5SWZkjKLsXOuAyx4xARETUZFpB0V9nFKkxcG4PFf1yBRgBGdvTEL9N7o30r\n",
              "O7GjGQRbCzO8EHXrjOwEdiGJiMhosICkeh1NzMGQ5dGITsiBhZkUi0aHYenjEbBW6HzelUmb2NMP\n",
              "9lZmSM4uxY7Y62LHISIiahIsIKkOtUbAJ3uuYPya48guVqGNmw1+md4bY7p4ix3NINnethZyxb5E\n",
              "VKs1IiciIiK6fywgqdbNogo89fVxLNuXAEEAxnbxwvZpvRHkZit2NIM2sacfHKzMkJxTih2xXAtJ\n",
              "RESGjwUkAQAOXcnG4GXROJacCytzGT55PBwfjw6HpTnvHnS/bBRyTO4TAABYvi+BXUgiIjJ4LCBN\n",
              "XLVag0W/X8LEtTHILa1EO3db/DKjNx7r6CV2NKMyoYcvHK3NkZJbhm1n2YUkIiLDxgLShGUWluPJ\n",
              "1cfx+f4kCALwZDcfbJvWCwEuNmJHMzrWCjkm96lZC/knu5BERGTYWECaqP2XbmLIsmjEpOTBRiHH\n",
              "inEdseCxUFiYcWTdXCb08IWTtTmu5ZZhyxmekU1ERIaLBaSJqVJrsHDXRTz77Qnkl1UhxNMOO2f0\n",
              "xrBwD7GjGT0rczle7KvtQn72ZyKq2IUkIiIDxQLShFwvKMfjK49h5aFkAMAzPf2weWpP+Dlbi5zM\n",
              "dDzV3RfONuZIzSvD1tPsQhIRkWFiAWki9ly4gSHLonE6tQC2FnJ89VQnvPtoByjkHFm3JCtzOab0\n",
              "1Z6RvWJ/AruQRERkkFhAGrnKag3e/+UCXvjuJArLqxDupcSumVF4OKSV2NFM1vhuvnC2USAtrxyb\n",
              "T6WLHYeIiEhnLCCNWFpeGcZ8dRTfHLkKAHi+d2tsnNIT3o5WIiczbZbmMkzpe+uM7ERUVrMLSURE\n",
              "hoUFpJH6LT4TQ5ZHIza9EEpLM6ye0AXzhgbDXM7/5frgqe6+cLFV4HpBOTaxC0lERAaG1YSRqahS\n",
              "453t8Zjyv9MorqhGJx977JoVhQeD3cSORrexMJNhas1ayM/3swtJRESGhQWkEUnJKcWoL49i3bFr\n",
              "AIAX+/pj/Ys94GlvKXIyqs+T3XzgWtOF3HgqTew4REREDcYC0kjsiM3A0BWHcT6jCI7W5lj7bFfM\n",
              "HdweZjL+L9ZXFmYyvNSvpgv5ZyJU1WqRExERETUMqwsDV1Glxtwt5zDzpzMoUVUj0s8Ru2ZGoX9b\n",
              "V7GjUQM8EekDNzsFMgorsOEk10ISEZFhYAFpwJKySzDi8yP4KSYVEgkwY0AgfnyhG9yVFmJHowbS\n",
              "diEDAQBf7GcXkoiIDAMLSAO19Uw6hq04jEtZxXC2Mcd3z0XilYfaQs6RtcF5vKs33O0skFlYgfUn\n",
              "uBaSiIj0H6sNA1NeqcarG2Pxn/WxKKtUo4e/E3bNjEJUkIvY0aiRLMxkmNZfuxbyi/1JqKhiF5KI\n",
              "iPQbC0gDcuVGMR797DA2nkqHRAK8/EAQ/jepG1ztOLI2dGO7esNDaYGsInYhiYhI/7GANACCIGDD\n",
              "yTQ8+tlhJNwsgYutAj9M6oaXH2gDmVQidjxqAgq5DC/1r1kLeSCRXUgiItJrOhWQubm5iIiIqH20\n",
              "adMGcrkceXl5zZXP5JWqqvHKhli8tikOFVUaRAU5Y/esKPQMcBY7GjWxMV284KG0wI0iFX6KSRU7\n",
              "DhER0V3JddnYyckJZ8+erf3z4sWLcfDgQTg6OjZ1LgJwMbMI0348jeTsUkglwCsPtcXUvgGQsuto\n",
              "lBRyGaYNCMSbW+PxxYEkjIv0gYWZTOxYREREd7ivEfaaNWvw/PPPN1UWqiEIAn48norhnx9BcnYp\n",
              "3O0s8PPkHpjWP5DFo5Eb09kbnvaWyC5W4Yfj7EISEZF+anQBefToUeTn52Po0KH1Pq9SqVBUVFTn\n",
              "oVZzXde9FFdUYebPZ/HG1nOorNagf1sX7JoVhcjW7PKaAnO5FNMHaNdCfnWQZ2QTGZOCskq8/PMZ\n",
              "bDnNmwaQ4Wt0AblmzRpMmDABcnn9U/CFCxdCqVTWecTExDQ6qCmIv16IYSsO45fYDMilEswd3A5r\n",
              "JnaFo7W52NGoBY3u7AUvB20X8n9/XRM7DhE1kbe3n8e2sxmYszkOFzKKxI5DdF8aVUCWlJRgw4YN\n",
              "eO655+66zdy5c1FYWFjnERkZ2eigxkwQBHx3LAUjvziKlNwyeNpbYv2LPfAi1zuaJDOZFDNqu5DJ\n",
              "KK9kF5LI0O0+l4kdsRkAgCq1gNkbY1FZrRE5FVHjNaqAXL9+PcLDw9GuXbu7bqNQKGBnZ1fnIZPx\n",
              "hIB/Kiyvwks/nMbb28+jUq3BA+3d8OvM3ujs6yB2NBLRyE5e8Ha0RE4Ju5BEhi63RIW3tsUDAJ7s\n",
              "5gMHKzNcyCzC5/sTRU5G1HiNKiB58kzTiE0rwNAV0dgdnwUzmQTzhgZj9YTOsLfiyNrUmcmkmNE/\n",
              "CIB2LWRZZbXIiYiosd7ecR65pZVo62aLd4YFY/6IEADA5/sTEX+9UOR0RI3TqALy6NGjePbZZ5s6\n",
              "i8kQBAFrDl/F6K+OIi2vHN6Oltg0pSee790aEglH1qT1WCdP+DhaIbe0Et8fYxeSyBDtjMvAr3GZ\n",
              "kEklWDwmHAq5DEPDPDAk1B3VGo6yyXDxTjQtrKCsEi98dwrzd15AlVrA4BB37JwRhXBve7GjkZ65\n",
              "fS3kykPJ7EISGZjsYhXm1Yyup/ULQKiXsva5+cND4GRtjktZxVjxZ4JYEYkajQVkCzp1LR+PLD+M\n",
              "vRdvwFwmxfvDO+CL8Z2gtDQTOxrpqcc6esLPyQp5pZX4jl1IIoMhCALe2nYO+WVVaOdui+kDguo8\n",
              "72SjwAc1o+wvDiQhLr1AhJREjccCsgVoNAJWHkzC4yuP4XpBOfycrLDlpZ6Y0MOPI2v6V3KZFDNq\n",
              "fvGsOpSMUhW7kESGYEdsBn4/fwNyqQRLxobDXH7nr9vBoa0wLNwD6ppRtqqaV1wgw8ECspnllVbi\n",
              "+XUnsHD3JVRrBAwL98AvM3ojxFN5728mAjA8wgOtna2RV1qJdcdSxI5DRPdws7gC7+w4DwCYPiAQ\n",
              "HTzu/nn/3qMd4Gxjjis3SrBsL0fZZDhYQDajmKt5GLIsGvsvZ0Mhl2LBY6FY/kQEbC04sqaGk9+2\n",
              "FnLVoWSUsAtJpLcEQcCbW+NRUFaF4FZ2mNY/8F+3d7Q2xwcjQgFor7hwNq2gBVIS3T8WkM1AoxHw\n",
              "+f5EjFv9F7KKKuDvYo1t03rhyW4+HFlTozwa7gF/Z2sUlFVh3dEUseMQ0V1sO3sdey7cgJlMO7o2\n",
              "k9371+zDIe4YEeEBjQC8suEsb2FKBoEFZBPLKVFh4toYLPr9MtQaASM7euKX6b3RvpWd2NHIgMll\n",
              "UswcqF0LuTo6GcUVVSInIqJ/ulFUgXe2a0fXswYG6fS5/+6jHeBiq0BSdik+2XOluSISNRkWkE3o\n",
              "aFIOBi+LRnRCDizMpPh4dBiWjA2HtaL++4UT6WJYuAcCXNiFJNJHgiDgjS3nUFRRjVBPJab0DdDp\n",
              "++2tzLHwMe0oe3V0Mk5dy2+OmERNhgVkE1BrBHy69wqe+vo4sotVCHK1wY7pvTG2izdH1tRkZFLJ\n",
              "bV3IqyhiF5JIb2w+fR37Lt2EuUyKJWPDIW/A6PqfHgh2w8hOntAIwKsbYznKJr3GAvI+3SyqwNNr\n",
              "juPTvQnQCMDYLl7YMb032rjZih2NjNDQMA8EutqgsLwK3x5JETsOEQHILCzHe79oR9cvPxh0X5//\n",
              "7wztADc7BZJzSrH498tNFZGoybGAvA/RCdkYsjwaR5NyYWUuw9Kx4fh4dDgszWViRyMjdXsX8uvo\n",
              "ZBSWswtJJCZBEPD65nMorqhGuLc9Jkf539frKa3M8OHIMADAmiNXcSIlryliEjU5FpCNUK3WYPHv\n",
              "lzHhmxjklFSinbstdkzvjZGdvMSORibgkdBWCHK1QVFFNdYeuSp2HCKTtuFkGg5eyYa5XIolY8Ia\n",
              "Nbr+p/7tXDG2ixeEmlF2eSVH2aR/WEDqKKuwAk+uPo7P9idCEIAnu/lg27ReCHS1ETsamQiZVIJZ\n",
              "D2i7kGsOX2UXkkgk1wvK8cHOiwCAVx5sg0DXplu69NbQYLRSWiAltwwf/36pyV6XqKmwgNTB/ss3\n",
              "MWR5NGJS8mCjkGP5uI5Y8FgoLMw4sqaWNSSkFdq42aC4ohrfHGYXkqilaUfXcShWVaOjjz0m3efo\n",
              "+p/sLMzw4SjtKHvtkRT8lZzbpK9PdL9YQDZAlVqDhbsv4tm1J5BXWokOHnbYOaM3Hg33EDsamSip\n",
              "VIJZA9sAAL45fBWFZexCErWkn2LSEJ2QA4VcisVjwiGTNv0VN/q2ccG4SG8AwGub4lDKu1CRHmEB\n",
              "eQ/XC8rx+MpjWHkwGQAwsYcvNk/tCT9na5GTkakbHOKOdu62KFZVY83hZLHjEJmMtLwy/PfXCwCA\n",
              "Vwe1RYBL8y1hemNIe3jaWyI1rwwf/cZRNukPFpD/Ys+FGxiyLBqnUwtgayHHl+M74b3hIRxZk17Q\n",
              "diG1ayG/OZKCgrJKkRMRGT+NRsCczXEorVSjq58Dnu3Vuln3Z2thho9qRtnfHbuGo0k5zbo/ooZi\n",
              "AVmPymoN5u+8gBe+O4nC8iqEeynx64woDA5tJXY0ojoGddB2IUtU1fg6mmshiZrbDzGpOJqUCwsz\n",
              "KRaNbp7R9T/1DnLG+G4+ALSj7BKOskkPsID8h7S8MoxZeQxrak5MeL53a2yc0hM+TlYiJyO6k1Qq\n",
              "wcsPaNdCfns0Bfml7EISNZfU3DIs3KU963rOw+1adCnT3CHt4eVgifT88toMRGJiAXmb3+IzMWR5\n",
              "NGLTCqC0NMPqCV0wb2gwzOU8TKS/BnVwQ3ArO20XkmshiZqFRiPg1U2xKKtUI7K1Iyb28GvR/dso\n",
              "5Ph4tHaU/cPxVEQnZLfo/on+iZURAFW1Gu9sj8eU/51GcUU1OvnY49eZvfFgsJvY0YjuSSL5+7qQ\n",
              "3x5JQR67kERN7rtjKTh+NQ9W5jIsHh0OaQuMrv+pZ4AzJvbwBQDM2RSH4gpefYHEY/IFZEpOKUZ9\n",
              "eRTrjl0DALzY1x/rX+wBLweOrMlwPBTshg4ediitVGN1NLuQRE0pJacUH/2mvS/164Pbibqkac7g\n",
              "dvBxtEJGYQUWcJRNIjLpAnJnXAaGrjiM+OtFcLAyw9pnumLu4PYwa4JbURG1JInk77WQ646mILdE\n",
              "JXIiIuNwa3RdXqVGD38nPNXNV9Q8VuZyLKoZZf8Uo72NIpEYTLJSqqhS442t5zD9xzMoUVWjq58D\n",
              "ds2KQv92rmJHI2q0B9q7ItRTibJKNVaxC0nUJNYeTcGJlHxYm8vw8egwUUbX/9TN3wnP9vIDoB1l\n",
              "83amJAaTKyCTsksw4vMj+PF4KiQSYHr/QPz0Qne0UlqKHY3ovmi7kNq1kN8dvcYuJNF9Ss4uwcc1\n",
              "F+9+45H28HbUn6VNrw1qBz8nK2QVVeCDnRfEjkMmyKQKyK1n0jFsxWFcyiqGk7U5vnsuErMHtYWc\n",
              "I2syEgPauSLMS4nyKjVWHWIXkqix1BoBr26Kg6pag96Bzngy0kfsSHVYmsuweEw4JBJg46l07L90\n",
              "U+xIZGJ0rpxUKhWmT5+OoKAghIaG4qmnnmqOXE2qvFKN1zbF4j/rtZdg6OHvhN2zohAV5CJ2NKIm\n",
              "VacLeewactiFJGqUbw5fxalr+bBRyPHR6DBIJOKPrv+pi58jnq+5E87rW+JQWMZRNrUcnQvI119/\n",
              "HRKJBFeuXMG5c+ewePHi5sjVZBJuFGP454ex4WQ6JBLg5QeC8L9J3eBqZyF2NKJm0b+tK8K97VFe\n",
              "pcbKg0lixyEyOIk3S7DoD+1Z1289or0Xtb6aPagt/J2tcaNIhfd2nhc7DpkQnQrI0tJSrFmzBv/9\n",
              "739r/zXm7u7eLMGawsaTaRj22WFcuVECF1sFfpjUDS8/0KZFbj1FJJbbu5Df/3UNN4srRE5EZDiq\n",
              "1Rq8sjEWldUa9Gnjgse7eosd6V9ZmMmweGw4pBJgy+nr2HPhhtiRyEToVEAmJSXB0dERCxYsQJcu\n",
              "XRAVFYV9+/bVu61KpUJRUVGdh1qtbpLQ91Kqqsb/bTiLVzfFoaJKg6ggZ+yaGYWeAc4tsn8isfVr\n",
              "44IIb3tUVGmw8iDXQhI11Oroq4hNK4CthRwfjQrVy9H1P3XyccALUf4AgDe2nkNBGW8mQM1PpwKy\n",
              "uroa165dQ3BwME6ePInly5fj8ccfx40bd/6LZ+HChVAqlXUeMTExTRb832QVVeC3+CxIJcCrg9pi\n",
              "3bORcLFVtMi+ifSBRCLBfx7UXhfyf+xCEjVIwo1ifLLnCgBg3tBgg7o6x38ebIMAF2tkF6vw7g6O\n",
              "sqn56VRA+vj4QCqVYvz48QCAjh07onXr1jh37twd286dOxeFhYV1HpGRkU2T+h4CXGywZEw4fp7c\n",
              "A9P6B+rFdbuIWlqfIGd08rGHqlqDrw6wC0n0b2pH12oN+rd1wZjOXmJH0omFmQxLxkZAKgG2nc3A\n",
              "b/FZYkciI6dTAens7IyBAwfi999/BwBcvXoVV69eRfv27e/YVqFQwM7Ors5DJpM1TeoGGBzaCpGt\n",
              "HVtsf0T65va70/xw/BpuFrELSXQ3Kw8lIy69EHYWciwcqZ9nXd9LhLc9pvQNAAC8te0c8ko5yqbm\n",
              "o/NZ2F999RUWLVqE0NBQjBgxAitXroSnp2dzZCOi+xQV5IzOvg5QVWvwxQGekU1Un0tZRfh0r3Z0\n",
              "/e6jHeCuNNyrdMx6IAht3GyQU1KJt7fHix2HjJjOBaS/vz/279+Pc+fOITY2FqNGjWqOXETUBCQS\n",
              "Cf5T04X8MSYVWYXsQhLdrkqtweyNsahSC3igvRse62jYDRGFXIYlYyIgk0qwMy4Tu85lih2JjBRv\n",
              "wUJk5HoFOqGrnwMqqzX48kCi2HGI9MqXB5IQf70ISkszLHgsxCBH1/8U6qXES/1ujbLjeUMBahYs\n",
              "IImM3O1dyJ9i0tiFJKpxPqMQy/clAADeH97BqG4wMWNAENq52yKvtBLztsVDEASxI5GRYQFJZAJ6\n",
              "BDghsrUjKtUafMEuJBEqqzWYvTEO1RoBgzq44dFwD7EjNSlzuRSLx4RDLpVgd3wWdsZxlE1NiwUk\n",
              "kQm4/e40P8ekIaOgXOREROL6bH8iLmYWwcHKDB+MMIwLhusqxFOJaf0DAQBvb49HdjFH2dR0WEAS\n",
              "mYieAc7oxi4kEeKvF+KL/dq/A+8PDzHqG01M6x+I4FZ2yC+rwlvbznGUTU2GBSSRCbl1d5r1J9Jw\n",
              "nV1IMkGqajVmb4xFtUbAkFB3DA1rJXakZnVrlG0mk+D38zewIzZD7EhkJFhAEpmQ7v5O6OHvhCq1\n",
              "gM/3swtJpmfFvkRcyiqGk7U55g83jrOu7yXYww4zB2iXsLy9/TxvKkBNggUkkYm51YXceDIN6fll\n",
              "Iqchajlx6QX48qD2gvofjAiBk43xjq7/aUq/AIR6KlFYXoU3tnKUTfePBSSRiYls7Yhegbe6kLw7\n",
              "DZkGVbUar2yIhVojYFi4BwaHGvfo+p/MZNpRtrlMir0Xb2LrmetiRyIDxwKSyATdukf2xpNpSMtj\n",
              "F5KM36d7E5BwswTONuZ479EOYscRRVt3W8yquRrDuzvO85qwdF9YQBKZoK5+jugd6IxqDddCkvE7\n",
              "k5qPlbWj61A4WpuLnEg8L/bxR7iXEkUV1Zi7JY6jbGo0FpBEJuo/D2o7EZtOpbMLSUarokp71rVG\n",
              "AEZEeODhEHexI4lKfmuULZdi/+VsbDyVLnYkMlAsIIlMVGdfR0QFabuQK/5MEDsOUbNYuucKkrJL\n",
              "4WKrwLsmOrr+pyA3W/xfzcl083+5gMxCXtKLdMcCksiE3Toje/Pp60jNZReSjMupa3lYHZ0MAFj4\n",
              "WCjsrUx3dP1PL0T5o6OPPYpV1ZizmWdlk+5YQBKZsE4+DujbxgVqdiHJyJRXqjF7YxwEARjZyRMP\n",
              "BLuJHUmvyKQSLB4TDoVcikNXsrH+RJrYkcjAsIAkMnG37pG95cx1pOSUipyGqGks/uMyruaUws1O\n",
              "gXeGcnRdnwAXG7w6qC0A4INfL/K6sKQTFpBEJq6jjwP6tb3VheQZ2WT4Yq7m4ZsjVwEAH44Mg9LK\n",
              "TORE+uvZXq3RxdcBJapqvM5RNumABSQR1V4XcuuZdFxlF5IMWFllNV7bFAtBAMZ28UL/dq5iR9Jr\n",
              "MqkEH48Og4WZFIcTc/BjTKrYkchAsIAkIkR422NAO1doBGDFPq6FJMP18W+XkZJbhlZKC7w1NFjs\n",
              "OAbB38UGrw1qBwD4768XeVkvahAWkEQE4O+1kNvOXkdydonIaYh091dyLr49mgIA+HBUGOwsOLpu\n",
              "qGd6+iHSzxFllWq8tikOGg1H2fTvWEASEQAgzMseD7Sv6UJyLSQZmFJVNV7dFAsAGBfpjb5tXERO\n",
              "ZFikUgkWjQmDpZkMx5Jz8b/j18SORHqOBSQR1Zo1ULsWcvvZ60hiF5IMyIe7LyEtrxye9pZ4Y0h7\n",
              "seMYJF8na7w+WDvKXrjrEq8NS/+KBSQR1Qr1UuKB9m7QCMByroUkA3E0MQff/6XtmH00Kgy2HF03\n",
              "2tPdfdHd3xHlVWrM3hTLUTbdFQtIIqrj1lrIHbEZSLxZLHIaon9XoqrGq5viAADju/mgd5CzyIkM\n",
              "m1QqwaLR4bAylyHmah7WHUsROxLpKRaQRFRHiKcSDwW7QRCAZfu4FpL024JdF3G9oBxeDpaYy9F1\n",
              "k/B2tKpdBvDRb5d4aS+qFwtIIrrDretC7ozLQMINdiFJP0UnZOPH49rrFn48Ogw2CrnIiYzH+G4+\n",
              "6BXohIoqDV7dGAs1R9n0DzoXkH5+fmjbti0iIiIQERGB9evXN0cuIhJRsIcdHu7gXtOF5FpI0j/F\n",
              "FVWYUzO6ntjDFz0DOLpuShKJBB+NCoO1uQwnr+Vjbc2dfYhuaVQHcv369Th79izOnj2Lxx9/vKkz\n",
              "EZEemFWzFvLXc5m4wi4k6Zn//noRGYUV8HG0wpyaM4epaXk5WNVejH3R75d5ZQaqgyNsIqpX+1Z2\n",
              "GBxS04Xcyy4k6Y8Dl2/i5xNpAIBFo8NgZc7RdXN5oqs3ooKcoarWYDZH2XSbRhWQEyZMQGhoKJ5/\n",
              "/nlkZ2fXu41KpUJRUVGdh1qtvq+wRNSybu9CXsoqEjkNEVBYXoXXN58DADzbyw/d/J1ETmTcbo2y\n",
              "bRVynEktwNfRyWJHIj2hcwF56NAhxMXF4fTp03B2dsbEiRPr3W7hwoVQKpV1HjExMfcdmIhaTjt3\n",
              "OzwS2goAu5CkHz7YeQFZRRXwc7KqvX8zNS8Pe0vMqxllL9lzhZf3IgCNKCB9fHwAAGZmZnj55ZcR\n",
              "HR1d73Zz585FYWFhnUdkZOT9pSWiFjfrgSBIJMDu+CxczGQXksTz56Ub2HgqHRIJsHhMOCzNZWJH\n",
              "MhljunihX1sXVFZr8MrGOFSrNWJHIpHpVECWlpaioKCg9s8//fQTOnbsWO+2CoUCdnZ2dR4yGf+y\n",
              "ExmaNm627EKS6ArL/h5dP9+rNbr4OYqcyLRIJBJ8ODIMthZyxKYVYBVH2SZPpwLyxo0b6N+/P8LC\n",
              "whAaGoqDBw/iu+++a65sRKQnZg3UdiF/O5+F8xmFYschE/TeL+dxs1gFf2drzB7UVuw4JsldaYF3\n",
              "h3UAAHy6JwGXszjKNmU6FZD+/v44c+YM4uLicO7cOWzfvh1+fn7NFI2I9EWQmy2GhnkAYBeSWt6e\n",
              "Czew5cx1SCXA4rHhsDDjNEssIzt5YmA7V1SqtWdlV3GUbbJ4GR8iapBZAwMhkQB/XLiB+OvsQlLL\n",
              "yC+txBtbtaPrF/r4o5OPg8iJTJtEIsGCkaFQWprh3PVCrDyYJHYkEgkLSCJqkEBXWzwaru1Cfsou\n",
              "JLWQd385j+xiFQJdbfCfmltskrjc7Czw3qPaUfayfQk8uc5EsYAkogabOTAIUgmw9yK7kNT8fovP\n",
              "xPazGdrR9RiOrvXJ8AgPPBTshiq1gFc2cJRtilhAElGDBbjYYHiEJwDg071XRE5DxiyvtBJvbYsH\n",
              "AEzpG4AIb3txA1EdEokEHzwWAnsrM1zILMLn+xPFjkQtjAUkEelkxoDAmi7kTcSlF4gdh4zU29vj\n",
              "kVNSiTZuNrV3RCL94mprgfeHhwAAPvszkVdoMDEsIIlIJ/4uNhhR24XkWkhqer/GZWJnXCZkUgmW\n",
              "jImAQs7Rtb4aFtYKg0PcUa3RjrIrqznKNhUsIIlIZzMGBkEmleDPSzdxNq1A7DhkRHJKVJi3XTu6\n",
              "fqlfAEK9lCInon8jkUgwf0QIHK3NcSmrGJ/9yX9UmgoWkESks9bO1rd1IbkWkpqGIAiYty0eeaWV\n",
              "aOduixkDOLo2BM42CsyvGWV/fiAJ59I5yjYFLCCJqFFmDgyETCrBgcvZOJOaL3YcMgK/xGVid3wW\n",
              "5FIJFo8Jh7mcv6IMxSNhrfBIWCuoNQJmb4yFqlotdiRqZvzbSUSN4utkjZEduRaSmsbN4gq8XTO6\n",
              "nj4gECGeHF0bmvnDQ+BsY47LN4qxfB8/E4wdC0giarTpA7RdyINXsnHqGruQ1DiCIODNrfEoKKtC\n",
              "cCs7TOsfKHYkagRHa3N8MCIUAPDlgSTEcn20UWMBSUSN5utkjVGduBaS7s/2sxnYc+EGzGTa0bWZ\n",
              "jL+aDNXDIe4YHuEBjQC8sjEWFVUcZRsr/i0lovsyY0AQ5FIJohNycOpanthxyMDcLKrAOzvOAwBm\n",
              "DghCsIedyInofr07rAOcbRRIvFmCT/gPS6PFApKI7ou3oxVGd/YCAHyyh+ueqOEEQcAbW8+hsLwK\n",
              "oZ5KTOkXIHYkagIO1uZY8Jj2rOzVh5JxmifZGSUWkER036b1D4RcKsHhxBycSGEXkhpmy+nr2Hvx\n",
              "JsxlUo6ujcxDHdwxsqMnNAIwm6Nso8S/rUR037wdrTCmizcAroWkhskqrMC7v2hH17MeCEJbd1uR\n",
              "E1FTe2dYB7jaKpCcXYolf1wWOw41MRaQRNQkpvUPgJlMgiOJuYi5yi4k3Z0gCHh9SxyKK6oR7qXE\n",
              "i338xY5EzUBpZYYPR2nPyv768FWc5HTCqLCAJKIm4eXwdxfykz3sQtLdbTyZjgOXs2Eu146u5Rxd\n",
              "G60B7dwwurMXBAF4dVMcyis5yjYW/FtLRE1mWv9AmMkkOJaci7+Sc8WOQ3ooo6Ac83deAAC88mAb\n",
              "BLlxdG3s5g0NhrudBa7mlGLR7xxlGwsWkETUZDztLfF4V3YhqX6CIGDO5jgUq6rR0ccek6I4ujYF\n",
              "Ssu/R9lrj17Fcf7j0iiwgCSiJjWtfyDMZVIcv5qHY0n8RUF/+/lEGqITcqCoGV3LpBKxI1EL6dfW\n",
              "FU909a4dZZdVVosdie4TC0gialKtlJZ4IrKmC7n3CgRBEDkR6YP0/DJ8UDO6fnVQWwS42IiciFra\n",
              "m4+0h4fSAql5Zfho9yWx49B9YgFJRE1uar8AmMukiGEXkvD36Lq0Uo0uvg54tldrsSORCGwtzPDR\n",
              "6DAAwLpj1/jZYOBYQBJRk2ultMQ4diGpxg/HU3EkMRcWZlIs4ujapEUFueDJbj4AgFc3xaJUxVG2\n",
              "oWIBSUTN4qX+gTCXS3EiJR9HEtlpMFVpeWVYsOsiAOC1Qe3Q2tla5EQktjeGtIenvSXS88uxcPdF\n",
              "seNQI7GAJKJm4WZngScjtZ0GdiFNk0Yj4NVNsSirVCPSzxHP9PQTOxLpARuFHItqRtn/+ysVRxJz\n",
              "RE5EjdHoAnLt2rWQSCTYtm1bE8YhImPyUr8AKORSnLqWj8P8JWFyvv/rGv5KzoOlmQyLxoRBytE1\n",
              "1egZ6Iynu/sCAF7bFIfiiiqRE5GuGlVApqSkYPXq1ejevXtT5yEiI+JqZ4Hx3bS/JD7Zwy6kKbmW\n",
              "W4oPa860nTukHXydOLqmul4f3A7ejpa4XlCOBbt4Vrah0bmA1Gg0mDRpElasWAGFQtEcmYjIiEzp\n",
              "6w+FXIrTqQU4lMAupCnQaAS8ujEO5VVqdPd3xFM1/4ggup21Qo5Fo8MBAD/FpOLQlWyRE5EudC4g\n",
              "ly5dil69eqFz587/up1KpUJRUVGdh1rNe2ASmRpXOws81Z1dSFPy7dEUxKTkwcpchkWjwzm6prvq\n",
              "7u9UuzZ2zuY4FHGUbTB0KiDj4+OxefNmvPXWW/fcduHChVAqlXUeMTExjQ5KRIZrSt8AWJhJcTat\n",
              "AAfYZTBqV3NK8fHv2nHkG0Paw9vRSuREpO9ee7gtfJ2skFlYUXuxedJ/OhWQ0dHRSElJQVBQEPz8\n",
              "/PDXX39h8uTJ+PLLL+/Ydu7cuSgsLKzziIyMbLLgRGQ4XGwVtQvmP2UX0mipNQJe3RiLiioNegc6\n",
              "Y3zN9f6I/o2VuXaULZEAG06mY//lm2JHogbQqYCcOnUqMjMzkZKSgpSUFHTv3h2rVq3C1KlT79hW\n",
              "oVDAzs6uzkMmkzVZcCIyLC/2DYClmQyx6YU4cJldSGO09shVnLyWDxuFHB+OCoVEwtE1NUxka0c8\n",
              "V3OHotc3x6GwjKNsfcfrQBJRi3C2UWBCj5q1kLwupNFJvFmCRb9fBqC957GXA0fXpJvZD7WFv7M1\n",
              "bhSp8D5H2XrvvgrIAwcOYMSIEU0UhYiM3Qt9/GFpJkNceiH+vMQxlbFQawTM3hgLVbUGUUHOeKKr\n",
              "t9iRyABZmmuvFyqRAJtPp2PvhRtiR6J/wQ4kEbUYZxsFJvSsWQu5N4FdSCOxOjoZZ9MKYKuQ46NR\n",
              "YRxdU6N19nXEC1H+AIA3tp5DQVmlyInoblhAElGLerFPAKzMZTh3vRB7L7ILaegSbhRj6Z4rAIB5\n",
              "w4LhYW8pciIydP/3YBsEuFjjZrEK7/3CUba+YgFJRC3K0docE2uu+/Yp10IatGq1BrM3xqKyWoP+\n",
              "bV0wprOX2JHICFiYybB4TDikEmDrmev4/XyW2JGoHiwgiajFTY7yh7W5DOczirCH65wM1spDyYhN\n",
              "L4SthRwLR3J0TU2no48DJvcJAAC8uTUe+aUcZesbFpBE1OIcrM3xTC8/AFwLaaguZxXj073a0fW7\n",
              "wzrAXWkhciIyNi8/EIQgVxvklKjwzo7zYsehf2ABSUSimNTbHzYKOS5kFuH38+xCGpKqmtF1lVrA\n",
              "A+1dMbKTp9iRyAjdGmXLpBLsiM3A7nOZYkei27CAJCJROFib194D99O9V6DRsAtpKL46kIRz1wuh\n",
              "tDTDgsd4wXBqPuHe9pjaVzvKfmtbPHJLVCInoltYQBKRaCZFtYatQo5LWcVcKG8gLmQUYfmfCQCA\n",
              "9x7tAFc7jq6pec0YGIh27rbILa3E29s5ytYXLCCJSDT2VuZ49ra1kOxC6rfK6r9H1w8Fu2F4hIfY\n",
              "kcgEKOR/j7J/PZeJnXEZYkcisIAkIpE939sfthZyXL5RjN/YhdRrn+9PxIXMIjhYmeG/HF1TCwrx\n",
              "VGJa/0AAwLxt8cgu5ihbbCwgiUhUSiszPNerNQBgGbuQeiv+eiE+358IAHh/eAhcbBUiJyJTM71/\n",
              "INq3skN+WRXe2naOV28QGQtIIhLdc71b13Yhd8XzTEt9c2t0Xa0RMDjEHUPDWokdiUyQuVyKJWPC\n",
              "IZdK8Pv5G9gRy1G2mFhAEpHolJZmeL73311INbuQemXFnwm4lFUMR2tzzB8RwtE1iSbYww4zBgQB\n",
              "AN7ZcR43iytETmS6WEASkV54rndr2FnIkXCzBL/yem96Iy69AF8cSAIAzB8eAmcbjq5JXC/1D0AH\n",
              "DzsUlFXhza3xHGWLhAUkEekFOwszTIryBwAs23uFXUg9oKpWY/bGWKg1AoaGtcIjHF2THjCTSbFk\n",
              "bDjMZBLsuXAD285eFzuSSWIBSUR649leflBamiEpu5SX6tADy/Ym4MqNEjjbmOP94SFixyGq1c7d\n",
              "Di8/0AYA8M7287hRxFF2S2MBSUR6w9bCDC9EaddCLt/HtZBiOptWgK8OakfXH4wIhaO1uciJiOp6\n",
              "sY8/wryUKKqoxtwtPCu7pbGAJCK9MrGnH+yttF3IX3iWpSgqqtR4ZcNZaARgeIQHHg5xFzsS0R3k\n",
              "Mu1Z2eYyKf68dBObTqWLHcmksIAkIr2i7UJq10Iu35eAarVG5ESm55M9V5CUXQoXWwXeHdZB7DhE\n",
              "dxXkZov/PKgdZb+/8wIyC8tFTmQ6WEASkd6Z2NMPDlZmSM4p5bXeWtipa/lYHZ0MAFjwWCgcOLom\n",
              "PfdCVGtEeNujuKIar2/mKLulsIAkIr1jo5DjhT7sQra0iio1Xt0YC40AjOzoiQeD3cSORHRPcpkU\n",
              "i8eEw1wuxcEr2dhwMk3sSCaBBSQR6aWJPbRdyJTcMmw/yy5kS1j8+2Uk55TC1VaBdzi6JgMS6GqD\n",
              "2Q9pR9nzd17E9QKOspsbC0gi0kvWCjkm9wkAoL0TCruQzetESh7WHLkKAPhwVCiUVmYiJyLSzfO9\n",
              "/dHJxx4lqmq8vjmOo+xmxgKSiPTWhB6+cLQ2R0puGbae4cWCm0t5pXZ0LQjAmM5eGNCOo2syPDKp\n",
              "BIvHhEMhlyI6IQc/xXCU3ZxYQBKR3rJWyPFizVrIFX8moopdyGbx8e+XkJJbhlZKC7w1NFjsOESN\n",
              "5u9ig9cebgcA+O+vF5CWVyZyIuPFApKI9NrTPXzhbGOO1LwybD3NLmRT+ys5F2uPpAAAPhwVBqUl\n",
              "R9dk2J7t6Yeufg4orVRjzuY4aHhDgmahcwH50EMPISwsDBEREYiKisKZM2eaIxcREQDAylyOF2+t\n",
              "hdyfwC5kEyqrrMZrm+IAAE909UbfNi4iJyK6f1KpBItGh8PCTIqjSbn44fg1sSMZJZ0LyA0bNiAu\n",
              "Lg5nz57F//3f/+GZZ55phlhERH97qru2C5mWV44tp3m3iaby0e5LSM0rg4fSAm8+0l7sOERNxs/Z\n",
              "Gq/XjLIX7r6E1FyOspuazgWkvb197X8XFhZCIpE0ZR4iojtYmsswpe+tM7ITUVnNLuT9OpqUg3XH\n",
              "tJ2Zj0eHw9aCo2syLhN6+KFba0eUVarx6qZYjrKbWKPWQE6YMAHe3t6YN28evv/++3q3UalUKCoq\n",
              "qvNQq9X3FZaITNf4br5wtlEgPb8cm9mFvC8lqr9H109280HvIGeRExE1vVujbCtzGY5fzcN3x1LE\n",
              "jmRUGlVAfvfdd0hLS8MHH3yAOXPm1LvNwoULoVQq6zxiYmLuKywRmS5Lcxmm9tN2IT9jF/K+LNx1\n",
              "Een55fC0t8QbQzi6JuPl42SFuYO1o+wPf7uElJxSkRMZj/s6C3vixInYv38/cnNz73hu7ty5KCws\n",
              "rPOIjIy8n90RkYkb380HrrYKXC8ox8ZTvMZbYxxOyMEPx1MBAItGh8FGIRc5EVHzGt/NFz0DnFBR\n",
              "peEouwnpVEAWFBQgI+PvW4pt27YNTk5OcHR0vGNbhUIBOzu7Og+ZTHb/iYnIZFmY/d2F/PzPRKiq\n",
              "uSxGF8UVVZizWTu6ntDDFz0DObom4yeVSvDRqDBYm8twIiUfa4+miB3JKOhUQBYWFmLEiBEIDQ1F\n",
              "eHg4PvvsM+zcuZMn0hBRixkXqe1CZhRWYONJroXUxYJd2nsE+zhaYU7NGapEpsDb0QpvPqK9SP7H\n",
              "v11CcnaJyIkMn04FpK+vL2JiYnDu3DnExsZi7969iIiIaKZoRER3sjCT4aVbXcj97EI21MEr2bW3\n",
              "dvt4dBisObomEzMu0htRQc5QVWswe2Ms1Bxl3xfeiYaIDM4TkT5wt7NAZmEFNpzgWsh7Kaqowus1\n",
              "o+tnevqhu7+TyImIWp5EIsGHo7Trfk+nFmDN4WSxIxk0FpBEZHAszGR4qf+tLmQSKqrYhfw3H+y8\n",
              "gMzCCvg5WeG1h9uKHYdINJ72lpg3VHvlgcV/XEHiTY6yG4sFJBEZpMe7eqOV0gJZRRVYzy7kXe2/\n",
              "dBMbTqZDIgEWjQmHlTlH12TaxnbR3razslqDVzbGopq3R20UFpBEZJAUchle6h8IAPjiQCK7kPUo\n",
              "LKvC61u0o+vnerVGV787r5hBZGq0o+xQ2FrIEZtWgNXRV8WOZJBYQBKRwRrbxQseSgvcKFLh55hU\n",
              "sePonfd2nseNIhX8na0x+yGOroluaaW0xNtDtWdlf7LnCq7cKBY5keFhAUlEBqtuF5JrIW+398IN\n",
              "bDl9HdKa0bWlOa/DS3S70Z29MKCdKyrV2rOyOcrWDQtIIjJoY7t4w9PeEjeLVfjxOLuQAFBQVom5\n",
              "W88BAF6I8kdnXweRExHpH4lEgoUjQ2FnIUdceiFWHuJZ2bpgAUlEBs1cLsW0mi7klwfZhQSAd3ec\n",
              "R3axCgEu1vjPg23EjkOkt9zsLPDe8A4AgE/3XsGlrCKRExkOFpBEZPBGd/aCp70lsotV+N9f18SO\n",
              "I6rf4rOw7WwGpBJgydgIWJhxdE30b0ZEeOKB9m6oUgt4ZUMsqjjKbhAWkERk8MzlUswYoO1CfnUw\n",
              "GeWVptmFzCutxFvbtKPrF/sGIMLbXtxARAZAIpFgwcgQ2FuZ4XxGEb48kCR2JIPAApKIjMKozl7w\n",
              "crBETokKPxw3zS7kOzvOI6ekEm3cbPDyA0FixyEyGK62FnjvUe0oe/m+BJzPKBQ5kf5jAUlERsFM\n",
              "dnsXMgllldUiJ2pZu85l4pfYDMikEiweEw6FnKNrIl08Gu6Bhzu4o1ojYPbGOFRWc5T9b1hAEpHR\n",
              "GNnJCz6OVsgpqTSptZA5JSq8tS0eADC1bwDCvOzFDURkgCQSCeaPCIGDlRkuZhbhs/2JYkfSaywg\n",
              "ichomMmkmF7ThVx5MNkkupCCIGDetnjklVainbstZgwMFDsSkcFysVVg/ogQAMDn+xMRf52j7Lth\n",
              "AUlERmVkR0/4Olkht7QS3x0z/i7kzrhM7I7Pgpyja6ImMTTMA4+EtoJaI2D2xlioqk3zpLx7YQFJ\n",
              "REZFLpNixgDtCSSrDiWjVGW8XcjsYhXe3q4dXU/rH4gQT6XIiYiMw/vDO8DJ2hyXsoqxYh9H2fVh\n",
              "AUlERmdEhAf8nKyQZ8RdSEEQ8Na2c8gvq0JwK7vai6kT0f1zslHgg5pR9pcHkxCbViBuID3EApKI\n",
              "jE7dLmQSSoywC7kjNgO/n79RO7o2l/PjnKgpDQ5thWHhHrWjbN7lqi5+4hCRURoe4YHWztbIL6vC\n",
              "uqMpYsdpUjeLKvD29vMAgJkDgxDsYSdyIiLj9P6jHeBso0DCzRIs25cgdhy9wgKSiIySXCbFzJoz\n",
              "kldHJ6O4okrkRE1DEAS8sfUcCsurEOJph6n9AsSORGS0HKzNseAx7Sh75cEknEnNFzmR/mABSURG\n",
              "69FwT/i7WKPAiLqQW89cx96LN2Emk2DJmAiYyfgxTtScHurgjsc6ekIjgKPs2/CTh4iMlkwqwayB\n",
              "2rWQq6OvosjAu5BZhRV4d4d2dP3yA23Q1t1W5EREpuGdYcFwsVUgKbsUS/dcETuOXmABSURGbWiY\n",
              "BwJcrFFYXoV1R1LEjtNogiBg7pY4FFVUI8xLiRf7+Isdichk2FuZY+FjoQC0S2JOXcsTOZH4WEAS\n",
              "kVGTSSWYWduFTDbYLuTGU+nYfzkb5jIplowJh5yja6IW9UCwG0Z18oIgALM3xqG80rRH2fwEIiKj\n",
              "NzTMA4GuNiiqqMbawylix9FZZmE55v9yAQDwfw+1QZAbR9dEYnh7WDDc7BS4mlOKxX9cFjuOqFhA\n",
              "EpHRu30t5NeHk1FYbjhdSEEQMGfzORSrqtHRxx4vRHF0TSQWpaUZPhwVBgD45shVxFw13VG2TgVk\n",
              "RUUFRowYgTZt2iA8PBwPPvggEhN5ix8i0n+PhLZCGzcbFFdU45vDV8WO02DrT6Th0JVsmMulWDQ6\n",
              "HDKpROxIRCatf1tXjO2iHWW/uikWZZXGd6OChtC5Azl58mRcvnwZsbGxGD58OCZNmtQcuYiImpRU\n",
              "KsGsgW0AAN8cvorCMv3vQl4vKMcHv14EALz6UFsEutqInIiIAOCtocFopbTAtdwyfPybaY6ydSog\n",
              "LSwsMGTIEEgk2n8Bd+/eHSkpKc2Ri4ioyQ0OcUdbN1sUq6qx5oh+dyEFQcCcTXEoUVWjs68Dnuvd\n",
              "WuxIRFTDzsIMH9WMsr89moK/knNFTtTy7msN5LJlyzB8+PB6n1OpVCgqKqrzUKtN+4wlIhKXVCrB\n",
              "rAe0ayHX6nkX8seYVBxOzIGFmRSLRodxdE2kZ/q0ccG4SB8A2lF2qcq0RtmNLiAXLFiAxMRELFy4\n",
              "sN7nFy5cCKVSWecRExPT6KBERE3h4Q7uaOeu7UJ+fThZ7Dj1Sssrw39vja4HtYO/C0fXRProjSHt\n",
              "4GlvibS8cny4+5LYcVpUowrIxYsXY8uWLdi9ezesrKzq3Wbu3LkoLCys84iMjLyvsERE90sqleDl\n",
              "W13IIykoKKsUOVFdGo2A1zbFoaxSjUg/Rzzb00/sSER0F7a3jbK//+sajibmiJyo5ehcQC5duhQ/\n",
              "/fQT9uzZA3t7+7tup1AoYGdnV+chk8nuJysRUZN4KNgd7VvZoURVjdXR+tWF/N/xaziWnAtLMxk+\n",
              "Hh0GKUfXRHqtd5Aznup+a5StXbdsCnQqINPT0/HKK6+goKAA/fv3R0REBLp169Zc2YiImsXtXchv\n",
              "j6Qgr1Q/upCpuWVYuEs7Bnt9cDv4OVuLnIiIGmLu4PbwcrDE9YJyLNh1Uew4LUKnAtLLywuCICAp\n",
              "KQlnz57F2bNncfz48ebKRkTUbB4KdkNwKzuUVqrxtR50ITUaAbM3xaK8So3u/o54uruv2JGIqIGs\n",
              "FXIsGh0OAPjxeCoOXckWOVHz451oiMgkSSR/dyHXHRW/C7nuWApirubBylyGRaPDObomMjA9Apww\n",
              "sYf2H36vb45DUYX+XuWhKbCAJCKT9WCwG0I8tV3IVYfE60JezSnFR79pR9dzh7SHt2P9JycSkX6b\n",
              "M7gdfBytkFFYgQW/GvcomwUkEZksiUSCl2vuTvPdsRTklqhaPINaI+DVjbGoqNKgV6ATxtdcV46I\n",
              "DI+VuRyLx4RDIgF+PpGGA5dvih2p2bCAJCKTNrC9K8K8lCgTqQu59shVnLyWD2tzGT4axbOuiQxd\n",
              "ZGtHPNtTe+eo1zefQ2G5cY6yWUASkUm7fS3kd8euIacFu5BJ2SVY9Lv2PrpvPhIMLweOromMwauD\n",
              "2qK1szWyiiowf+cFseM0CxaQRGTy+rd1RbiXEuVVLdeFVGsEzN4YC1W1BlFBzhgX6d0i+yWi5mdp\n",
              "LsOi0WGQSIBNp9Lx56UbYkdqciwgicjkabuQf6+FzC5u/i7k19HJOJNaAFuFHB+NCoNEwtE1kTHp\n",
              "4ueISb1vG2WXGdcomwUkERGAfm1dEOFtj4oqDVYeTGrWfSXeLMaSPVcAAPOGBsPD3rJZ90dE4njl\n",
              "obbwd7HGzWIV3vvlvNhxmhQLSCIi1F0L+b/j13CzuKJZ9lOt1uCVjXGorNagX1sXjOni1Sz7ISLx\n",
              "WZjJsHhMOKQSYMuZ6/jjfJbYkZoMC0giohp927igo4+2C/nVgeZZC7kqOhmxaQWwtZDjw5EcXRMZ\n",
              "u04+Dnihjz8A4I2t8cjXk1un3i8WkERENSQSCf5Tsxbyh+PXcLOoabuQl7OK8emeBADAO8M6wF1p\n",
              "0aSvT0T66T8PtEGgqw1ySlR410hG2SwgiYhuExXkjM6+DlBVa/BlE66FrFJrMHtjLCrVGgxs54pR\n",
              "nTyb7LWJSL9ZmMmwZEw4ZFIJtp/NwG/xmWJHum8sIImIbnP7WsgfjqfiRhN1IVceTMK564VQWpph\n",
              "wchQjq6JTEy4tz1erBllv7k1XpQ7XzUlFpBERP/QO9AZXXwdUFmtwZcH7r8LeTGzCMv2aUfX7z4a\n",
              "DDc7jq6JTNGsB4LQxs0GuaWVeHuHYY+yWUASEf2DRCLBfx7UroX8MSYVWYWN70JWqTV4ZUMsqtQC\n",
              "Hgx2w4gIjq6JTJVCLsOSMRGQSSX4NS4Tv8YZ7iibBSQRUT16Bjgh0s8RldUafHEgsdGv8/n+RFzI\n",
              "LIK9lRn++1gIR9dEJi7US4lp/QIAAPO2x7fo7VObEgtIIqJ6SCQSvPygdi3kzzFpyCgo1/k1zmcU\n",
              "4rM/tcXn+8ND4GrL0TURAdMHBKGduy3ySisxb1s8BEEQO5LOWEASEd1FzwBndGvtiEq17mshK6u1\n",
              "o+tqjYDBIe4YFtaqmVISkaExl0uxZGw45FIJdsdn4RcDHGWzgCQi+he37pG9/oRuXcjP/kzApaxi\n",
              "OFqbY/4Ijq6JqK4OHkpMHxAIAHh7e3yz3f2qubCAJCL6Fz0CnNDdX9uF/Hx/w9ZCnksvxOc1Hcv5\n",
              "w0PgbKNozohEZKCm9Q9EcCs7FJRV4c2thjXKZgFJRHQPt+5Os+FkGtLzy/51W1W1GrM3xkKtEfBI\n",
              "WCs8wtE1Ed2FmUw7yjaTSbDnwg1sP5shdqQGYwFJRHQP3fyd0DPACVVqAZ/v//e1kMv3JeDyjWI4\n",
              "25hj/vCQFkpIRIaqfSs7zBygPWHvnR3nm+zmBc2NBSQRUQPcui7kxpNpSMurvwsZm1ZQe7LNByNC\n",
              "4Ght3mL5iMhwTekXgFBPJQrLq/DGlnMGMcpmAUlE1ABd/RzRO9AZ1Rqh3utCVlSp8crGWGgE4NFw\n",
              "DzwcwtE1ETWMmUyKxWPCYS6TYt+lm9hy+rrYke6JBSQRUQPdukf2xpPpd3QhP92bgMSbJXC2UeC9\n",
              "RzuIEY+IDFhbd9vaa8+++8v5+7oDVktgAUlE1EBd/BwRFaTtQt66QDgAnE7Nx6pD2tH1gsdC4MDR\n",
              "NRE1wuQof4R726O4ohqvb4nT61G2TgXkzJkz4efnB4lEgrNnzzZTJCIi/XXrupCbTqcjNbcMFVXa\n",
              "s641AjCyoyce6uAuckIiMlRymRRLxoTBXC7FgcvZ2HgyXexId6VTATl69GgcPnwYvr6+zZWHiEiv\n",
              "dfZ1QJ82LlBrBKz4MwFL/riM5OxSuNoq8M4wjq6J6P4EutrilZqT9ubvvNCo26i2BJ0KyD59+sDL\n",
              "y6u5shARGYT/1KyF3HLmOr4+fBUAsHBkKJRWZmLGIiIjMSnKHx197FGsqsaczfo5ym62NZAqlQpF\n",
              "RUV1Hmq1url2R0TUYjr6OKBfW20XUhCA0Z29MLC9m9ixiMhIyKQSLB4TDoVciuiEHPx8Ik3sSHdo\n",
              "tgJy4cKFUCqVdR4xMTHNtTsiohb1nwfaQCaVoJXSAvOGBosdh4iMTICLDV4d1BYAsPpQMqrVGpET\n",
              "1SURGtEX9fPzw7Zt2xAREXHXbVQqFVQqVZ2vzZs3D8uWLdM5JBGRPrqcVQwHazO42lqIHYWIjJBa\n",
              "I+Dz/YmY0MMX9lb6dXUHeXO9sEKhgEKhqPM1mUzWXLsjImpxbd1txY5AREZMJpVg5sAgsWPUS6cR\n",
              "9osvvggvLy+kp6dj0KBBCAwMbK5cRERERKSndOpArly5srlyEBEREZGB4J1oiIiIiEgnLCCJiIiI\n",
              "SCcsIImIiIhIJywgiYiIiEgnLCCJiIiISCcsIImIiIhIJywgiYiIiEgnLCCJiIiISCcsIImIiIhI\n",
              "JxJBEISW2tnIkSPh5+fXIvtSq9WIiYlBZGQk78F9Gx6Xu+OxqR+Py93x2NSPx+XueGzqx+Nyd2Ic\n",
              "G19fX8yaNetft2nRArIlFRUVQalUorCwEHZ2dmLH0Rs8LnfHY1M/Hpe747GpH4/L3fHY1I/H5e70\n",
              "9dhwhE1EREREOmEBSUREREQ6YQFJRERERDox2gJSoVDgnXfegUKhEDuKXuFxuTsem/rxuNwdj039\n",
              "eFzujsemfjwud6evx8ZoT6IhIiIiouZhtB1IIiIiImoeLCCJiIiISCcsIImIiIhIJwZfQCYkJKBn\n",
              "z55o06YNunbtivPnz9e73Zo1axAUFISAgAC88MILqKqqauGkLashx+XAgQOwtLRERERE7aO8vFyE\n",
              "tC1n5syZ8PPzg0QiwdmzZ++6nam9X4CGHRtTfM9UVFRgxIgRaNOmDcLDw/Hggw8iMTGx3m137tyJ\n",
              "du3aISgoCCNHjkRRUVELp205DT0uKSkpkMlkdd4zSUlJIiRuWQ899BDCwsIQERGBqKgonDlzpt7t\n",
              "TO2zpiHHxRQ/Z25Zu3YtJBIJtm3bVu/zevUZIxi4/v37C2vXrhUEQRA2btwodOnS5Y5tkpOThVat\n",
              "WgmZmZmCRqMRhg0bJnz22WctnLRlNeS47N+/XwgPD2/ZYCI7ePCgkJaWJvj6+gpnzpypdxtTfL8I\n",
              "QsOOjSm+Z8rLy4Vff/1V0Gg0giAIwooVK4S+ffvesV1xcbHg6uoqXLx4URAEQZg2bZowe/bsloza\n",
              "ohp6XK5evSoolcqWDacH8vPza/97y5YtQlhY2B3bmOJnTUOOiyl+zgiC9u9Kjx49hO7duwtbt269\n",
              "43l9+4wx6A7kzZs3cfLkSTz11FMAgFGjRiEtLe2OfwVv2rQJjz76KNzd3SGRSDBlyhT89NNPYkRu\n",
              "EQ09LqaoT58+8PLy+tdtTO39cktDjo0psrCwwJAhQyCRSAAA3bt3R0pKyh3b7d69Gx07dkS7du0A\n",
              "AC+99JJRv28aelxMlb29fe1/FxYW1h6n25niZ01Djosp0mg0mDRpElasWHHXy/Xo22eMQReQaWlp\n",
              "aNWqFeRyOQBAIpHAx8cHqampdbZLTU2Fr69v7Z/9/Pzu2MaYNPS4AEBSUhI6deqErl274osvvmjp\n",
              "qHrJ1N4vujL198yyZcswfPjwO75e3/smMzMT1dXVLRlPNHc7LgBQWlqKrl27olOnTnj//fehVqtb\n",
              "OJ04JkyYAG9vb8ybNw/ff//9Hc+b6mfNvY4LYHqfM0uXLkWvXr3QuXPnu26jb58xclH2SnqhU6dO\n",
              "SE9Ph1KpRHp6OoYMGQJnZ2eMHTtW7Gikp0z9PbNgwQIkJiZi3759YkfRK/92XFq1aoXr16/D1dUV\n",
              "eXl5ePzxx7FkyRK89tprIiRtWd999x0AYN26dZgzZw527dolciL9cK/jYmqfM/Hx8di8eTMOHTok\n",
              "dhSdGHQH0tvbu071LQgCUlNT4ePjU2c7Hx8fXLt2rfbPKSkpd2xjTBp6XOzs7KBUKgEAXl5eGDdu\n",
              "HKKjo1s8r74xtfeLLkz5PbN48WJs2bIFu3fvhpWV1R3P1/e+uX0SYKzudVwUCgVcXV0BAI6Ojnju\n",
              "uedM5j1zy8SJE7F//37k5ubW+bqpf9bc7biY2udMdHQ0UlJSEBQUBD8/P/z111+YPHkyvvzyyzrb\n",
              "6dtnjEEXkK6urujUqRP+97//AQA2b94MLy8vBAYG1tlu1KhR2LFjB7KysiAIAr766is88cQTYkRu\n",
              "EQ09LpmZmdBoNACA4uJi7Ny5Ex07dmzxvPrG1N4vujDV98zSpUvx008/Yc+ePXXWcN3u4YcfxunT\n",
              "p3Hp0iUAwBdffGH075uGHJebN2/WnlmsUqmwZcsWo3/PFBQUICMjo/bP27Ztg5OTExwdHetsZ2qf\n",
              "NQ09Lqb2OTN16lRkZmYiJSUFKSkp6N69O1atWoWpU6fW2U7vPmNEO32niVy6dEno3r27EBQUJHTu\n",
              "3FmIi4sTBEEQnn/+eWH79u21261atUrw9/cX/P39heeee06orKwUK3KLaMhxWbFihRAcHCyEhYUJ\n",
              "wcHBwjvvvFN7RqWxmjx5suDp6SnIZDLB1dVVCAgIEASB7xdBaNixMcX3TFpamgBA8Pf3F8LDw4Xw\n",
              "8HAhMjJSEARBmDdvnvDll1/Wbrt9+3ahbdu2QkBAgDB8+HChoKBArNjNrqHHZfPmzUKHDh1q3zPT\n",
              "p08XKioqxIze7FJSUoSuXbsKISEhQlhYmDBw4MDaKxuY8mdNQ4+LKX7O3K5v3761Z2Hr82cM74VN\n",
              "RERERDox6BE2EREREbU8FpBEREREpBMWkERERESkExaQRERERKQTFpBEREREpBMWkERERESkExaQ\n",
              "RERERKQTFpBEREREpBMWkERERESkExaQRERERKQTFpBEREREpJP/B788tBWlTXMwAAAAAElFTkSu\n",
              "QmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-ecbef7ca-7db7-43e2-ba0d-5158540ccaa9\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-ecbef7ca-7db7-43e2-ba0d-5158540ccaa9\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "_df_15['Shell Companies Involved'].plot(kind='line', figsize=(8, 4), title='Shell Companies Involved')\n",
              "plt.gca().spines[['top', 'right']].set_visible(False)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-710fd9a6-b529-42db-b1f2-fa6396592d4f\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAApAAAAFuCAYAAAA2xSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABGV0lEQVR4nO3deXhU9d3//+dM9nUmJASSkACBQAxZAAWpVFzAVrxlMWCrxVbr\n",
              "Qm/b+y7etQraUqgbVFtaqr+61KVqv9VebEoBpaIguKINkhAgECAkgYQQkkkGss+c3x/BKSmgDCQ5\n",
              "yczrcV25LjJzJvOaj+PklfeZc8ZiGIaBiIiIiMg5spodQERERER6FxVIEREREfGKCqSIiIiIeEUF\n",
              "UkRERES8ogIpIiIiIl5RgRQRERERr6hAioiIiIhXVCBFRERExCsqkCIiIiLiFRVIEQFg06ZNWCwW\n",
              "2trazvtnLFy4kG9+85ue76+88kp++ctfdka8Xmny5Mk8/PDDZsfoFCUlJVgsFoqLi7vsPn75y19y\n",
              "5ZVXdtnPF5HOowIp4icOHDjAzTffTGJiIpGRkSQmJnLddddRUVFhaq6CggJuuukmEhISiIyMZNCg\n",
              "Qdx8883k5eWZmqszvPXWW8yfP79LfvZtt93GLbfc0iU/W0Tk66hAiviJ6667jqioKHbs2MHx48fZ\n",
              "tm0b3/3ud7FYLKZl2rRpE2PHjqVfv358/PHHOJ1OvvjiC6655hqWLVtmWi4REflqKpAifuDYsWPs\n",
              "3r2b//7v/6ZPnz4A9OvXj1tvvZX+/ft32PaNN95g2LBhREVFcc0113Do0CHPdU1NTTz44IMMGTKE\n",
              "mJgYJkyYwLZt2847149+9CNmzpzJ0qVLGTRoEBaLBbvdzu23386iRYs82/3lL38hMzOT6OhoMjMz\n",
              "efnllz3Xfblr9cUXXyQ7O5uIiAi++c1vUl5ezlNPPcXAgQOx2+386Ec/wuVyeW5nsVhYsmQJY8aM\n",
              "ITIykrFjx/L55597rt+0aROXXXYZsbGxxMTEcPXVV/PFF190uN5isbB8+fKzrtd/7sI/dOgQ3/ve\n",
              "90hKSiI+Pp6bb76Zo0ePeq5/6qmnGDJkCFFRUfTr14/bbrvtnNfyyiuvZM6cOXzve9/DZrORnJzM\n",
              "008/DYDb7SY5OZlXX321w22WLFlCdnb2Oa3zqerq6ggPD2fLli0dLv/pT3/K1KlTPd+/8sor5OTk\n",
              "YLPZGDFiBK+//nqH7V999VXS0tKIiooiNzcXh8Nxzo9XRExmiIhfyMrKMi655BLjxRdfNLZv3264\n",
              "XK4O12/cuNEAjO9973uGw+EwHA6Hcdlllxk/+MEPPNvceuutxsSJE42ysjKjtbXVePLJJ42+ffsa\n",
              "tbW1hmEYxoIFC4zx48d7tr/iiiuMX/ziF2fMs2fPHgMw/vnPf35l7uXLlxtRUVHGhg0bjLa2NuOd\n",
              "d94xIiIijFWrVhmGYRgHDhwwAOOaa64xjhw5YjidTmP8+PHGsGHDjPvvv99oamoy9u7da9hsNuNv\n",
              "f/ub5+cCxpAhQ4zCwkKjqanJWLBggREXF2c4HA7DMAzjgw8+MD788EOjubnZqK+vN+666y4jJSXF\n",
              "aG5uPuf1OvXxNzU1GcOHDzfuvfde4/jx44bT6TRuueUWY9KkSZ71CAsLMwoKCgzDMAyn02m8//77\n",
              "Z12XW2+91Zg1a1aH+4qOjjbeffddw+VyGcuXLzesVquxd+9ewzAMY/78+cYVV1zR4WdcdNFFxtKl\n",
              "S71a5y9/3ve//33j1ltv9fysxsZGIyYmxnjzzTcNwzCMl156yUhOTjY+++wzw+VyGVu2bDGioqKM\n",
              "LVu2GIZhGB9++KERGBhorF692mhtbTVWr15thIaGnpZRRHomFUgRP1FdXW3Mnz/fGDNmjBESEmLE\n",
              "xMQY9957r9HU1GQYxr8L0cGDBz23eeqpp4z09HTP7QFj9+7dHX7u0KFDjVdffdUwDO8K5AcffGAA\n",
              "xs6dO78y97e+9S3jnnvu6XDZT3/6U+Pb3/62YRj/LjabN2/2XP+HP/zBCA8PN9ra2jyXXX/99R1+\n",
              "DmD88Y9/9HzvcrmM/v37G6+88soZc9TU1BiAkZ+fbxjG16/Xfz7+FStWGImJiYbb7fZcX15ebgBG\n",
              "WVmZsX//fiM0NNR4/fXXjbq6uq9cE8M4c4H84Q9/2GGbuLg44/XXX/es06mF8oMPPjBCQkKMY8eO\n",
              "GYZx7uv85e3ff/99Izw83JP1r3/9q5GQkOBZ86ysLOOZZ57p8PPuvPNO44477vD8Ozc3t8P1ubm5\n",
              "KpAivYR2YYv4idjYWB566CG2bt1KXV0dL774In/+85877CoGSExM9Pw7IiICp9MJ4Dn69tJLL8Vu\n",
              "t3u+Dh06RHl5udd54uPjAb72tmVlZQwZMqTDZUOHDqW0tLTDZQkJCR1y9+3bl4CAgDM+li8NHjzY\n",
              "82+r1crAgQMpKysDID8/nylTppCUlER0dLRn26qqqg4/42zr9Z/27t3LkSNHiImJ8azdiBEjCAkJ\n",
              "obS0lMGDB/P666/z0ksvkZKSwpgxY3jttde+cm3+06lZ/jPPoEGDmDhxIi+88AIAzz//PLm5uZ63\n",
              "NJzrOn9pwoQJDBgwwJPx+eef57bbbvOs+d69e7n33ns7PFdee+01Dh8+DLT/dz91/YHTvheRnksF\n",
              "UsQPhYSEMH36dCZNmnTORzt/+V7J/Px8HA6H56uhoYF58+Z5nSEtLY1hw4ad9r68/5ScnMy+ffs6\n",
              "XLZv3z5SUlK8vs//VFJS4vm32+2mtLSUAQMGAHDjjTcyZMgQduzYQX19PQcOHADAMIzzuq/+/fsz\n",
              "cODADmvncDhoamrisssuA2DatGm8/fbbVFdXc9999zFr1iz27NlzYQ/yFHfeeScvv/wyNTU1LFu2\n",
              "jDvvvNNz3fms8x133MHzzz9PcXExmzdv5o477ujweP/0pz91eKzHjx9n3bp1AAwYMKDD+gOnfS8i\n",
              "PZcKpIgfqK2tZd68eeTn59Pc3IzL5eLdd99l48aNTJgw4Zx+xsCBA5k+fTo/+clPOHjwIABOp5O3\n",
              "3nrrvE8F9Oyzz7Js2TJ+9rOfcfDgQQzDoL6+nldeeYVf/OIXQHvpefHFF9m0aRMul4v33nuPF154\n",
              "gdmzZ5/XfZ5q6dKl7Nq1i5aWFh599FFaWlo8B4HU1dURHR2NzWajpqaGe++994LuKzc3l9bWVubP\n",
              "n09dXR3QPs38+9//DkBRURHr1q3j+PHjBAYGYrPZADpMUS/U9OnTaW1t9Rw8ddVVV3muO591vvXW\n",
              "W9m+fTv/93//xxVXXNFhgnnPPffw8MMP89lnn+F2u2lubuazzz7jX//6l+e2q1evZu3atbhcLtau\n",
              "XesplyLS86lAiviB4OBgqqurufHGG4mLiyM2NpY5c+Ywd+5cr4rR3/72Ny6++GKuueYaoqKiGD58\n",
              "OH/+85/Peyp35ZVX8umnn3Lo0CHGjh1LVFQU2dnZvP3228ycORNonwT+7ne/48c//jF2u53//d//\n",
              "ZenSpeTm5p7XfZ7q7rvv5vvf/z59+vRh9erVrFu3DrvdDsCLL77IsmXLiIqKYty4cUyePPmC7isq\n",
              "KoqPP/6Y0tJSsrKyiI6O5rLLLmPz5s0AnhL75S7ze++9l1deeeW03coXIjg4mO9///usWbOG22+/\n",
              "vcMpnM5nnfv168f111/PmjVrOkwzAebMmcPChQs9R/4nJSVx3333ceLECQC++c1v8txzzzFnzhzs\n",
              "djsvvPACt99+e6c9VhHpWhbjfF/5RUR6MYvFwjvvvMOkSZPMjiIi0utoAikiIiIiXlGBFBERERGv\n",
              "BJodQETEDHr3jojI+dMEUkRERES8ogIpIiIiIl5RgRQRERERr6hAioiIiIhXurVALl26tDvvTkRE\n",
              "RES6QLcWyC8//kxEREREei/twhYRERERr6hAioiIiIhXVCBFRERExCsqkCIiIiLiFRVIEREREfGK\n",
              "CqSIiIiIeEUFUkRERES8ogIpIiIiIl5RgRQRERERr6hAioiIiIhXVCBFREREeqimVpfZEc5IBVJE\n",
              "RESkByqraeCbv9nIXz48gNttmB2nAxVIERERkR7GMAweXFVA9fFm1u2oNDvOaVQgRURERHqYFXmH\n",
              "2LK3muBAK4tzs7BaLWZH6kAFUkRERKQHOeps5uE1OwH4v0nDSO0baXKi06lAioiIiPQgC1cXUtfY\n",
              "yojEaO66fLDZcc5IBVJERESkh3h7RyVrCyoIsFr4zYxsAgN6ZlXrmalERERE/ExdYyu/enMHAD+a\n",
              "kEpmks3kRGenAikiIiLSAyxat4sqZzOpcRH8dGKa2XG+kgqkiIiIiMk+Kq7m9c/KAFg8I5vQoACT\n",
              "E301FUgREREREzW2uJi3sgCAW8alMHZwH5MTfT0VSBERERETLXmniNKaBhJsocy9Nt3sOOdEBVJE\n",
              "RETEJNvLHLzwwQEAHr0hk6jQIJMTnRsVSBERERETtLS5mbsiH7cB00YmcnV6P7MjnTMVSBERERET\n",
              "PPP+PnZXOukTEcyvrs8wO45XVCBFREREutneI06efG8vAAumZBAbGWJyIu+oQIqIiIh0I5fbYO6K\n",
              "fFpdBlenxzM1J9HsSF5TgRQRERHpRq98XEJeqYPIkEAemZ6JxWIxO5LXVCBFREREuklZTQOPv10E\n",
              "wLzJ6STaw0xOdH5UIEVERES6gWEYPLiqgMZWF2MH9+F7Y1PMjnTeVCBFREREusGKvENs2VtNcKCV\n",
              "xblZWK29b9f1l1QgRURERLrYUWczD6/ZCcA9k9JI7RtpcqILowIpIiIi0sUWri6krrGVEYnR3HV5\n",
              "qtlxLpgKpIiIiEgXWl9YydqCCgKsFn4zI5uggN5fv3r/IxARERHpoeoaW5n/xg4AZk9IJTPJZnKi\n",
              "zqECKSIiItJFFq3bRZWzmdS4COZMTDM7TqdRgRQRERHpAh8VV/P6Z2UALJ6RTWhQgMmJOo8KpIiI\n",
              "iEgna2xxMW9lAQC3jEth7OA+JifqXCqQIiIiIp1syTtFlNY0kGALZe616WbH6XReF8h169YxevRo\n",
              "Ro4cSWZmJi+//HJX5BIRERHplbaXOXjhgwMAPHpDJlGhQSYn6nyB3mxsGAa33HILmzZtIjs7m5KS\n",
              "EtLT08nNzSUqKqqrMoqIiIj0Ci1tbuauyMdtwLSRiVyd3s/sSF3C6wmkxWLB4XAAUF9fT2xsLCEh\n",
              "IZ2dS0RERKTXeeb9feyudNInIphfXZ9hdpwu49UE0mKx8Pe//53c3FwiIiKora1l5cqVBAcHn7Zt\n",
              "c3Mzzc3NHS5zuVwXllZERESkh9p7xMlT7xUDsGBKBrGRvjtg82oC2dbWxiOPPMLKlSs5ePAg7777\n",
              "Lt///veprq4+bdtFixZhs9k6fG3durXTgouIiIj0FC63wdwV+bS43FydHs/UnESzI3UprwrkF198\n",
              "weHDh5kwYQIAY8aMYcCAAWzbtu20bR944AHq6uo6fI0dO7ZzUouIiIj0IK98XEJeqYPIkEAemZ6J\n",
              "xWIxO1KX8moXdnJyMhUVFezatYuLLrqI4uJi9u3bx/Dhw0/bNiQk5LT3RgYE+M4JNEVEREQAymoa\n",
              "eGJ9EQBzJ6eTaA8zOVHX86pA9uvXj+eee47vfOc7WK1W3G43Tz31FCkpKV2VT0RERKTHMgyDB1cV\n",
              "0NDiYuygPswa6x+dyKsCCXDzzTdz8803d0UWERERkV5lRd4htuytJjjQyuIZWVitvr3r+kv6JBoR\n",
              "ERGR83DU2czDa3YCcM+kNFL7RpqcqPuoQIqIiIich4WrC6lrbGVEYjR3XZ5qdpxupQIpIiIi4qX1\n",
              "hZWsLaggwGrhNzOyCQrwr0rlX49WRERE5ALVNbYy/40dAMyekEpmks3kRN1PBVJERETEC4vW7aLK\n",
              "2UxqXARzJqaZHccUKpAiIiIi5+ij4mpe/6wMgEW5WYQG+ec5rlUgRURERM5BY4uLeSsLALhlXAqX\n",
              "psaanMg8KpAiIiIi52DJO0WU1jSQYAtl7rXpZscxlQqkiIiIyNfYXubghQ8OAPDoDZlEhQaZnMhc\n",
              "KpAiIiIiX6Glzc3cFfm4DZg2MpGr0/uZHcl0KpAiIiIiX+GZ9/exu9JJn4hgfnV9htlxegQVSBER\n",
              "EZGz2HvEyVPvFQOwYEoGsZEhJifqGVQgRURERM7A5TaYuyKfFpebq9PjmZqTaHakHkMFUkREROQM\n",
              "Xvm4hLxSB5EhgTwyPROLxWJ2pB5DBVJERETkP5TVNPDE+iIA5k5OJ9EeZnKinkUFUkREROQUhmHw\n",
              "4KoCGlpcjB3Uh1ljU8yO1OOoQIqIiIicYkXeIbbsrSY40MriGVlYrdp1/Z9UIEVEREROOups5uE1\n",
              "OwG4Z1IaqX0jTU7UM6lAioiIiJy0cHUhdY2tjEiM5q7LU82O02OpQIqIiIgA6wsrWVtQQYDVwm9m\n",
              "ZBMUoJp0NloZERER8Xt1ja3Mf2MHALMnpJKZZDM5Uc+mAikiIiJ+b9G6XVQ5m0mNi2DOxDSz4/R4\n",
              "KpAiIiLi1z4qrub1z8oAWJSbRWhQgMmJej4VSBEREfFbjS0u5q0sAOCWcSlcmhprcqLeQQVSRERE\n",
              "/NaSd4oorWkgwRbK3GvTzY7Ta6hAioiIiF/aXubghQ8OAPDoDZlEhQaZnKj3UIEUERERv9PS5mbu\n",
              "inzcBkwbmcjV6f3MjtSrqECKiIiI33nm/X3srnQSEx7Er67PMDtOr6MCKSIiIn5l7xEnT71XDMDC\n",
              "qSOIjQwxOVHvowIpIiIifsPlNpi7Ip8Wl5ur0+OZmpNodqReSQVSRERE/MarH5eQV+ogMiSQR6Zn\n",
              "YrFYzI7UK6lAioiIiF8oq2ng8fVFAMydnE6iPczkRL2XCqSIiIj4PMMweHBVAQ0tLsYO6sOssSlm\n",
              "R+rVVCBFRETE563MO8SWvdUEB1pZPCMLq1W7ri+ECqSIiIj4tKPOZh5asxOAeyalkdo30uREvZ8K\n",
              "pIiIiPi0hasLqWtsZURiNHddnmp2HJ+gAikiIiI+a31hJWsLKgiwWvjNjGyCAlR9OoNWUURERHxS\n",
              "XWMr89/YAcDsCalkJtlMTuQ7VCBFRETEJy1at4sqZzOD4yKYMzHN7Dg+RQVSREREfM5HxdW8/lkZ\n",
              "AItzswgNCjA5kW9RgRQRERGf0tjiYt7KAgBuGZfCpamxJifyPSqQIiIi4lN+v2EPpTUNJNhCmXtt\n",
              "utlxfJIKpIiIiPiM7WUOnt+yH4BHb8gkKjTI5ES+SQVSREREfEJLm5u5K/JxGzBtZCJXp/czO5LP\n",
              "UoEUERERn/Ds+/vYXekkJjyIX12fYXYcn6YCKSIiIr3e3iNOnnyvGICFU0cQGxliciLfpgIpIiIi\n",
              "vZrLbTB3RT4tLjdXp8czNSfR7Eg+TwVSREREerVXPy4hr9RBZEggj0zPxGKxmB3J56lAioiISK9V\n",
              "VtPA4+uLAJg7OZ1Ee5jJifyDCqSIiIj0SoZh8OCqAhpaXIwd1IdZY1PMjuQ3VCBFRESkV1qZd4gt\n",
              "e6sJDrSyaEYWVqt2XXcXFUgRERHpdY46m3lozU4A7pmUxpC+kSYn8i8qkCIiItLrLFxdSF1jKxkJ\n",
              "0dx1earZcfyOCqSIiIj0KusLK1lbUEGA1cLjM7MJClCd6W5er3hzczP/8z//Q1paGllZWdxyyy1d\n",
              "kUtERETkNHWNrcx/YwcAsyekkplkMzmRfwr09gbz5s3DYrGwZ88eLBYLlZWVXZFLRERE5DSL39pF\n",
              "lbOZwXERzJmYZnYcv+VVgTxx4gQvvPAC5eXlnpN09u/fv0uCiYiIiJzqo+JqXttaBsDi3CxCgwJM\n",
              "TuS/vNqFvW/fPvr06cNjjz3GJZdcwuWXX8677757xm2bm5upr6/v8OVyuToltIiIiPiXxhYX81YW\n",
              "AHDLuBQuTY01OZF/86pAtrW1cfDgQTIyMvj888/54x//yHe/+12OHDly2raLFi3CZrN1+Nq6dWun\n",
              "BRcRERH/8fsNeyitaSDBFsrca9PNjuP3vCqQKSkpWK1WZs2aBcCoUaMYPHgwBQUFp237wAMPUFdX\n",
              "1+Fr7NixnZNaRERE/Mb2MgfPb9kPwKM3ZBIVGmRyIvGqQMbFxTFx4kTWr18PwIEDBzhw4AAXXXTR\n",
              "aduGhIQQHR3d4SsgQO9VEBERkXPX0uZm7op83AZMG5nI1en9zI4knMdR2M888wx33HEHc+fOxWq1\n",
              "8uyzz5KUlNQV2URERMTPPfv+PnZXOokJD+JX12eYHUdO8rpApqamsnHjxq7IIiIiIuKx94iTJ98r\n",
              "BmDh1BHERoaYnEi+pFO3i4iISI/jchvMXZFPi8vNVcP7MjUn0exIcgoVSBEREelxXv24hLxSBxHB\n",
              "ATx6Q5bn/NPSM6hAioiISI9SXtvA4+uLAJh33UUk2sNMTiT/SQVSREREegzDMHhw1Q4aWlyMHdSH\n",
              "WWNTzI4kZ6ACKSIiIj3GyrxDbN5zlOBAK4tmZGG1atd1T6QCKSIiIj3CUWczD63ZCcA9k9IY0jfS\n",
              "5ERyNiqQIiIi0iMs/EchdY2tZCREc9flqWbHka+gAikiIiKmW19Yydr8CgKsFh6fmU1QgCpKT6b/\n",
              "OiIiImKqusZW5r+xA4DZE1LJTLKZnEi+jgqkiIiImGrxW7uocjYzOC6CORPTzI4j50AFUkREREzz\n",
              "UXE1r20tA2BxbhahQQEmJ5JzoQIpIiIipmhscTFvZQEAsy5N4dLUWJMTyblSgRQRERFT/H7DHkpr\n",
              "GkiwhTJvcrrZccQLKpAiIiLS7baXOXh+y34AHr0hk6jQIJMTiTdUIEVERKRbtbS5mbsiH7cBU3MS\n",
              "uTq9n9mRxEsqkCIiItKtnn1/H7srncSEB7FgSobZceQ8qECKiIhItymucvLke8UALJw6gtjIEJMT\n",
              "yflQgRQREZFu4XIb3L88nxaXm6uG92VqTqLZkeQ8qUCKiIhIt3j14xLySh1EBAfw6A1ZWCwWsyPJ\n",
              "eVKBFBERkS5XXtvA4+uLAJh33UUk2sNMTiQXQgVSREREupRhGDy4agcNLS7GDurDrLEpZkeSC6QC\n",
              "KSIiIl1qZd4hNu85SnCglUUzsrBateu6t1OBFBERkS5z1NnMQ2t2AjBnYhpD+kaanEg6gwqkiIiI\n",
              "dJmF/yikrrGVjIRoZk9INTuOdBIVSBEREekS6wsrWZtfQYDVwuMzswkKUO3wFfovKSIiIp2urrGV\n",
              "+W/sAOCuy1PJTLKZnEg6kwqkiIiIdLrFb+2iytnM4LgI7pmUZnYc6WQqkCIiItKpPtpXzWtbywBY\n",
              "nJtFaFCAyYmks6lAioiISKdpbHHxwMoCAGZdmsKlqbEmJ5KuoAIpIiIineb3G/Zw8FgDCbZQ5k1O\n",
              "NzuOdBEVSBEREekU28scPL9lPwCP3pBJVGiQyYmkq6hAioiIyAVraXMzd0U+bgOm5iRydXo/syNJ\n",
              "F1KBFBERkQv27Pv72F3pJCY8iAVTMsyOI11MBVJEREQuSHGVkyffKwZgwZQRxEaGmJxIupoKpIiI\n",
              "iJw3l9vg/uX5tLjcXDW8L9NGJpodSbqBCqSIiIict1c/LiGv1EFEcACP3pCFxWIxO5J0AxVIERER\n",
              "OS/ltQ08vr4IgHmT00m0h5mcSLqLCqSIiIh4zTAMHly1g4YWF2MGxTDr0oFmR5JupAIpIiIiXluZ\n",
              "d4jNe44SHGhl8YxsrFbtuvYnKpAiIiLilaPOZh5euxOAORPTGNI30uRE0t1UIEVERMQrC/9RiKOh\n",
              "lYyEaGZPSDU7jphABVJERETO2T8LK1mbX0GA1cLjM7MJClCV8Ef6ry4iIiLnpK6xlflv7gDgrstT\n",
              "yUyymZxIzKICKSIiIudk8Vu7OFLfzOC4CO6ZlGZ2HDGRCqSIiIh8rY/2VfPa1jIAFudmERoUYHIi\n",
              "MZMKpIiIiHylxhYXD6wsAGDWpSlcmhprciIxmwqkiIiIfKXfb9jDwWMNJNhCmTc53ew40gOoQIqI\n",
              "iMhZbS9z8PyW/QA8Mj2TqNAgkxNJT6ACKSIiImfU0uZm7op83AZMzUlk4kX9zI4kPYQKpIiIiJzR\n",
              "s+/vY3elk5jwIBZMyTA7jvQgKpAiIiJymuIqJ0++VwzAgikjiI0MMTmR9CQqkCIiItKBy21w//J8\n",
              "Wlxurhrel2kjE82OJD2MCqSIiIh08OrHJeSVOogIDuDRG7KwWCxmR5Ie5rwL5EsvvYTFYuGNN97o\n",
              "xDgiIiJipvLaBh5fXwTAvMnpJNrDTE4kPdF5FciSkhL+/Oc/M27cuM7OIyIiIiYxDIMHV+2gocXF\n",
              "mEExzLp0oNmRpIfyukC63W7uvPNOnnzySUJC9IZaERERX7Ey7xCb9xwlONDK4hnZWK3adS1nFujt\n",
              "DZYsWcL48eO5+OKLv3K75uZmmpubO1zmcrm8vTsRERHpBkedzTy8dicAcyamMaRvpMmJpCfzagK5\n",
              "Y8cOVqxYwS9/+cuv3XbRokXYbLYOX1u3bj3voCIiItJ1Fv6jEEdDKxkJ0cyekGp2HOnhvCqQW7Zs\n",
              "oaSkhLS0NAYNGsQnn3zC7Nmzefrpp0/b9oEHHqCurq7D19ixYzstuIiIiHSOfxZWsja/ggCrhcdn\n",
              "ZhMUoJO0yFfzahf23Xffzd133+35/sorr+See+5h+vTpp20bEhJy2nskAwICzi+liIiIdIm6xlbm\n",
              "v7kDgLsuTyUzyWZyIukN9CeGiIiIH1v81i6O1DczOC6CeyalmR1HegmvD6I51aZNmzophoiIiHS3\n",
              "j/ZV89rWMgAW52YRGqQ9hXJuNIEUERHxQ40tLh5YWQDArEtTuDQ11uRE0puoQIqIiPih32/Yw8Fj\n",
              "DSTYQpk3Od3sONLLqECKiIj4mfxyB89v2Q/AI9MziQoNMjmR9DYqkCIiIn6kpc3N/cvzcRswNSeR\n",
              "iRf1MzuS9EIqkCIiIn7k2ff3sbvSSUx4EAumZJgdR3opFUgRERE/UVzl5Mn3igFYMGUEsZEhX3ML\n",
              "kTNTgRQREfEDLrfB/cvzaXG5uWp4X6aNTDQ7kvRiKpAiIiJ+4NWPS8grdRARHMAjN2RhsVjMjiS9\n",
              "mAqkiIiIjyuvbeDx9UUAzJucTpI9zORE0tupQIqIiPgwwzB4cNUOGlpcjBkUw6xLB5odSXyACqSI\n",
              "iIgPW5l3iM17jhIcaGXxjGysVu26lgunAikiIuKjjjqbeXjtTgDmTExjSN9IkxOJr1CBFBER8VEL\n",
              "/1GIo6GVjIRoZk9INTuO+BAVSBERER/0z8JK1uZXEGC18PjMbIIC9CtfOo+eTSIiIj6mrrGV+W/u\n",
              "AOCuy1PJTLKZnEh8jQqkiIiIj1n81i6O1DczOC6CeyalmR1HfJAKpIiIiA/5aF81r20tA2BRbhah\n",
              "QQEmJxJfpAIpIiLiIxpbXDywsgCAWZemMC411uRE4qtUIEVERHzE7zfs4eCxBvpHhzJvcrrZccSH\n",
              "qUCKiIj4gPxyB89v2Q/AI9MziQoNMjmR+DIVSBERkV6upc3N/cvzcRswNSeRSRn9zI4kPk4FUkRE\n",
              "pJd79v197K50EhMexIIpGWbHET+gAikiItKLFVc5efK9YgAWTBlBbGSIyYnEH6hAioiI9FJut8Hc\n",
              "FQW0uNxcNbwv00Ymmh1J/IQKpIjIeXK5DVpdbrNjiB979ZOD/OtgLRHBATxyQxYWi8XsSOInAs0O\n",
              "ICLS27jdBn/99CBPvF1Ei8tN9gAbo1NiGJViZ3RKDPHRoWZHFD9QXtvAb97eDcC8yekk2cNMTiT+\n",
              "RAVSRMQLB4+d4P7l+Xx6oMZz2WcltXxWUuv5PskexqgUO6NSYhidYicjMZqQQH0aiHQewzB4cNUO\n",
              "GlpcjBkUw6xLB5odSfyMCqSIyDlwuw1e/riEx98uorHVRVhQAHOvHc430+LIK3WwrdTBttJaio44\n",
              "OeRo5JCjkTX5FQAEB1rJTIw+WSjbJ5WJmhbJBVi17RCb9xwlONDK4hnZWK3adS3dSwVSRORrHKg+\n",
              "wf3Lt3umjONS+/D4jBxSYsMBGBofxXcuSQbgeHMb28vay2TeyVJZ29BKXqmDvFIHL3AAgP7RoZ5d\n",
              "3qMH2hmRaNNnFss5Oeps5qE1OwGYMzGNIX0jTU4k/kgFUkTkLFxug5c+PMAT64tobnMTHhzAA9dd\n",
              "xKyxKWed+ESGBDJ+aBzjh8YB7bsaS441nCyUtWwrdbC70kllfRNv7ajkrR2VAAQFWMhItDEq2c7o\n",
              "gTGMSrYzICZMB0XIaRb+oxBHQysZCdHMnpBqdhzxUyqQIiJnsO/oce5btp28UgcA44fGsjg3m+Q+\n",
              "4V79HIvFwuC4CAbHRZA7egAADS1t5JfXeQrlttJaqo+3sL3MwfYyB3/5qASAvlEhHQpl9gA7YcGa\n",
              "UvqzfxZWsja/ggCrhcdnZhMUoJOpiDlUIEVETuFyGzy/ZT+/e2cPLW1uIkMCefC6i7h5bHKnTQPD\n",
              "gwMZlxrLuNRYoH1KWVbTyLayWvIO1rKtzMHOw/UcdTbzz51H+OfOIwAEWC1clBDV4YjvlD7hmlL6\n",
              "ibrGVua/uQOAuy5PJTPJZnIi8WcqkCIiJ+094uS+5fl8UeYA4PK0OBbPyO7y06NYLBZSYsNJiQ1n\n",
              "2sgkABpbXOw4XNdeKEsd5JXWUuVsZsehenYcqueVjw8CEBsR7Dnie1SKnZwBdiJC9NLuixa/tYsj\n",
              "9c0MjovgnklpZscRP6dXGRHxe20uN89t2c8f3tlLi8tNVEgg86/P4MZLBpg23QsLDmDMoD6MGdQH\n",
              "aJ9SHq5r6lAoCw/XcexECxt2VbFhVxUAVgsM7x/N6FNOIzQ4LkJTyl7uo33VvLa1DIBFuVk64EpM\n",
              "pwIpIn6tqNLJfcu3k19eB8CVw/uyKDeLBFvPOs2OxWIhyR5Gkj2MKTntH1fX1Oqi8HA92055L+Xh\n",
              "uiZ2VdSzq6Ke//dpKQD28CBGJds9pxHKSbYRFRpk5sMRLzS2uHhgZQEAsy5N8bz1QcRMKpAi4pda\n",
              "XW6efX8fS9/dS6vLICo0kAVTRjBjdFKvmdaFBgVw8cAYLh4Y47mssq6pwxHf+YfqcDS0srHoKBuL\n",
              "jgJgscCw+CjP+yhHpdgZ0jdS5xLsof6wYQ8HjzXQPzqUeZPTzY4jAqhAiogf2lVRz8+XbafwcD0A\n",
              "E9PjeSw3i34+8BGE/W2hTM5KYHJWAgAtbW52VdSTd8p5KctrGyk64qToiJPXP2vfLRoVGsjI5H8X\n",
              "ylHJMdjCNaU0W365gz9v2Q/AI9MzNTmWHkMFUkT8Rkubmz9tKuap94ppcxvYwoJYODWD6SN7z9TR\n",
              "W8GBVnKS7eQk2/nh+PbLqpxNnvdRbit1kF/uwNnUxpa91WzZW+257ZC+ESdPdN5eKtPiowjQlLLb\n",
              "tLrc3L88H7cBU3MSmZTRz+xIIh4qkCLiF3YcquO+5fnsqmifOn4rox+P3JBJfFTvnzp6Kz4qlG+P\n",
              "6M+3R/QH2otKUaXTUyjzSms5eKyBfUdPsO/oCZb9qxxoP0l6TrKtw5QyJiLYzIfi0559fx+7K53E\n",
              "hAexYEqG2XFEOlCBFBGf1tLm5qn39vKnTftocxvEhAfx62mZTMlO8Nmpo7eCAqxkJtnITLLxg2+0\n",
              "X3bseHP7gTllteQddLC93MHx5jY+LD7Gh8XHPLcdHBfhOY3Q6BQ7w/tFEaiTW1+w4ionf3y3GIAF\n",
              "U0YQGxliciKRjlQgRcRnFZTXcd/y7eyudAIwObM/D03LpG+Ufhl/ndjIECZl9PPsNnW5DYoqnZ5C\n",
              "ua2slv1HT3Cguv1rZd4hAMKDA8geYPMc8T0qxU6cyo9X3G6DuSsKaHG5uXJ4X6aNTDQ7kshpVCBF\n",
              "xOc0t7n447t7eeb9/bjcBn0ignl4Wib/lZ1gdrReK8BqISMxmozEaGZdOhCA2hMtfFHuYNvJT8/5\n",
              "otSBs7mNT/bX8Mn+Gs9tU/qEe85LOSrFzkUJ0foIvq/w6icH+dfBWiKCA3j0hixNyqVHUoEUEZ/y\n",
              "RZmD+5ZtZ2/VcQCuz07g11O1C7ArxEQEc9XweK4aHg+0TymLq453OI3Q3qrjlNY0UFrTwBtfHAYg\n",
              "NMhKdpK9w67veB84Ar4zlNc28Ju3dwMwb3J6l38Kksj5UoEUEZ/Q1Ori9xv28OfN+3EbEBcZzCPT\n",
              "M7k2U1PH7hJgtTC8fxTD+0dx09gUoP3zm7eX/fuI722ltdQ3tbG1pIatJf+eUibZwzqcl3JEoo3g\n",
              "QP+aUhqGwYOrdtDQ4mLMoBjPpFekJ1KBFJFe718Ha7l/+Xb2HT0BwLSRiSycMkJHCPcAtrAgJgzr\n",
              "y4RhfYH29/ftrz7RoVAWHXFyyNHIIUcja/IrgPbTD2UmRp8slDGMHmjvcZ8O1NlWbTvE5j1HCQ60\n",
              "snhGtk7sLj2aCqSI9FpNrS5+988inv/gAIYBfaNCeHR6Jt86eXoa6XmsVgtD4yMZGh/Jdy5JBsDZ\n",
              "1Ep+ed3JXd/tpbK2oZW8Ugd5pQ7gAAD9o0MZPbD99EGjB7ZPKX3lM6GPOpt5aM1OAOZMTGNI30iT\n",
              "E4l8NRVIEemVPi+p4f7l+eyvbp865o5O4lfXZ2AP19Sxt4kKDWL80DjGD40D2nfllhxr8LyXMu+g\n",
              "g92V9VTWN7GuoJJ1BZUABAVYyEi0eQ7QGZ1iJ8ke1isPOln4j0IcDa1kJEQze0Kq2XFEvpYKpIj0\n",
              "Ko0tLp5YX8RLH7VPHftFh/DYDVlMvEif0uErLBYLg+MiGBwXQe7oAQCcaG5rn1KePI3QF2W1VB9v\n",
              "YXuZg+1lDl76sARon0L/u1DGkJVkIyy4Z08p/1lYydr8CgKsFh6fma0j1KVXUIEUkV7j0/3HuH9F\n",
              "PgePNQBw48UD+OX1GdjC9PnAvi4iJJBvDInlG0NigfYpZVlN48lC2X4aoZ2H6znqbGZ94RHWFx4B\n",
              "INBq4aKE6A4H6KT0Ce8xU8r6plbmv7kDgLsuTyUzyWZyIpFzowIpIj3eieY2Hn97Ny9/fBCABFso\n",
              "j+VmeU4fI/7HYrGQEhtOSmw400YmAe3T6R2H69oL5cmPZKxyNlNwqI6CQ3W8cvL5ExsR7DmF0KgU\n",
              "OzkD7ESEmPPrcNG63Rypb2ZwXAT3TEozJYPI+VCBFJEe7aN91cxdkU9ZTSMAN41J5sH/uojoUE0d\n",
              "paOw4ADGDOrDmEF9gPYp5eG6pg6FsvBwHcdOtLBhVxUbdlUBYLXA8P7RjD5lSjk4LqLLp5Qf7avm\n",
              "ta2lACzKzfKZA4LEP6hAikiPdLy5jcVv7eKvn7T/gk2yh7EoN8tzOhiRr2OxWEiyh5FkD2NKTvvH\n",
              "ATa1uig8XM+2U04jdLiuiV0V9eyqqOf/fdr+fLOHBzEq2e45jVBOso2oTvyjpbHFxQMrCwD43qUp\n",
              "jEuN7bSfLdIdVCBFpMf5YG/71PGQo33qOOvSFOZNTu/UX+Din0KDArh4YAwXD4zxXFZZ1/TvI75L\n",
              "HRQcqsPR0MrGoqNsLDoKgMUCw+KjOpxGKDUu8rzP1fiHDXs4eKyB/tGhzJuc3imPTaQ7eVUgm5qa\n",
              "uOmmm9i5cydhYWHEx8fz9NNPM3To0K7KJyJ+xNnUymPrdnt26w2ICeM3M7I9p3cR6Qr9baFMzkpg\n",
              "clb7pxa1tLnZWVHf4byU5bWNFB1xUnTEyWtbywCIDg1kZEpM+6RyYAwjB9ixhX/9Hzn55Q7+vGU/\n",
              "AI9Mz9TbMaRX8noCOXv2bCZPnozFYuGpp57izjvvZNOmTV0QTUT8yft7jvLAinwO1zUB8INvDGTu\n",
              "temmHdwg/is40MrIZDsjk+38cHz7ZVXOJs/7KLeVOsgvd1Df1MbmPUfZvOeo57ZD4yM9hXJUip20\n",
              "+CgCTplStrrc3L88H7cBU3ISmZSh009J72QxDMM43xt//vnnzJw5k5KSknPa/mc/+xlLliw537sT\n",
              "ER9U39TKo2t28ffP26c6KX3C+c2MbM/pWkR6olaXm6JKp6dQ5pXWek4vdarIkEBGJts9pxH618Fa\n",
              "ntpYTEx4EO/87AriIkNMSC9y4S7oT/ulS5cybdq0M17X3NxMc3Nzh8tcLteF3J2I+JiNu6t4YGUB\n",
              "lfXtU8fbLhvE/dcOJzxYU0fp2YICrGQm2chMsvGDb7Rfdux4c/uBOSdPdr693MHx5jY+KK7mg+Lq\n",
              "DrdfMGWEyqP0auf9Kv3YY49RXFzMu+++e8brFy1axK9//esOl40bN+58705EfEhdQysPrdnJirxy\n",
              "AAbFhvP4zBzGDu5jcjKR8xcbGcKkjH6e3dJtLjd7jhz3TCm3ldayv/oEkzP7M21koslpRS7Mee3C\n",
              "/u1vf8vrr7/Ohg0bsNvtZ9zmTBPI+fPns3Tp0vMKKiK+YcPOIzy4qoAqZzMWC9w+fjA//9bwHv9x\n",
              "cyKdobHFRUig9byP3hbpKbyeQC5ZsoTXXnvtK8sjQEhICCEhHcfzAQH6BSHirxwNLfz6HztZte0Q\n",
              "AKlxETxxYzYXD9TUUfyH/lASX+FVgSwvL+fee+8lNTWVq666Cmgvip9++mmXhBMR37C+sJJfrNpB\n",
              "9fFmrJb2z/z9v2uG6ZM3RER6Ka8K5IABA7iAg7ZFxM/UnGhh4epCVm8/DLSf4uSJmdmMSon5mluK\n",
              "iEhPpkMdRaRLrCuo4Fdv7qD6eAtWC/zoiiHMmZimqaOIiA9QgRSRTlV9vJkFbxaytqACgGH9Inli\n",
              "Zg45yXZzg4mISKdRgRSRTmEYBmvyK1iwupCaEy0EWC3cfcUQ/nfiUEICNXUUEfElKpAicsGOOpuZ\n",
              "/8YO3i6sBCC9fxRPzMwha4DN5GQiItIVVCBF5LwZhsHq7YdZsLoQR0MrgVYLP7lqKD+5aijBgVaz\n",
              "44mISBdRgRSR81JV38Qv3tjBOzuPAJCREM0TN2YzIlFTRxERX6cCKSJeMQyDVdsO8et/7KSusZWg\n",
              "AAv/e3Uad185hKAATR1FRPyBCqSInLPKuiYeXFXAe7urAMhMiua3N+aQ3j/a5GQiItKdVCBF5GsZ\n",
              "hsGyf5Xz8JqdOJvaCA6wMmdSGrMnpGrqKCLih1QgReQrHXY08sDKAt7fcxSAnAE2nrgxh2H9okxO\n",
              "JiIiZlGBFJEzMgyDv39WxqNrd+FsbiM40MrPrhnGnd8cTKCmjiIifk0FUkROc8jRyLwV+WzZWw3A\n",
              "qBQ7T8zMZmi8po4iIqICKSKnMAyDv20t5bG1uzjR4iIk0MrPvzWc2785mACrxex4IiLSQ6hAiggA\n",
              "ZTUNzF2Rz0f7jgFwycAYHp+ZTWrfSJOTiYhIT6MCKeLn3G6Dv356kMVv7aahxUVokJX7vp3ObZcN\n",
              "0tRRRETOSAVSxI8dPHaC+5fn8+mBGgDGDurD4zOzGRQXYXIyERHpyVQgRfyQ223w8sclPP52EY2t\n",
              "LsKCApg3OZ3vjxuIVVNHERH5GiqQIn7mQPUJ5i7PZ2tJ+9RxXGofHp+RQ0psuMnJRESkt1CBFPET\n",
              "LrfBSx8e4Lf/LKKp1U1EcADzrruIWWNTNHUUERGvqECK+IF9R49z37Lt5JU6ABg/NJbFudkk99HU\n",
              "UUREvKcCKeLDXG6D57fsZ8k7e2hucxMZEsiD113EzWOTsVg0dRQRkfOjAinio4qrnPx8WT5flDkA\n",
              "uDwtjsUzskmyh5kbTEREej0VSBEf0+Zy89yW/fxhw15a2txEhQQy//oMbrxkgKaOIiLSKVQgRXxI\n",
              "UaWT+5ZvJ7+8DoCrhvflsdwsEmyaOoqISOfx2QLZ5nITGGA1O4ZIt2h1uXn2/X0sfXcvrS6D6NBA\n",
              "fjVlBDNGJ2nqKCIinc5nC+SVv91ETHgwo1LsjE6JYVSKnZQ+4fplKj5nV0U9P1+2ncLD9QBMTI/n\n",
              "sdws+kWHmpxMRER8lU8WyIq6Rspr278KDtXxyscHAYiNaC+Uo04WypwBdiJCfHIJxA+0tLn506Zi\n",
              "/r+NxbS6DGxhQSycmsH0kZo6iohI1/LJ9tQ/OpQP511N3sFatpU6yCutpfBwHcdOtLBhVxUbdlUB\n",
              "YLXA8P7RjD5ZKken2BkcF6FfvtLjFR6u4+fL8tlV0T51/FZGPx65IZP4KE0dRUSk6/lkgbRYLCTZ\n",
              "w0iyhzElJxGAplYXhYfr2VbaXiq3ldZyuK6JXRX17Kqo5/99WgqAPTyIUclfFsoYcpJtRIUGmflw\n",
              "RDxa2tw89d5e/rRpH21ug5jwIH49LZMp2Qn6w0dERLqNTxbIMwkNCuDigTFcPDDGc1llXRPbSmvJ\n",
              "O1kq8w/V4WhoZWPRUTYWHQXAYoFh8VGe91KOHmgnNS5SH/0m3a6gvI77lm9nd6UTgMmZ/XloWiZ9\n",
              "o0JMTiYiIv7GbwrkmfS3hTI5K4HJWQlA+3RnV0W9p1DmldZSXttI0REnRUecvP5ZGQDRoYGMTIlh\n",
              "VLKd0QNjGDnAji1cU0rpGs1tLv747l6eeX8/LrdBn4hgHp6WyX9lJ5gdTURE/JRfF8j/FBxoJSfZ\n",
              "Tk6ynR+Ob7+sytl0cpd3e6HML3dQ39TG5j1H2bznqOe2Q+MjPYVyVIqdtPgoAjSllAu0vczBz5dt\n",
              "Z2/VcQCuz07g11NHEBupqaOIiJhHBfJrxEeF8u0R/fn2iP5A+/n2iiqdHaaUB481UFx1nOKq4yz7\n",
              "VzkAkSGB5CTbPKcQGpUcQ0xEsJkPRXqRplYXf9iwl+c278NtQFxkMI9Mz+TaTE0dRUTEfCqQXgoK\n",
              "sJKZZCMzycYPvtF+2bHjze1TyrJa8g462F7u4HhzGx8WH+PD4mOe2w6Oi/CcRmh0ip3h/aJ0snM5\n",
              "TV5pLfct286+oycAmDYykYVTRugPEBER6TFUIDtBbGQIkzL6MSmjHwAut0FRpdNTKLeV1bL/6AkO\n",
              "VLd/rcw7BEB4cADZA76cUrZPKuO0a9JvNbW6+N0/i3jhgwO4DegbFcKj0zP51snpt4iISE+hAtkF\n",
              "AqwWMhKjyUiMZtalAwGoPdHCF+UOth2sZVuZgy9KHTib2/hkfw2f7K/x3DalT/gp56WMIT0hiiBN\n",
              "KX3e5yU13L88n/3V7VPH3NFJ/Or6DOzhmjqKiEjPowLZTWIigrlqeDxXDY8H2qeUxVXHO5xGaG/V\n",
              "cUprGiitaeCNLw4DEBpkJTvJ3mHXd7w+os5nNLa4eGJ9ES99dADDgH7RITx2QxYTL+pndjQREZGz\n",
              "UoE0SYDVwvD+UQzvH8VNY1MAqGtsZXuZw1Mot5XWUt/UxtaSGraW/HtKmWQP6/AZ3yMSbQQHakrZ\n",
              "23y6/xj3r8jn4LEGAG68eAC/vD4DW5hOCSUiIj2bCmQPYgsLYsKwvkwY1hcAt9tgf/WJDoWy6IiT\n",
              "Q45GDjkaWZNfAbSffigzMdrzXsrRA+0k2MLMfCjyFRpa2nj87SL+8lEJAAm2UB7LzfJMp0VERHo6\n",
              "FcgezGq1MDQ+kqHxkXznkmQAnE2t5JfXndz13V4qaxtaySt1kFfqAA4A7Z8HPnpg++mDRg9sn1KG\n",
              "BgWY92AEgI/2VTN3RT5lNY0A3DQmmQf/6yKi9XGZIiLSi6hA9jJRoUGMHxrH+KFxABiGQcmxBs97\n",
              "KfMOOthdWU9lfRPrCipZV1AJQFCAhYxEm+dk56NT7CTZw/T5yd3keHMbi9/axV8/af/M9SR7GIty\n",
              "szzTZhERkd5EBbKXs1gsDI6LYHBcBLmjBwBwormtfUp58jRCX5TVUn28he1lDraXOTy7TvtGhXQ4\n",
              "4jsryUZYsKaUne3D4mruX57PIUf71HHWpSnMm5xOlKaOIiLSS6lA+qCIkEC+MSSWbwyJBdqnlGU1\n",
              "jScLZftphHYerueos5n1hUdYX3gEgECrhYsSojscoJPSJ1xTyvPkbGrlsXW7eW1r+9RxQEwYv5mR\n",
              "7Zkei4iI9FYqkH7AYrGQEhtOSmw400YmAe2nj9lxuK69UJ78SMYqZzMFh+ooOFTHKx8fBCA2Ithz\n",
              "CqFRKXZyBtiJCNHT5uts3nOUeSvyOVzXBMAPvjGQudema+1ERMQn6LeZnwoLDmDMoD6MGdQHaJ9S\n",
              "Hq5r6lAoCw/XcexECxt2VbFhVxUAVgsM7x99yq5vO4PjIjSlPKm+qZVH1+zi75+XAe0nhv/NjGzP\n",
              "NFhERMQXqEAK0D6lTLKHkWQPY0pOItD+0XqFh+vZdspphA7XNbGrop5dFfX8v0/bd83aw4MYlfzv\n",
              "91LmJNv88v19G3dX8cDKAirr26eOt102iPuvHU54sP43ExER36LfbHJWoUEBXDwwhosHxnguq6xr\n",
              "+vcR36UOCg7V4WhoZWPRUTYWHQXAYoFh8VGe91KOHmgnNS4Sq9U3p5R1Da08tGYnK/LKARgUG87j\n",
              "M3MYO7iPyclERES6hgqkeKW/LZTJWQlMzkoAoKXNzc6K+g7npSyvbaToiJOiI05e/6x9V250aCAj\n",
              "U2I8pxEaOcCOLbz3Tyk37DzCg6sKqHI2Y7HA7eMH8/NvDdfR7CIi4tNUIOWCBAdaGZlsZ2SynR+O\n",
              "b7+sytnkeR/ltlIH+eUO6pva2LznKJv3HPXcdmh8pKdQjkqxkxYfRUAvmVI6Glr49T92smrbIQBS\n",
              "4yJ44sZsLh6oqaOIiPg+FUjpdPFRoXx7RH++PaI/AK0uN0WVTk+hzCut5eCxBoqrjlNcdZxl/2rf\n",
              "9RsZEkhOss1zCqFRyTHERASb+VDOaH1hJb9YtYPq481YLXDX5an83zXD9Ek/IiLiN1QgpcsFBVjJ\n",
              "TLKRmWTjB99ov+zY8eb2A3NOnux8e7mD481tfFh8jA+Lj3luOzguwnMaodEpdob3iyIwwGrK46g5\n",
              "0cLC1YWs3n4YaJ+gPjEzm1EpMV9zSxEREd+iAimmiI0MYVJGPyZl9AOgzeVmz5HjninlttJa9lef\n",
              "4MDJr5V57buKw4MDyB5g8xzxPSrFTlxkSJfnfauggvlv7qD6eAtWC/zoiiHMmZimqaOIiPglFUjp\n",
              "EQIDrGQkRpORGM0t4wYCUHuihS/KHJ4DdL4oa59SfrK/hk/213hum9In3HNeylEpdi5KiCaok6aU\n",
              "1cebWfBmIWsLKgAY1i+SJ2bmkJNs75SfLyIi0ht5XSD37t3LrbfeSnV1NTabjb/85S+MGDGiK7KJ\n",
              "n4uJCOaq9HiuSo8HwOU2KK467jmN0LZSB3urjlNa00BpTQNvfNG+azk0yEp2kr3Dru/46FCv7tsw\n",
              "DNYWVPCrNwupOdFCgNXCj68cwv9cPZSQQE0dRUTEv3ldIH/0ox8xe/ZsbrvtNpYvX85tt93GZ599\n",
              "1hXZRDoIsFoY3j+K4f2juGlsCgB1ja1sL3N02PVd39TG1pIatpb8e0qZZA/r8BnfIxJtBAeeeUp5\n",
              "1NnM/Dd28HZhJQDp/aP47Y05ZCbZuv5BioiI9AIWwzCMc924qqqKoUOHUlNTQ2BgIIZhkJCQwAcf\n",
              "fMDQoUO/9vY/+9nPWLJkyQUFFvkqbrfB/uoTJwtle6ksOuLkP5/lwYFWMhOjTxbK9pOd948OZfX2\n",
              "wyxYXYijoZVAq4WfXDWUn1w19KxlU0RExB95NYEsKysjISGBwMD2m1ksFlJSUigtLT2tQDY3N9Pc\n",
              "3NzhMpfLdYFxRb6a1WphaHwkQ+Mj+c4lyQA4m1rJL69r/5zvk++prG1oJa/UQV6pAzgAQEx4ELUN\n",
              "rQBkJETzxI3ZjEjU1FFEROQ/ddlBNIsWLeLXv/51h8vGjRvXVXcnclZRoUGMHxrH+KFxQPv7G0uO\n",
              "NZwslO2nEdpdWU9tQytBARb+9+o07r5ySKcdiCMiIuJrumwX9pkmkPPnz2fp0qWdk1ykE51obmNn\n",
              "RT2J9jCS7GFmxxEREenRvBqxxMfHM3r0aP76178CsGLFCgYMGHDG9z+GhIQQHR3d4SsgQEevSs8U\n",
              "ERLImEF9VB5FRETOgde7sJ999lluu+02HnvsMaKjo3nppZe6IpeIiIiI9FBeF8jhw4fz8ccfd0UW\n",
              "EREREekFdJSAiIiIiHhFBVJEREREvKICKSIiIiJeUYEUEREREa+oQIqIiIiIV1QgRURERMQrKpAi\n",
              "IiIi4hUVSBERERHxigqkiIiIiHjFYhiG0V13lpuby6BBg7rlvlwuF1u3bmXs2LH6DO5TaF3OTmtz\n",
              "ZlqXs9PanJnW5ey0NmemdTk7M9Zm4MCBzJkz5yu36dYC2Z3q6+ux2WzU1dURHR1tdpweQ+tydlqb\n",
              "M9O6nJ3W5sy0LmentTkzrcvZ9dS10S5sEREREfGKCqSIiIiIeEUFUkRERES84rMFMiQkhAULFhAS\n",
              "EmJ2lB5F63J2Wpsz07qcndbmzLQuZ6e1OTOty9n11LXx2YNoRERERKRr+OwEUkRERES6hgqkiIiI\n",
              "iHhFBVJEREREvNLrC+TevXu57LLLGDZsGGPGjKGwsPCM273wwgukpaUxZMgQ7rrrLlpbW7s5afc6\n",
              "l3XZtGkTYWFhjBw50vPV2NhoQtru89Of/pRBgwZhsVj44osvzrqdvz1f4NzWxh+fM01NTUyfPp1h\n",
              "w4aRk5PDNddcQ3Fx8Rm3XbNmDenp6aSlpZGbm0t9fX03p+0+57ouJSUlBAQEdHjO7Nu3z4TE3etb\n",
              "3/oW2dnZjBw5kssvv5xt27adcTt/e605l3Xxx9eZL7300ktYLBbeeOONM17fo15jjF7uqquuMl56\n",
              "6SXDMAxj2bJlxiWXXHLaNvv37zcSEhKMiooKw+12G1OmTDGeeuqpbk7avc5lXTZu3Gjk5OR0bzCT\n",
              "vf/++0ZZWZkxcOBAY9u2bWfcxh+fL4Zxbmvjj8+ZxsZGY+3atYbb7TYMwzCefPJJ44orrjhtO6fT\n",
              "acTHxxu7du0yDMMwfvKTnxg///nPuzNqtzrXdTlw4IBhs9m6N1wPUFtb6/n3ypUrjezs7NO28cfX\n",
              "mnNZF398nTGM9v9XvvGNbxjjxo0zVq1addr1Pe01pldPIKuqqvj888+55ZZbAJgxYwZlZWWn/RW8\n",
              "fPlypk6dSv/+/bFYLPz3f/83r732mhmRu8W5ros/mjBhAgMGDPjKbfzt+fKlc1kbfxQaGsp1112H\n",
              "xWIBYNy4cZSUlJy23VtvvcWoUaNIT08H4Mc//rFPP2/OdV38ld1u9/y7rq7Os06n8sfXmnNZF3/k\n",
              "dru58847efLJJ896up6e9hrTqwtkWVkZCQkJBAYGAmCxWEhJSaG0tLTDdqWlpQwcONDz/aBBg07b\n",
              "xpec67oA7Nu3j9GjRzNmzBj+9Kc/dXfUHsnfni/e8vfnzNKlS5k2bdppl5/peVNRUUFbW1t3xjPN\n",
              "2dYF4MSJE4wZM4bRo0fz0EMP4XK5ujmdOX7wgx+QnJzM/PnzefXVV0+73l9fa75uXcD/XmeWLFnC\n",
              "+PHjufjii8+6TU97jQk05V6lRxg9ejTl5eXYbDbKy8u57rrriIuL4zvf+Y7Z0aSH8vfnzGOPPUZx\n",
              "cTHvvvuu2VF6lK9al4SEBA4dOkR8fDw1NTV897vf5Xe/+x3333+/CUm71yuvvALAyy+/zNy5c1m3\n",
              "bp3JiXqGr1sXf3ud2bFjBytWrGDz5s1mR/FKr55AJicnd2jfhmFQWlpKSkpKh+1SUlI4ePCg5/uS\n",
              "kpLTtvEl57ou0dHR2Gw2AAYMGMDNN9/Mli1buj1vT+Nvzxdv+PNz5re//S0rV67krbfeIjw8/LTr\n",
              "z/S8OXVPgK/6unUJCQkhPj4egD59+nD77bf7zXPmS7feeisbN27k2LFjHS7399eas62Lv73ObNmy\n",
              "hZKSEtLS0hg0aBCffPIJs2fP5umnn+6wXU97jenVBTI+Pp7Ro0fz17/+FYAVK1YwYMAAhg4d2mG7\n",
              "GTNmsHr1aiorKzEMg2eeeYabbrrJjMjd4lzXpaKiArfbDYDT6WTNmjWMGjWq2/P2NP72fPGGvz5n\n",
              "lixZwmuvvcY777zT4T1cp7r22mvJy8tj9+7dAPzpT3/y+efNuaxLVVWV58ji5uZmVq5c6fPPGYfD\n",
              "weHDhz3fv/HGG8TGxtKnT58O2/nba825rou/vc7cfffdVFRUUFJSQklJCePGjeO5557j7rvv7rBd\n",
              "j3uNMe3wnU6ye/duY9y4cUZaWppx8cUXG/n5+YZhGMYdd9xhvPnmm57tnnvuOSM1NdVITU01br/9\n",
              "dqOlpcWsyN3iXNblySefNDIyMozs7GwjIyPDWLBggeeISl81e/ZsIykpyQgICDDi4+ONIUOGGIah\n",
              "54thnNva+ONzpqyszACM1NRUIycnx8jJyTHGjh1rGIZhzJ8/33j66ac927755pvG8OHDjSFDhhjT\n",
              "pk0zHA6HWbG73Lmuy4oVK4wRI0Z4njP/8z//YzQ1NZkZvcuVlJQYY8aMMTIzM43s7Gxj4sSJnjMb\n",
              "+PNrzbmuiz++zpzqiiuu8ByF3ZNfY/RZ2CIiIiLilV69C1tEREREup8KpIiIiIh4RQVSRERERLyi\n",
              "AikiIiIiXlGBFBERERGvqECKiIiIiFdUIEVERETEKyqQIiIiIuIVFUgRERER8YoKpIiIiIh4RQVS\n",
              "RERERLzy/wO8UFVlqxVFLwAAAABJRU5ErkJggg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-710fd9a6-b529-42db-b1f2-fa6396592d4f\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-710fd9a6-b529-42db-b1f2-fa6396592d4f\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x78367ecc3eb0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">2-d categorical distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "import pandas as pd\n",
              "plt.subplots(figsize=(8, 8))\n",
              "df_2dhist = pd.DataFrame({\n",
              "    x_label: grp['Country'].value_counts()\n",
              "    for x_label, grp in _df_16.groupby('Transaction ID')\n",
              "})\n",
              "sns.heatmap(df_2dhist, cmap='viridis')\n",
              "plt.xlabel('Transaction ID')\n",
              "_ = plt.ylabel('Country')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-d0e504a9-af17-4001-8b85-f3c6adf32429\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAooAAAKfCAYAAADpb+mkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABDSklEQVR4nO3de3wU9b3/8fckwGIlCVDBgCGJEC5KIBfEKsilityqAQkVqqiR\n",
              "W5BSsJFCU0RAW2OrtWKVA2iKiMqdelDr5eCvaijVSkOAIiABQgBF5JZFNEuSnd8fyOo2A2RhhiXD\n",
              "63ke83iwM9/d7zcz7enn8f7OfMcwTdMUAAAA8F8iwj0AAAAAXJgoFAEAAGCJQhEAAACWKBQBAABg\n",
              "iUIRAAAAligUAQAAYIlCEQAAAJYoFAEAAGCJQhEAAACWKBQBAADCZPz48UpMTJRhGCoqKrJsU1JS\n",
              "op49eyomJkapqanVjufn56t169Zq1aqVRo0apYqKihodqwkKRQAAgDAZPHiwVq9erYSEhFO2iY6O\n",
              "1m9/+1u98sor1Y7t3LlTU6dOVUFBgYqLi/XFF19o7ty5ZzxWUxSKAAAAYdK9e3fFxcWdtk3jxo11\n",
              "ww036NJLL612bNmyZcrIyFBsbKwMw9CYMWO0cOHCMx6rqTohtQYAAMBp+Xw++Xy+oH0ej0cej8f2\n",
              "vkpLS4PSyMTERJWWlp7xWE3VikKx5cwnwz0EfGvHhJxwDwEAgDPy72sTtr7zZt+hGTNmBO2bNm2a\n",
              "pk+fHp4BnYNaUSgCAADUFrm5ucrJCQ5WnEgTJSk+Pl7bt28PfC4pKVF8fPwZj9UU9ygCAADX8Yfx\n",
              "/zwej6Kjo4M2pwrFzMxMrVy5Uvv27ZNpmpo9e7aGDh16xmM1RaEIAAAQJtnZ2YqLi9OePXvUp08f\n",
              "JSUlSZJGjhyplStXSpK+/vprxcXF6ac//ak++eQTxcXFKTc3V5LUsmVLzZgxQ127dlVSUpKaNGmi\n",
              "7OzsMx6rKcM0TdPGv9cR3KN44eAeRQBAbVC5LylsfdeJLQ5b33bjHkUAAOA6VaY/bH27qbhi6hkA\n",
              "AACW3FT0AgAASJL8uuDvrKsVSBQBAABgiUIRAAAAlph6BgAAruNX+B5mcRMSRQAAAFgiUQQAAK5T\n",
              "deEvE10rkCgCAADAEokiAABwHZbHsQeJIgAAACxRKAIAAMASU88AAMB1qph6tgWJIgAAACyRKAIA\n",
              "ANfhYRZ7kCgCAADAEoUiAAAALDH1DAAAXIc3s9iDRBEAAACWSBQBAIDr+MM9AJcgUQQAAIAlEkUA\n",
              "AOA6LLhtDxJFAAAAWKJQBAAAgCWmngEAgOtUMfNsCxJFAAAAWCJRBAAArsPyOPYgUQQAAIAlCkUA\n",
              "AABYYuoZAAC4TpWMcA/BFUgUAQAAYIlEEQAAuI6f5XFsQaIIAAAASySKAADAdbhH0R4kigAAALBE\n",
              "oQgAAABLTD0DAADXYerZHiSKAAAAsESiCAAAXMdvkijagUQRAAAAligUAQAAYImpZwAA4Do8zGIP\n",
              "EkUAAABYIlEEAACuU0UWZgvHCsUNGzac9njHjh2d6hoAAAA2cKxQHDBgwCmPGYahHTt2ONU1AAC4\n",
              "yLE8jj0cKxR37tzp1E8DAADgPHCsUDx27JguvfRSeb1ey+PR0dFOdQ0AAAAbOFYoduvWTYWFhWrY\n",
              "sKEMw5BpmoFjhmGoqqrKqa4BAMBFjuVx7OFYoVhYWChJ8vv9TnUBAAAABzn+7Pgjjzyiffv2Be2b\n",
              "O3eu090CAICLWJUZEbbNTRz/a5544gl1795dmzdvDuybPXu2090CAADgHDleKLZs2VJz585V//79\n",
              "9cEHH0hS0P2KAAAAuDA5/mYWwzDUs2dPvfbaa7rtttv08MMPyzC4wRQAADjHz5tZbOF4oXgyPUxO\n",
              "Ttb777+vW265RVu3bnW6WwAAAJwjxwvFF154IfDv5s2b64MPPtCKFStO2d7n88nn8wXtMysrZdTh\n",
              "tdQAAKBmWB7HHo7nsikpKZJOFIBer1d+v18DBw48Zfu8vDzFxMQEbUf+712nhwkAAID/4nih+OGH\n",
              "H+qqq67SD37wAzVq1CiwnUpubq7KysqCtoY33+T0MAEAgIuwPI49HJ/PnTBhgl544QWNGTNGH3zw\n",
              "gZ5++mnVr1//lO09Ho88Hk/QPqadAQAAzj/Hy96Kigr96Ec/UmVlpaKiojRlyhQtWrTI6W4BAABw\n",
              "jhyP6up8mwb+8Ic/VGFhoVq0aKEvv/zS6W4BAMBFzM/DLLZwvFD82c9+poMHD+o3v/mNevTooYqK\n",
              "Cv32t791ulsAAACcI0cLRb/fry5duuiHP/yhevfurUOHDqm8vFxRUVFOdgsAAC5yVSy4bQtHz2JE\n",
              "RIRGjx4d+Fy3bl2KRAAAgFrC8XK7devWKi4udrobAAAA2MzxexQPHTqk1NRUdenSRQ0aNAjsP93b\n",
              "WQAAAM6F29YzDBfHC8V77rlH99xzjyTJMIzAu58BAABwYXOsUPzmm2/0wgsvqFGjRrr99ts1adIk\n",
              "vfXWW2rXrp1mzpzpVLcAAADy8zCLLRw7i6NGjdIbb7yhuXPnqk+fPjpy5Igef/xxXXnllRozZoxT\n",
              "3QIAANQa48ePV2JiogzDUFFR0Snb5efnq3Xr1mrVqpVGjRqliooKSdK8efOUmpoa2C677DINGjRI\n",
              "klRSUqLIyMig49u3bw9pfI4lioWFhfrkk09UXl6uZs2a6e2331ZERIT69eunDh06ONUtAACAqsza\n",
              "seD24MGDNWnSJN1www2nbLNz505NnTpVhYWFuvzyyzVgwADNnTtXP//5z3Xvvffq3nvvDbRNTk7W\n",
              "nXfeGfgcFRV12gL0TBxLFE++r7l+/fpKTExURMR3XdWtW9epbgEAAGqN7t27Ky4u7rRtli1bpoyM\n",
              "DMXGxsowDI0ZM0YLFy6s1u6jjz7S/v37lZGRYdv4HEsUy8vLtXHjRpmmGfRv6cT9iwAAAG7k8/nk\n",
              "8/mC9nk8nkCIFqrS0lIlJCQEPicmJqq0tLRau/z8fN11111BgdyxY8fUuXNnVVVVaeDAgZoyZYoi\n",
              "IyNr3LejD7N8v6L9/r8No3bEwQAAoHYK55tZ8vLyNGPGjKB906ZN0/Tp0x3r89ixY1q0aJE+/PDD\n",
              "wL5mzZpp7969atq0qQ4dOqQhQ4boj3/8oyZNmlTj33WsUCwpKXHqpwEAAC5Yubm5ysnJCdp3tmmi\n",
              "JMXHxwc9hFJSUqL4+PigNkuXLlX79u119dVXB/XZtGlTSVLjxo01fPhwvfLKKyEVijw7DgAAXMdv\n",
              "RoRt83g8io6ODtrOpVDMzMzUypUrtW/fPpmmqdmzZ2vo0KFBbfLz8zVixIigffv37w88He3z+bRi\n",
              "xQqlpaWF1DeFIgAAQJhkZ2crLi5Oe/bsUZ8+fZSUlCRJGjlypFauXClJatmypWbMmKGuXbsqKSlJ\n",
              "TZo0UXZ2duA3tm7dqqKiIg0ZMiTot1evXq20tDSlpKQoPT1dsbGxmjJlSkjjM8xa8KqUljOfDPcQ\n",
              "8K0dE3LO3AgAgDBbXNw5bH0PSfo4bH3bzfFX+AEAAJxv4XyYxU04iwAAALBEoggAAFyntryZ5UJH\n",
              "oggAAABLFIoAAACwxNQzAABwHT9ZmC04iwAAALBEoggAAFynyiQLswNnEQAAAJZIFAEAgOv4xfI4\n",
              "diBRBAAAgCUKRQAAAFhi6hkAALgOD7PYg7MIAAAASySKAADAdarIwmzBWQQAAIAlCkUAAABYYuoZ\n",
              "AAC4jt9kHUU7kCgCAADAEokiAABwHR5msQdnEQAAAJZIFAEAgOv4WXDbFpxFAAAAWKJQBAAAgCWm\n",
              "ngEAgOtUieVx7ECiCAAAAEskigAAwHV4mMUenEUAAABYolAEAACAJaaeAQCA6/Awiz1IFAEAAGCJ\n",
              "RBEAALgOD7PYg7MIAAAASySKAADAdapIFG3BWQQAAIAlCkUAAABYYuoZAAC4jp/lcWxBoggAAABL\n",
              "JIoAAMB1eJjFHpxFAAAAWKJQBAAAgKVaMfW8Y0JOuIeAb7Wc+WS4h4Dv4b8bAGDNb/Iwix1IFAEA\n",
              "AGCpViSKAAAAoagiC7MFZxEAAACWSBQBAIDrcI+iPUgUAQAAYIlCEQAAAJaYegYAAK7jJwuzBWcR\n",
              "AAAAlkgUAQCA61TxMIstSBQBAABgiUIRAAAAlph6BgAArsM6ivYgUQQAAIAlEkUAAOA6fpMszA6c\n",
              "RQAAAFgiUQQAAK5TJe5RtAOJIgAAACxRKAIAAMASU88AAMB1WB7HHiSKAAAAsESiCAAAXIflcezB\n",
              "WQQAAIAlCkUAAABYYuoZAAC4jp91FG1BoggAAABLJIoAAMB1qlgexxYkigAAAGEyfvx4JSYmyjAM\n",
              "FRUVnbJdfn6+WrdurVatWmnUqFGqqKiQJL333nu65JJLlJqaGti++eabM36vpigUAQCA6/jNiLBt\n",
              "oRg8eLBWr16thISEU7bZuXOnpk6dqoKCAhUXF+uLL77Q3LlzA8fbtm2roqKiwHbJJZfU6Hs1QaEI\n",
              "AAAQJt27d1dcXNxp2yxbtkwZGRmKjY2VYRgaM2aMFi5ceMbfPtvvfR+FIgAAgI18Pp+8Xm/Q5vP5\n",
              "zvr3SktLgxLHxMRElZaWBj5v375d6enp6ty5s2bNmlXj79UEhSIAAHAdv2mEbcvLy1NMTEzQlpeX\n",
              "58jfmZ6erj179qiwsFB//etfNXv2bC1ZssS236dQBAAAsFFubq7KysqCttzc3LP+vfj4eO3atSvw\n",
              "uaSkRPHx8ZKk6OhoxcTESJLi4uL0s5/9TAUFBWf8Xk1RKAIAANfxywjb5vF4FB0dHbR5PJ6z/lsy\n",
              "MzO1cuVK7du3T6Zpavbs2Ro6dKgk6fPPP5ff75ckHT16VK+//rrS0tLO+L2aolAEAAAIk+zsbMXF\n",
              "xWnPnj3q06ePkpKSJEkjR47UypUrJUktW7bUjBkz1LVrVyUlJalJkybKzs6WJC1fvlwdOnRQSkqK\n",
              "rrvuOt1888269957z/i9mjJM0zRt/Hvhci1nPhnuIeB7dkzICfcQAOCCdOdHo8LW98s/ei5sfduN\n",
              "N7MAAADX8fNmFlsw9QwAAABLJIoAAMB1Qn1DCqxxFgEAAGCJQhEAAACWmHoGAACuw8Ms9iBRBAAA\n",
              "gCUSRQAA4Dp+kSjagUQRAAAAlkgUAQCA63CPoj1IFAEAAGCJQhEAAACWmHoGAACuw9SzPRxNFLdt\n",
              "26Z+/fqpefPmaty4cWADAADAhc/RQnHUqFHKyspSo0aN9P7772vw4MGaOHGik10CAADIbxph29zE\n",
              "0ULR6/VqyJAhioiIUIcOHTRnzhy9+uqrTnYJAAAAmzhaKNatW1eSFBUVpZKSEvl8Ph04cMDJLgEA\n",
              "AGATRx9m6d69uw4ePKhx48apU6dOqlevnoYOHepklwAAAK6bAg4XRwvFxx9/XJJ0xx13qFu3bior\n",
              "K1NycrKTXQIAAMAmji+P4/f7tW/fPpmmqejoaJWWlio+Pt7pbgEAwEWMdz3bw9FC8YUXXtD48eNV\n",
              "t25dRUScuB3SMAzt37/fyW4BAABgA0cLxUceeUQff/yx2rZt62Q3AAAAQbhH0R6OPvV82WWXUSQC\n",
              "AADUUo4WigMHDtRTTz2l/fv3y+v1BjYAAABc+Bydep4yZYokKScnR4ZhyDRNGYahqqoqJ7sFAAAX\n",
              "Oaae7eFooej3+0P+js/nk8/nC9rn8Xjk8XjsGhYAAABqwNGp57ORl5enmJiYoC0vLy/cwwIAALUI\n",
              "73q2hyOJYo8ePfT++++rUaNGMozvTtjJqedDhw6d8ru5ubnKyckJ2keaCAAAcP45UiguWrRIklRU\n",
              "VBTyd5lmBgAAuDA4Uig2a9ZMkpSQkODEzwMAAJyW26aAw8XRh1kKCwv1m9/8Rjt27FBlZWVg/44d\n",
              "O5zsFgAAADZwtFC85557NG7cOF1//fWKjIx0sisAAIAAk0TRFo4WipGRkcrOznayCwAAADjE0eVx\n",
              "unbtqrVr1zrZBQAAQDV+GWHb3MSRRDEtLU2GYaiiokLPPfeckpKSVL9+/cDxwsJCJ7oFAACAjRwp\n",
              "FKdPn65Dhw6pZcuWQft37Nihxo0bO9ElAAAAbOZIofjmm2/q5ptvVo8ePYL2Hzx4UG+99ZYGDBjg\n",
              "RLcAAACSWB7HLo7co/ivf/1LmZmZ1fYPGjRIH3zwgRNdAgAAwGaOJIrfXzPxv0VEXHCvlwYAAC7D\n",
              "8jj2cKRqq6iokNfrrba/rKxMFRUVTnQJAAAAmzlSKA4dOlR33XWXDh8+HNh3+PBh3XvvvRo6dKgT\n",
              "XQIAAMBmjhSKDz74oBo2bKgWLVooLS1NaWlpatGihaKiojR16lQnugQAAAjwm0bYNjdx5B7FyMhI\n",
              "zZ8/Xw899FBgzcT09HS1atXKie4AAADgAEdf4deqVSuKQwAAcN7xMIs9eAQZAAAAlhxNFAEAAMLB\n",
              "bfcKhguJIgAAACxRKAIAAMASU88AAMB1TDPcI3AHEkUAAABYIlEEAACu4xcPs9iBRBEAAACWKBQB\n",
              "AABgialnAADgOryZxR4kigAAALBEoggAAFyHN7PYg0QRAAAAlkgUAQCA67Dgtj1IFAEAAGCJQhEA\n",
              "AACWmHoGAACuw/I49iBRBAAAgCUSRQAA4DokivYgUQQAAIAlCkUAAABYYuoZAAC4Dm9msQeJIgAA\n",
              "ACyRKAIAANfhzSz2IFEEAAAIk/HjxysxMVGGYaioqOiU7fLz89W6dWu1atVKo0aNUkVFhSTp//2/\n",
              "/6drr71WV199tdq3b69JkybJ7/dLkkpKShQZGanU1NTAtn379pDGR6EIAABcxzSNsG2hGDx4sFav\n",
              "Xq2EhIRTttm5c6emTp2qgoICFRcX64svvtDcuXMlSY0aNdKiRYv0ySef6N///rfWrFmjF198MfDd\n",
              "qKgoFRUVBbZWrVqFND4KRQAAgDDp3r274uLiTttm2bJlysjIUGxsrAzD0JgxY7Rw4UJJUlpamlq2\n",
              "bClJql+/vlJTU1VSUmLb+LhHEQAAwEY+n08+ny9on8fjkcfjOavfKy0tDUocExMTVVpaWq3dvn37\n",
              "tGzZMr3++uuBfceOHVPnzp1VVVWlgQMHasqUKYqMjKxx3ySKAADAdcI59ZyXl6eYmJigLS8vz9G/\n",
              "1+v16tZbb9WkSZN0zTXXSJKaNWumvXv36uOPP9aqVatUUFCgP/7xjyH9LoUiAACAjXJzc1VWVha0\n",
              "5ebmnvXvxcfHa9euXYHPJSUlio+PD3w+evSo+vbtqwEDBignJyew3+PxqGnTppKkxo0ba/jw4Soo\n",
              "KAipbwpFAADgOmYYN4/Ho+jo6KDtbKedJSkzM1MrV67Uvn37ZJqmZs+eraFDh0qSvvrqK/Xt21d9\n",
              "+/bVgw8+GPS9/fv3B56O9vl8WrFihdLS0kLqm0IRAAAgTLKzsxUXF6c9e/aoT58+SkpKkiSNHDlS\n",
              "K1eulCS1bNlSM2bMUNeuXZWUlKQmTZooOztbkjRz5kz961//0ooVKwJL4Pzud7+TJK1evVppaWlK\n",
              "SUlRenq6YmNjNWXKlJDGZ5gmS1Ki5lrOfDLcQ8D37JiQc+ZGAHARarvi4bD1vXXQQ2Hr22489QwA\n",
              "AFwn1PUMYY2pZwAAAFgiUQQAAO7DjXW2IFEEAACAJQpFAAAAWGLqGQAAuA4Ps9iDRBEAAACWSBQB\n",
              "AIDrsEq0PUgUAQAAYIlEEQAAuA73KNqDQhEh4ZVxAC50vGr0wsH/ZtR+TD0DAADAEokiAABwH6ae\n",
              "bUGiCAAAAEskigAAwHVYHsceJIoAAACwRKEIAAAAS0w9AwAA92Hq2RYkigAAALBEoggAAFyHN7PY\n",
              "g0QRAAAAlkgUAQCA+3CPoi1IFAEAAGCJQhEAAACWmHoGAACuw8Ms9iBRBAAAgCUSRQAA4D48zGIL\n",
              "EkUAAABYolAEAACAJaaeAQCAC/Ewix1IFAEAAGCJRBEAALgPD7PYgkQRAAAAlkgUAQCA+5Ao2oJE\n",
              "EQAAAJYoFAEAAGCJqWcAAOA+vOvZFiSKAAAAsESiCAAAXMfkYRZbkCgCAADAkmOF4rZt29SvXz81\n",
              "b95cjRs3DmwAAACoHRwrFEeNGqWsrCw1atRI77//vgYPHqyJEyc61R0AAMB3zDBuLuJYoej1ejVk\n",
              "yBBFRESoQ4cOmjNnjl599VWnugMAAIDNHHuYpW7dupKkqKgolZSUKDY2VgcOHHCqOwAAgO+wPI4t\n",
              "HCsUu3fvroMHD2rcuHHq1KmT6tWrpyFDhjjVHQAAAGzmWKH4+OOPS5LuuOMOdevWTWVlZUpOTnaq\n",
              "OwAAgADDZfcKhotj9yi+9tprOnLkiCSpRYsWuuKKK/TGG2841R0AAABs5lihOHXqVDVs2DDwuWHD\n",
              "hpo6dapT3QEAAMBm5+3NLIZhqKqq6nx1BwAALmZMPdvCsUQxKipKa9asCXz+xz/+oaioKKe6AwAA\n",
              "gM0cSxT/8Ic/6LbbblO7du0knXhTy1//+lenugMAAPgOy+PYwrFC8frrr9fmzZv1z3/+U5LUpUuX\n",
              "oHsWAQAAcGELeer5uuuu0yuvvKKKiooztm3UqJH69++v/v37UyQCAADUMiEXig8//LCWLFmixMRE\n",
              "TZ06VXv37g063qNHD0knisTGjRsHtpOfAQAAHMe7nm0R8tRz79691bt3b5WWlmr27Nnq3Lmzunbt\n",
              "qvvvv19du3bVokWLJElFRUV2jxUAAADn0Vnfo3j48GF98cUXioiIULNmzTRu3Dh17dpVzzzzjKqq\n",
              "qjRixAitWrXKzrECAADUjMuSvXAJeep54cKF6tq1q4YNG6brrrtO27Zt09NPP621a9cG3rwSGRmp\n",
              "r7/+Wn6/3/YBAwAA4PwIOVF85ZVXNGPGDPXq1Stof2RkpJ5++unA586dO+uWW27RsGHD1KBBg8D+\n",
              "jIyMcxguAABADZAo2iKkQrGqqkoNGzasViSedOuttwb+vWHDBknSc889F9hnGAaFIgAAQC0RUqEY\n",
              "GRmpTz/99LRt5s+fr3vuuUezZs3SVVdddU6DAwAAQPiEfI/ij3/8Y40ePVpr1qzRhg0bAttJJ6ef\n",
              "77zzTvtGCQAAEArTCN/mIiHfo7h48WJJ0v/93/8F9hmGoR07dgQ+33fffdq7d69ycnKqff/JJ588\n",
              "m3ECAADgPAu5UNy5c+dpjy9dulRLly5VRESEYmJigo4ZhruqbAAAcGEyeJjFFiEXigMHDtSrr756\n",
              "yn0tW7bU5MmT1bx5c911112BNlVVVXrttdfOabAAAAA4f0K+R7G0tLTavu3bt1fbd7JI3Lp1qyZN\n",
              "mqQrrrhCv/3tb89iiAAAAAiHGieKc+bM0ezZs/Xpp58qPT09sL+srEzt27cPavv1119r8eLFev75\n",
              "57Vz50598803+uc//6l27drZN3IAAIBTYerZFjUuFPv27au2bdvqvvvu05/+9KfA/ujoaHXs2DHw\n",
              "edSoUVqxYoW6d++uX//61+rXr59at25NkQgAAFDL1LhQTEhIUEJCgjZv3nzadosWLdI111yj7Oxs\n",
              "9enTR4Zh8BALAABALRTyPYolJSW677771Lt3b914442B7aTPP/9cw4YN08MPP6yEhAQ9+OCDqqio\n",
              "sHXQAAAAbjB+/HglJibKMAwVFRWdsl1+fr5at26tVq1aadSoUUG11dkeq4mQC8Xbb79dDRs21Lhx\n",
              "4/TAAw8EtpMaNGigESNGaM2aNXrrrbdUXl6u48ePq0uXLpo1a1ao3QEAAITMMMO3hWLw4MFavXq1\n",
              "EhISTtlm586dmjp1qgoKClRcXKwvvvhCc+fOPadjNRVyoVheXq68vDxlZGToJz/5SWCzcvXVV+uJ\n",
              "J57Q3r179cADD+iNN94ItTsAAADX6t69u+Li4k7bZtmyZcrIyFBsbKwMw9CYMWO0cOHCczpWUyGv\n",
              "o5icnKzS0lLFx8fXvJM6dZSZmanMzMwztvX5fPL5fEH7PB6PPB5PqEMFAAA47+yuZUpLS4MSx8TE\n",
              "xMByhWd7rKZCThS//PJLpaSkqH///ho0aFBgs0teXp5iYmKCtry8PNt+HwAAXATC+K5nN9UyISeK\n",
              "w4YN07Bhw5wYiyQpNze32juiSRMBAEBtYXctEx8fH/Ryk5KSksDM7tkeq6mQC8V77rkn1K+EhGlm\n",
              "AABwzsK44LbdtUxmZqZuuOEGTZ8+XZdffrlmz56toUOHntOxmgq5UBw+fLjl/r/85S9BnysrK7V8\n",
              "+XJt375dlZWVgf0PPfRQqF0CAAC4UnZ2tt544w3t27dPffr0UVRUlIqLizVy5EhlZGQoIyNDLVu2\n",
              "1IwZM9S1a1dJUs+ePZWdnS1JZ32spgzTNEOquZ999tnAv8vLy7V8+XKlp6frmWeeCWo3ePBg7du3\n",
              "T9dee60iIyMD+x9//PGQBggAQChaznwy3EPAt3ZMyDlzI4e0fCp8/znYcX/4/m67hZwo/vznPw/6\n",
              "fN999ykjI6Nau40bN2rLli28lQUAAJx/vOvZFiE/9fzf6tevrz179lTb36JFCx0/fvxcfx4AAABh\n",
              "EnKi+P2neKqqqrR27VolJycH9j399NOSpKSkJPXs2VO33Xab6tevHzg+fvz4cxkvAADAGYX6hhRY\n",
              "C7lQjImJ+e7Ldepo/PjxQesorlu3LvDvdu3aafPmzYHPTEMDAADUHiEXitOmTTvt8Xnz5kmSDhw4\n",
              "oMsuuyzo2IEDB0LtDgAAAGES8j2KR48e1c9//nO1adNGbdq00bhx43T06NFq7Xr37l2jfQAAALYz\n",
              "w7i5SMiF4tixY1VZWaklS5Zo6dKl8vv9Gjt2bOD48ePH5fV6VVVVpaNHj8rr9crr9Wr37t06duyY\n",
              "rYMHAACAc0Keet6wYYPWr18f+Dxr1iylpKQEPufl5WnGjBkyDCPofsbo6Gg98MAD5zhcAACAGnBZ\n",
              "shcuISeKJ5PCk44ePaqqqqrA52nTpsnv92v06NHy+/2B7ciRI5o6dao9owYAAIDjzupdz9ddd52G\n",
              "DBkiSVqyZInuvffeau3+53/+59xHBwAAcBZYHsceNS4UvV6vDh06pF/96ldKTk7Wu+++K+nEPYvD\n",
              "hg2r1j4iIsJyOZzvp48AAAC4cNW4UJw0aZJuvvlmJSYmql+/furXr58kacWKFZo8eXK1BPH709Pf\n",
              "fPONXnzxRYpEAACAWqTG9yj+61//UmZmZrX9gwYN0gcffFBt/6WXXhrYLrvsMuXk5GjZsmXnNloA\n",
              "AICaMI3wbS5S40KxsrLy1D8Sceaf2bJlCwtuAwAA1CI1nnquqKiQ1+tVdHR00P6ysjJVVFRUa9+o\n",
              "UaPAPYoni8w///nP5zJWAACAmuFhFlvUuFAcOnSo7rrrLr3wwgtq1KiRJOnw4cMaMWKEhg4dWq19\n",
              "UVHRd53UqaPY2FhFRkae+4gBAABwXtR46vnBBx9Uw4YN1aJFC6WlpSktLU0tWrRQVFSU5fqICQkJ\n",
              "atKkifbu3atdu3bJ5/PZOnAAAAA4q8aJYmRkpObPn6+HHnpIhYWFkqT09HS1atXKsv2aNWuUmZmp\n",
              "2NhYSdIXX3yh5cuX6/rrr7dh2AAAAKfGOor2CHnB7VatWp2yOPy+k085d+3aVdKJwvGXv/ylPvzw\n",
              "w9BHCQAAgPMu5EKxpr755ptAkShJXbp0UXl5uVPdAQAAfIdE0RYhv+u5pho0aKBVq1YFPr/77ru6\n",
              "9NJLneoOAAAANnMsUZw5c6YyMzMVGRkp0zRlmqZWrFjhVHcAAAAB3KNoD8cKxWuuuUbFxcXaunWr\n",
              "JKlt27aqW7euU90BAADAZo5NPUtS3bp19YMf/ECrVq3S22+/7WRXAAAAsJnthWKvXr0Ci21/9tln\n",
              "uuaaa/T2229r4sSJ+v3vf293dwAAANWZYdxcxPZCce/evUpNTZUkvfLKK+rRo4fefPNN/fOf/9TL\n",
              "L79sd3cAAABwiO33KF5yySWBf69Zs0b9+/eXdOLdz3XqOHZLJAAAwHdcluyFi+2JYkREhPbs2aOv\n",
              "vvpK77//vnr06BE49vXXX9vdHQAAABxie8T3m9/8RmlpaapTp45+/OMfq02bNpJOpIuJiYl2dwcA\n",
              "AACH2F4oDho0SF26dNEXX3yhjh07BvYnJiZq7ty5dncHAABQDeso2sORmwZjY2MVGxsbtK958+ZO\n",
              "dAUAAACHOLqOIgAAAGovCkUAAABYYr0aAADgPtyjaAsSRQAAAFiiUAQAAIAlpp4BAIDrsDyOPUgU\n",
              "AQAAYIlEEQAAuA+Joi1IFAEAAGCJQhEAAACWmHoGAADuw9SzLUgUAQAAYIlEEQAAuA7L49iDRBEA\n",
              "AACWSBQBAID7kCjagkQRAAAAligUAQAAYImpZwAA4Do8zGIPEkUAAABYIlEEAADuQ6JoCxJFAAAA\n",
              "WKJQBAAAgCWmngEAgPsw9WwLEkUAAABYIlEEAACuw/I49qBQBAAbtJz5ZLiHgG/tmJAT7iEArkGh\n",
              "CAAA3IdE0RbcowgAAABLFIoAAACwxNQzAABwH6aebUGiCAAAAEskigAAwHVYHsceJIoAAACwRKEI\n",
              "AAAAS0w9AwAA92Hq2RYkigAAALBEoggAAFyHh1nsQaIIAAAQJtu2bVOXLl3Upk0bde7cWZs2barW\n",
              "xu/3a+LEiUpOTla7du00YsQIHT9+XJL09ttvKzU1NbA1b95c6enpge8ahqEOHToEjhcUFIQ0PgpF\n",
              "AADgPmYYtxBkZ2dr9OjR+vTTTzV58mRlZWVVa5Ofn6/CwkIVFhZq8+bNioiI0MyZMyVJffr0UVFR\n",
              "UWBLT0/XnXfeGfT9goKCwPFu3bqFND4KRQAAgDDYv3+/1q5dq2HDhkmSMjMztXv3bhUXFwe1W79+\n",
              "vXr16qV69erJMAz169dPCxYsqPZ7n332md59913dddddto2RQhEAAMBGPp9PXq83aPP5fNXa7d69\n",
              "W82aNVOdOiceGTEMQ/Hx8SotLQ1q16lTJ61cuVJer1cVFRVasmSJSkpKqv3eCy+8oP79+6tp06ZB\n",
              "+2+66SalpKQoJydHx44dC+lvoVAEAADuE8ap57y8PMXExARteXl5Z/2nZGVlqW/fvurRo4d69Oih\n",
              "Nm3aBIrLwJ9rmvrLX/6iESNGBO3ftWuX/v3vf2vNmjX68ssv9atf/SqkvikUAQAAbJSbm6uysrKg\n",
              "LTc3t1q7Fi1a6PPPP1dlZaWkE8VeaWmp4uPjg9oZhqHp06dr3bp1WrNmja6++mq1b98+qM3777+v\n",
              "8vJy9enTJ2j/yd+69NJLNXbsWB5mAQAAMMK4eTweRUdHB20ej6faGJs2bar09HS99NJLkqTly5cr\n",
              "Li5OSUlJQe3Ky8t1+PBhSdKBAwf02GOPadKkSUFt8vPzlZWVpcjIyMC+w4cP6+uvv5Z04snpxYsX\n",
              "Ky0tLaTzyDqKAAAAYTJnzhxlZWXp0UcfVXR0tObNmydJGjlypDIyMpSRkaGysjL17NlTERER8vv9\n",
              "mjBhgm699dbAb5SVlWnFihXauHFj0G9v2bJF2dnZMgxDlZWVSk9PDzwtXVOGaZosSQkA56jlzCfD\n",
              "PQR8a8eEnHAPAReAjjl/ClvfG578Zdj6thuJIgAAcB9iMFtwjyIAAAAskSgCAADX4V3P9iBRBAAA\n",
              "gCUKRQAAAFhi6hkAALgPU8+2IFEEAACAJRJFAADgPiSKtiBRBAAAgCUSRQAA4Dosj2MPEkUAAABY\n",
              "olAEAACAJaaeAQCA+zD1bAsSRQAAAFgiUQQAAK7Dwyz2IFEEAACAJQpFAAAAWGLqGQAAuA9Tz7Yg\n",
              "UQQAAIAlEkUAAOA6PMxiDxJFAAAAWCJRBAAA7kOiaAsSRQAAAFiiUAQAAIAlpp4BAID7MPVsCxJF\n",
              "AAAAWCJRBAAArsPyOPYgUQQAAIAlCkUAAABYYuoZAAC4D1PPtiBRBAAAgCUSRQAA4DqGSaRoh/Oe\n",
              "KB47dux8dwkAAICz4EiheP311wf+fddddwUd69atmxNdAgAAfMcM4+YijhSK5eXlgX9v2rQp6JhJ\n",
              "FAwAAFArOD71/N+FoWEYTncJAAAAGzjyMMv3i0EKQwAAcL7xZhZ7OFIobtiwQY0bN5Ykeb3ewL9N\n",
              "09RXX33lRJcAAACwmSOF4vbt2534WQAAgJohUbSFI4ViQkKC5f6DBw9qwYIFuv/++53oFgAAADY6\n",
              "L+sovv3227r99tt15ZVXavXq1adt6/P55PV6gzafz3c+hgkAAIDvcaxQ3LVrlx566CElJCTowQcf\n",
              "1N///nft3r1by5YtO+338vLyFBMTE7Tl5eU5NUwAAOBChhm+zU0cKRRvvvlmXXvttTp69Khef/11\n",
              "ffzxx2rQoIFiYmLO+N3c3FyVlZUFbbm5uU4MEwAAAKfhyD2KxcXFuuKKK9S2bVslJiZKqvkyOR6P\n",
              "Rx6Px4lhAQCAi4XLkr1wcSRR3Llzpx5//HF98MEHSkxM1LBhw4Le1gIAAIALn2P3KN5000165ZVX\n",
              "VFxcrOuuu06XX365WrRooUmTJjnVJQAAgCTuUbSL4089N2rUSOPGjdO6dev06quv6tixY053CQAA\n",
              "ABs49maW7zMMQ02bNlWnTp3UqVMnJ7oEAACAzRwpFAcMGFBt34EDB9SqVSstW7ZMSUlJTnQLAABw\n",
              "gsumgMPFkUJx586dlvtffPFFjR8/Xn/729+c6BYAAAA2Oi9vZjnp7rvv1r59+85nlwAA4CLEwyz2\n",
              "OK+FoiRVVVWd7y4BAABwFhyZevZ6vdX2HTx4UHPmzFFKSooTXQIAAMBmjhSKDRs2lGEYMs0T+ath\n",
              "GGrSpIn69Omjp556yokuAQAAvmO6bA44TBwpFP1+vxM/CwAAgPPIkUIRAAAgnNz2UEm4nPeHWQAA\n",
              "AFA7kCgCAAD3IVG0BYkiAAAALFEoAgAAwBJTzwAAwHUMFmCxBYkiAAAALJEoAgAA9+FhFluQKAIA\n",
              "AMAShSIAAAAsMfUMAABchzez2INEEQAAAJZIFAEAgPuYRIp2IFEEAAAIk23btqlLly5q06aNOnfu\n",
              "rE2bNlVr4/f7NXHiRCUnJ6tdu3YaMWKEjh8/LkkqKSlRZGSkUlNTA9v27dsD33399dfVrl07tW7d\n",
              "WoMGDZLX6w1pfBSKAADAdQwzfFsosrOzNXr0aH366aeaPHmysrKyqrXJz89XYWGhCgsLtXnzZkVE\n",
              "RGjmzJmB41FRUSoqKgpsrVq1kiR99dVXGjFihF599VVt27ZNzZs31yOPPBLS+CgUAQAAwmD//v1a\n",
              "u3athg0bJknKzMzU7t27VVxcHNRu/fr16tWrl+rVqyfDMNSvXz8tWLDgjL//5ptvKi0tTe3atZMk\n",
              "jR07VgsXLgxpjBSKAAAANvL5fPJ6vUGbz+er1m737t1q1qyZ6tQ58ciIYRiKj49XaWlpULtOnTpp\n",
              "5cqV8nq9qqio0JIlS1RSUhI4fuzYMXXu3Fnp6el6+OGHVVVVJUkqLS1VQkJCoF1iYqI+//xzVVZW\n",
              "1vhvoVAEAADuY4Zvy8vLU0xMTNCWl5d31n9KVlaW+vbtqx49eqhHjx5q06ZNoLhs1qyZ9u7dq48/\n",
              "/lirVq1SQUGB/vjHP551X/+NQhEAAMBGubm5KisrC9pyc3OrtWvRokVQwmeapkpLSxUfHx/UzjAM\n",
              "TZ8+XevWrdOaNWt09dVXq3379pIkj8ejpk2bSpIaN26s4cOHq6CgQJIUHx+vXbt2BX6npKQkKMGs\n",
              "CQpFAADgOuF8mMXj8Sg6Ojpo83g81cbYtGlTpaen66WXXpIkLV++XHFxcUpKSgpqV15ersOHD0uS\n",
              "Dhw4oMcee0yTJk2SdOI+x4qKCkknprxXrFihtLQ0SVLfvn1VWFioLVu2SJJmzZqloUOHhnQeWUcR\n",
              "AAAgTObMmaOsrCw9+uijio6O1rx58yRJI0eOVEZGhjIyMlRWVqaePXsqIiJCfr9fEyZM0K233ipJ\n",
              "Wr16tR566CFFRkaqsrJSN954o6ZMmSLpxNPQzz//vAYOHKjKykolJydr/vz5IY3PME1WpASAc9Vy\n",
              "5pPhHgK+tWNCTriHgAtAt9ueCFvfBX+dGLa+7UaiCAAA3IcczBbcowgAAABLJIoAAMB1Qn1DCqyR\n",
              "KAIAAMAShSIAAAAsMfUMAADch6lnW5AoAgAAwBKJIgAAcB0eZrEHiSIAAAAskSgCAAD38RMp2oFE\n",
              "EQAAAJYoFAEAAGCJqWcAAOA+zDzbgkQRAAAAlkgUAQCA67A8jj1IFAEAAGCJQhEAAACWmHoGAADu\n",
              "YzL3bAcSRQAAAFgiUQQAAK7Dwyz2IFEEAACAJRJFAADgPiSKtiBRBAAAgCUKRQAAAFhi6hkAALiO\n",
              "wfI4tiBRBAAAgCUSRaAWaznzyXAPAd/aMSEn3EMA8H3+cA/AHUgUAQAAYIlCEQAAAJaYegYAAK7D\n",
              "wyz2IFEEAACAJRJFAADgPgSKtiBRBAAAgCUSRQAA4D7co2gLEkUAAABYolAEAACAJaaeAQCA6xjM\n",
              "PNuCRBEAAACWSBQBAID78DCLLUgUAQAAYIlCEQAAAJaYegYAAK5j+MM9AncgUQQAAIAlEkUAAOA+\n",
              "PMxiCxJFAAAAWCJRBAAA7kOgaAsSRQAAAFiiUAQAAIAlpp4BAIDrGDzMYgsSRQAAAFgiUQQAAO5D\n",
              "omgLEkUAAABYolAEAACAJaaeAQCA+/CuZ1uQKAIAAMASiSIAAHAdlsexB4kiAAAALJEoAgAA9yFR\n",
              "tAWJIgAAACxRKAIAAMASU88AAMB9mHq2BYkiAAAALJEoAgAA92HBbVuQKAIAAMAShSIAAAAsMfUM\n",
              "AABchzez2INEEQAAAJZIFAEAgPuQKNqCRBEAACBMtm3bpi5duqhNmzbq3LmzNm3aVK2N3+/XxIkT\n",
              "lZycrHbt2mnEiBE6fvy4JGnjxo3q3r272rVrp+TkZA0fPlzffPNN4LuGYahDhw5KTU1VamqqCgoK\n",
              "QhofhSIAAHAf0wzfFoLs7GyNHj1an376qSZPnqysrKxqbfLz81VYWKjCwkJt3rxZERERmjlzpiSp\n",
              "fv36euaZZ7RlyxatX79ex44d0+9///ug7xcUFKioqEhFRUXq1q1bSOOjUAQAAAiD/fv3a+3atRo2\n",
              "bJgkKTMzU7t371ZxcXFQu/Xr16tXr16qV6+eDMNQv379tGDBAklS69at1bFjR0lSZGSkOnfurJKS\n",
              "EtvGSKEIAABgI5/PJ6/XG7T5fL5q7Xbv3q1mzZqpTp0Tj4wYhqH4+HiVlpYGtevUqZNWrlwpr9er\n",
              "iooKLVmyxLIYPHbsmJ5//nkNGDAgaP9NN92klJQU5eTk6NixYyH9LRSKAADAfcI49ZyXl6eYmJig\n",
              "LS8v76z/lKysLPXt21c9evRQjx491KZNm0BxedLx48c1ZMgQ9e7dW7fddltg/65du/Tvf/9ba9as\n",
              "0Zdffqlf/epXIfVNoQgAAGCj3NxclZWVBW25ubnV2rVo0UKff/65KisrJUmmaaq0tFTx8fFB7QzD\n",
              "0PTp07Vu3TqtWbNGV199tdq3bx84XlFRoSFDhqhZs2aBexdPOvlbl156qcaOHcvDLAAAAPKHb/N4\n",
              "PIqOjg7aPB5PtSE2bdpU6enpeumllyRJy5cvV1xcnJKSkoLalZeX6/Dhw5KkAwcO6LHHHtOkSZMk\n",
              "SZWVlRo6dKgaN26suXPnyjCMwPcOHz6sr7/++sTp8Pu1ePFipaWlhXQaWUcRAAAgTObMmaOsrCw9\n",
              "+uijio6O1rx58yRJI0eOVEZGhjIyMlRWVqaePXsqIiJCfr9fEyZM0K233ipJWrx4sVasWKGOHTsG\n",
              "isCuXbvq2Wef1ZYtW5SdnS3DMFRZWan09PRqieOZGKbJipRAbdVy5pPhHgK+tWNCTriHAOB7+raf\n",
              "Era+39r0u7D1bTcSRQAA4Dq869ke3KMIAAAASySKAADAfUgUbUGiCAAAAEuOFIpHjx495bF169Y5\n",
              "0SUAAABs5kiheOutt1q+qqaoqEj9+/d3oksAAIDv+M3wbS7iSKF41VVX6fbbb5ff7w/sW79+vfr3\n",
              "769Zs2Y50SUAAABs5kihOGvWLP3gBz9QVlaWJGnjxo3q27ev/vznPwe9fxAAAMARYXzXs5s4Uiga\n",
              "hqEFCxbo0KFDuuuuuwJFYmZmphPdAQAAwAGOLI+zcuVKSdKwYcP0i1/8Qn379lW9evUC+zMyMk75\n",
              "XZ/PV+3+Ro/HY/mORAAAAEsuS/bCxZFC8U9/+lPg38nJydqzZ09gn2EYpy0U8/LyNGPGjKB906ZN\n",
              "0/Tp050YKgAAAE7BkULx73//+1l/Nzc3Vzk5we9MJU0EAAA4/xwpFDds2BD02TAMNW3aVJdffvkZ\n",
              "v8s0MwAAOGdMPdvCkUJxwIAB1fYdOHBArVq10tKlS9W6dWsnugUAAICNHCkUd+7cabn/xRdf1IQJ\n",
              "E/S3v/3NiW4BAABOcNnC1+FyXt/1fPfdd2vfvn3ns0sAAACcpfNaKEpSVVXV+e4SAAAAZ8GRqWev\n",
              "11tt38GDBzVnzhylpKQ40SUAAMB3TP+Z2+CMHCkUGzZsKMMwZH77xJFhGGrSpIn69Omjp556yoku\n",
              "AQAAYDNHCkW/nyoeAACEEcvj2OK836MIAACA2sGRRBEAACCsWB7HFiSKAAAAsEShCAAAAEtMPQMA\n",
              "APfhYRZbkCgCAADAEokiAABwHxJFW5AoAgAAwBKFIgAAACwx9QwAANyHqWdbkCgCAADAEokiAABw\n",
              "H78/3CNwBRJFAAAAWCJRBAAA7sM9irYgUQQAAIAlCkUAAABYYuoZAAC4D1PPtiBRBAAAgCUSRQAA\n",
              "4D5+EkU7kCgCAADAEoUiAAAALDH1DAAAXMc0eTOLHUgUAQAAYIlEEQAAuA8Ps9iCRBEAAACWSBQB\n",
              "AID7sOC2LUgUAQAAYIlCEQAAAJaYegYAAO7jZ3kcO5AoAgAAwBKJIgAAcB8eZrEFiSIAAAAsUSgC\n",
              "AADAElPPAADAdUweZrEFiSIAAAAskSgCAAD34WEWW5AoAgAAwBKJIgAAcB8/iaIdSBQBAABgiUIR\n",
              "AAAAlph6BgAA7mOyPI4dSBQBAABgiUQRAAC4jsnDLLYgUQQAAIAlCkUAAABYYuoZAAC4Dw+z2IJE\n",
              "EQAAAJZIFAEAgOvwMIs9SBQBAADCZNu2berSpYvatGmjzp07a9OmTdXa+P1+TZw4UcnJyWrXrp1G\n",
              "jBih48ePB46//vrrateunVq3bq1BgwbJ6/XW6FhNUCgCAAD3Mf3h20KQnZ2t0aNH69NPP9XkyZOV\n",
              "lZVVrU1+fr4KCwtVWFiozZs3KyIiQjNnzpQkffXVVxoxYoReffVVbdu2Tc2bN9cjjzxyxmM1RaEI\n",
              "AAAQBvv379fatWs1bNgwSVJmZqZ2796t4uLioHbr169Xr169VK9ePRmGoX79+mnBggWSpDfffFNp\n",
              "aWlq166dJGns2LFauHDhGY/VFIUiAACAjXw+n7xeb9Dm8/mqtdu9e7eaNWumOnVOPDJiGIbi4+NV\n",
              "Wloa1K5Tp05auXKlvF6vKioqtGTJEpWUlEiSSktLlZCQEGibmJiozz//XJWVlac9VlM8zHKe+Hw+\n",
              "5eXlKTc3Vx6PJ9zDuai56VrsmJAT7iGcEzddi9qOa3Fh4Xqcu//zLw1b39OnT9eMGTOC9k2bNk3T\n",
              "p08/q9/LysrSrl271KNHD11yySXq1auX3nnnHRtGemaGaZo8FnQeeL1excTEqKysTNHR0eEezkWN\n",
              "a3Hh4FpcOLgWFxauR+3m8/mqJYgej6da0b9//34lJSXp0KFDqlOnjkzTVLNmzbR69WolJSWd8vcX\n",
              "LVqkZ599VgUFBVq6dKny8/P11ltvSZI++eQT9e7dW3v27DntsZpi6hkAAMBGHo9H0dHRQZtVMty0\n",
              "aVOlp6frpZdekiQtX75ccXFx1YrE8vJyHT58WJJ04MABPfbYY5o0aZIkqW/fviosLNSWLVskSbNm\n",
              "zdLQoUPPeKymmHoGAAAIkzlz5igrK0uPPvqooqOjNW/ePEnSyJEjlZGRoYyMDJWVlalnz56KiIiQ\n",
              "3+/XhAkTdOutt0qSoqKi9Pzzz2vgwIGqrKxUcnKy5s+ff8ZjNcXU83nCNMKFg2tx4eBaXDi4FhcW\n",
              "rgcuFEw9nycej0fTpk3jpuQLANfiwsG1uHBwLS4sXA9cKEgUAQAAYIlEEQAAAJYoFAEAAGCJQhEA\n",
              "AACWXFEopqamKjU1VVdffbUiIyMDn4cMGaIPP/xQ8fHxOnLkSKD9T3/6U02bNk2S5Pf79Ytf/EKt\n",
              "WrVSUlKSnnnmmUA7tx9744031KlTJ3k8Ht1///1nefaDcS3O7tjTTz+t5ORkdejQQR07dgysqXUu\n",
              "uBZnd+zZZ59Vhw4dlJqaquTkZD399NNnewkCuBZnd+yk/fv36/LLL9fAgQNDPPPVcS3O7tj06dPV\n",
              "pEmTwPm68847z/YSoLYxXWTnzp1mTExMtf2TJ08277zzTtM0TfOll14yU1NTzePHj5umaZrz5883\n",
              "b7zxRrOystI8ePCgGR8fb/7nP/+5KI5t3brVLCoqMqdMmWJOmDCBaxHGY6tWrTKPHDlimqZplpaW\n",
              "mj/84Q/N4uJirkUYjp28DqZpmmVlZWaLFi3MwsJCrkUYjp00cOBAc/jw4eaAAQNsuQ6mybUI9di0\n",
              "adNs/98J1A6uSBTP5OGHH9b69ev15z//WQ888IDmz5+vunXrSpIWL16sUaNGKTIyUo0bN9aQIUO0\n",
              "cOHCi+JYmzZtlJKSEngZ+fnAtbA+dtNNNykmJkaS1KJFC8XGxmr37t1cizAcO3kdJOnYsWOqqKhw\n",
              "9DpIXItTHZOk/Px8XXnllerWrZvTl0ES1+J01wIXp4uiUKxXr57mzp2r8ePH67777lPHjh0Dx0pL\n",
              "S5WQkBD4nJiYqNLS0oviWDhwLc58LVatWqXDhw+rc+fOlufQLlyLU1+LZcuWqX379kpMTNTEiROV\n",
              "lpZ22nN5rrgW1sd27typ2bNn63e/+90Zz6FduBan/u/F0qVLlZKSohtvvFF///vfT3se4R4XRaEo\n",
              "SX/9618VFxenoqKicA/lose1OLWNGzfq3nvv1eLFi3XppZc63h/XwtrgwYO1adMmbd26VS+99JK2\n",
              "bt3qeJ9ci2CmaWr48OF65plndMkll5zXvrkW1Y0ZM0YlJSVav369HnnkEQ0ZMkS7du0K97BwHlwU\n",
              "heI//vEPLVmyROvWrVNJSYlefvnlwLH4+Pig/7CXlJQoPj7+ojgWDlyLU1+LTz75RLfccov+8pe/\n",
              "6IYbbjjtebQD1+LM/71ITEzUj370I73++uuW59AuXIvqx7xerzZs2KAhQ4YEkt133nlHN910U01O\n",
              "6VnjWlgfi42NDUzBd+3aVWlpaVq7du3pTybcIdw3SdrJ6ubkr776ykxKSjLfeust0zRNs6ioyLz8\n",
              "8svNzz//3DRN05w3b161m3c3bNhwURw7yYmblLkWoR375JNPzISEhMC54VqE79imTZsC52n//v1m\n",
              "69atzXfeeYdrEYZj3zdv3jzHH2bhWpz62O7duwPn6dNPPzWbNm1qbt261bbrgQuX6wvFsWPHmqNG\n",
              "jQraN2PGDDMjI8M0TdOsrKw0x44da1555ZVmy5YtzaeeeirQzu3HVq1aZV5xxRVmVFSU2aBBA/OK\n",
              "K64w//d//7fG5/t0uBahHevVq5fZsGFDMyUlJbDZVTRyLUI7Nnr0aPOqq64yU1JSzI4dO5rPPvts\n",
              "jc/1mXAtQjv2feejUORanPrY3XffbbZv395MSUkx09PTzaVLl9b4XKN2413PAAAAsHRR3KMIAACA\n",
              "0FEoAgAAwBKFInARON1ryy407733nt56663A588++8z2xZanT58eeG3le++9p0suuURpaWlq3769\n",
              "2rdvr5ycHB0+fNjWPgGgNjp/r+QAEDYn14MrKSlRamqq5fpwlZWV5/UtPafy3nvv6ciRI+rbt68k\n",
              "qXnz5iooKHC0z7Zt22rdunWSpKNHjyonJ0c33XSTPv74Y0VGRjraNwBcyEgUgYtYYmKiJk+erGuv\n",
              "vVb33HOP9u3bpx//+Mfq1KmT2rdvr3Hjxsnv90uSXnjhBfXq1Us/+9nP1KFDB11zzTXasWOHJGnb\n",
              "tm3q2rWrUlJS1KFDBz344IOSpHfffVfXX399IK3Lz88P9F1WVqaRI0cqOTlZKSkpGj58uIqKijR7\n",
              "9my9/PLLSk1N1cMPP6ySkhI1bNgw8L23335b6enp6tixo3r06KFPPvlE0okCMzk5WWPHjlVKSora\n",
              "t29/Vuu8RUVFadasWTpw4EBQsgkAF6PwxwcAwurgwYP66KOPZBiGysvL9dprr6lBgwaqqqrSgAED\n",
              "tGTJEg0dOlSS9PHHH6uoqEhXXnmlfv3rX+v3v/+95syZo2eeeUa33HKLcnNzJUmHDh2SJKWnp2v1\n",
              "6tWKjIzUoUOHlJaWpj59+iguLk7333+/LrnkEm3YsEERERH68ssv1aRJE40ZM0ZHjhzRU089JelE\n",
              "CnrS/v37dccdd+i9995Thw4d9PLLLwfeoiJJW7ZsUX5+vmbNmqXZs2drypQpevvtt0M+J3Xr1lVa\n",
              "Wpo2bdqkn/zkJ+dwdgGgdiNRBC5yWVlZMgxDkuT3+zV58mSlpKQE3rzw/Wnq66+/XldeeWXg39u3\n",
              "b5ckde/eXc8995ymTJmid955J5AAHjx4UD/96U+VnJysG2+8UQcPHtR//vMfSdLrr7+uiRMnKiLi\n",
              "xP8batKkyRnH+tFHH6lDhw7q0KGDJOnOO+/UZ599pr1790qSkpKS9KMf/aja+M4GK4cBAIUicNFr\n",
              "0KBB4N9PPvmk9u/fr48++kgbNmzQHXfcofLy8sDx+vXrB/4dGRmpyspKSVJmZqb+8Y9/qG3btoF0\n",
              "UTrxftgbbrhBGzduVFFRkdq0aRP0e3Y71fhCVVFRoaKiIiUnJ9s1NAColSgUAQQcPnxYsbGxql+/\n",
              "vvbt26elS5fW6Hvbtm3T5Zdfrrvvvlt/+MMf9OGHHwZ+LyEhQYZh6IMPPtD69esD38nIyNATTzwR\n",
              "uAfyyy+/lCRFR0errKzMsp/rrrtOGzduDKSSixYt0hVXXKErrrjirP/m//bVV1/pF7/4hS677DL1\n",
              "6dPHtt8FgNqIexQBBEyYMEGDBw9W+/bt1bx5c/Xq1atG31u2bJleeukl1atXT36/X7Nnz5YkPfbY\n",
              "Yxo7dqweeeQRpaamBqaFJelPf/qTfvnLX6pDhw6qW7euOnfurOeee0633XabFixYoNTUVA0aNEh3\n",
              "33134DtNmjTRyy+/rLvvvluVlZVq1KiRli5dGpg6P1tbt25VamqqKioqZJqm+vTpo3fffZcnngFc\n",
              "9HiFHwAAACwx9QwAAABLFIoAAACwRKEIAAAASxSKAAAAsEShCAAAAEsUigAAALBEoQgAAABLFIoA\n",
              "AACwRKEIAAAASxSKAAAAsEShCAAAAEv/H0ZVKnCXu3NpAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-d0e504a9-af17-4001-8b85-f3c6adf32429\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-d0e504a9-af17-4001-8b85-f3c6adf32429\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "import pandas as pd\n",
              "plt.subplots(figsize=(8, 8))\n",
              "df_2dhist = pd.DataFrame({\n",
              "    x_label: grp['Transaction Type'].value_counts()\n",
              "    for x_label, grp in _df_17.groupby('Country')\n",
              "})\n",
              "sns.heatmap(df_2dhist, cmap='viridis')\n",
              "plt.xlabel('Country')\n",
              "_ = plt.ylabel('Transaction Type')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-216ea315-a003-45be-b67a-ef8251cb2f05\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAooAAAKfCAYAAADpb+mkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABY8klEQVR4nO3dd3xUVf7/8fckgQBCEpAWN4QAgaBSkiBIB5VqQRdQIosapURc\n",
              "FvxlWZRFpVgQUQELUgyg0ttXs7qCIiggWCAEQUBpIVRDCQk1be7vD9ZZsxkkA/cwYXw993Efy9y5\n",
              "cz8nmZV9+zn33OuwLMsSAAAA8D/8vD0AAAAAlEwERQAAALhFUAQAAIBbBEUAAAC4RVAEAACAWwRF\n",
              "AAAAuEVQBAAAgFsERQAAALhFUAQAAIBbBEUAAAAvGTx4sCIiIuRwOJSamur2mLS0NLVv317BwcGK\n",
              "jo4u8n5SUpLq1q2rOnXqqH///srLyyvWe8VBUAQAAPCSnj17au3atapZs+ZFjwkKCtILL7yguXPn\n",
              "Fnlv7969evbZZ7VmzRrt2rVLv/zyi6ZNm3bJ94qLoAgAAOAlbdu2VVhY2O8eU6lSJbVu3VrXXXdd\n",
              "kfcWL16sbt26qXr16nI4HHr88cc1b968S75XXAEeHQ0AAIDflZOTo5ycnEL7AgMDFRgYaHut9PT0\n",
              "Qt3IiIgIpaenX/K94iIoAoANak963dtDwH/sGZLo7SGgBHAeqee12mOn9Nbo0aML7Rs5cqRGjRrl\n",
              "nQFdAYIiAACAjYYPH67ExML/wmKimyhJ4eHh2r17t+t1WlqawsPDL/lecXGNIgAA8DlOL/4nMDBQ\n",
              "QUFBhTZTQbFHjx5KTk7WkSNHZFmWpkyZori4uEu+V1wERQAAAC9JSEhQWFiYDhw4oM6dOysyMlKS\n",
              "1K9fPyUnJ0uSzp49q7CwMN1///3atm2bwsLCNHz4cElS7dq1NXr0aLVq1UqRkZGqUqWKEhISLvle\n",
              "cTksy7Js/HkB4A+JaxRLDq5RhCTlH4n0Wu2A6ru8VttuXKMIAAB8ToHl9FptXwpXTD0DAADALV8K\n",
              "vQAAAJIkp7iyzg50FAEAAOAWQREAAABuMfUMAAB8jlPeW8ziS+goAgAAwC06igAAwOcUcJtoW9BR\n",
              "BAAAgFt0FAEAgM/h9jj2oKMIAAAAtwiKAAAAcIupZwAA4HMKmHq2BR1FAAAAuEVHEQAA+BwWs9iD\n",
              "jiIAAADcIigCAADALaaeAQCAz+HJLPagowgAAAC36CgCAACf4/T2AHwEHUUAAAC4RUcRAAD4HG64\n",
              "bQ86igAAAHCLoAgAAAC3mHoGAAA+p4CZZ1vQUQQAAIBbtncUs7Ozf/f9oKAgu0sCAAAUwu1x7GF7\n",
              "UAwJCZHD4ZD1mzui//ra4XCooKDA7pIAAAAwwPag6HSS4QEAAHwBi1kAAIDPKZDD20PwCcYWs+zc\n",
              "uVNdu3bVDTfcoEqVKrk2AAAAXBuMBcX+/fsrPj5eFStW1FdffaWePXtq6NChpsoBAAC4OC3vbb7E\n",
              "WFDMzs5Wr1695Ofnp4YNG2rq1Kn68MMPTZUDAACAzYxdo1iqVClJUoUKFZSWlqbq1avr2LFjpsoB\n",
              "AAC4cI2iPYwFxbZt2+r48eMaNGiQmjRpotKlSysuLs5UOQAAANjMWFAcP368JKl3795q06aNsrKy\n",
              "1KBBA1PlAAAAYDNj1yi+8MIL+uabb+R0OlWjRg1CIgAAuGoK5PDa5kuMBUXLsjRs2DBVrVpV3bp1\n",
              "0xtvvKEff/zRVDkAAADYzGH99ll7Bpw5c0ZLlizRyJEjlZ6eziP8APik2pNe9/YQ8B97hiR6ewgo\n",
              "ATan1/Ba7cbh+71W227GrlFcsWKFVqxYoZUrV8rpdOr+++9Xx44dTZUDAACAzYwFxU6dOqlly5Z6\n",
              "7bXX1KZNG1NlAAAAYIixaxS3bt2qBx54QOPHj9dNN92k+Ph4zZkzx1Q5AAAAFxaz2MP4NYp79+7V\n",
              "p59+qvHjx2v//v3Kz883WQ4AvIJrFEsOrlGEJKWkh3utdmx4utdq283Y1HNCQoK++OILSVKHDh30\n",
              "yiuv6I477jBVDgAAwKXA3KTpH4qxoHjLLbfo6aefVq1atUyVAAAAgEHGgmL//v1NnRoAAOB3OS3f\n",
              "ulbQW4z1ZXfu3KmuXbvqhhtuUKVKlVwbAAAArg3GgmL//v0VHx+vihUr6quvvlLPnj01dOhQU+UA\n",
              "AABgM2NBMTs7W7169ZKfn58aNmyoqVOn6sMPPzRVDgAAwIXb49jDWFAsVaqUJKlChQpKS0tTTk6O\n",
              "jh07ZqocAAAAbGZsMUvbtm11/PhxDRo0SE2aNFHp0qUVFxdnqhwAAIBLgcXtcexgJChalqXExERd\n",
              "f/316t27t9q0aaOsrCw1aNDARDkAAAAYYCxud+zY0fXnGjVqEBIBAACuMUY6ig6HQ2FhYTp27Jgq\n",
              "V65sogQAAMBFOXkyiy2MXaNYvnx5RUdH684771T58uVd+19/neehAgAAXAuMBcWGDRuqYcOGpk4P\n",
              "AABwUb52mxpvMRYUR44caerUAAAAuApsD4pjxoz53fefe+45u0sCAAAUwu1x7GF7UDx16pQk6cCB\n",
              "A/riiy/UrVs3ORwOJScn64477rC7HAAAAAyxPSiOHz9ektSpUyelpqbqhhtukHSh0xgfH293OQAA\n",
              "ABhi7BrFQ4cOuUKiJIWGhurgwYOmygEAALg4WcxiC2MT+GFhYRo5cqT279+v/fv3a9SoUQoLCzNV\n",
              "DgAAADYzFhRnzZql7du3Kzo6WtHR0dqxY4dmzZplqhwAAIBLgfy8tvkSY1PP1atX18KFC02dHgAA\n",
              "AIYZC4qS9O2332r37t3Kz8937Xv44YdNlgQAAIBNjAXFgQMHavny5YqOjpa/v7+kC8+AJigCAADT\n",
              "uI+iPYwFxRUrVmjbtm0qU6aMqRIAAAAwyFhQDA0NVWBgoKnTAwAAXJTTxxaVeIvtv8Xk5GQlJyfr\n",
              "1ltvVc+ePbVw4ULXvuTkZLvLAQAAXLMGDx6siIgIORwOpaamXvS4pKQk1a1bV3Xq1FH//v2Vl5cn\n",
              "SZo5c6brDjPR0dGqXLmyunfvLklKS0uTv79/ofd3797t0fhs7yhOmDCh0Ot33nnH9WeHw6Fu3brZ\n",
              "XRIAAKCQAuvauOF2z549NWzYMLVu3fqix+zdu1fPPvusUlJSVK1aNd17772aNm2a/vrXv+rRRx/V\n",
              "o48+6jq2QYMG+stf/uJ6XaFChd8NoJdie1BctWqV3acEAADwSW3btr3kMYsXL1a3bt1UvXp1SdLj\n",
              "jz+ul156SX/9618LHfftt98qIyPD1qacsQn8Zs2aFWvf/8rJyVF2dnahLScnx8QQAQAAbGd3lklP\n",
              "T1fNmjVdryMiIpSenl7kuKSkJD300EMqVaqUa9+ZM2fUtGlTxcbGasyYMSooKPCotrGg+Nt7J0pS\n",
              "Xl6eTp06dcnPjR07VsHBwYW2sWPHmhomAADwQd58Mos3ssyZM2c0f/589e3b17UvNDRUBw8e1Pff\n",
              "f68VK1ZozZo1eu211zw6r+1Bcdy4capYsaK2bNmiSpUqubagoKBitVeHDx+urKysQtvw4cPtHiYA\n",
              "AIARdmeZ8PBw7du3z/U6LS1N4eHhhY5ZtGiRbr75Zt10002ufYGBgapataokqVKlSnrssce0Zs0a\n",
              "j2rbfo3i448/rl69emngwIGaMmWKa39QUJAqVqx4yc8HBgZyWx0AAHBFnF684bbdWaZHjx5q3bq1\n",
              "Ro0apWrVqmnKlCmKi4srdExSUlKhbqIkZWRkqGLFiipVqpRycnK0dOlSxcTEeFTb9t9icHCwIiIi\n",
              "9Omnn6pmzZqurTghEQAA4I8kISFBYWFhOnDggDp37qzIyEhJUr9+/Vy3Faxdu7ZGjx6tVq1aKTIy\n",
              "UlWqVFFCQoLrHD/99JNSU1PVq1evQudeu3atYmJi1LhxY8XGxqp69eoaMWKER+NzWJZlXeHPWMiD\n",
              "Dz6oefPmKSYmRg5H0aXpKSkpdpYDgBKh9qTXvT0E/MeeIYneHgJKgAW7mnqtdq/I771W2262Tz0P\n",
              "HTpUkjRx4kS7Tw0AAFAsBTyZxRa2B8Xk5GSdPn1aLVq0UOnSpe0+PQAAAK4S24NiRkaGEhIStH//\n",
              "frVo0UK33Xabbr/9djVr1kz+/v52lwMAACjiWnkyS0lne1/2nXfe0Y4dO7Rr1y717dtX6enpeuSR\n",
              "R1SxYkXddddddpcDAACAIbZ3FH8VGhqqHj16KDQ0VNWrV9e8efOu6FmDAAAAuLpsD4qrV6/Wl19+\n",
              "qVWrVungwYNq3ry52rZtq08++UR169a1uxwAAEARThaz2ML2oNi+fXs1b95czz33nLp06WL36QEA\n",
              "AHCVGOsovvrqqxo0aJCaNWum9u3bq3379qpXr57d5QAAAIoo8OKTWXyJ7UGxdevWat26tZ555hnl\n",
              "5ubq22+/1apVq9StWzedPn1aBw4csLskAAAADDC2mOXQoUNatWqVvvzyS61cuVIZGRlq3bq1qXIA\n",
              "AAAuTnF7HDvYHhT79++vr776SocOHXLdR3H27Nlq2rSpAgKM5VIAAADYzPbkVqNGDSUlJal58+Yq\n",
              "VaqU3acHAADAVWJ7UHzuuefsPiUAAIBHWMxiD36LAAAAcIuLBgEAgM8poBdmC36LAAAAcMtYRzE/\n",
              "P19LlizR7t27lZ+f79rPNYwAAADXBmNBMS4uTkeOHFGzZs3k7+9vqgwAAEARTov7KNrBWFDcsmWL\n",
              "duzYIYeDLwoAAOBaZCwo1qhRQ7m5uQoMDDRVAgAAwC0Ws9jD9qD4xhtvSJIiIyPVvn17/fnPf1aZ\n",
              "MmVc7w8ePNjukgAAADDA9qC4adMm15/r16+v7du3u14zDQ0AAK4GJzfctoXtQXHmzJl2nxIAAABe\n",
              "YCxuT506VVlZWZKkQYMG6ZZbbtHq1atNlQMAAIDNjAXFt99+W8HBwfr666+1ZcsWvfjiixo6dKip\n",
              "cgAAAC4Fcnht8yXGgmJAwIVZ7ZUrV+rhhx9W586dC914GwAAACWbsdvj+Pn5acGCBVqwYIE++eQT\n",
              "SVJubq6pcgAAAC4sZrGH0annefPmqX///qpZs6Z+/vln3X777abKAQAAwGbGOoq33nqrPvzwQ9fr\n",
              "evXque6xCAAAgJLPWFCUpIULFyo1NVXnz5937Xv99ddNlgQAAPC5RSXeYmzqefDgwfrggw80a9Ys\n",
              "ORwOLV682HW7HAAAAJR8xoLiqlWr9NFHH6lKlSp67bXX9N133+nAgQOmygEAALg4LT+vbb7E2E9T\n",
              "pkwZ+fn5yeFwKC8vT9WrV9ehQ4dMlQMAAIDNjF2jWKFCBZ09e1atW7dWnz59VL16dZUrV85UOQAA\n",
              "AJcCH+vseYux3+K8efMUEBCg8ePHq1GjRipVqpQWL15sqhwAAABsZntHMTs7WydOnFBERIRr34gR\n",
              "I7R3714FBwfbXQ4AAACG2N5RHDZsmDZu3Fhk/6ZNm/TUU0/ZXQ4AAKAIpxxe23yJ7UHxu+++U48e\n",
              "PYrs7969u1avXm13OQAAABhi+9Rzfn7+Rd/z8+PCUgAAYB6LWexh+28xLy9P2dnZRfZnZWUpLy/P\n",
              "7nIAAAAwxPagGBcXp4ceekiZmZmufZmZmXr00UcVFxdndzkAAAAYYntQfOaZZxQSEqIaNWooJiZG\n",
              "MTExqlGjhipUqKBnn33W7nIAAABFOC2H1zZfYvs1iv7+/nrvvff03HPPKSUlRZIUGxurOnXq2F0K\n",
              "AAAABhl7MkudOnUIhwAAwCsKzD1T5A+F3yIAAADcMtZRBAAA8BZfu1bQW+goAgAAwC2CIgAAANxi\n",
              "6hkAAPgcJ70wW/BbBAAAgFt0FAEAgM8pYDGLLegoAgAAwC2CIgAAANxi6hkAAPgc7qNoDzqKAAAA\n",
              "cIuOIgAA8DlOi16YHfgtAgAAwC06igAAwOcUiGsU7UBHEQAAAG4RFAEAAOAWU88AAMDncHsce9BR\n",
              "BAAAgFt0FAEAgM/h9jj24LcIAAAAtwiKAAAAcIupZwAA4HOc3EfRFnQUAQAA4BYdRQAA4HMKuD2O\n",
              "LegoAgAAeMngwYMVEREhh8Oh1NTUix6XlJSkunXrqk6dOurfv7/y8vIkSV9++aXKli2r6Oho13bu\n",
              "3LlLfq64CIoAAMDnOC0/r22e6Nmzp9auXauaNWte9Ji9e/fq2Wef1Zo1a7Rr1y798ssvmjZtmuv9\n",
              "qKgopaamurayZcsW63PFQVAEAADwkrZt2yosLOx3j1m8eLG6deum6tWry+Fw6PHHH9e8efMuee7L\n",
              "/dxvERQBAABslJOTo+zs7EJbTk7OZZ8vPT29UMcxIiJC6enprte7d+9WbGysmjZtqsmTJxf7c8XB\n",
              "YhbgGlZ70uveHgL+Y8+QRG8PAcBvePNZz2PHjtXo0aML7Rs5cqRGjRple63Y2FgdOHBAwcHBOnDg\n",
              "gO68805VrlxZDzzwgC3np6MIAABgo+HDhysrK6vQNnz48Ms+X3h4uPbt2+d6nZaWpvDwcElSUFCQ\n",
              "goODJUlhYWF68MEHtWbNmkt+rrgIigAAwOc45fDaFhgYqKCgoEJbYGDgZf8sPXr0UHJyso4cOSLL\n",
              "sjRlyhTFxcVJkg4fPiyn0ylJOnXqlD7++GPFxMRc8nPFRVAEAADwkoSEBIWFhenAgQPq3LmzIiMj\n",
              "JUn9+vVTcnKyJKl27doaPXq0WrVqpcjISFWpUkUJCQmSpCVLlqhhw4Zq3Lixmjdvro4dO+rRRx+9\n",
              "5OeKy2FZlmXjzwvgKuIaxZKDaxSBkuUv3/b3Wu05t073Wm27sZgFAAD4HG8uZvElTD0DAADALTqK\n",
              "AADA53j6hBS4x28RAAAAbhEUAQAA4BZTzwAAwOewmMUedBQBAADgFh1FAADgc5yio2gHOooAAABw\n",
              "i44iAADwOVyjaA86igAAAHCLoAgAAAC3mHoGAAA+h6lne9BRBAAAgFt0FAEAgM+ho2gPOooAAABw\n",
              "i6AIAAAAt5h6BgAAPoepZ3vQUQQAAIBbdBQBAIDP4VnP9qCjCAAAALfoKAIAAJ/DNYr2oKMIAAAA\n",
              "twiKAAAAcIupZwAA4HOYerYHHUUAAAC4RUcRAAD4HDqK9qCjCAAAALcIigAAAHCLqWcAAOBzmHq2\n",
              "Bx1FAAAAuGU0KC5fvtzk6QEAANyyLIfXNl9iNCiOGTNGUVFRmjRpkrKzs02WAgAAgM2MBsWvv/5a\n",
              "8+fP19atW1WvXj098cQT2rZtm8mSAAAAcsrhtc2XGL9GMSYmRtOnT9eyZcv08ccfq1GjRurYsaO2\n",
              "bNliujQAAACugPGguGLFCt17773q3r27/vrXv+rIkSNKSEjQn//8Z9OlAQAAcAWM3h7nxhtvVOXK\n",
              "lTV48GB1795d/v7+kqSePXsqKSnJZGkAAPAHxu1x7GE0KM6ePVtNmjRx+96nn35qsjQAAACukNGp\n",
              "540bN+rEiROu18ePH9f06dNNlgQAAOD2ODYxGhQnT56sSpUquV5ff/31mjx5ssmSAAAAsInRoGhZ\n",
              "VpF9BQUFJksCAADAJkaDYmhoqBYuXOh6vWDBAoWGhposCQAAIKfl8NrmS4wuZpk4caLuvfdeDRs2\n",
              "TJJUrlw5ffTRRyZLAgAAwCZGg2L9+vW1bds2/fTTT5KkqKgo1y1yAAAATPG1RSXeYjQoSpLD4VBI\n",
              "SIjy8/N18OBBSVJ4eLjpsgAAALhCRoPirFmzNHjwYJUqVUp+fhcuh3Q4HMrIyDBZFgAA/MH52rWC\n",
              "3mI0KD7//PP6/vvvFRUVZbIMAAAADDC66rly5cqERAAAgGuU0aB43333aeLEicrIyFB2drZrAwAA\n",
              "MMmyvLf5EqNTzyNGjJAkJSYmyuFwyLIsORwObroNAABwDTAaFJ1Op8nTAwAAuOUUi1nsYHTqWZI2\n",
              "btyoDz74QJJ08uRJHT582HRJAAAA2MBoUJw8ebIee+wxjRo1SpJ0/Phx9e7d22RJAAAA2MRoUJw2\n",
              "bZq++eYbBQUFSZLq1Kmjo0ePmiwJAAAgy3J4bfMlRoNiYGCgypYtW2hfQIDxh8EAAADABkZTW5Uq\n",
              "VfTzzz/L4biQrmfNmsXj+wAAgHE8mcUeRoPixIkT9eCDD2rHjh2qUaOGgoKC9PHHH5ssCQAAAJsY\n",
              "C4oFBQXavHmzvv32W/3000+yLEtRUVHy9/c3VRIAAECS79342luMXaPo7++vF198UX5+frrxxht1\n",
              "0003ERIBAACuIUYXs8TGxmrt2rUmSwAAAMAQo9cofvPNN5o1a5Zq166t8uXLu/anpKSYLAsAAP7g\n",
              "fO02Nd5iNCi+/fbbJk8PAAAAg4wuZhk4cKC2bdtmqgQAAIBbdBTtYXQxS5UqVXT27FlTJQAAAGCQ\n",
              "0annyMhItWrVSvfff3+haxQHDx5ssiwAAABsYDQoOp1ORUdHa+fOna59vz6lBQAAwBSezGIPo0Fx\n",
              "5syZJk8PAAAAg4wGxffff9/t/ocffthkWQAA8AfHk1nsYfSG2//6179c26JFizRkyBDNmzfPZEkA\n",
              "AIBrxuDBgxURESGHw6HU1NSLHpeUlKS6deuqTp066t+/v/Ly8iRJK1euVLNmzXTTTTfp5ptv1rBh\n",
              "w+R0OiVJaWlp8vf3V3R0tGvbvXu3R+Mz2lFctGhRodd79+7ViBEjTJYEAAC4Zm6P07NnTw0bNkyt\n",
              "W7e+6DF79+7Vs88+q5SUFFWrVk333nuvpk2bpr/+9a+qWLGi5s+fr9q1a+v8+fPq0KGD3n//fcXH\n",
              "x0uSKlSo8LsB9FKMdhT/V61atfTjjz9ezZIAAAAlVtu2bRUWFva7xyxevFjdunVT9erV5XA49Pjj\n",
              "j7tmaGNiYlS7dm1JUpkyZRQdHa20tDTbxme0o5icnOz6c0FBgb799lsFBgb+7mdycnKUk5NTaF9g\n",
              "YOAlPwcAAFAS2J1l0tPTVbNmTdfriIgIpaenFznuyJEjWrx4sT7++GPXvjNnzqhp06YqKCjQfffd\n",
              "pxEjRsjf37/YtY12FCdMmODaJk+erKysLC1YsOB3PzN27FgFBwcX2saOHWtymAAAwMdYlsNrmzey\n",
              "THZ2tu655x4NGzZMt9xyiyQpNDRUBw8e1Pfff68VK1ZozZo1eu211zw6r9GO4qpVqzz+zPDhw5WY\n",
              "mFhoH91EAABwrbA7y4SHhxdahJKWlqbw8HDX61OnTqlLly669957C9UNDAxU1apVJUmVKlXSY489\n",
              "prlz52rYsGHFrm20ozht2jSdOHHC9fr48eOaPn36734mMDBQQUFBhTaCIgAA8ITlxc3uLNOjRw8l\n",
              "JyfryJEjsixLU6ZMUVxcnCTp9OnT6tKli7p06aJnnnmm0OcyMjJcq6NzcnK0dOlSxcTEeFTbaFCc\n",
              "PHmyKlWq5Hp9/fXXa/LkySZLAgAAXDMSEhIUFhamAwcOqHPnzoqMjJQk9evXz7XWo3bt2ho9erRa\n",
              "tWqlyMhIValSRQkJCZKkSZMm6bvvvtPSpUtdt8B58cUXJUlr165VTEyMGjdurNjYWFWvXt3ju884\n",
              "LMvcLSkbN26szZs3F9rXqFEj/fDDD6ZKAn8otSe97u0h4D/2DEm89EEArpqopWO8Vvun7s95rbbd\n",
              "jHYUQ0NDtXDhQtfrBQsWKDQ01GRJAAAAry5m8SVGF7NMnDhR9957r+uiyXLlyumjjz4yWRIAAAA2\n",
              "MRoU69evr23btumnn36SJEVFRXl07x4AAIDLwrOebWE0KErSxo0btWLFCklSp06dXPf2AQAAQMlm\n",
              "/PY4PXv2VEZGho4ePaoePXro3XffNVkSAAAANjHaUXzrrbe0ceNGValSRZL0z3/+U3fccYf69etn\n",
              "siwAAPiD87VFJd5itKMoyRUS//fPAAAAKNmMdhTr1q2rESNGuG4KOX36dNWtW9dkSQAAAJm7S/Qf\n",
              "i9GO4pQpU7R7927FxsaqSZMm2rVrl9555x2TJQEAAGATox1Fh8Oh+fPnF9p37NgxkyUBAAC4RtEm\n",
              "RjuKnTp1KtY+AAAAlDxGOoq5ubk6f/68CgoKdOrUKf36OOmsrCydOXPGREkAAADYzEhHcezYsQoJ\n",
              "CdHWrVsVHByskJAQhYSEqGHDhurTp4+JkgAAAP9lOby3+RAjQXHkyJFyOp0aMGCAnE6nazt58qSe\n",
              "ffZZEyUBAABgM6OLWd555x2lp6dr9erVcjgcatu2rWrUqGGyJAAAALfHsYnRxSxz585VTEyMlixZ\n",
              "osWLFys2NrbIKmgAAACUTEY7imPGjNGGDRtUq1YtSVJaWpq6dOmiuLg4k2UBAABgA6NBsVy5cq6Q\n",
              "KEkREREqV66cyZIAAAASU8+2MDr1fNddd2nUqFE6cOCA9u/frzFjxuiee+5Rdna2srOzTZYGAADA\n",
              "FXJYlrnLPf38Lp5DHQ6HCgoKTJUG/hBqT3rd20PAf+wZkujtIQD4jVpzxnqt9t6/DPdabbsZnXp2\n",
              "Op0mTw8AAACDjE49T506VefOnTNZAgAAoCjLi5sPMRoUV69erVq1aun//b//p127dpksBQAAAJsZ\n",
              "DYpz5szR5s2bdf311+uOO+5Q165d9e9//9tkSQAAANjEaFCUpGrVqumZZ57Re++9px9//FF9+vRR\n",
              "/fr19cUXX5guDQAA/qAsy+G1zZcYXcxy/vx5zZ49W2+//bbKlSun8ePHq2fPntq0aZN69uyptLQ0\n",
              "k+UBAABwBTwOiocPH9ZPP/2k9u3bKz8/X06nU6VLl3Z7bEREhDp27Khp06apadOmrv233HKLOnbs\n",
              "ePmjBgAA+D0+tqjEWzyael68eLGaN2+u+Ph4SdKPP/6o++67z+2xBQUFeuWVV/TBBx8UCom/mj59\n",
              "useDBQAAwNXjUVAcO3asUlJSVLFiRUlS48aNtW/fPrfH+vv7a+LEiVc8QAAAAHiHR0HR399f119/\n",
              "faF9F5t2lqTY2FitXbv28kYGAABw2Rxe3HyHR9coVqhQQb/88oscjgu/hC+++EKVKlW66PHffPON\n",
              "Zs2apdq1a6t8+fKu/SkpKZc5XAAAAFwtHgXFcePGqWvXrtqzZ49at26tvXv36pNPPilyXHZ2tk6c\n",
              "OKG333670P49e/b8brAEAACwBYtZbOFRULzlllu0atUqrVu3TpZlqWXLlgoJCSly3LBhw9SxY0f1\n",
              "6NGj0P7jx49r2bJluvfee69o0AAAADDP4xtuZ2Vl6fjx48rMzNSpU6fcHvPdd98VCYmS1L17d61e\n",
              "vdrzUQIAAHiCZz3bwqOgOHfuXMXExGjp0qVavHixYmNjNX/+/CLH5efnX7ygn/GHwQAAAMAGHk09\n",
              "jxkzRhs2bFCtWrUkSWlpaerSpYvi4uIKHZeXl6fs7GwFBQUV2p+VlaW8vLwrHDIAAACuBo/ae+XK\n",
              "lXOFROnCk1fKlStX5Li4uDg99NBDyszMdO3LzMzUo48+WiRUAgAA2M5yeG/zIR4FxbvuukujRo3S\n",
              "gQMHtH//fo0ZM0b33HOPsrOzlZ2d7TrumWeeUUhIiGrUqKGYmBjFxMSoRo0aqlChgp599lnbfwgA\n",
              "AADYz2FZVrEvu/y96wsdDocKCgoK7du9e7frnomxsbGqU6fOZQ4TgDu1J73u7SHgP/YMSfT2EAD8\n",
              "Rs0Zr3it9r7Hhnmttt08ukbR6XR6dPI6deoQDgEAAK5RHk09T506VefOnTM1FgAAAJQgHgXF1atX\n",
              "q1atWvp//+//adeuXabGBAAAcGW4j6ItPAqKc+bM0ebNm3X99dfrjjvuUNeuXfXvf//b1NgAAADg\n",
              "RR7f/bpatWp65pln9N577+nHH39Unz59VL9+fX3xxRcmxgcAAOA5bo9ji2IFxZ07d0qSzp8/r3ff\n",
              "fVcxMTEaMWKExo8fr6NHj2r27Nnq27ev0YECAADg6irWqudevXopJSVFERER6tixo6ZNm6amTZu6\n",
              "3r/lllvUsWNHY4MEAADwhMPHrhX0lmIFxV9vtbhp0yaFhoa6PWb69On2jQoAAABeV6ygmJWVpX/9\n",
              "61+62L25u3XrZuugAAAA4H3FCopHjx7VhAkT3AZFh8NBUAQAACULU8+2KFZQjIyM1MqVK02PBQAA\n",
              "ACWIR4/wAwAAuCb42G1qvKVYt8dhahkAAOCPp1hBcfTo0abHAQAAgBKGqWcAAOB7WMxiC48f4QcA\n",
              "AIA/BjqKAADA99BRtIVHQTElJUX//Oc/tWfPHuXn57v279mzx/aBAQAAwLs8CoqPPPKIBg0apBYt\n",
              "Wsjf39/UmAAAAK4MHUVbeBQU/f39lZCQYGosAAAAKEE8WszSqlUrbdiwwdRYAAAAUIJ41FFcvXq1\n",
              "pk+frsjISJUpU8a1PyUlxfaBAQAAXDaezGILj4LiW2+9ZWocAAAAKGE8Cort2rWTJB06dEiSdMMN\n",
              "N9g/IgAAgCvkYDGLLTy6RnH79u26+eabXVvDhg21Y8cOU2MDAACAF3kUFJ944gmNGDFCmZmZyszM\n",
              "1IgRIzRw4EBTYwMAAIAXeRQUMzMz1bt3b9fruLg4ZWZm2j4oAACAK2J5cfMhHgVFf39/bdu2zfV6\n",
              "27Zt3HgbAADAR3m0mOWll15S27Zt1ahRI0nSli1bNGfOHCMDAwAAgHd51FHs3Lmztm3bpsTERCUm\n",
              "Jmr79u3q1KmTqbEBAAD4tMGDBysiIkIOh0OpqakXPS4pKUl169ZVnTp11L9/f+Xl5V3xe8XhUVCU\n",
              "pKpVq+ruu+/W3XffrcqVK3v6cQAAAOMclvc2T/Ts2VNr165VzZo1L3rM3r179eyzz2rNmjXatWuX\n",
              "fvnlF02bNu2K3iuuYgXFX++fWLFiRVWqVMm1/foaAAAAnmvbtq3CwsJ+95jFixerW7duql69uhwO\n",
              "hx5//HHNmzfvit4rrmJdozh//nxJ+t2WKAAAAKScnBzl5OQU2hcYGKjAwMDLOl96enqhjmNERITS\n",
              "09Ov6L3iKlZQDA0NlSR98MEHeuaZZwq998ILLxTZZ7fak143en4U354hid4eAn6D7wMALsKLz3oe\n",
              "O3asRo8eXWjfyJEjNWrUKO8M6Ap4dI3i0qVLi7UPAADgj2r48OHKysoqtA0fPvyyzxceHq59+/a5\n",
              "XqelpSk8PPyK3iuuYnUUly9frmXLlungwYNKTPxvByMrK8ujYgAAAFeFF298fSXTzO706NFDrVu3\n",
              "1qhRo1StWjVNmTJFcXFxV/RecRWro1imTBmFhITIz89PwcHBrq1BgwZ0FAEAAC5TQkKCwsLCdODA\n",
              "AXXu3FmRkZGSpH79+ik5OVmSVLt2bY0ePVqtWrVSZGSkqlSpooSEhCt6r7gclmUVO3Nv3rxZjRs3\n",
              "9qiAHbhGseTgmjgAwLWg9kTvZYc9T/rO/1d6dI3im2++qePHj7teHzt2zONkCgAAYBzPeraFR0Fx\n",
              "48aNuv76612vK1eurO+//972QQEAAMD7PHrWc35+fqHXlmUpNzfX1gEBAABcKU+fkAL3POooNm/e\n",
              "XIMGDdK+ffuUlpamv/3tb2revLmpsQEAAMCLPAqKr732ms6ePaumTZvq1ltvVU5OjiZMmGBqbAAA\n",
              "APAij6aeg4KCNGPGDFNjAQAAsAdTz7bwKChK0qFDh7R161adP3/eta9bt262DgoAAADe51FQnDFj\n",
              "hsaMGaMTJ06obt262rx5s5o3b05QBAAAJQsdRVt4dI3ihAkTtGnTJtWpU0cbN27UypUrVa9ePVNj\n",
              "AwAAgBd5FBRLly6tihUrum6T07ZtW6WmppoYFwAAwGVzWN7bfIlHU8+BgYGyLEv16tXTxIkTVbNm\n",
              "TZ0+fdrU2AAAAOBFHgXFF154QdnZ2XrllVf0+OOPKzMzU5MnTzY1NgAAAHiRR0Hx9ttvlyQFBwfr\n",
              "888/NzIgAACAK2Y5vD0Cn+DRNYrPPfecTp48KcuydNddd6ly5cpasmSJqbEBAADAizwKih999JFC\n",
              "QkK0YsUKBQQE6Ouvv9YLL7xgamwAAACXx/Li5kM8Cop+fhcO/+qrr3T//fcrKipKDgetXQAAAF/k\n",
              "0TWK1113ncaNG6f58+fr66+/lmVZys3NNTU2AAAAeJFHHcVZs2bp8OHDeuWVV1StWjXt3r1bffr0\n",
              "MTU2AACAy8J9FO3hUUcxMjJSEydOLPT66aeftntMAAAAKAE8CoppaWkaN26cdu/e7Xo6iyStXLnS\n",
              "9oEBAABcNh/r7HmLR0HxgQce0B133KFBgwbJ39/f1JgAAABQAngUFM+fP6+xY8eaGgsAAIAtfO1a\n",
              "QW/xaDFLgwYNlJ6ebmosAAAAKEE86igePXpUjRs3VosWLVSmTBnX/qVLl9o+MAAAAHiXR0GxT58+\n",
              "xbodjmVZOnLkiEJDQy97YAAAAJeNqWdbeBQUH3nkkWIf27FjR23dutXjAQEAAKBk8CgoStLChQuV\n",
              "mpqq8+fPu/a9/vrrhY5xOBwKCwvTsWPHVLly5SsfJQAAgCfoKNrCo6A4ePBg7d27Vxs3btSDDz6o\n",
              "RYsWqWPHjm6PLV++vKKjo3XnnXeqfPnyrv3/GyoBAABQMnkUFFetWqXNmzcrJiZGr732mv7xj39c\n",
              "dDq6YcOGatiwoS2DBAAAwNXnUVAsU6aM/Pz85HA4lJeXp+rVq+vQoUNujx05cqQtAwQAAPAU91G0\n",
              "h0f3UaxQoYLOnj2r1q1bq0+fPhoyZIjKlSvn9tj9+/fr7rvvVnR0tCQpNTVVEyZMuOIBAwAA4Orw\n",
              "KCjOmzdPAQEBGj9+vBo1aqRSpUpp8eLFbo9NSEhQXFycLOtCpG/QoIFmzJhx5SMGAADAVVHsoFhQ\n",
              "UKChQ4eqdOnSKlu2rEaMGKFXX31VNWrUcHt8RkaG+vTpIz+/CyUCAgIUEODxImsAAAB4SbGTm7+/\n",
              "v37++efinzggwNVNlKTMzMxCrwEAAIwhctjCoxbfbbfdpgEDBig+Pr7QLW8aNWpU5Nj7779fCQkJ\n",
              "ys7O1rvvvqspU6aoX79+Vz5iAAAAXBXFCooPPvig5s2bpwULFkiSPv/8c9d7DodDe/bsKfKZv//9\n",
              "75o3b56ysrL02WefKTExUb1797Zp2AAAADCtWEFxx44dkqS9e/de8tgePXpoyZIleuWVVzRs2DA9\n",
              "+OCDVzZCAAAAD3F7HHsUazGLw+Eo9gl/+uknWZal+fPnX/agAAAA4H3F6ij+8MMPqlSpUpH9lmXJ\n",
              "4XDoxIkTrn233nqrKlSooJycnEKfcXcsAACAEXQUbVGsoBgVFaV///vfxTphUlKSXnrpJd1+++3F\n",
              "/gwAAABKnmIFxcDAQNWsWbPYJ61WrZrWrVun4OBgSRe6iadPn1aFChUub5QAAAC46op1jeLl3P8w\n",
              "MTFRJ0+eVG5urqKjo1WtWjVNnjzZ4/MAAAB4zPLi5kOKFRQ3bdrk8Yk3btyokJAQLVu2TDExMTpy\n",
              "5IimTJni8XkAAADgHcaeqfdrF3LNmjW6++67FRQUJH9/f1PlAAAAXLg9jj2K/axnT1WvXl0DBw7U\n",
              "okWL1KFDB+Xl5amgoMBUOQAAANjMWFCcM2eOoqKiNH/+fIWEhOjgwYNKTEw0VQ4AAOC/uEbRFsam\n",
              "nitXrqwnn3zS9ToiIkLx8fGmygEAAMBmxoJiSkqKRowYod27dys/P9+1391zoQEAAFDyGAuKjzzy\n",
              "iAYNGqQWLVqwiAUAAFxVLGaxh7Gg6O/vr4SEBFOnBwAAgGHGFrO0atVKGzZsMHV6AACAi2Mxiy2M\n",
              "dRRXr16t6dOnKzIyUmXKlHHtT0lJMVUSAAAANjIWFN966y1TpwYAAMBVYCwotmvXztSpAQAAfp+P\n",
              "TQF7i7GgeO7cOb355ptKTU3V+fPnXfuXLl1qqiQAAABsZGwxS//+/ZWWlqZ169bptttu0759+1Sz\n",
              "Zk1T5QAAAFwclvc2X2IsKG7evFmTJ09WUFCQ/va3v+nLL7/Uxo0bTZUDAACAzYxNPZctW/ZCgYAA\n",
              "nTlzRhUqVNDRo0dNlQMAAPgvH+vseYuxoFipUiVlZmbqzjvvVOfOnVW5cmWFhYWZKgcAAACbGQuK\n",
              "n3zyifz9/fX8889r7ty5yszM1MMPP2yqHAAAAGxmJCgWFBSoc+fOWrFihRwOh/7yl7+YKAMAAOAe\n",
              "U8+2MLKYxd/fX2fPnpXT6TRxegAAAFwFxqaemzZtqrvvvlt9+vRR+fLlXfu7detmqiQAAIAk37tN\n",
              "jbfYHhTbtWunr776Sj/88IMkafr06a73HA4HQREAAOAaYXtQzM7OliStWrXK7lMDAADgKrI9KDqd\n",
              "Tp06dUqW5b7nGxQUZHdJAACAwph6toXtQXHLli0KCQlxGxQdDocKCgrsLgkAAAADbF/13LhxYxUU\n",
              "FMjpdBbZCIkAAOBq4FnP9rA9KDocDrtPCQAA4JN27typli1bql69emratKl+/PHHIsc4nU4NHTpU\n",
              "DRo0UP369dW3b1/l5uZKkpYvX67o6GjXdsMNNyg2Ntb1WYfDoYYNG7reX7NmjUfjsz0ohoaG2n1K\n",
              "AAAAz1he3DyQkJCgAQMG6Oeff9ZTTz2l+Pj4IsckJSUpJSVFKSkp2r59u/z8/DRp0iRJUufOnZWa\n",
              "muraYmNjizzoZM2aNa7327Rp49H4bA+Kn3zyid2nBAAA8DkZGRnasGGD+vTpI0nq0aOH9u/fr127\n",
              "dhU6bvPmzerQoYNKly4th8Ohrl276oMPPihyvkOHDumLL77QQw89ZNsYjTyZ5Urk5OQoOzu70Gbl\n",
              "53t7WAAAAMXiLsvk5OQUOW7//v0KDQ1VQMCFtcUOh0Ph4eFKT08vdFyTJk2UnJys7Oxs5eXlaeHC\n",
              "hUpLSytyvlmzZunOO+9U1apVC+2/44471LhxYyUmJurMmTMe/SwlLiiOHTtWwcHBhbaTn3/h7WEB\n",
              "AIBriRennt1lmbFjx172jxIfH68uXbqoXbt2ateunerVq+cKl64f17I0Y8YM9e3bt9D+ffv2aePG\n",
              "jVq3bp2OHj2qf/zjHx7VdlgXu+GhTXJychQYGOjR8f+buhtPnyJHgLGnDcIDe4YkensIAABcUsPE\n",
              "CV6rvWHsE0WyTGBgYJE8lJGRocjISJ04cUIBAQGyLEuhoaFau3atIiMjL3r++fPn6+233y60MOXL\n",
              "L79Unz59tG/fPvn7+7v93Pr16zVgwABt2bKl2D+LsY7iDz/8oAYNGqhOnTqSpI0bN2rYsGGX/Fxg\n",
              "YKCCgoIKbYREAADgCYcXN3dZxl3TrGrVqoqNjdXs2bMlSUuWLFFYWFiRkHj+/HllZmZKko4dO6aX\n",
              "X365SKZKSkpSfHx8oZCYmZmps2fPSrqwcnrBggWKiYnx6PdoLCgOHjxYU6ZMUZUqVSRJsbGxLHQB\n",
              "AAD4jalTp2rq1KmqV6+eXn75Zc2cOVOS1K9fPyUnJ0uSsrKy1LJlS918881q06aNHn/8cd1zzz2u\n",
              "c2RlZWnp0qV67LHHCp17x44dat68uRo3bqyGDRvq+PHjmjhxokfjM9aqO336tFq3bu167XA4VLp0\n",
              "aVPlAAAArjlRUVFav359kf3vvvuu68/VqlXT9u3bL3qO4OBgt4tUWrRooR9++OGKxmcsKAYEBCgv\n",
              "L891A+79+/dfdM4cAADAVj72hBRvMTb1PGjQIN133306evSonnnmGbVp06ZY1ygCAACgZDDWUezT\n",
              "p49q166tjz76SLm5uZo9e3ahqWgAAABTfO2Zy95iJCgWFBSoYcOG2rZtm1q2bGmiBAAAAAwzMvXs\n",
              "7++vKlWquJZkAwAA4NpjbOo5MjJSrVq10v3336/y5cu79g8ePNhUSQAAgAuYeraFsaDodDoVHR2t\n",
              "nTt3uvb9ugIaAAAAJZ+xoPjrDSMBAACuOjqKtjAWFPPz8zVhwgR9/vnnkqTOnTtryJAhRR5iDQAA\n",
              "gJLJWGpLTEzU7t279cQTT8jhcOjdd9/Vvn379MYbb5gqCQAAIInb49jFWFD88ssvlZqaKj+/Cwur\n",
              "77rrLsXGxpoqBwAAAJsZezKLZVlyOp2FXlsW8R4AAOBaYayj2KVLF3Xq1Enx8fGSpPfff19du3Y1\n",
              "VQ4AAOC/6E3ZwlhQHDdunKZOnark5GRJUs+ePTVgwABT5QAAAGAzY0HRz89PAwcO1MCBA02VAAAA\n",
              "cIvFLPYwFhRPnjypqVOnavfu3crPz3ftnzFjhqmSAAAAsJGxoNizZ09VqVJFLVq0kL+/v6kyAAAA\n",
              "MMRYUDx8+LBWrFhh6vQAAAAXx9SzLYzdHqdOnTo6efKkqdMDAADAMNs7iomJiZKkcuXKKTY2Vl26\n",
              "dFGZMmVc77/++ut2lwQAACiExSz2sD0oBgcHu/77xhtvtPv0AAAAuEpsD4ojR460+5QAAACeoaNo\n",
              "C2PXKD733HM6efKkLMvSXXfdpcqVK2vJkiWmygEAAMBmxoLiRx99pJCQEK1YsUIBAQH6+uuv9cIL\n",
              "L5gqBwAAAJsZfTKLJH311Ve6//77FRUVJYfDYaocAADAfzH1bAtjQfG6667TuHHjNH/+fH399dey\n",
              "LEu5ubmmygEAAMBmxqaeZ82apcOHD+uVV15RtWrVtHv3bvXp08dUOQAAABeH5b3Nl9jeUWzRooXW\n",
              "r1+vt956SxMnTnTtj4yM1NNPP213OQAAABhie1A8efKkfvnlF61atUqnTp2SZRWO1kFBQXaXBAAA\n",
              "gAG2B8UHHnhAtWrVUk5Ojuvm2w6HQ5ZlyeFwqKCgwO6SAAAAhfnYFLC32H6N4ujRo3X27Fk1b95c\n",
              "TqdTTqdTBQUFrv8GAADAtcH2oNiiRQtJ0i233GL3qQEAAIrFYVle23yJsWsUv/zyS65RBAAAuIZx\n",
              "jSIAAPA9vtXY8xquUQQAAIBbxm64/fXXX+v48eM6fvy4qRIAAAAwyEhQnDlzpiIiIlSlShVVqVJF\n",
              "ERERmjFjholSAAAARfBkFnvYfo3i+++/r/Hjx+udd95xrYBet26dhg4dKn9/fz3yyCN2lwQAAIAB\n",
              "tgfF1157TcuWLVN4eLhr35133qmbbrpJ3bp1IygCAADzfKyz5y22Tz3n5+cXCom/ioiIYDELAADA\n",
              "NcT2oJibm6vz588X2X/u3Dnl5OTYXQ4AAACG2B4Uu3fvroceekgnT5507cvMzNTDDz+s7t27210O\n",
              "AACgCBaz2MP2oPjCCy+oVKlSCgsLU0xMjGJiYlSjRg0FBAToxRdftLscAAAADLF9MUupUqU0d+5c\n",
              "7dq1S5s2bZIkxcTEKDIy0u5SAAAA7vlYZ89bbA+Kv4qMjCQcAgAAXMOMBUUAAABv8bVrBb3F2CP8\n",
              "AAAAcG0jKAIAAMAtpp4BAIDvYerZFnQUAQAA4BYdRQAA4HNYzGIPOooAAABwi6AIAAAAt5h6BgAA\n",
              "vsdi7tkOdBQBAADgFh1FAADgc1jMYg86igAAAHCLjiIAAPA9dBRtQUcRAAAAbhEUAQAA4BZTzwAA\n",
              "wOc4nN4egW+gowgAAAC36CgCAADfw2IWW9BRBAAAgFsERQAAALjF1DMAAPA5PJnFHnQUAQAA4BYd\n",
              "RQAA4HssWop2oKMIAADgJTt37lTLli1Vr149NW3aVD/++GORY5xOp4YOHaoGDRqofv366tu3r3Jz\n",
              "cyVJaWlp8vf3V3R0tGvbvXu367Mff/yx6tevr7p166p79+7Kzs72aHwERQAA4HMclvc2TyQkJGjA\n",
              "gAH6+eef9dRTTyk+Pr7IMUlJSUpJSVFKSoq2b98uPz8/TZo0yfV+hQoVlJqa6trq1KkjSTp9+rT6\n",
              "9u2rDz/8UDt37tQNN9yg559/3qPxERQBAAC8ICMjQxs2bFCfPn0kST169ND+/fu1a9euQsdt3rxZ\n",
              "HTp0UOnSpeVwONS1a1d98MEHlzz/p59+qpiYGNWvX1+S9MQTT2jevHkejZGgCAAAYKOcnBxlZ2cX\n",
              "2nJycooct3//foWGhiog4MKSEYfDofDwcKWnpxc6rkmTJkpOTlZ2drby8vK0cOFCpaWlud4/c+aM\n",
              "mjZtqtjYWI0ZM0YFBQWSpPT0dNWsWdN1XEREhA4fPqz8/Pxi/yzXxGKWPUMSvT0E/EftSa97ewj4\n",
              "Df7ZAIri76mSw6t/R3lxLcvYsWM1evToQvtGjhypUaNGXdb54uPjtW/fPrVr105ly5ZVhw4d9Nln\n",
              "n0mSQkNDdfDgQVWtWlUnTpxQr1699Nprr2nYsGFX+mNIoqMIAABgq+HDhysrK6vQNnz48CLH1ahR\n",
              "o1CHz7IspaenKzw8vNBxDodDo0aN0qZNm7Ru3TrddNNNuvnmmyVJgYGBqlq1qiSpUqVKeuyxx7Rm\n",
              "zRpJUnh4uPbt2+c6T1paWqEOZnEQFAEAgM/x5mKWwMBABQUFFdoCAwOLjLFq1aqKjY3V7NmzJUlL\n",
              "lixRWFiYIiMjCx13/vx5ZWZmSpKOHTuml19+2dUxzMjIUF5enqQLU95Lly5VTEyMJKlLly5KSUnR\n",
              "jh07JEmTJ09WXFycR7/Ha2LqGQAAwBdNnTpV8fHxeumllxQUFKSZM2dKkvr166du3bqpW7duysrK\n",
              "Uvv27eXn5yen06khQ4bonnvukSStXbtWzz33nPz9/ZWfn6/bb79dI0aMkHRhNfS7776r++67T/n5\n",
              "+WrQoIHee+89j8bnsCzuSIni49qfkoVrFIGi+Huq5PDm31Ft/vyq12qv+b+hXqttNzqKAADA99AH\n",
              "swXXKAIAAMAtOooAAMDnePqEFLhHRxEAAABuERQBAADgFlPPAADA9zD1bAs6igAAAHCLjiIAAPA5\n",
              "LGaxBx1FAAAAuEVHEQAA+B4nLUU70FEEAACAWwRFAAAAuMXUMwAA8D3MPNuCjiIAAADcoqMIAAB8\n",
              "DrfHsQcdRQAAALhFUAQAAIBbTD0DAADfYzH3bAc6igAAAHCLjiIAAPA5LGaxBx1FAAAAuEVHEQAA\n",
              "+B46iragowgAAAC3CIoAAABwi6lnAADgcxzcHscWdBQBAADgFh1FAADge5zeHoBvoKMIAAAAtwiK\n",
              "AAAAcIupZwAA4HNYzGIPIx1Fy7J0+PBhE6cGAADAVWJs6rljx46mTg0AAPD7LC9uPsRIUHQ4HAoL\n",
              "C9OxY8dMnB4AAABXgbFrFMuXL6/o6GjdeeedKl++vGv/66+/bqokAADABVyjaAtjQbFhw4Zq2LCh\n",
              "qdMDAADAMGNBceTIkaZODQAAgKvA2GKW/fv36+6771Z0dLQkKTU1VRMmTDBVDgAAwMVheW/zJcaC\n",
              "YkJCguLi4mT95xqBBg0aaMaMGabKAQAAwGbGgmJGRob69OkjP78LJQICAhQQwP29AQDAVWBZ3tt8\n",
              "iLGgGBAQ4OomSlJmZmah1wAAACjZjAXF+++/XwkJCcrOzta7776rjh07ql+/fqbKAQAAwGbG5oL/\n",
              "/ve/a968ecrKytJnn32mxMRE9e7d21Q5AAAAF4fT2yPwDbYHxR49emjJkiV65ZVXNGzYMD344IN2\n",
              "lwAAAMBVYPvU808//STLsjR//ny7Tw0AAFA8LGaxhe0dxVtvvVUVKlRQTk6OKlWq5NpvWZYcDodO\n",
              "nDhhd0kAAAAYYHtQTEpK0ksvvaTbb79d//73v+0+PQAAwKX5VmPPa4wsZqlWrZrWrVun4OBgSRe6\n",
              "iadPn1aFChVMlAMAAIABxm6Pk5iYqJMnTyo3N1fR0dGqVq2aJk+ebKocAAAAbGYsKG7cuFEhISFa\n",
              "tmyZYmJidOTIEU2ZMsVUOQAAABeHZXlt8yXGguKvT2FZs2aN7r77bgUFBcnf399UOQAAANjMWFCs\n",
              "Xr26Bg4cqEWLFqlDhw7Ky8tTQUGBqXIAAAD/xe1xbGEsKM6ZM0dRUVGaP3++QkJCdPDgQSUmJpoq\n",
              "BwAAAJsZe4Rf5cqV9eSTT7peR0REKD4+3lQ5AAAA2MxYUExJSdGIESO0e/du5efnu/bv2bPHVEkA\n",
              "AIALeNazLYwFxUceeUSDBg1SixYtWMQCAABwDTIWFP39/ZWQkGDq9AAAABfla7ep8RZji1latWql\n",
              "DRs2mDo9AAAADDPWUVy9erWmT5+uyMhIlSlTxrU/JSXFVEkAAIAL6CjawlhQfOutt0ydGgAAAFeB\n",
              "saDYrl07U6cGAADAVWAsKJ47d05vvvmmUlNTdf78edf+pUuXmioJAABwAVPPtjC2mKV///5KS0vT\n",
              "unXrdNttt2nfvn2qWbOmqXIAAACwmbGguHnzZk2ePFlBQUH629/+pi+//FIbN240VQ4AAOC/nF7c\n",
              "fIixoFi2bFlJUkBAgM6cOaMKFSro6NGjpsoBAADAZsauUaxUqZIyMzN15513qnPnzqpcubLCwsJM\n",
              "lQMAAIDNjAXFTz75RP7+/nr++ec1d+5cZWZm6uGHHzZVDgAAwIUns9jDSFAsKChQ586dtWLFCjkc\n",
              "Dv3lL38xUQYAAAAGGQmK/v7+Onv2rJxOp/z8jF0GCQAA4B4dRVsYS3FNmzbV3Xffrblz5yo5Odm1\n",
              "AQAA4IKdO3eqZcuWqlevnpo2baoff/yxyDFOp1NDhw5VgwYNVL9+ffXt21e5ubmSpC1btqht27aq\n",
              "X7++GjRooMcee0znzp1zfdbhcKhhw4aKjo5WdHS01qxZ49H4bA+Kvz6R5YcfftC5c+c0ffp0TZgw\n",
              "QRMmTNDEiRPtLgcAAFCUZXlv80BCQoIGDBign3/+WU899ZTi4+OLHJOUlKSUlBSlpKRo+/bt8vPz\n",
              "06RJkyRJZcqU0VtvvaUdO3Zo8+bNOnPmjMaNG1fo82vWrFFqaqpSU1PVpk0bj8Zn+9Rzdna2JGnV\n",
              "qlV2nxoAAMBnZGRkaMOGDfrss88kST169NCgQYO0a9cuRUZGuo7bvHmzOnTooNKlS0uSunbtqlGj\n",
              "Rukf//iH6tat6zrO399fTZs21datW20bo+0dRafTqVOnTik7O9vtdik5OTlFPpOTk2P3MAEAAIwo\n",
              "bpbZv3+/QkNDFRBwoW/ncDgUHh6u9PT0Qsc1adJEycnJys7OVl5enhYuXKi0tLQi5ztz5ozeffdd\n",
              "3XvvvYX233HHHWrcuLESExN15swZj34W24Pili1bFBIS4narWLHiJT8/duxYBQcHF9rGjh1r9zAB\n",
              "AIAv8+LUs91ZJj4+Xl26dFG7du3Url071atXzxUuf5Wbm6tevXqpU6dO+vOf/+zav2/fPm3cuFHr\n",
              "1q3T0aNH9Y9//MOj2g7LsndZUExMjDZt2nTZn8/JySmSugMDAxUYGHilQ4MNak963dtDwG/sGZLo\n",
              "7SEAJQ5/T5Uc3vw7qkvDEV6r/dGG54qVZTIyMhQZGakTJ04oICBAlmUpNDRUa9euLTT1/L/mz5+v\n",
              "t99+27UwJS8vTw888IAqV66sadOmyeFwuP3c+vXrNWDAAG3ZsqXYP4vt1yhebHDFRSgEAABXzIvP\n",
              "XC5ulqlatapiY2M1e/ZsxcfHa8mSJQoLCysSEs+fP69z586pYsWKOnbsmF5++WU9//zzkqT8/HzF\n",
              "xcWpUqVKRUJiZmamAgMDVa5cOTmdTi1YsEAxMTEe/Sy2B8XQ0FC7TwkAAOCTpk6dqvj4eL300ksK\n",
              "CgrSzJkzJUn9+vVTt27d1K1bN2VlZal9+/by8/OT0+nUkCFDdM8990iSFixYoKVLl6pRo0auENiq\n",
              "VSu9/fbb2rFjhxISEuRwOJSfn6/Y2FjXaunisn3qGb6NKZ2ShalnoCj+nio5vDr1fLP3pp6X/fii\n",
              "12rbzdizngEAALyFZz3bg+frAQAAwK2r0lG0LEunT59WhQoVrkY5AADwR0dH0RbGOop9+/bVyZMn\n",
              "lZubq+joaFWrVk2TJ082VQ4AAAA2MxYUN27cqJCQEC1btkwxMTE6cuSIpkyZYqocAAAAbGZs6vnX\n",
              "xdRr1qzR3XffraCgIPn7+5sqBwAA8F9Opp7tYKyjWL16dQ0cOFCLFi1Shw4dlJeXp4KCAlPlAAAA\n",
              "YDNjQXHOnDmKiorS/PnzFRISooMHDyoxkXu+AQCAq8CLz3r2JcamntPT0/Xkk0+6XkdERCg4ONhU\n",
              "OQAAANjMWEfxoYceUnp6uuv18uXL9c9//tNUOQAAgP+io2gLY0FxypQpuu+++5Sdna01a9Zo0KBB\n",
              "+uSTT0yVAwAAgM2MTT23adNGTz/9tDp16qQTJ07o448/Vu3atU2VAwAAgM1sD4pvvPFGodf5+flq\n",
              "06aNli9fruXLl2vw4MF2lwQAACjMx6aAvcX2oLhp06ZCrxs2bCin01lkPwAAAEo224PizJkz7T4l\n",
              "AACAZ7jhti2MLWYZOXKkjh8/7np97NgxjR492lQ5AAAA2MxYUPzoo490/fXXu15XrlxZH330kaly\n",
              "AAAAsJmxVc9Op7PIvtzcXFPlAAAA/ssqmkPgOWMdxaioKL3yyisqKChQfn6+xo0bp/r165sqBwAA\n",
              "AJsZC4qTJk3SsmXLVLZsWV133XVasWKF3nrrLVPlAAAA/osns9jC2NTzDTfcoJUrV+rMmTOSpOuu\n",
              "u85UKQAAABhgLChK0qJFi/T5559Lkjp37qwePXqYLAcAAHABt8exhbGp5zFjxmjs2LG66aabdPPN\n",
              "N2vs2LF64YUXTJUDAACAzYx1FBcvXqxvvvlG5cqVkyT169dPLVq00DPPPGOqJAAAAGxkLChaluUK\n",
              "idKFaxQtH7vAEwAAlFBkDlsYC4rNmjXTQw89pP79+0uSkpKS1KxZM1PlAAAAYDNj1yi+8cYb+tOf\n",
              "/qTExEQlJiYqNDRUb7zxhqlyAAAA/8XtcWxhrKO4e/duvfzyy4X2/fDDD2rUqJGpkgAAALCRsY5i\n",
              "fHx8sfYBAACgZLK9o5iRkaEjR47o3Llz2rJli2sBS1ZWluvm2wAAAEb52BSwt9geFOfNm6eJEyfq\n",
              "0KFD6tatm2t/cHCwhg0bZnc5AAAAGGJ7UBwyZIiGDBmi559/Xs8++6zdpwcAALg0p9PbI/AJxq5R\n",
              "/DUk7tmzRxMnTtTHH39sqhQAAAAMsD0odujQQampqZKkQ4cO6ZZbbtHy5cs1dOhQjRs3zu5yAAAA\n",
              "RXF7HFvYHhQPHjyo6OhoSdLcuXPVrl07ffrpp1q/fr3mzJljdzkAAAAYYntQLFu2rOvP69at0513\n",
              "3ilJqlixogICjN22EQAAADazPSj6+fnpwIEDOn36tL766iu1a9fO9d7Zs2ftLgcAAFAUU8+2sL3F\n",
              "989//lMxMTEKCAjQbbfdpnr16km60F2MiIiwuxwAAAAMsT0odu/eXS1bttQvv/xS6HF9ERERmjZt\n",
              "mt3lAAAAinL6VmfPW4xcNFi9enVVr1690L4bbrjBRCkAAAAYYuw+igAAALi2sQwZAAD4HMviySx2\n",
              "oKMIAAAAt+goAgAA38NiFlvQUQQAAIBbdBQBAIDv8bEbX3sLHUUAAAC4RVAEAACAW0w9AwAA3+Pk\n",
              "9jh2oKMIAAAAt+goAgAA38NiFlvQUQQAAIBbBEUAAAC4xdQzAADwORaLWWxBRxEAAABu0VEEAAC+\n",
              "h8UstqCjCAAAALfoKAIAAN/jpKNoBzqKAAAAcIugCAAAALeYegYAAL7H4vY4dqCjCAAAALfoKAIA\n",
              "AJ9jsZjFFnQUAQAA4BZBEQAAAG4x9QwAAHwPi1lsQUcRAAAAbtFRBAAAPofFLPagowgAAOAlO3fu\n",
              "VMuWLVWvXj01bdpUP/74Y5FjnE6nhg4dqgYNGqh+/frq27evcnNzXe9//PHHql+/vurWravu3bsr\n",
              "Ozu7WO8VB0ERAAD4Hsvpvc0DCQkJGjBggH7++Wc99dRTio+PL3JMUlKSUlJSlJKSou3bt8vPz0+T\n",
              "Jk2SJJ0+fVp9+/bVhx9+qJ07d+qGG27Q888/f8n3iougCAAA4AUZGRnasGGD+vTpI0nq0aOH9u/f\n",
              "r127dhU6bvPmzerQoYNKly4th8Ohrl276oMPPpAkffrpp4qJiVH9+vUlSU888YTmzZt3yfeKi6AI\n",
              "AABgo5ycHGVnZxfacnJyihy3f/9+hYaGKiDgwpIRh8Oh8PBwpaenFzquSZMmSk5OVnZ2tvLy8rRw\n",
              "4UKlpaVJktLT01WzZk3XsRERETp8+LDy8/N/973iYjHLVZKTk6OxY8dq+PDhCgwM9PZwLtueIYne\n",
              "HsIV85XvwhfwXZQcvvRd8PcUJOlz5yKv1R41apRGjx5daN/IkSM1atSoyzpffHy89u3bp3bt2qls\n",
              "2bLq0KGDPvvsMxtGemkOy7JYFnQVZGdnKzg4WFlZWQoKCvL2cP7Q+C5KDr6LkoPvomTh+7i25eTk\n",
              "FOkgBgYGFgn9GRkZioyM1IkTJxQQECDLshQaGqq1a9cqMjLyouefP3++3n77ba1Zs0aLFi1SUlKS\n",
              "li1bJknatm2bOnXqpAMHDvzue8XF1DMAAICNAgMDFRQUVGhz1xmuWrWqYmNjNXv2bEnSkiVLFBYW\n",
              "ViQknj9/XpmZmZKkY8eO6eWXX9awYcMkSV26dFFKSop27NghSZo8ebLi4uIu+V5xMfUMAADgJVOn\n",
              "TlV8fLxeeuklBQUFaebMmZKkfv36qVu3burWrZuysrLUvn17+fn5yel0asiQIbrnnnskSRUqVNC7\n",
              "776r++67T/n5+WrQoIHee++9S75XXEw9XyVMI5QcfBclB99FycF3UbLwfaCkYOr5KgkMDNTIkSO5\n",
              "KLkE4LsoOfguSg6+i5KF7wMlBR1FAAAAuEVHEQAAAG4RFAEAAOAWQREAAABuERQ9FBERoaioKEVH\n",
              "R+vGG29U7969debMGdvOHx0drVOnTrlqpaam2nZuX5Sfn6/Ro0erfv36atCggaKjozVgwAB9+OGH\n",
              "io6OdvuZQ4cOqU2bNld3oCXU0qVL1aRJE0VHR6t+/fq6/fbb5XR69kD7/zVq1CidP3/e9To+Pl4T\n",
              "J04s9udPnTql8uXLq2/fvoX27969W7GxsYqJiXHdPuK3+F4vzuFw6OTJk4X2/e/fL5ZlqVatWrrj\n",
              "jjsKHZeWliZ/f39FR0e7tltvvfUqjPqP4VLfTfv27fXhhx9KkpxOpwYOHKi2bdsqKyvr6g4Uf1jc\n",
              "R/EyLFiwQNHR0XI6nbrnnns0a9Ys/fWvfy10TEFBgfz9/T0+N8HQM3379tWJEye0fv16VaxYUZZl\n",
              "afHixTpx4sRFP3PDDTdozZo1V3GUJdPhw4c1YMAAbdy40fUs0JSUFDkcjis67+jRo/Xkk0+qTJky\n",
              "l/X5BQsWqEmTJlq6dKkmTZqk8uXLS5IWL16spk2baurUqUU+k5+fz/d6hb744guFhITohx9+0N69\n",
              "e1WrVi3XexUqVODvJi/Ly8vTww8/rNOnT2v58uUqW7ast4eEPwg6ilcgNzdXZ8+eVcWKFTVr1izd\n",
              "dttt6tGjhxo2bKjvvvtOr7/+upo2baro6Gg1bdpU69evl3QhDP72386DgoJcz4R092+XcG/Xrl1a\n",
              "tGiRZs6cqYoVK0q68Pu7//77Vbt2beXn5+uJJ55Q48aNdfPNN2vDhg2SLnRIQkJCXOdxOBx66aWX\n",
              "1KxZM9WqVatQt2ro0KGu77Bt27b66aefrurPaNIvv/wif39/VapUybUvNjbWFRQ3bNigli1bqlGj\n",
              "RmrWrJm+/vprSUV/f6dPn3Z95vHHH5cktWnTRtHR0crIyJAkbd++XXfccYfq1aun7t27Kzc396Lj\n",
              "SkpK0lNPPaW2bdtqwYIFkqT3339fEyZM0NKlSxUdHa1t27apffv2Gjx4sFq0aKFOnToVGdf69evV\n",
              "unVrNW7cWI0aNdJHH30kybe/0yuRlJSk/v37q3fv3poxY4a3h4PfOHfunO677z75+/vr//7v/wiJ\n",
              "uLoseKRmzZpWvXr1rMaNG1vBwcHW7bffbuXl5VkzZ860ypYta+3YscN1bEZGhuvP69evt6Kiooqc\n",
              "b+XKlVadOnWsAwcOWJZlWZKszMxMV61NmzYZ/XmuZQsWLLAaNWrk9r1Vq1ZZ/v7+1jfffGNZlmW9\n",
              "8847VqdOnSzLsqy9e/dawcHBrmMlWa+++qplWZa1fft2q3z58lZeXp5lWYW/w3nz5lmdO3c28aN4\n",
              "RUFBgdW9e3erYsWK1n333We98sorrv8d5uTkWDVq1LCWLVtmWZZlrVmzxqpWrZp16tSpIr+/U6dO\n",
              "Wb/9q+S3/xu2LMt65JFHrGbNmllnzpyx8vPzrZYtW1pz5851O6Yff/zR+tOf/mTl5+dbH330kdWi\n",
              "RQvXeyNHjrSGDBniet2uXTurc+fOVm5urmVZhb/X48ePW1WrVrVWr17t+lmPHz9uWZZvf6cX87/f\n",
              "iWUV/vvl+PHjVkhIiJWZmWlt3rzZCgsLswoKCizLuvB79fPzsxo3buzaevfufZV/At91qe+mXbt2\n",
              "1vXXX2/17t3bcjqdV3+A+MNj6vky/Dr1nJ+fr4SEBD311FNq2LChWrZsqaioKNdxmzZt0osvvqjj\n",
              "x48rICBAP/30k86dO+f6t8GtW7fq0Ucf1ccff6w//elP3vpxfFZkZKTrWqoWLVro1Vdfveixf/nL\n",
              "XyRJ9evXV0BAgI4cOaKwsDB9/vnnevPNN3Xq1Ck5nc7fndK+1vj5+WnJkiXasWOHvvrqK3366ad6\n",
              "8cUXtWHDBp07d05+fn7q3LmzJKl169aqVq2aUlNTFRYW5nGtP//5zypXrpwkqVmzZtq9e7fb45KS\n",
              "kvTwww/L399fd955pxISErR9+3bdeOONbo/v06ePSpUqVWT/+vXrFRUV5bpm0c/Pz9U59eXv1FO/\n",
              "doLnzJmjrl27KiQkRCEhIapWrZqWL1+url27SmLq2Rt+ewlI586dtXLlSm3ZskWNGjXy4qjwR8TU\n",
              "8xUICAhQjx49tGzZMklyXUslXZiW7t69u1599VVt3bpVq1evliTl5ORIunDh/X333aeZM2eqQYMG\n",
              "V3/wPiA2NlY7d+7U8ePH3b7/22vk/P39lZ+ff9FzuTs2PT1dgwYN0uzZs7V161bNnz+/0CINX1G/\n",
              "fn0lJCToww8/VPPmzZWcnOz2uF//jysgIEAFBQWu/cX5nRTnu8jLy9MHH3yg9957TxEREYqMjNTZ\n",
              "s2eVlJR00fP+9p+54vijfKf/q0qVKkX+OTl27JiqVq0q6UJAX7lypSIiIhQREaG9e/f+7u8d9rnU\n",
              "dyNJ999/vyZNmqROnToR2HHVERSv0MqVKwt1EX91/vx55ebmKjw8XJL05ptvut47deqU7rrrLo0e\n",
              "PVq33XbbVRurr4mMjFSPHj3Ut29f13WdlmVpyZIl2rNnzxWfPysrS6VKlVJoaKgsy9Jbb711xecs\n",
              "SQ4ePOi67lCSMjMztXfvXtWpU0dRUVFyOp36/PPPJUnr1q3TkSNHFB0drerVq8uyLG3btk3ShesH\n",
              "f6tChQqXtSIzOTlZtWvX1sGDB5WWlqa0tDR98803+uCDD5SXl+fRuVq2bKmdO3e6Frf82jn09e/0\n",
              "Yjp37lxoEdD777+v2rVrKzQ0VBs3btTRo0d16NAh1+999+7dWr58uY4ePerFUf8x/N5381sPPPCA\n",
              "3nrrLXXp0kWbNm262sPEHxhB8TL06tVL0dHRatCggbZv365JkyYVOSYoKEgvvPCCmjVrpiZNmqh0\n",
              "6dKu95YuXaodO3Zo/PjxrgUtU6ZMuZo/gs+YMWOGGjdurFtvvVU333yzbrrpJn322WeFFmhcroYN\n",
              "GyouLk4333yzmjZt6gr9viI/P19jxoxRvXr1FB0drTZt2uiRRx7Rvffeq9KlS2vp0qUaOXKkGjVq\n",
              "pCeffFKLFy9W+fLlFRAQoDfffFN33323mjZtWiTE/f3vf1fHjh0LLWYpjqSkJNclAL+68cYb9ac/\n",
              "/Un/+te/PPrZKlasqP/7v//T008/rUaNGik2NlZff/21z3+nFzNx4kQdPnxYjRo1UnR0tObOnatF\n",
              "ixZJuvB7j4uLk5/ff//vICQkRB07dtQHH3wg6cK/3P52Ad5vb+OFK/N7383/6tmzpyZPnqwuXbpo\n",
              "48aNV3mk+KPiWc8AAABwi44iAAAA3CIoAgAAwC2CIgCPXOyxiSZuFJ+Wlsb1uwDgRQRFAB7p27ev\n",
              "NmzYoPXr12vr1q3atGmTOnbsaOR+hJcKir93yyMAwJVjMQuAYtu1a5caNWqk9PR0Va5cucj748eP\n",
              "16xZs+Tn56dGjRpp8uTJCg4O1qhRo3Ty5ElNnDhRkvTWW29pw4YNmjVrlmbNmqXZs2erSpUq2rp1\n",
              "qwIDA7Vw4ULVrl1b9evX1759+xQVFaXw8HAlJycrIiJCvXr10qpVq1S3bl1lZWWpd+/e6t27tyTp\n",
              "s88+07PPPqtvv/32av5qAMAn0VEEUGwpKSmqW7eu25D46aefasaMGfr666+1ZcsWXXfddXr66aeL\n",
              "dd7vv/9eL730krZs2aIOHTpo3LhxkqQpU6YoKipKqamphW4Efvz4cX377beaM2eOhgwZUuh+iG+/\n",
              "/bYGDRp0hT8pAEAiKAKwyYoVK9SrVy+FhIRIkgYOHOi6YfeltGjRQrVq1XL9+WKP+PtVfHy860kx\n",
              "HTt2VFZWljZt2qR9+/bpu+++0wMPPHD5PwgAwIVnPQMott8+NvH666//3WN/+6zaSz32z5PHLUpF\n",
              "H903ePBgvfnmm6pWrZoee+wxBQYGXvJnAQBcGh1FAMX2e49NrF27thYuXKjs7GxJ0tSpU9WpUyfX\n",
              "5zZs2KCCggKdPXtWS5YsKVa9oKCgYj0O8KGHHtLy5cs1c+ZMPf7445f3wwEAiqCjCMAjM2bM0Asv\n",
              "vKBbb71VAQEBcjqdatu2rcaNG6ezZ8+qRYsWhRazSFL37t21aNEi3XjjjQoLC1NMTIzOnj17yVqN\n",
              "GjXSzTffrAYNGqh27dqFrlP8rXLlyql79+46dOiQatSoYevPCwB/ZKx6BnDNKygoUJMmTfTmm2+q\n",
              "TZs23h4OAPgMpp4BXNOSk5NVp04dtWjRgpAIADajowgAAAC36CgCAADALYIiAAAA3CIoAgAAwC2C\n",
              "IgAAANwiKAIAAMAtgiIAAADcIigCAADALYIiAAAA3CIoAgAAwK3/D13fCqUe1yYxAAAAAElFTkSu\n",
              "QmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-216ea315-a003-45be-b67a-ef8251cb2f05\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-216ea315-a003-45be-b67a-ef8251cb2f05\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "import pandas as pd\n",
              "plt.subplots(figsize=(8, 8))\n",
              "df_2dhist = pd.DataFrame({\n",
              "    x_label: grp['Date of Transaction'].value_counts()\n",
              "    for x_label, grp in _df_18.groupby('Transaction Type')\n",
              "})\n",
              "sns.heatmap(df_2dhist, cmap='viridis')\n",
              "plt.xlabel('Transaction Type')\n",
              "_ = plt.ylabel('Date of Transaction')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-682788ed-a25c-4838-b4c1-104abe946ca8\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAooAAAKfCAYAAADpb+mkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABunUlEQVR4nO3de1xVdb7/8fcWlSjF3IppKmzYYt4FBE0rUKnUOpmmZcNUGt44\n",
              "2tEZc3LI8Sg2E53GqZypNBuoQZ2c1Gw40+luEU5NmkrjdJm8AZYXUAzRBAXW7w8f7p+7DQr6XWzF\n",
              "13Me6/Fgfdda+/PdeyPz6bvW9/N1WJZlCQAAAPiRJv7uAAAAAC5OJIoAAACoEYkiAAAAakSiCAAA\n",
              "gBqRKAIAAKBGJIoAAACoEYkiAAAAakSiCAAAgBqRKAIAAKBGJIoAAAB+MmPGDLlcLjkcDuXl5dV4\n",
              "Tn5+vgYPHqxWrVopKirK53hGRoYiIyPldrs1efJknTx5sk7H6oJEEQAAwE/Gjh2rDRs2KCwsrNZz\n",
              "goOD9etf/1p//vOffY7t3r1b8+bNU25urnbs2KEDBw5o2bJl5zxWVySKAAAAfhIfH69OnTqd9Ryn\n",
              "06kbb7xRV111lc+xNWvWaOTIkWrfvr0cDodSUlL0yiuvnPNYXTWt19kAAAA4q4qKClVUVHi1BQYG\n",
              "KjAw0HiswsJCr9FIl8ulwsLCcx6rKxJFAH4Vsfgpf3cBDWjXzFn+7gIuE9X7u/otdvrSJKWlpXm1\n",
              "zZ8/XwsWLPBPhy4AiSIAAIBBqampmjXL+z+K7BhNlKTQ0FDt3LnTs5+fn6/Q0NBzHqsrnlEEAACN\n",
              "TrUf/xcYGKjg4GCvza5EccyYMcrOztb+/ftlWZaWLl2qe++995zH6opEEQAAwE+mTp2qTp066dtv\n",
              "v9WwYcPUpUsXSdKkSZOUnZ0tSfrhhx/UqVMn3X333fryyy/VqVMnpaamSpIiIiKUlpamG264QV26\n",
              "dFFISIimTp16zmN15bAsyzL4fj1OnDihvXv3SpKuvfZaNW/e3I4wAC5xPKN4eeEZRTSUyv1d/Ba7\n",
              "afsdfottmvERxX379umee+5Rq1atdP3112vAgAFq1aqV7rnnHn333XemwwEAAPiosqr9tjUmxhPF\n",
              "+++/XwMGDFBxcbH279+vAwcOqLi4WP3799f9999vOhwAAABsYjxR/Pbbb/Xwww+rRYsWnrYWLVpo\n",
              "9uzZnlvRAAAAdqqW5betMTGeKF5xxRX66KOPfNpzcnJsm/EDAAAA84zXUXzhhRd03333qVmzZp5q\n",
              "4Pn5+aqsrNSKFStMhwMAAIBNjCeKAwYM0DfffKPNmzd7lokJDQ1Vv3795HA4TIcDAADwUa3GNanE\n",
              "X2xZmcXhcKhPnz5q27atpFPlcUgSAQAALi2UxwEAAI1OlWX5bWtMGqQ8TlFREeVxAAAALjENUh6n\n",
              "ZcuWlMcBAAANhvI4ZlAeBwAAADWiPA4AAABqRHkcAADQ6FQ1slvA/mJbeZzY2FjFxsba8fIAAABo\n",
              "AMafUTzTggULzroPAABgByazmGFrotihQ4ez7gMAAODiZWuiOHXq1LPuAwAA4OJlyzOKhw8f1rp1\n",
              "67wms4waNUpOp9OOcAAAAF4a2wop/mJ8RHHt2rXq1q2b3nnnHR0/flzHjx/X22+/rR49emjt2rWm\n",
              "wwEAAMAmxkcU586dq08//VQul8urfffu3RoxYoTGjBljOiQAAICXan93oJEwPqJYVVXlkyRKUnh4\n",
              "uCorK02HAwAAgE2MjyjGxcUpOTlZKSkpnpVZCgoKtHTpUuoqAgCABkHBbTOMjyhmZGQoPDxcycnJ\n",
              "ioyMVGRkpCZOnKiwsDBlZmaaDgcAAACbOCyLaUEA/Cdi8VP+7gIa0K6Zs/zdBVwm9nznv9rNnTvu\n",
              "81ts0yiPAwAAGp0qhsGMsK08zttvv015HAAAgEsY5XEAAECjQ3kcMyiPAwAAgBoZTxRPl8fZuHGj\n",
              "Dhw4oAMHDmjjxo1KTk6mPA4AAMAlhPI4AACg0amSw29bY0J5HAB+RXmcywvlcdBQtn97rd9iR3ba\n",
              "67fYptlSHgcAAMCfqhkGM8L4reczxcTEnHUfAAAAFy9bRxTfeOONs+4DAADYobE9K+gvto4odujQ\n",
              "4az7AAAAuHjZUkdxyZIlio+Pl8vlksvlUnx8vJ5//nlVVVWZDgcAAACbGL/1PG3aNO3fv1+pqame\n",
              "wtv5+flaunSp8vLytGzZMtMhAQAAvHDr2QzjieL69eu1fft2r7bu3btr+PDh6tq1q+lwAAAAsInx\n",
              "RNHhcKi4uFghISFe7cXFxaJkIwAAaAjVFiOKJhhPFOfMmaOoqCjdeeedCgsLkyQVFBQoOztbaWlp\n",
              "psMBAADAJsYTxYkTJ2ro0KFau3atCgsLJUkRERHKzc1VeHi46XAAAACwiS11FMPDwzV79mw7XhoA\n",
              "AOCcmMxiBuVxAAAAUCPK4wAAgEanyt41RS4blMcBAABAjSiPAwAAGh3K45hBeRwAAADUyJbyOImJ\n",
              "iVqzZg3lcQAAAC5htpTHcblclMcBAAB+Q3kcM5gSBAAAgBrZMqIIAADgT1UWY2Em8CkCAACgRg2a\n",
              "KB46dKghwwEAAOACNOit5+joaM9MaAAAALtUc9PUCOOJYnZ2dq3HysvLTYcDAACATYwniqNHj1ZC\n",
              "QkKNq7CUlZWd8/qKigpVVFR4tQUGBiowMNBYHwEAQONGeRwzjCeKkZGRyszMlMvl8jnWuXPnc16f\n",
              "np7us4LL/PnztWDBAkM9BAAAQF0YTxTHjx+vgwcP1pgopqSknPP61NRUzZo1y6uN0UQAAFAflMcx\n",
              "w3iimJqaWuuxuXPnnvN6bjMDAABcHGyZ9VxVVaWcnBzPDOfQ0FAlJCQoICDAjnAAAACwgfFEMTc3\n",
              "V0lJSerYsaPCwsIkSfn5+dq7d69Wrlyp+Ph40yEBAAC8VDOZxQjjieL06dO1bt06xcbGerVv2rRJ\n",
              "ycnJ2rZtm+mQAAAAsIHxRLG8vNwnSZSkuLg4n7I3AAAAdqii4LYRxj9Ft9uthQsXqqioyNNWVFSk\n",
              "tLQ0hYeHmw4HAAAAmxhPFLOyslRQUCC3262goCAFBQXJ7XaroKBAy5cvNx0OAAAANjF+6zkkJEQZ\n",
              "GRnKyMhQSUmJJMnpdJoOAwAAUCvqKJph66fodDrldDqVnp5uZxgAAADYoEHS7dWrVzdEGAAAAElS\n",
              "tZr4bWtMGuTdWJbVEGEAAAAuKTNmzJDL5ZLD4VBeXl6t52VkZCgyMlJut1uTJ0/WyZMnJUkvvfSS\n",
              "oqKiPFvbtm111113STpVxzogIMDr+M6dO+vVvwZJFBctWtQQYQAAACRJVZbDb1t9jB07Vhs2bPAs\n",
              "UlKT3bt3a968ecrNzdWOHTt04MABLVu2TJL04IMPKi8vz7O1b99eP/3pTz3XtmzZ0uu42+2uV/8a\n",
              "JFFMTEyUJB06dKghwgEAAFwS4uPj1alTp7Oes2bNGo0cOVLt27eXw+FQSkqKXnnlFZ/zPv30UxUV\n",
              "FWnkyJHG+tegN9Kjo6MbMhwAAECDq6io0JEjR7y2C1l0pLCw0GvE0eVyqbCw0Oe8jIwM3X///WrW\n",
              "rJmn7dixY4qLi1NMTIwWLlyoqqqqesU2Xh4nOzu71mPl5eWmwwEAAPjw58os6enpSktL82qbP3++\n",
              "FixYYFvMY8eOadWqVfrHP/7haevQoYO+++47tWvXTiUlJRo3bpx+97vf6ZFHHqnz6xpPFEePHq2E\n",
              "hIQaJ7CUlZWZDgcAAHBRSU1N1axZs7zaAgMDz/v1QkNDvSah5OfnKzQ01Ouc1atXq2fPnurRo4dX\n",
              "zHbt2kk6VbIwOTlZf/7zn/2bKEZGRiozM1Mul8vnWOfOnU2HAwAA8FHtx4LbgYGBF5QY/tiYMWN0\n",
              "4403asGCBbrmmmu0dOlS3XvvvV7nZGRkaOLEiV5tRUVFat26tZo1a6aKigq99tpr9X4M0PinOH78\n",
              "eB08eLDGYykpKabDAQAAXLKmTp2qTp066dtvv9WwYcPUpUsXSdKkSZM8j/NFREQoLS1NN9xwg7p0\n",
              "6aKQkBBNnTrV8xr//ve/lZeXp3Hjxnm99oYNGxQdHa2+ffsqJiZG7du319y5c+vVP4dFkUMAfhSx\n",
              "+Cl/dwENaNfMWec+CTDgLzvi/BZ7XJdNfottmvFbz5JUVVWlnJwcz4yc0NBQJSQkKCAgwI5wAAAA\n",
              "Xvw5maUxMZ4o5ubmKikpSR07dvRM5c7Pz9fevXu1cuVKxcfHmw4JAAAAGxhPFKdPn65169YpNjbW\n",
              "q33Tpk1KTk7Wtm3bTIcEAADwUt8VUlAz4+Oy5eXlPkmiJMXFxV1QsUkAAAA0LOOJotvt1sKFC1VU\n",
              "VORpKyoqUlpamsLDw02HAwAAgE2MJ4pZWVkqKCiQ2+1WUFCQgoKC5Ha7VVBQoOXLl5sOBwAA4KNa\n",
              "Tfy2NSbGn1EMCQlRRkaGMjIyVFJSIulUNXAAAABcWmxNe51Op5xOp9LT0+0MAwAA4KXKauK3rTFp\n",
              "kHezevXqhggDAAAAg2wpuP1jLP4CAAAaUrUoj2NCg4woLlq0qCHCAAAAwCDbE8XKyko5nU6Vlpba\n",
              "HQoAAAAGGU8U169frzZt2qht27bKycnRoEGDlJSUJLfbrZycHNPhAAAAfDCZxQzjzyimpqbq/fff\n",
              "1/fff68xY8bo1Vdf1dChQ7Vx40Y9/PDDys3NNR0SAAAANjCeKJ44cUJRUVGSpKuvvlpDhw6VJPXv\n",
              "319Hjx41HQ4AAMBHVSMrfO0vxj/F6upqz893332317GqqirT4QAAAGAT44liv379dOTIEUnyKrS9\n",
              "c+dOBQcHmw4HAAAAmxi/9ZyZmVlje1hYmN59913T4QAAAHxUW9RRNKHBbuA3bdpUP/zwQ0OFAwAA\n",
              "wAVqkJVZTouOjlZhYWFDhgQAAJchJrOYYTxRzM7OrvVYeXm56XAAAACwifFEcfTo0UpISKhxfeey\n",
              "sjLT4QAAAHxUN7LC1/5iPFGMjIxUZmamXC6Xz7HOnTubDgcAAACbGE+3x48fr4MHD9Z4LCUlxXQ4\n",
              "AAAA2MSWJfxqM3fuXNPhAAAAfFSJ8jgm2DLruaqqSjk5OZ4ZzqGhoUpISFBAQIAd4QAAAGAD44li\n",
              "bm6ukpKS1LFjR4WFhUmS8vPztXfvXq1cuVLx8fGmQwIAAHhhMosZxhPF6dOna926dYqNjfVq37Rp\n",
              "k5KTk7Vt2zbTIQEAAGAD4+l2eXm5T5IoSXFxcaqoqDAdDgAAADYxnii63W4tXLhQRUVFnraioiKl\n",
              "paUpPDzcdDgAAAAfVXL4bWtMjCeKWVlZKigokNvtVlBQkIKCguR2u1VQUKDly5ebDgcAAACbGH9G\n",
              "MSQkRBkZGcrIyFBJSYkkyel0mg4DAABQKyazmGFLeZzTSBABAAAuXcbT7Z07d2rIkCGKiIjQrFmz\n",
              "VF5e7jk2cOBA0+EAAAB8VFlN/LY1JsbfzbRp0zR27FitXr1aBw8eVGJiosrKyiTJK2kEAADAxc14\n",
              "olhUVKTp06erX79+ysrK0u23367ExESVlpbK4WhcM4EAAAAaM+PPKB4/ftxr/9FHH1Xz5s29RhYB\n",
              "AADsVN3IytT4i/ERxe7du+utt97yaps9e7aSkpK0c+dO0+EAAABgE+MjiqtWraqxfdasWRo3bpzp\n",
              "cAAAAD4a26QSfzGeKAYGBtZ6rGPHjqbDAQAAwCak2wAAAKiRrQW3AeBcds2c5e8uoAFFLH7K311A\n",
              "A/Lnv+9qi8ksJjCiCAAAgBoxoggAABqdKsbCjOBTBAAAQI1sG1E8ceKE9u7dK0m69tpr1bx5c7tC\n",
              "AQAAeOEZRTOMjyju27dP99xzj1q1aqXrr79eAwYMUKtWrXTPPffou+++Mx0OAAAANjGeKN5///0a\n",
              "MGCAiouLtX//fh04cEDFxcXq37+/7r//ftPhAAAAYBPjieK3336rhx9+WC1atPC0tWjRQrNnz/bc\n",
              "igYAALBTtZr4bWtMjL+bK664Qh999JFPe05OzllXbQEAAMDFxfhklhdeeEH33XefmjVrprCwMElS\n",
              "fn6+KisrtWLFCtPhAAAAfFQxmcUI44nigAED9M0332jz5s0qLCyUJIWGhqpfv35yOPjSAAAALhW2\n",
              "lMdxOBzq06eP2rZtK+lUeRySRAAAgEsL5XEAAECjU205/LY1Jg1SHqeoqIjyOAAAAJeYBimP07Jl\n",
              "S8rjAACABlNtNfHb1phQHgcAAAA1ojwOAABodKrUuJ4V9BfK4wAAAKBGtpXHiY2NVWxsrB0vDwAA\n",
              "gAZg6xOXCxYsOOs+AACAHSiPY4atiWKHDh3Oug8AAICLly23nk+bOnXqWfcBAADs0NjK1PiLLYni\n",
              "4cOHtW7dOq/JLKNGjZLT6bQjHAAAAGxgPN1eu3atunXrpnfeeUfHjx/X8ePH9fbbb6tHjx5au3at\n",
              "6XAAAACwifERxblz5+rTTz+Vy+Xyat+9e7dGjBihMWPGmA4JAADgpZo6ikYYH1GsqqrySRIlKTw8\n",
              "XJWVlabDAQAAwCbGRxTj4uKUnJyslJQUz8osBQUFWrp0KXUVAQBAg6hqZGVq/MX4iGJGRobCw8OV\n",
              "nJysyMhIRUZGauLEiQoLC1NmZqbpcAAAAJesGTNmyOVyyeFwKC8vr9bzMjIyFBkZKbfbrcmTJ+vk\n",
              "yZOSpA8//FBBQUGKiorybMePHz/ndXVlPFEMCgrSvHnz9K9//UtHjhzRkSNHtG3bNs2fP19XXnml\n",
              "6XAAAAA+qq0mftvqY+zYsdqwYYPnLmxNdu/erXnz5ik3N1c7duzQgQMHtGzZMs/x6667Tnl5eZ4t\n",
              "KCioTtfVhS1Fhg4fPqzMzEwtWLBACxYsUGZmpkpKSuwIBQAAcMmKj49Xp06dznrOmjVrNHLkSLVv\n",
              "314Oh0MpKSl65ZVXzvna53vdmWwrj/P2229THgcAAFx2KioqPHdVT28VFRXn/XqFhYVeI44ul8tT\n",
              "q1qSdu7cqZiYGMXFxen555+v83V1QXkcAADQ6PhzzeX09HSlpaV5tc2fP18LFiwwHismJkbffvut\n",
              "WrVqpW+//Va33Xab2rZtq3vuucfI61MeBwAAwKDU1FSVlpZ6bampqef9eqGhoSooKPDs5+fnKzQ0\n",
              "VJIUHBysVq1aSZI6deqkn/zkJ8rNzT3ndXVlPFE8XR5n48aNOnDggA4cOKCNGzcqOTmZ8jgAAKBB\n",
              "VMvhty0wMFDBwcFeW2Bg4Hm/lzFjxig7O1v79++XZVlaunSp7r33XknSvn37VF1dLUkqKyvT3/72\n",
              "N0VHR5/zurqiPA4AAICfTJ06VZ06ddK3336rYcOGqUuXLpKkSZMmKTs7W5IUERGhtLQ03XDDDerS\n",
              "pYtCQkI0depUSafmhvTu3Vt9+/bV9ddfr1tuuUUPPvjgOa+rK4dlWZbB9wsAQK0iFj/l7y6gAe2a\n",
              "OctvsX/66WS/xV454EW/xTbN+GQWAAAAf/PnZJbGxJY6iqfFxMScdR8AAAAXL1tHFN94442z7gMA\n",
              "ANihviukoGa2foodOnQ46z4AAAAuXg2abs+dO7chwwEAAOACNGiiuHz58oYMBwAALlPVlsNvW2Ni\n",
              "/BnF2iasWJaloqIi0+EAAABgE+OJ4q5du/TKK6/oyiuv9Gq3LEvjxo0zHQ4AAMBHtRrXyJ6/GE8U\n",
              "o6Oj1apVKw0aNMjnWPPmzU2HAwAAgE2MJ4ovv/yygoODazz2zTffmA4HAADgo7E9K+gvxhPFsLCw\n",
              "Wo8FBQWZDgcAAACbGJ/1XFVVpSVLlig+Pl4ul0sul0vx8fF6/vnnVVVVZTocAAAAbGJ8RHHatGna\n",
              "v3+/UlNT5XK5JEn5+flaunSp8vLytGzZMtMhAQAAvHDr2QzjieL69eu1fft2r7bu3btr+PDh6tq1\n",
              "q+lwAAAAsInxRNHhcKi4uFghISFe7cXFxbIsy3Q4AAAAH4wommE8UZwzZ46ioqJ05513eia2FBQU\n",
              "KDs7W2lpaabDAQAAwCbGE8WJEycqMTFRa9asUWFhoSQpIiJCubm5Cg8PNx0OAAAANjGeKEqSy+XS\n",
              "7Nmz7XhpAACAc+LWsxnGy+MAAACgcbBlRBEAAMCfWOvZDEYUAQAAUKMGHVE8dOiQ2rRp05AhAQDA\n",
              "ZYhnFM1o0BHF6OjohgwHAACAC2B8RDE7O7vWY+Xl5abDAQAAwCbGE8XRo0crISGhxlVYysrKTIcD\n",
              "AADwwa1nM4wnipGRkcrMzJTL5fI51rlz53NeX1FRoYqKCq+2wMBABQYGmuoiAAAA6sD4M4rjx4/X\n",
              "wYMHazyWkpJyzuvT09PVqlUrry09Pd10NwEAQCNWbTn8tjUmDqume8R+xIgiADReEYuf8ncX0IB2\n",
              "zZzlt9iJH/gv9vtDGs/vuS3lcaqqqpSTk+NZ6zk0NFQJCQkKCAg457UkhQAAABcH44libm6ukpKS\n",
              "1LFjR4WFhUmS8vPztXfvXq1cuVLx8fGmQwIAAHhpbLeA/cV4ojh9+nStW7dOsbGxXu2bNm1ScnKy\n",
              "tm3bZjokAAAAbGA8USwvL/dJEiUpLi7O59lDAAAAO1iMKBphfNaz2+3WwoULVVRU5GkrKipSWlqa\n",
              "wsPDTYcDAACATYwnillZWSooKJDb7VZQUJCCgoLkdrtVUFCg5cuXmw4HAADgo1oOv22NifFbzyEh\n",
              "IcrIyFBGRoZKSkokSU6n03QYAAAA2Mz4iOKZnE6nnE4nBbMBAAAuQbYmiqetXr26IcIAAABIYmUW\n",
              "UxokUbzIFn8BAABAHdiyMsuPLVq0qCHCAAAASKI8jikNMqKYmJgoSTp06FBDhAMAAIABDZIonhYd\n",
              "Hd2Q4QAAAHABjN96zs7OrvVYeXm56XAAAAA+GtukEn8xniiOHj1aCQkJNU5gKSsrMx0OAAAANjGe\n",
              "KEZGRiozM1Mul8vnWOfOnU2HAwAA8MFkFjOMP6M4fvx4HTx4sMZjKSkppsMBAADAJsZHFFNTU2s9\n",
              "NnfuXNPhAAAAfPCMohm21FGsqqpSTk6OCgsLJUmhoaFKSEhQQECAHeEAAABgA+OJYm5urpKSktSx\n",
              "Y0eFhYVJkvLz87V3716tXLlS8fHxpkMCAADABsYTxenTp2vdunWKjY31at+0aZOSk5O1bds20yEB\n",
              "AAC8sHqwGcYns5SXl/skiZIUFxeniooK0+EAAABgE+OJotvt1sKFC1VUVORpKyoqUlpamsLDw02H\n",
              "AwAA8FEth9+2xsR4opiVlaWCggK53W4FBQUpKChIbrdbBQUFWr58uelwAAAAsInxZxRDQkKUkZGh\n",
              "jIwMlZSUSJKcTqfpMAAAALCZ8RHFMzmdTjmdTqWnp9sZBgAAwItlOfy2NSa2JoqnrV69uiHCAAAA\n",
              "wCBbCm7/mMUcdQAA0IBYmcWMBhlRXLRoUUOEAQAAgEG2jyhWVlbK6XSqtLRUrVq1sjscAAAABbcN\n",
              "MT6iuH79erVp00Zt27ZVTk6OBg0apKSkJLndbuXk5JgOBwAAAJsYH1FMTU3V+++/r++//15jxozR\n",
              "q6++qqFDh2rjxo16+OGHlZubazokAAAAbGA8UTxx4oSioqIkSVdffbWGDh0qSerfv7+OHj1qOhwA\n",
              "AICPxlamxl+M33qurq72/Hz33Xd7HauqqjIdDgAAADYxPqLYr18/HTlyRMHBwV6Ftnfu3Kng4GDT\n",
              "4QAAAHwwomiG8UQxMzOzxvawsDC9++67psMBAADAJg1SR1GSmjZtqh9++KGhwgEAAOACNcjKLKdF\n",
              "R0ersLCwIUMCAIDLECuzmGE8UczOzq71WHl5uelwAAAAsInxRHH06NFKSEiocX3nsrIy0+EAAAB8\n",
              "sDKLGcafUYyMjFRmZqY++OADn61t27amwwEAAFyyZsyYIZfLJYfDoby8vFrPy8jIUGRkpNxutyZP\n",
              "nqyTJ09KOrUiXv/+/dWjRw/17NlTjzzyiKdUYX5+vgICAhQVFeXZdu7cWa/+GU8Ux48fr4MHD9Z4\n",
              "LCUlxXQ4AAAAH5bl8NtWH2PHjtWGDRsUFhZW6zm7d+/WvHnzlJubqx07dujAgQNatmyZJKl169Za\n",
              "tWqVvvzyS23evFkff/yxsrKyPNe2bNlSeXl5ns3tdterf8YTxdTUVMXGxtZ4bO7cuabDAQAAXLLi\n",
              "4+PVqVOns56zZs0ajRw5Uu3bt5fD4VBKSopeeeUVSacmCkdEREiSrrjiCkVFRSk/P99Y/2yZ9VxV\n",
              "VaWcnBzPDOfQ0FAlJCQoICDAjnAAAAAXjYqKClVUVHi1BQYGKjAw8Lxer7Cw0GvE0eVy1VhFZv/+\n",
              "/VqzZo3+9re/edqOHTumuLg4VVVVadSoUZo7d2698jHjI4q5ublyuVx69NFH9eabb+rNN99Uamqq\n",
              "XC6XPvroI9PhAAAAfPjz1nN6erpatWrltZ25Wp0djhw5ojvuuEOPPPKI585uhw4d9N1332nTpk16\n",
              "7733lJubq9/97nf1el3jI4rTp0/XunXrfG4/b9q0ScnJydq2bZvpkAAAABeN1NRUzZo1y6vtfEcT\n",
              "pVN3Zs+chJKfn6/Q0FDPfllZmYYPH64777zTK25gYKDatWsnSXI6nUpOTtaf//xnPfLII3WObXxE\n",
              "sby8vMZnFOPi4nyGYQEAAOxg+XELDAxUcHCw13YhieKYMWOUnZ2t/fv3y7IsLV26VPfee68k6ejR\n",
              "oxo+fLiGDx+uX/3qV17XFRUVeWZHV1RU6LXXXlN0dHS9YhtPFN1utxYuXKiioiJPW1FRkdLS0hQe\n",
              "Hm46HAAAwCVr6tSp6tSpk7799lsNGzZMXbp0kSRNmjTJs4hJRESE0tLSdMMNN6hLly4KCQnR1KlT\n",
              "JUmLFy/Wxo0b9dprr3lK4PzmN7+RJG3YsEHR0dHq27evYmJi1L59+3pPLHZYNVXGvgDFxcX65S9/\n",
              "qVdffVWVlZWSTq3zfPfdd+uJJ57wDIECAC4/EYuf8ncX0IB2zZx17pNsct1rC/0W+993/bffYptm\n",
              "/BnFkJAQZWRkKCMjQyUlJZJO3RcHAABoKPWtZ4ia2VIe5zQSRAAAgEuX8WcUd+7cqSFDhigiIkKz\n",
              "Zs1SeXm559jAgQNNhwMAAPDlz9ksjYjxRHHatGkaO3asVq9erYMHDyoxMVFlZWWS5JU0AgAA4OJm\n",
              "PFEsKirS9OnT1a9fP2VlZen2229XYmKiSktL5XDwvAAAAMClwvgzisePH/faf/TRR9W8eXOvkUUA\n",
              "AAA7MZnFDOMjit27d9dbb73l1TZ79mwlJSV5VRUHAADAxc34iOKqVatqbJ81a5bGjRtnOhwAAIAP\n",
              "s1WiL1/GE8WzLVHTsWNH0+EAAABgE1vrKAIAAPgDzyiaYfwZRQAAADQOJIoAAACoEbeeAQBA48Ot\n",
              "ZyMYUQQAAECNbBtR3LVrlwoLCyVJoaGhioiIsCsUAACAF8rjmGE8Ufzqq680fvx47dmzR6GhoZKk\n",
              "wsJCde7cWS+99JJ69uxpOiQAAABsYDxRnDBhgubMmaMxY8Z4ta9Zs0YPPvigNm7caDokAAAAbGD8\n",
              "GcXvv//eJ0mUpLFjx6q0tNR0OAAAAF+WH7dGxHii2LZtWy1fvlzV1dWeturqav3pT39SmzZtTIcD\n",
              "AACATep963nfvn3avXu3KisrPW3x8fGen//0pz9p6tSp+q//+i916NDBc01MTIxefvnlC+8xAADA\n",
              "ObAyixn1ShR/85vf6Le//a0iIiIUEBAgSXI4HF7PHXbp0kXvv/++iouLtWfPHklS586dFRISYrDb\n",
              "AAAAsFu9EsXMzEzt3LmzTreQy8rKdOTIEc/PJIoAAKDBNLJnBf2lXoniNddcc84k8csvv9SECRMo\n",
              "jwMAAHCJq1eieMstt+hnP/uZkpKSdMUVV3ja+/Tp4/n5wQcfpDwOAABAI1CvRDErK0uS9Ne//tXT\n",
              "5nA4tGvXLs/+2crjzJ0793z7CQAAUGdMZjGjXoni7t27z3nO6fI4P/3pT9WkyanqO9XV1Vq+fDnl\n",
              "cQAAAC4h9S6Ps3HjRr333nuSpFtvvVWxsbFexymPAwAA/I7JLEbUK1FctmyZfv3rX+uuu+6Sw+HQ\n",
              "mDFjNG/ePE2aNMlzDuVxAAAAGod6JYrPPvusNm/e7En6Hn30USUmJnoliqeFhISQHAIAAFzC6r2E\n",
              "35nJ37kSwSlTppx1HwAAwB4OP26NR70SxcjISM2dO1eFhYUqLCzUvHnzFBkZWev5d9xxx1n3AQAA\n",
              "cPGqV6K4dOlS7dy5UzExMYqJidGOHTu0ZMmSWs8nUQQAAH5h+XFrROr1jGJISIhWrVp1zvO++uor\n",
              "vfLKKyosLJQkhYaGaty4cazKAgAAcAmpU6KYk5OjhIQEZWdn13h85MiRnp+fe+45/fa3v9W4ceM0\n",
              "YMAASVJ+fr5uv/12zZ49Ww899JCBbgMAAJxFIxvZ85c6JYorVqxQQkKCnn76aZ9jDofDK1FcvHix\n",
              "tm7dqtatW3udN2fOHA0YMIBEEQAA4BJRp0TxxRdflCR98MEH5zy3urraJ0mUpKuvvlrV1dX17B4A\n",
              "AAD8pV7PKPbv318bN248a9uIESN0yy23aPLkyQoLC5MkFRQU6MUXX9Rtt91moMsAAADnwFrPRtQr\n",
              "UaysrPTaP3nypMrKyrzafv/732v58uXKysrymsxy33336f7777/A7gIAAKCh1ClR/J//+R898cQT\n",
              "Onr0qJxOp6f9+PHjeuCBB7zOdTgceuCBB3zaAQAAGorFZBYj6pQopqSkaNy4cfrP//xPLV261NMe\n",
              "HBxc4/OINZXHuffee9WjRw9D3QYAAIDd6lRwu1WrVnK5XHrhhRd0zTXXKCwsTGFhYQoKCtKePXu8\n",
              "zn3uuec0YsQIVVRUaMCAARowYIAqKip022236dlnn7XlTQAAAMC8ej2jOHbsWH300UeefcuyNHbs\n",
              "WH366aeeNsrjAAAAv+PWsxH1WsLvxIkTuuKKKzz7QUFBqqio8DqH8jgAAACNQ71GFB0Oh4qKitSu\n",
              "XTtJ0v79+2X96GlRyuMAAAC/ozyOEfVKFGfMmKGBAwd6ytysWLFC8+fP9zqH8jgAAACNQ70SxQcf\n",
              "fFDh4eH6v//7P0nSSy+9pJtuusnrHMrjAAAAf3PwjKIR9UoUJWnw4MEaPHhwnc7dv3+/2rdvX+s+\n",
              "AAAALl71ShSPHz+uP/zhD8rLy1N5ebmn/bXXXqvx/BEjRmjr1q217gMAAODiVa9Zz5MnT1Z+fr4+\n",
              "/vhjDRkyRAUFBZ4JKzX5cVJIkggAABqE5cetEalXovj555/r+eefV3BwsP7rv/5LH374oTZv3mxX\n",
              "3wAAAOBH9UoUg4KCJElNmzbVsWPH1LJlSxUXF/ucl5WVpYULF2rLli1e7enp6RfQVQAAgDqyHP7b\n",
              "GpF6JYpOp1OHDx/WbbfdpmHDhmnUqFHq1KmT1zm//OUv9eKLL6q4uFi33367Fi9e7Dm2evVqM70G\n",
              "AACA7eo1meWNN95QQECAHnvsMf35z3/W4cOHfcrgvPHGG9q8ebOaN2+uuXPnauTIkfrhhx+Umprq\n",
              "U5wbAAAAF696JYpVVVUKCAiQw+HQwIED9eWXX+qqq67yOseyLDVv3lyS1L59e7333nsaMWKEqqqq\n",
              "5HA0ruFYAABwkWJsyoh63Xq+4YYbVFZWpkOHDummm27SE088oenTp3udc9VVVyk/P9+zHxwcrLff\n",
              "fltvvvmmvvjiCyOdBgAAgP3qlShWVlaqZcuWeuONNzR+/Hht2LBBf//7373OeeKJJ1RaWurV1qJF\n",
              "C73zzjs+y/0BAADYgvI4RtTr1vOJEyckSR9++KGSkpIkSQEBAV7nDBkypMZrr7rqKj366KPn00cA\n",
              "AAD4Qb1GFIcMGaIePXro73//uxISEnT48GE1beqda1ZVVWnJkiWKj4+Xy+WSy+VSfHy8nn/+eVVV\n",
              "VRntPAAAQI0YUTSiXiOKf/jDH/T5558rIiJCzZo1U1VVlV588UWvc6ZNm6b9+/crNTVVLpdLkpSf\n",
              "n6+lS5cqLy9Py5YtM9Z5AAAA2KdeiaLD4VCfPn20f/9+ff/995KkNm3aeJ2zfv16bd++3aute/fu\n",
              "Gj58uLp27XphvQUAAECDqVei+PLLL2vGjBlq1qyZmjQ5ddfa4XCoqKjIc47D4VBxcbFCQkK8ri0u\n",
              "LqaOIgAAaBiNbIUUf6lXovjYY49p06ZNuu6662o9Z86cOYqKitKdd96psLAwSVJBQYGys7OVlpZ2\n",
              "Yb0FAABAg6lXoti2bduzJomSNHHiRCUmJmrNmjUqLCyUJEVERCg3N1fh4eHn31MAAIA6cnAT04h6\n",
              "JYqjRo3SM888o6SkJF1xxRWe9uDgYK/zXC6XZs+ebaaHAAAA8It6JYpz586VJM2aNUsOh0OWZcnh\n",
              "cFD2BgAAoBGqV6JYXV1tVz8AAADM4dazEfUquA0AAIDLR70Sxe3bt2vEiBG69tpr5XQ6PVtdHTp0\n",
              "qN4dBAAAgH/UK1GcPHmyJkyYoNatWysnJ0djx46t16SV6OjoencQAACgsZoxY4ZcLpccDofy8vJq\n",
              "PS8jI0ORkZFyu92aPHmyTp48ecHH6qJeieKRI0c0btw4NWnSRL1799YLL7yg119/3euc7OzsWrfy\n",
              "8vJ6dQ4AAOB8OCz/bfUxduxYbdiwwVN7uia7d+/WvHnzlJubqx07dujAgQOeJZHP91hd1WsyS7Nm\n",
              "zSRJLVu2VH5+vtq3b6+DBw96nTN69GglJCTUuApLWVlZvToHAADQmMXHx5/znDVr1mjkyJFq3769\n",
              "JCklJUWPP/64pk+fft7H6qpeiWJ8fLwOHTqkhx56SP369VPz5s117733ep0TGRmpzMxMuVwun+s7\n",
              "d+58zhgVFRWqqKjwagsMDFRgYGB9ugoAAOAXpnOZwsJCrxFHl8vlWdTkfI/VVZ1vPVuWpVmzZqlN\n",
              "mzZKSkpSXl6e3n33XT399NNe540fP95nlPG0lJSUc8ZJT09Xq1atvLb09PS6dhMAAODUWs9+2hpT\n",
              "LlOvEcVbbrlF//rXvySdGh2saYQwNTW11utPF+w+m9TUVM2aNcurjdFEAABwqTCdy4SGhmrnzp2e\n",
              "/fz8fIWGhl7Qsbqq84iiw+FQp06dah0tPFNVVZXWr1+vl19+WS+//LLWr19f59VbAgMDFRwc7LWR\n",
              "KAIAgHqx/LeZzmXGjBmj7Oxs7d+/X5ZlaenSpZ5H/873WF3VaURx+/btioyMVIsWLRQVFaXbbrtN\n",
              "LVq08Bx/6qmnPD/n5uYqKSlJHTt29NwXz8/P1969e7Vy5co6PbQJAABwOZg6dareeOMN7d+/X8OG\n",
              "DVPLli21Y8cOTZo0SSNHjtTIkSMVERGhtLQ03XDDDZKkwYMHa+rUqZJ03sfqymHVND35R2JiYrRl\n",
              "yxalpaXVeHz+/Pmen/v06aPMzEzFxsZ6nbNp0yYlJydr27Zt9eogAKDxiFj81LlPQqOxa+asc59k\n",
              "k4hn/Pe7tutn/nvfptVpRPF0LnlmQlib8vJynyRRkuLi4nxmAAEAANiCtZ6NqFOiWFpaqv/93/+t\n",
              "sTaiJI0cOdLzs9vt1sKFC5WSkqJ27dpJkoqKirRkyRKFh4cb6DIAAAAaQp0SxeLiYj399NM1JooO\n",
              "h8MrUczKytIvf/lLud1uVVZWngrStKnuvvtuLV++3FC3AQAAalffFVJQszolil26dNH69evr9IIh\n",
              "ISHKyMhQRkaGSkpKJElOp/P8ewgAAAC/qNdaz/XldDrldDov2SKTAAAAl7M6JYpn3lo+H6tXr76g\n",
              "6wEAAOrFj3UUG5M6JYq1lcWpqzpU4AEAAMBFpl5L+J2vRYsWNUQYAACAUxijMqJOI4qHDx++oCCJ\n",
              "iYmSpEOHDl3Q6wAAAKDh1ClRPJ3o3XPPPRcULDo6+oKuBwAAqAuH5b+tManTrefjx4/r008/1bZt\n",
              "27Rt2zafZw779Onj+Tk7O7vW1ykvLz/PbgIAAKCh1SlR/NnPfqYHH3xQu3fv9pkB7XA4tGvXLs/+\n",
              "6NGjlZCQUOMElrKysgvsLgAAABpKnRLFqVOnaurUqbr77rvPWeomMjJSmZmZcrlcPsc6d+58Xp0E\n",
              "AACoF8vh7x40CvUquL169Wr98MMP+vjjj/Xxxx/rhx9+8Dln/PjxOnjwYI3Xp6SknF8vAQAA0ODq\n",
              "VR7nk08+0V133aVrrrlGDodDBw4c0Nq1azVw4EDPOampqbVeP3fu3PPvKQAAQF01skkl/lKvRPHn\n",
              "P/+51qxZoxtuuEGS9PHHH+vnP/+5/vGPf3idV1VVpZycHBUWFkqSQkNDlZCQoICAAEPdBgAAgN3q\n",
              "lSgeP37ckyRK0qBBg3xmMufm5iopKUkdO3ZUWFiYJCk/P1979+7VypUrFR8fb6DbAAAAsFu9EsUW\n",
              "LVrovffe08033yxJev/993XVVVd5nTN9+nStW7dOsbGxXu2bNm1ScnKytm3bdoFdBgAAOLvGVs/Q\n",
              "X+qVKC5evFhjxozx3EKurq7Wa6+95nVOeXm5T5IoSXFxcaqoqLiArgIAAKAh1StRjI2N1Y4dO/Tv\n",
              "f/9bknTdddepWbNmXue43W4tXLhQKSkpateunSSpqKhIS5YsUXh4uKFuAwAAnAUjikbUqzyOJDVr\n",
              "1ky9evVSr169fJJEScrKylJBQYHcbreCgoIUFBQkt9utgoICLV++3EinAQAAYL96jSjWRUhIiDIy\n",
              "MpSRkaGSkhJJktPpNB0GAACgVjyjaEa9RxTrw+l0yul0Kj093c4wAAAAsMF5JYr1nZRyrmX/AAAA\n",
              "cPGpV6L4z3/+U7169ZLb7ZYkbd68WY888sg5r7Msxn8BAEADsvy4NSL1ShRnzJihpUuXKiQkRJIU\n",
              "ExOjN95445zXLVq06Px6BwAAAL+pV6J49OhR3XjjjZ59h8Oh5s2bn/WayspKOZ1OlZaWnl8PAQAA\n",
              "6osRRSPqlSg2bdpUJ0+elMPhkCTt2bPHZ/3m9evXq02bNmrbtq1ycnI0aNAgJSUlye12Kycnx1zP\n",
              "AQAAYKt6lcd56KGHNGrUKBUXF+tXv/qVVqxYoSeffNLrnNTUVL3//vv6/vvvNWbMGL366qsaOnSo\n",
              "Nm7cqIcffli5ublG3wAAAADsUa9E8b777lNERIT++te/6sSJE1qxYoXXrWhJOnHihKKioiRJV199\n",
              "tYYOHSpJ6t+/v44ePWqm1wAAAGdBHUUz6pUoZmZmKjk5WYMGDfJpO626utrz89133+11fVVV1fn2\n",
              "EwAAAA2sXs8oPvvssz5tzz33nNd+v379dOTIEUnyKrS9c+dOBQcHn08fAQAA4Ad1GlHcuHGjPvnk\n",
              "ExUXF+v3v/+9p720tNSn+HZmZmaNrxEWFqZ33333AroKAACAhlSnRHHfvn3Ky8vTDz/8oK1bt3ra\n",
              "g4OD9fLLL9ctUNOmKi0tVVBQ0Hl1FAAAoM54RtGIOiWKd955p+688069+eabGjFixHkHi46OVmFh\n",
              "4XlfDwAAgIZTr8ksI0aM0N69e/Wvf/1L5eXlnvaRI0d6fs7Ozq71+jOvAQAAwMWtXoniSy+9pLS0\n",
              "NJWUlCgyMlKff/65rr/+eq9EcfTo0UpISKhxfeeysrIL7zEAAMA5UB7HjHolik899ZS2bt2qoUOH\n",
              "avPmzfroo498nlGMjIxUZmamXC6Xz/WdO3e+kL4CAACgAdWrPE7z5s3VunVrVVZWSpLi4+OVl5fn\n",
              "dc748eN18ODBGq9PSUk5v14CAADUB2s9G1GvEcXAwEBZlqWuXbvqmWeeUVhYmM9qK6mpqbVeP3fu\n",
              "3PPrJQAAABpcvRLFX//61zpy5IiefPJJpaSk6Pvvv9fzzz/vc15VVZVycnI8M5xDQ0OVkJCggIAA\n",
              "M70GAACA7eqVKJ5et7lVq1a1Fs/Ozc1VUlKSOnbsqLCwMElSfn6+9u7dq5UrVyo+Pv4CuwwAAHAO\n",
              "jewWsL/UOVHctGmTFi1apC+++EKS1KtXL82ePVuxsbFe502fPl3r1q3zad+0aZOSk5O1bds2A90G\n",
              "AACA3eqUKH7yySe67bbblJKSop/85CeyLEsbN27UrbfeqjfffFMDBgzwnFteXu6TJEpSXFycz3J/\n",
              "AAAAdqA8jhl1ShSffPJJZWZmavTo0Z620aNH6/rrr1d6erpef/11T7vb7dbChQuVkpKidu3aSZKK\n",
              "ioq0ZMkShYeHm+09AAAAbFOn8jhffPGFV5J42p133qkvv/zSqy0rK0sFBQVyu90KCgpSUFCQ3G63\n",
              "CgoKtHz5cjO9BgAAOBvK4xhRpxHFK6+8stZjV111ldd+SEiIMjIylJGRoZKSEkmS0+m8gC4CAADA\n",
              "H+qUKFZUVGjbtm01Lst3tvWbSRABAAAuXXVKFI8fP+61nvOZHA6H1/7OnTs1adIkFRQUaNSoUXr8\n",
              "8cd1xRVXSJIGDhyoTz755AK7DAAAcHZMZjGjTolifn5+nV9w2rRpGjt2rK6//notXrxYiYmJeuut\n",
              "t9SyZcuzjj4CAADg4lKvtZ7roqioSNOnT1e/fv2UlZWl22+/XYmJiSotLfUZfQQAALAFk1mMqNfK\n",
              "LHVx/Phxr/1HH31UzZs3V2JiosrKykyHAwAAgE2Mjyh2795db731llfb7NmzlZSUpJ07d5oOBwAA\n",
              "AJsYH1FctWpVje2zZs3SuHHjTIcDAADw1chuAfuL8UQxMDCw1mMdO3Y0HQ4AAAA2MZ4oAgAA+Bvl\n",
              "ccy4JBLFiMVP+bsLaEC7Zs7ydxcA2IR/38Cl5ZJIFAEAAOqFEUUjjM96BgAAQONAoggAAIAacesZ\n",
              "AAA0Ptx6NsK2RPHEiRPau3evJOnaa69V8+bN7QoFAAAAGxi/9bxv3z7dc889atWqla6//noNGDBA\n",
              "rVq10j333KPvvvvOdDgAAAAfDst/W2NiPFG8//77NWDAABUXF2v//v06cOCAiouL1b9/f91///2m\n",
              "wwEAAMAmxhPFb7/9Vg8//LBatGjhaWvRooVmz57tuRUNAACAi5/xRPGKK67QRx995NOek5Nz1uX9\n",
              "AAAAjLH8uDUixiezvPDCC7rvvvvUrFkzhYWFSZLy8/NVWVmpFStWmA4HAAAAmxhPFAcMGKBvvvlG\n",
              "mzdvVmFhoSQpNDRU/fr1k8PhMB0OAADAR2ObVOIvthTcdjgc6tOnj2JiYhQTE6M+ffqQJAIAAPzI\n",
              "9u3bNWjQIHXt2lVxcXH64osvfM6prq7W7Nmz1atXL3Xr1k0TJ07UiRMnJElvv/22oqKiPNu1116r\n",
              "mJgYz7UOh0O9e/f2HM/Nza1X/yiPAwAAGp9L5BnFqVOnasqUKfrmm280Z84cTZgwweecjIwMbdmy\n",
              "RVu2bNFXX32lJk2aaPHixZKkYcOGKS8vz7PFxMTopz/9qdf1ubm5nuM33XRTvfrXIOVxioqKKI8D\n",
              "AABwhqKiIn322We67777JEljxozRnj17tGPHDq/zPv/8c918881q3ry5HA6HRowYoeXLl/u83t69\n",
              "e/X+++8bzbcapDxOy5YtKY8DAAAuCxUVFTpy5IjXVlFR4XPenj171KFDBzVtemrKiMPhUGhoqGeO\n",
              "x2n9+vVTdna2jhw5opMnT+rVV19Vfn6+z+u9/PLLuu2229SuXTuv9sTERPXt21ezZs3SsWPH6vVe\n",
              "KI8DAAAaHz/eek5PT1erVq28tvT09PN+KxMmTNDw4cOVkJCghIQEde3a1ZNcet6uZSkzM1MTJ070\n",
              "ai8oKNDmzZv18ccfq7i4WL/4xS/qFZvyOAAAAAalpqZq1qxZXm01DZZ17txZ+/btU2VlpZo2bSrL\n",
              "slRYWKjQ0FCv8xwOhxYsWKAFCxZIklatWqWePXt6nZOTk6Py8nINGzbMq/30a1111VWaNm2apkyZ\n",
              "Uq/3QnkcAADQ6Pgz4wgMDKzTXdR27dopJiZGK1as0IQJE7R27Vp16tRJXbp08TqvvLxcx48fV+vW\n",
              "rXXw4EE98cQTeuyxx7zOycjI0IQJExQQEOBpO3z4sAIDA3XllVequrpaf/nLXxQdHV2v92I8UZRO\n",
              "Zb6xsbGKjY214+UBAAAahRdeeEETJkzQ448/ruDgYL300kuSpEmTJmnkyJEaOXKkSktLNXjwYDVp\n",
              "0kTV1dWaOXOm7rjjDs9rlJaW6rXXXtO2bdu8Xvvrr7/W1KlT5XA4VFlZqZiYGM9s6bpyWJZlW0nK\n",
              "M4dJa9qvq4jFT5nrFC56u2bOOvdJAACcRZ9ZT/st9j+f+rnfYptmy4jiaR06dDjrPgAAgC1YmcUI\n",
              "W1ZmOW3q1Kln3QcAAMDFy5YRxcOHD2vdunVek1lGjRolp9NpRzgAAAAvrPVshvERxbVr16pbt256\n",
              "5513dPz4cR0/flxvv/22evToobVr15oOBwAAAJsYH1GcO3euPv30U7lcLq/23bt3a8SIERozZozp\n",
              "kAAAALCB8USxqqrKJ0mUpPDwcFVWVpoOBwAA4Itbz0YYv/UcFxen5ORkbdy4UQcOHNCBAwe0ceNG\n",
              "JScnU1cRAADgEmI8UczIyFB4eLiSk5MVGRmpyMhITZw4UWFhYcrMzDQdDgAAwJcf13puTIzfeg4K\n",
              "CtK8efM0b9480y8NAACABkR5HAAA0OhQHscM28rjvP3225THAQAAuIRRHgcAAAA1ojwOAABofLj1\n",
              "bATlcQAAAFAjyuMAAIBGx2H5b2tMKI8DAACAGhkfUQQAAEDjYGuiGBMTc9Z9AAAAW7AyixG2Jopv\n",
              "vPHGWfcBAABw8bJlZZbTOnTocNZ9AAAAOzS2SSX+YnxE8eTJk1q8eLF+//vfq7KyUq+++qruvPNO\n",
              "zZs3TydOnDAdDgAAADYxPqI4Y8YMHThwQMePH9cnn3yiiooKJSUlad26dXrkkUf0zDPPmA4JAADg\n",
              "jRFFI4wnihs2bNC2bdtUXl6udu3aaf/+/bryyis1evRo9evXz3Q4AAAA2MT4redmzZpJkq644gpF\n",
              "REToyiuvlCQ1b95cTZva+kgkAAAADDKeuVmWpaqqKgUEBOivf/2rp72yspK1ngEAQMPg1rMRxkcU\n",
              "lyxZopMnT0qSwsLCPO0FBQWaMWOG6XAAAACwifERxeuvv77GdrfbLbfbbTocAACAD8rjmGF8RLGq\n",
              "qkpLlixRfHy8XC6XXC6X4uPjtWTJElVVVZkOBwAAAJsYH1GcNm2a9u/fr9TUVLlcLklSfn6+li5d\n",
              "qq1bt2rZsmWmQwIAAMAGxhPF9evXa/v27V5t3bt31/Dhw9W1a1fT4QAAAHxx69kI47eeHQ6HiouL\n",
              "fdqLi4tlWXxrAAAAlwrjI4pz5sxRVFSU7rzzTs+s54KCAmVnZystLc10OAAAAB8OBqeMMJ4oTpw4\n",
              "UYmJiVqzZo0KCwslSREREcrNzVV4eLjpcAAAALCJLUuluFwuzZ49246XBgAAODcGFI0w/owiAAAA\n",
              "GgcSRQAAANTIllvPAAAA/sTKLGY06IjioUOHGjIcAAAALkCDJorR0dENGQ4AAFyuLD9ujYjxW8/Z\n",
              "2dm1HisvLzcdDgAAADYxniiOHj1aCQkJNa7CUlZWds7rKyoqVFFR4dVmVVbK0ZTHKQEAABqS8ewr\n",
              "MjJSmZmZcrlcPsc6d+58zuvT09N9VnC5etitaj1imKkuAgCARo7JLGYYf0Zx/PjxOnjwYI3HUlJS\n",
              "znl9amqqSktLvbarb0k03U0AAACcg/ERxdTU1FqPzZ0795zXBwYGKjAw0KuN284AAKBeGFE0wpYM\n",
              "rKqqSjk5OZ61nkNDQ5WQkKCAgAA7wgEAAMAGxhPF3NxcJSUlqWPHjgoLC5Mk5efna+/evVq5cqXi\n",
              "4+NNhwQAAPDCM4pmGE8Up0+frnXr1ik2NtarfdOmTUpOTta2bdtMhwQAAIANjE9mKS8v90kSJSku\n",
              "Ls6n7A0AAAAuXsYTRbfbrYULF6qoqMjTVlRUpLS0NIWHh5sOBwAA4IuVWYwwnihmZWWpoKBAbrdb\n",
              "QUFBCgoKktvtVkFBgZYvX246HAAAAGxi/BnFkJAQZWRkKCMjQyUlJZIkp9NpOgwAAECtmMxihvER\n",
              "xTM5nU45nU6lp6fbGQYAAAA2sDVRPG316tUNEQYAAAAGNciSJ5bF+C8AAGhA5B5GNMiI4qJFixoi\n",
              "DAAAAAxqkBHFxMRESdKhQ4fUpk2bhggJAAAuY0xmMaNBRhRPi46ObshwAAAAuADGRxSzs7NrPVZe\n",
              "Xm46HAAAgC9GFI0wniiOHj1aCQkJNU5gKSsrMx0OAAAANjGeKEZGRiozM1Mul8vnWOfOnU2HAwAA\n",
              "gE2MP6M4fvx4HTx4sMZjKSkppsMBAAD4cFT7b2tMjI8opqam1nps7ty5psMBAADAJraUx6mqqlJO\n",
              "To4KCwslSaGhoUpISFBAQIAd4QAAALwxmcUI44libm6ukpKS1LFjR4WFhUmS8vPztXfvXq1cuVLx\n",
              "8fGmQwIAAMAGxhPF6dOna926dYqNjfVq37Rpk5KTk7Vt2zbTIQEAAGAD44lieXm5T5IoSXFxcaqo\n",
              "qDAdDgAAwAcrs5hhfNaz2+3WwoULVVRU5GkrKipSWlqawsPDTYcDAACATYwnillZWSooKJDb7VZQ\n",
              "UJCCgoLkdrtVUFCg5cuXmw4HAADgy7L8tzUixhPFkJAQZWRkqKysTN99952+++47lZWVKTMzU+3a\n",
              "tTMdDgAA4JK1fft2DRo0SF27dlVcXJy++OILn3Oqq6s1e/Zs9erVS926ddPEiRN14sQJSacmDAcE\n",
              "BCgqKsqz7dy503Pt3/72N3Xr1k2RkZG66667dOTIkXr1z3iieCan0ymn06n09HQ7wwAAAHhxWP7b\n",
              "6mPq1KmaMmWKvvnmG82ZM0cTJkzwOScjI0NbtmzRli1b9NVXX6lJkyZavHix53jLli2Vl5fn2dxu\n",
              "tyTp6NGjmjhxol5//XVt375d1157rR577LF69c/WRPG01atXN0QYAACAS0ZRUZE+++wz3XfffZKk\n",
              "MWPGaM+ePdqxY4fXeZ9//rluvvlmNW/eXA6HQyNGjKjT43xvvvmmoqOj1a1bN0nStGnT9Morr9Sr\n",
              "jw2SKFqN7H49AABAbSoqKnTkyBGvrabKL3v27FGHDh3UtOmpIjQOh0OhoaGeBUtO69evn7Kzs3Xk\n",
              "yBGdPHlSr776qvLz8z3Hjx07pri4OMXExGjhwoWqqqqSJBUWFnpqWkuSy+XSvn37VFlZWef30iCJ\n",
              "4qJFixoiDAAAwCmW/7b09HS1atXKa7uQx/AmTJig4cOHKyEhQQkJCeratasnuezQoYO+++47bdq0\n",
              "Se+9955yc3P1u9/97rxj/ZjtiWJlZaWcTqdKS0vtDgUAAOB3qampKi0t9dpSU1N9zuvcubPXCJ9l\n",
              "WSosLFRoaKjXeQ6HQwsWLNDWrVv18ccfq0ePHurZs6ckKTAw0DNZ2Ol0Kjk5Wbm5uZJOLaFcUFDg\n",
              "eZ38/HyvEcy6MJ4orl+/Xm3atFHbtm2Vk5OjQYMGKSkpSW63Wzk5OabDAQAA+PDnZJbAwEAFBwd7\n",
              "bYGBgT59bNeunWJiYrRixQpJ0tq1a9WpUyd16dLF67zy8nIdPnxYknTw4EE98cQTeuSRRySdes7x\n",
              "5MmTkk7d8n7ttdcUHR0tSRo+fLi2bNmir7/+WpL0/PPP6957763X52h8ZZbU1FS9//77+v777zVm\n",
              "zBi9+uqrGjp0qDZu3KiHH37Yk+UCAABc7l544QVNmDBBjz/+uIKDg/XSSy9JkiZNmqSRI0dq5MiR\n",
              "Ki0t1eDBg9WkSRNVV1dr5syZuuOOOyRJGzZs0H//938rICBAlZWVGjp0qObOnSvp1GzoP/7xjxo1\n",
              "apQqKyvVq1cv/elPf6pX/xyW4Zkm0dHR2rp1qySpS5cuXjN3zjxWHxGLnzLWP1z8ds2c5e8uAAAu\n",
              "cTeN9t/8iNx1s/0W2zTjI4rV1dWen++++26vY6dn4QAAANiKiitGGH9GsV+/fp6q32fO8Nm5c6eC\n",
              "g4NNhwMAAIBNjI8oZmZm1tgeFhamd99913Q4AAAAH/VdIQU1a5A6ipLUtGlT/fDDDw0VDgAAABeo\n",
              "wRJFSZ7p2gAAALj4Gb/1nJ2dXeux8vJy0+EAAAB8cevZCOOJ4ujRo5WQkFDj+s5lZWWmwwEAAMAm\n",
              "xhPFyMhIZWZmyuVy+Rzr3Lmz6XAAAAA+mMxihvFnFMePH6+DBw/WeCwlJcV0OAAAANjEliX8anN6\n",
              "SRkAAABbVTOkaILxRFE6tQJLTk6OCgsLJUmhoaFKSEhQQECAHeEAAABgA+OJYm5urpKSktSxY0eF\n",
              "hYVJkvLz87V3716tXLlS8fHxpkMCAADABsYTxenTp2vdunWKjY31at+0aZOSk5O1bds20yEBAAC8\n",
              "cefZCOOTWcrLy32SREmKi4tTRUWF6XAAAACwifFE0e12a+HChSoqKvK0FRUVKS0tTeHh4abDAQAA\n",
              "+HBY/tsaE+OJYlZWlgoKCuR2uxUUFKSgoCC53W4VFBRo+fLlpsMBAADAJsafUQwJCVFGRoYyMjJU\n",
              "UlIiSXI6nabDAAAAwGa2lMc5jQQRAAD4RQ1LCaP+jN963rlzp4YMGaKIiAjNmjVL5eXlnmMDBw40\n",
              "HQ4AAAA2MZ4oTps2TWPHjtXq1at18OBBJSYmqqysTJK8kkYAAAC7MJnFDOOJYlFRkaZPn65+/fop\n",
              "KytLt99+uxITE1VaWiqHw2E6HAAAAGxi/BnF48ePe+0/+uijat68udfIIgAAgK0a2cievxgfUeze\n",
              "vbveeustr7bZs2crKSlJO3fuNB0OAAAANjE+orhq1aoa22fNmqVx48aZDgcAAACbGE8UAwMDaz3W\n",
              "sWNH0+EAAAB8OCiPY4TxW88AAABoHGwtuG3Krpmz/N0FNKCIxU/5uwtoQPz7BmCLan93oHFgRBEA\n",
              "AAA1IlEEAABAjS6JW88AAAD1wWQWM2xLFHft2qXCwkJJUmhoqCIiIuwKBQAAABsYTxS/+uorjR8/\n",
              "Xnv27FFoaKgkqbCwUJ07d9ZLL72knj17mg4JAADgjQFFI4wnihMmTNCcOXM0ZswYr/Y1a9bowQcf\n",
              "1MaNG02HBAAAgA2MT2b5/vvvfZJESRo7dqxKS0tNhwMAAPBlWf7bGhHjiWLbtm21fPlyVVf//wJG\n",
              "1dXV+tOf/qQ2bdqYDgcAAACbGE8U//SnP+nll1+W0+lU9+7d1b17dzmdTk87AAAALg3Gn1Hs0qWL\n",
              "3n//fRUXF2vPnj2SpM6dOyskJMR0KAAAgBo5GtcdYL+xreB2WVmZjhw5oiNHjqisrMyuMAAAALCJ\n",
              "8RHFL7/8UhMmTKA8DgAA8J9GNqnEX4wnig8++CDlcQAAABoByuMAAACgRpTHAQAAjY6j2n9bY0J5\n",
              "HAAAANSI8jgAAKDxYTKLEcYTxdNCQkJIDgEAAC5httVRlKQpU6acdR8AAMAWlh+3RsTWRPGOO+44\n",
              "6z4AAAAuXiSKAAAAqJEtzyh+9dVXeuWVV1RYWChJCg0N1bhx41iVBQAANAgHk1mMMD6i+Nxzz2nE\n",
              "iBGqqKjQgAEDNGDAAFVUVOj222/Xs88+azocAAAAbGJ8RHHx4sXaunWrWrdu7dU+Z84cDRgwQA89\n",
              "9JDpkAAAAN4YUTTC+IhidXW1T5IoSVdffbXXai0AAAC4uBkfURwxYoRuueUWTZ48WWFhYZKkgoIC\n",
              "vfjii7rttttMhwMAAIBNjCeKv//977V8+XJlZWV5TWa57777dP/995sOBwAA4IubmEYYTxQdDoce\n",
              "eOABPfDAA6ZfGgAAAA2owcrj3HvvverRo4cd4QAAALxQHseMBiuPc9ttt1EeBwAA4BJCeRwAAND4\n",
              "MKJoBOVxAAAAUCPK4wAAAKBGlMcBAACND7eejaA8DgAAAGpk/BnFM+3fv/+s+wAAALao9uPWiNia\n",
              "KI4YMeKs+wAAALh42Zoobt269az7AAAAuHjZsjLLjx06dEht2rRpiFAAAACszGKIrSOKp8XFxTVE\n",
              "GAAAABhkfETR6XT6tJWWlnraS0pKTIcEAADwxoiiEcZHFKOionT33Xdr8+bN2rp1q7Zs2aJrr71W\n",
              "W7du5RlFAACAM2zfvl2DBg1S165dFRcXpy+++MLnnOrqas2ePVu9evVSt27dNHHiRJ04cUKStG3b\n",
              "NsXHx6tbt27q1auXkpOTdfz4cc+1DodDvXv3VlRUlKKiopSbm1uv/hlPFNevX69u3brpwQcfVEVF\n",
              "hVwul5o1a6awsDDPSi0AAAC2siz/bfUwdepUTZkyRd98843mzJmjCRMm+JyTkZGhLVu2aMuWLfrq\n",
              "q6/UpEkTLV68WJJ0xRVX6Nlnn9XXX3+tzz//XMeOHdP//M//eF2fm5urvLw85eXl6aabbqpX/2x5\n",
              "RvHnP/+5nnvuOT344IN66qmnZDH8CwAA4KWoqEifffaZ7rvvPknSmDFjtGfPHu3YscPrvM8//1w3\n",
              "33yzmjdvLofDoREjRmj58uWSpMjISPXp00eSFBAQoLi4OOXn5xvro22TWXr27KmcnByVlJSoc+fO\n",
              "doUBAAC4qFRUVOjIkSNeW0VFhc95e/bsUYcOHdS06akpIw6HQ6GhoZ4lkE/r16+fsrOzdeTIEZ08\n",
              "eVKvvvpqjcngsWPH9Mc//lF33nmnV3tiYqL69u2rWbNm6dixY/V6L7bOem7atKl+/etf66OPPrIz\n",
              "DAAAgDc/3npOT09Xq1atvLb09PTzfisTJkzQ8OHDlZCQoISEBHXt2tWTXJ524sQJjRs3TrfeeqtG\n",
              "jx7taS8oKNDmzZv18ccfq7i4WL/4xS/qFdt4olhVVaUlS5bopptuksvlksvlUnx8vJ5//nlVVVWZ\n",
              "DgcAAHBRSU1NVWlpqdeWmprqc17nzp21b98+VVZWSpIsy1JhYaFCQ0O9znM4HFqwYIG2bt2qjz/+\n",
              "WD169FDPnj09x0+ePKlx48apQ4cOnmcXTzv9WldddZWmTZtW78ksxsvjTJs2Tfv379ejjz4ql8sl\n",
              "ScrPz9fSpUuVl5enZcuWmQ4JAADgzY9rLgcGBiowMPCc57Vr104xMTFasWKFJkyYoLVr16pTp07q\n",
              "0qWL13nl5eU6fvy4WrdurYMHD+qJJ57QY489JkmqrKzUvffeK6fTqWXLlsnhcHiuO3z4sAIDA3Xl\n",
              "lVequrpaf/nLXxQdHV2v92I8UVy/fr22b9/u1da9e3cNHz5cXbt2NR0OAADgkvXCCy9owoQJevzx\n",
              "xxUcHKyXXnpJkjRp0iSNHDlSI0eOVGlpqQYPHqwmTZqourpaM2fO1B133CFJ+stf/qLXXntNffr0\n",
              "8SSBN9xwg5577jl9/fXXmjp1qhwOhyorKxUTE+Mz4nguDsvwlOSuXbvq73//u0JCQrzai4qKNGjQ\n",
              "IJ+ZPMCPRSx+yt9dQAPaNXOWv7sAoBEa3nOu32K/9cVv/BbbNOMjinPmzFFUVJTuvPNOT93EgoIC\n",
              "ZWdnKy0tzXQ4AAAAH6z1bIbxRHHixIlKTEzUmjVrPNO7IyIilJubq/DwcNPhAAAAYBPjiaIkuVwu\n",
              "zZ49246XBgAAODdGFI2wtY4iAAAALl0kigAAAKiRLbeeAQAA/KqaW88mNOiI4qFDhxoyHAAAAC5A\n",
              "gyaK9a0GDgAAcF78uNZzY2L81nN2dnatx8rLy02HAwAAgE2MJ4qjR49WQkKCalrwpays7JzXV1RU\n",
              "qKKiwqutrmsmAgAASGp0I3v+YjxRjIyMVGZmplwul8+xzp07n/P69PR0nxVc5s+frwULFhjqIQAA\n",
              "AOrCeKI4fvx4HTx4sMZEMSUl5ZzXp6amatYs77VfGU0EAABoeMYTxdTU1FqPzZ177gW6uc0MAAAu\n",
              "GLeejbCljmJVVZVycnI8az2HhoYqISFBAQEBdoQDAACADYwnirm5uUpKSlLHjh0VFhYmScrPz9fe\n",
              "vXu1cuVKxcfHmw4JAADgjYLbRhhPFKdPn65169YpNjbWq33Tpk1KTk7Wtm3bTIcEAACADYwX3C4v\n",
              "L/dJEiUpLi7Op+wNAAAALl7GE0W3262FCxeqqKjI01ZUVKS0tDSFh4ebDgcAAODLqvbf1ogYTxSz\n",
              "srJUUFAgt9utoKAgBQUFye12q6CgQMuXLzcdDgAAADYx/oxiSEiIMjIylJGRoZKSEkmS0+k0HQYA\n",
              "AKB2lMcxwviI4pmcTqecTqfS09PtDAMAAAAb2JoonrZ69eqGCAMAAHBKteW/rRFpkETRYvgXAADg\n",
              "ktMgieKiRYsaIgwAAAAMsmUJvx9LTEyUJB06dEht2rRpiJAAAOByxt1MIxpkRPG06OjohgwHAACA\n",
              "C2B8RDE7O7vWY+Xl5abDAQAA+GJE0QjjieLo0aOVkJBQ4wSWsrIy0+EAAABgE+OJYmRkpDIzM+Vy\n",
              "uXyOde7c2XQ4AAAA2MT4M4rjx4/XwYMHazyWkpJiOhwAAIAvy/Lf1ogYH1FMTU2t9djcuXNNhwMA\n",
              "AIBNbCmPU1VVpZycHBUWFkqSQkNDlZCQoICAADvCAQAAeKuu9ncPGgXjiWJubq6SkpLUsWNHhYWF\n",
              "SZLy8/O1d+9erVy5UvHx8aZDAgAAwAbGE8Xp06dr3bp1io2N9WrftGmTkpOTtW3bNtMhAQAAvDWy\n",
              "ZwX9xfhklvLycp8kUZLi4uJUUVFhOhwAAABsYjxRdLvdWrhwoYqKijxtRUVFSktLU3h4uOlwAAAA\n",
              "sInxRDErK0sFBQVyu90KCgpSUFCQ3G63CgoKtHz5ctPhAAAAfFEexwjjzyiGhIQoIyNDGRkZKikp\n",
              "kSQ5nU7TYQAAAGAz4yOKZ3I6nXI6nUpPT7czDAAAgLdqy39bI2Jronja6tWrGyIMAAAADGqQRNFq\n",
              "ZPfrAQAALge2rMzyY4sWLWqIMAAAAJIky2JlFhNsH1GsrKyU0+lUaWmp3aEAAABgkPFEcf369WrT\n",
              "po3atm2rnJwcDRo0SElJSXK73crJyTEdDgAAwBeTWYwwfus5NTVV77//vr7//nuNGTNGr776qoYO\n",
              "HaqNGzfq4YcfVm5urumQAAAAsIHxRPHEiROKioqSJF199dUaOnSoJKl///46evSo6XAAAAC+mEhr\n",
              "hPFbz9XV///h0bvvvtvrWFVVlelwAAAAsInxRLFfv346cuSIJHkV2t65c6eCg4NNhwMAAIBNjN96\n",
              "zszMrLE9LCxM7777rulwAAAAvqopj2NCgxTclqSmTZvqhx9+aKhwAAAAuEANlihKUnR0dEOGAwAA\n",
              "lyvL8t/WiBi/9ZydnV3rsfLyctPhAAAAYBPjieLo0aOVkJBQ4/rOZWVlpsMBAADAJsYTxcjISGVm\n",
              "Zsrlcvkc69y5s+lwAAAAPiwmsxhh/BnF8ePH6+DBgzUeS0lJMR0OAAAANrFlCb/azJ0713Q4AAAA\n",
              "X41sUom/GE8UpVMrsOTk5KiwsFCSFBoaqoSEBAUEBNgRDgAAADYwnijm5uYqKSlJHTt2VFhYmCQp\n",
              "Pz9fe/fu1cqVKxUfH286JAAAgLdqRhRNMJ4oTp8+XevWrVNsbKxX+6ZNm5ScnKxt27aZDgkAAAAb\n",
              "GJ/MUl5e7pMkSlJcXJwqKipMhwMAAIBNjCeKbrdbCxcuVFFRkaetqKhIaWlpCg8PNx0OAADAl1Xt\n",
              "v60RMZ4oZmVlqaCgQG63W0FBQQoKCpLb7VZBQYGWL19uOhwAAABsYvwZxZCQEGVkZCgjI0MlJSWS\n",
              "JKfTaToMAABArSwmsxhhS3mc00gQAQAALl3Gbz3v3LlTQ4YMUUREhGbNmqXy8nLPsYEDB5oOBwAA\n",
              "AJsYTxSnTZumsWPHavXq1Tp48KASExNVVlYmSV5JIwAAgG2YzGKE8USxqKhI06dPV79+/ZSVlaXb\n",
              "b79diYmJKi0tlcPhMB0OAAAANjH+jOLx48e99h999FE1b97ca2QRAADATkxmMcP4iGL37t311ltv\n",
              "ebXNnj1bSUlJ2rlzp+lwAAAAl6zt27dr0KBB6tq1q+Li4vTFF1/4nFNdXa3Zs2erV69e6tatmyZO\n",
              "nKgTJ054jv/tb39Tt27dFBkZqbvuuktHjhyp07G6MJ4orlq1SkOGDPFpnzVrlvbs2WM6HAAAgK9L\n",
              "5BnFqVOnasqUKfrmm280Z84cTZgwweecjIwMbdmyRVu2bNFXX32lJk2aaPHixZKko0ePauLEiXr9\n",
              "9de1fft2XXvttXrsscfOeayujCeKgYGBCgwMrPFYx44dTYcDAAC4JBUVFemzzz7TfffdJ0kaM2aM\n",
              "9uzZox07dnid9/nnn+vmm29W8+bN5XA4NGLECM8iJm+++aaio6PVrVs3SacmFb/yyivnPFZXxhNF\n",
              "AACAy1lFRYWOHDnitVVUVPict2fPHnXo0EFNm56aMuJwOBQaGqrCwkKv8/r166fs7GwdOXJEJ0+e\n",
              "1Kuvvqr8/HxJUmFhocLCwjznulwu7du3T5WVlWc9Vle2FtzG+auoqFB6erpSU1NrHaFtrHbNnOXv\n",
              "LjS4y/n7vhzxfV9e+L79493q1X6LvWDBAqWlpXm1zZ8/XwsWLDiv15swYYIKCgqUkJCgoKAg3Xzz\n",
              "zXrnnXcM9PTcHJZlMS3oInTkyBG1atVKpaWlCg4O9nd3YDO+78sL3/flhe/78lNRUeEzgljTo3lF\n",
              "RUXq0qWLSkpK1LRpU1mWpQ4dOmjDhg3q0qVLra+/atUqPffcc8rNzdXq1auVkZHhmUj85Zdf6tZb\n",
              "b9W333571mN1xa1nAAAAgwIDAxUcHOy11TSa3K5dO8XExGjFihWSpLVr16pTp04+SWJ5ebkOHz4s\n",
              "STp48KCeeOIJPfLII5Kk4cOHa8uWLfr6668lSc8//7zuvffecx6rK249AwAA+MkLL7ygCRMm6PHH\n",
              "H1dwcLBeeuklSdKkSZM0cuRIjRw5UqWlpRo8eLCaNGmi6upqzZw5U3fccYckqWXLlvrjH/+oUaNG\n",
              "qbKyUr169dKf/vSncx6rK249X6S4VXF54fu+vPB9X174vnEp49bzRSowMFDz58/nwefLBN/35YXv\n",
              "+/LC941LGSOKAAAAqBEjigAAAKgRiSIAAABqRKIIAACAGl0WiWJlZaXS0tLUrVs39erVS1FRUZoy\n",
              "ZYq+//7783q9CRMm6JlnnjnneWlpaZo0aZJnf8OGDXI4HPrwww89bSkpKZo3b54+++wzjRs3TpL0\n",
              "/fff64knnvB6rcGDB+v111+vU/9mz5593tXfL5TD4Tjvz9VOpn8HarJgwQKVl5cbez2c24kTJzRn\n",
              "zhx16dJF3bt3V+/evX1KP8yfP1/dunXTgAEDatyvyYIFC/Szn/3Mzq57ue222xQVFaWoqCg5HA71\n",
              "7t1bUVFRuummm2yPPXHiRPXo0UOjR4+2PZZdXnvtNfXr109RUVHq1q2bhg4dqurqaknSM888o/37\n",
              "91/Q69f1b35N+G5xqbss6ihOnDhRJSUl+uSTT9S6dWtZlqU1a9aopKREV199tW1xhwwZouTkZM/+\n",
              "Bx98oAEDBujDDz/U4MGDPW1Lly5VbGys/vKXv0j6/4niL3/5S6P9Of2Hs0mTy+K/D7zU9XegsrLS\n",
              "s+ZmfaWlpelnP/uZrrjiCkO9rl1N/byQvl+qJkyYoIqKCn3++ee66qqrlJ+frxEjRqiyslITJ06U\n",
              "JD355JPatWuXOnToUOO+Xerzffzf//2f52eHw6Hc3Fyfv012fL8HDhzQqlWrdOTIEQUEBNT5uovp\n",
              "b8m+ffs0ZcoUbd682bOm7ZYtW+RwOCSdShQHDx6s9u3b+6V/fLe41DX634QdO3Zo9erVeumll9S6\n",
              "dWtJp/6x3n333YqIiND+/fs1ZMgQ9evXTz179tRDDz3k+Yfyj3/8w/Nfqb169dKSJUs8r/vVV18p\n",
              "MTFRXbt21V133aUTJ074xL7++uu1d+9ez1I5H374of77v//bM6K4b98+FRYWauDAgfrwww8VFRUl\n",
              "6dQoY1lZmaKiohQbG+t5vQ0bNuimm26S2+1WSkqKp33fvn0aNmyYevTooZtvvtlraZ4FCxZozJgx\n",
              "GjZsmHr16qV9+/Zp9uzZiouLU1RUlOLj4/Xvf/9bkrRs2TJNmTJF0qllfhwOh2ctyYULF2rhwoWS\n",
              "VOv1F6uz/Q4UFhaqZ8+emjhxoqKiorRy5Updc801+uGHHzzXJyUleb57h8OhX/3qV4qOjlbXrl21\n",
              "cuVKSfJ8HzfddJOioqJUVFSkoqIi3XXXXerdu7d69eqlF154wfOaX331lYYNG6Y+ffqoT58+Wrp0\n",
              "qSTfkeOxY8fq5ZdflnQqKUpOTlZ8fLx69eqlDz/80Kvv69at0/bt23X77bcrLi5Offr00bPPPut5\n",
              "LYfDoccff1z9+/dXeHi4p6hrbf357LPP1K1bN51ZGGHQoEF68803TXwtF2z79u16/fXXtWzZMl11\n",
              "1VWSTi14/7vf/c6zxuqgQYNUXl6uW2+9VTNmzPDZ3759u2644Qb17dtXvXv31q9+9SvP6+/bt093\n",
              "3HGHevTooaFDh6qkpESSVFVVpV/84hfq1auXevXqpf/6r//y/Pv/8XckScuXL9eAAQMUExOj+Ph4\n",
              "ff7553V+j4MHD9aMGTM0cOBA3XrrraqsrNSwYcMUGxurnj17KikpSceOHZN06u9Lr169NG3aNPXt\n",
              "21c9e/bUZ599JkkqLi7Wrbfeqt69e6tPnz568MEH9f3332vIkCEqLy9Xv379PHcxFi1apP79+ysm\n",
              "JkbDhw9XQUGBpJr/llwMDhw4oICAADmdTk9bTEyMHA6HFi5cqL1792rcuHGKiopSXl6ejh49quTk\n",
              "ZM/3d+Z6vN99953Gjh3r+ZzmzZvnEy83N1c9evTQZ599VuPnWld8t7hkWI3cX/7yF6tPnz61Hj9+\n",
              "/LhVVlZmWZZlVVZWWrfffrv1yiuvWJZlWSNHjrT+/Oc/e84tKSmxLMuyxo8fb/Xv3986duyYVVlZ\n",
              "aQ0aNMjrvDMlJiZaWVlZVnl5uRUeHm5ZlmW53W7r+PHj1sqVK60hQ4ZYlmVZH3zwgdW3b1/Lsixr\n",
              "9+7dVqtWrbxeJyEhwRo1apR18uRJ64cffrBcLpf18ccfW5ZlWWPHjrV+9atfWZZlWd9++63Vtm1b\n",
              "a/78+ZZlWdb8+fOtDh06WPv37/e8VlFRkefnV155xRo2bJhlWZa1c+dOTx+feeYZa+DAgdYvfvEL\n",
              "y7Is68Ybb7T+/ve/n/V6y7IsSdbhw4dr/rD95Gy/Ax988IHlcDisDz/80NOWlJRkvfDCC5ZlWdb+\n",
              "/futkJAQz++IJM9nvXPnTqt169bW7t27PcfOfO/33HOP9ctf/tKyLMs6cOCA1alTJ+uTTz6xTp48\n",
              "aUVGRnr9zhQXF1uWdep7Xrdunad9zJgx1ksvvWRZ1qnfuz59+lhHjhypse+VlZVWv379rK+++sqy\n",
              "LMs6duyY1bt3b2vjxo2e/i1atMiyLMv66quvrBYtWlgnT548a38GDRpkvf3225ZlWdaWLVusLl26\n",
              "WNXV1Wf/wBtIbd9rSUmJJcnze/rj7+XM/RkzZliPP/6459ihQ4csyzr17yYsLMw6ePCgZVmWNW7c\n",
              "OM95zz//vJWQkGCVl5dbJ0+etEaMGGE98cQTlmX5fkcbNmywRowYYZWXl1uWZVkfffSR1aNHj7O+\n",
              "rzP7l5CQYA0bNsw6ceKEZVmWVV1d7elTdXW1lZKSYqWnp1uWder3ISAgwPrHP/5hWZZlLVmyxLr1\n",
              "1lsty7Ksp556ypoyZYrP+/zx35qVK1dakyZNsiorKy3LsqysrCzrtttu83wmP/5bcjGoqqqy7rrr\n",
              "Lqt169bWqFGjrCeffNL69ttvPcfDwsKsrVu3evYfeeQRKykpyaqqqrKOHj1qRUVFWatWrbIsy7IG\n",
              "Dx7s9ftw+ndo/Pjx1tNPP22tWrXK6tu3r7Vr1y7Lsmr/XGvDd4tL0eV1n6oG1dXVmjNnjjZs2CDL\n",
              "slRUVKRevXrp3nvv1ZAhQ/TYY49p+/btGjp0qG688UbPdaNHj9aVV14pSerfv7927txZ4+sPGTJE\n",
              "H374ocLCwtS/f39Jp0YaP/nkE3344YcaMmRInfs6btw4NW3aVE2bNlVUVJR27typgQMH6v3339ei\n",
              "RYskSR07dtTIkSO9rrvtttt0zTXXePbfffdd/eEPf1BZWZmqq6s9IyURERGSpF27dum9995Tenq6\n",
              "Hn74YR09elRffvmlp/+1XX+pioiIUEJCgmd/5syZmjx5sqZMmaIXX3xRP/nJT9SiRQvP8dPPnUZE\n",
              "RCg+Pl4fffSRXC6Xz+u+99572rx5s6RT63neddddeu+999SyZUuVl5frJz/5iefctm3b1qmvd999\n",
              "t1q2bFlj3//973/riy++8FrHs6ysTF9++aXi4uIkST/96U8lSd26dVPTpk21f/9+lZaW1tqfmTNn\n",
              "6tlnn9Wtt96q5557TtOmTfPc0msM4uPj9Ytf/EJHjx5VQkKCbr75Zs+x4cOHq02bNpKkgQMHatu2\n",
              "bZJOfa8TJkzwFE+ePHmynnvuOc2ZM0eS93f017/+VZ9//rnX85AlJSU6fvy4goKC6tTH++67T82a\n",
              "NZMkWZalp59+Wm+88YYqKytVWlqqQYMGec7t0qWLJ9bAgQM9fxeuv/56Pf3003r44YcVHx+v4cOH\n",
              "1xjr9ddf16ZNm9SvXz9Jp0ZPz/TjvyUXgyZNmmjt2rX6+uuvlZOTozfffFO/+c1v9Nlnn/mslyud\n",
              "+v5+97vfqUmTJrrqqqv0wAMP6N1339Xtt9+uDRs26O233/acGxIS4vl5+fLlCggI0AcffOC5M1HX\n",
              "z7U2fLe4FDT6W88xMTHavn27Dh06VOPxp556SkVFRfr000/1z3/+U0lJSZ4JCT/72c/0xhtvqEOH\n",
              "Dnr00Uc1bdo0z3VnPocWEBCgysrKGl9/yJAh+uCDD/TBBx94nktMSEjwtA0dOrTO76WuMX/8f+Rn\n",
              "JjmFhYV66KGHtGLFCv3rX//SqlWrvCZg3HzzzXrzzTe1fft2JSQkyLIsrV27VgMHDlTTpk3Pef3F\n",
              "6Fy/A2d+PtKpxP/KK6/UBx98oGXLlmn69Olnff26Jk51Oa9p06Zef8B//Nn+uK9n7luWJafTqby8\n",
              "PM+2e/dujR8/3nNOXX+HTrvrrrv0z3/+U1u3blV2dna9bq3ZLTo6usbv9ZNPPlHnzp29/k++NmPG\n",
              "jNHf//53XXfddXr22Wf1H//xH55jJv69WZal8ePHe30n+/btq3OS+OPX+/Of/6z169crJydH27Zt\n",
              "0+zZs71+R2rr88CBA5WXl6cBAwbotddeU1xcnE+icLq/qampnr5u27bNkyD/uC8Xm27dumnq1Kl6\n",
              "/fXXdf311ys7O7tO19X132+fPn1UUlLi9XnU9XOtDd8tLgWNPlHs0qWLxowZo4kTJ3pmuJ5Ofnbt\n",
              "2qXDhw+rffv2uuKKK7R//36tXr3ac+2///1vhYeHa/LkyXr00Uf1j3/8o97x4+LiVFRUpJUrV3ol\n",
              "iqtWrdK+ffs8o3RnCg4O1vHjx2t87rEmN998szIzMyWdeq7qbH8gS0tL1axZM3Xo0EGWZXk9w3b6\n",
              "tX772996+jV06FDNnz/fM9JyrusvRuf6HajJzJkz9cADD6h79+7q2rWr17HTz/bl5+crNzfXM3ux\n",
              "ZcuWKi0t9Zx3880368UXX5R06jmi1157Tbfccouuu+46XXnllXrllVc85x48eNDT108//VSStHv3\n",
              "bm3YsKHO7/O6667zWlBeOvV85rlGfM/Wn6ZNmyolJUUjR47U6NGjbZ38VV+RkZG64447NGXKFM8z\n",
              "pfn5+Xr44YdrfLasJtu3b9c111yjBx54QE8++WSd/o3ffPPNysrK0okTJ1RZWak//vGPuvXWW2s8\n",
              "d+TIkVqxYoUKCwslnbqDcfrZsvNx+PBhtW3bVsHBwSorK/M8v3ouu3fvVosWLXTPPffoD3/4g775\n",
              "5hsdPXrU57xRo0Zp6dKlnt+ZkydPauvWrefd34bw3Xff6e9//7tn//Dhw9q9e7fcbrekU39Pf/zv\n",
              "MiMjQ5Zl6dixY1q+fLluvfVWtWjRQvHx8frd737nObe4uNjzc9++ffW///u/Sk5O1ltvvSWp7p9r\n",
              "XfDd4mLV6BNFScrMzFTfvn01YMAA9ezZUz169NA777wjp9OpmTNn6tNPP1XPnj11//33e916evbZ\n",
              "Z9WzZ09FR0frV7/6ldcfkLpq1qyZbrzxRpWVlalbt26SpK5du6qsrEw33nij57bDmZxOpx544AH1\n",
              "6dPHazJLbRYvXqx//OMf6tGjhx544IGzjlL27t1b9957r3r27Km4uDiFhoZ6HU9MTFRhYaHnc7jl\n",
              "lltUUFCgxMTEOl1/sTrb70BNxo4dq6NHj+qhhx7yOVZVVaXo6Gjdeuut+v3vf++57fzwww/rlltu\n",
              "8Uxm+f3vf6+vvvpKvXv31pAhQzR37lwNGDBATZs21V//+le99NJL6t27t/r27au1a9dKkh555BF9\n",
              "8MEH6t27t1JTU89awuXHmjZtqr/97W967bXX1KdPH89El+PHj5/zutr6I52aMf7dd9/V+Fn4W1ZW\n",
              "liIiItS7d291795d//Ef/6Ff/OIXmjx5cp2uX7NmjXr37q3o6GiNGzfOM6nobKZMmaKYmBjFxMQo\n",
              "KipKLper1lI6N910k5588kmNHj3aMwlh1apV9XmLXh544AH98MMPuu666zRixIg6l1j58MMPPRPz\n",
              "Bg0apN/+9rdq1aqVz3k//elPNWHCBA0ZMkR9+/ZVVFSU1q9ff979bQiVlZVauHChunbt6ik7M378\n",
              "eN15552SpBkzZmjy5MmeySzz5s1Ts2bN1Lt3bw0YMEAjR47UPffcI+nU7eXPPvtMPXv2VFRUlM9/\n",
              "CHfv3l1vv/22Zs6cqbVr19b5c60LvltcrFjrGajBZ599pqSkJH399ddeZSIcDocOHz58UY2s2W3N\n",
              "mjVasmSJ3n//fX93BQDQwC77ySzAj02aNEnvvPOO/vjHP172tcSGDx+ub775RuvWrfN3VwAAfsCI\n",
              "IgAAAGp0eQ+XAI3I6WXCevTooYCAAM/+6aUhLyYffvihZ0KAJO3du9fokmb//d//7Xn/LVq0UHh4\n",
              "uGf/Yi8QDwAXE0YUgUYmPz9fUVFRNa5jfbEs87dgwQJ9//33571+bn0MHjxYP/vZzzRq1CjbYwFA\n",
              "Y8OIItDIuVwuzZkzR/3799f48ePPumzlyy+/rJtvvlk/+clP1Lt3b8XGxnpKCNW23N3777+vgQMH\n",
              "Kjo6Wj179lRGRoYndmlpqSZNmqRevXqpb9++Sk5OVl5enpYuXaqVK1cqKipKCxcuVH5+vtcEobff\n",
              "flsxMTHq06ePEhIS9OWXX0o6+1Jm57J3797zWp5RkjZt2qShQ4cqNjZW0dHRXmW0AKBRa+CVYADY\n",
              "7MdLd4WFhVkTJ070LL13tmUrX3rpJSs4ONizRNmcOXM8y4PVttxdSUmJZ1mwQ4cOWaGhodaePXss\n",
              "y7KsCRMmWP/5n/9pVVVVWZb1/5dEmz9/vjVz5swa+3zgwAHL6XRa//znPy3LsqwVK1ZY3bt3t6qr\n",
              "q8+6lFltzlwW8XyWZzx8+LAVFRVl7d2717KsU8sbdu7c2WuZOABorBhRBC4DEyZM8KxAcXrZyr59\n",
              "+yo6OlqfffaZ8vLyPOcOHDhQ4eHhnp9PL08ZHx+vF198UXPnztU777zjGQE8dOiQ7r77bvXq1UtD\n",
              "hw7VoUOH9K9//UuS9Le//U2zZ8/2zB6vy2opn376qXr37q3evXtLOlX/be/evfruu+8k+S5lVtvy\n",
              "mTWZOXOmnnvuOUmq8/KMH3/8sXbt2qURI0YoKirKU2OUZx0BXA78/7ASANudmQyduWzlFVdcoVmz\n",
              "ZtVpqbAxY8Zo0KBBevfdd/Xss8/qmWee0f/93/8pJSVFt912m9auXSuHw6GYmBhbl3Ws7zKEZ/rx\n",
              "8ozvvffeWc93OByyLEs9e/bUxx9/fN59BoBLFSOKwGXmbMtWnk1ty90dPnxYYWFhcjgc+uijj/T5\n",
              "5597rhk5cqQWLVrkeQby9JJoP15W7UzXX3+9tm3b5hmVXLVqlTp27KiOHTue93s+U32XZxw0aJB2\n",
              "797tlVTm5eXVeYlNALiUMaIIXGZmzpypsWPHqmfPnrr22mu9lq08mzVr1mjFihVq3ry5qqurPcvd\n",
              "PfHEE5o2bZoee+wxRUVFeS07+PTTT+vnP/+5evfurWbNmikuLk4vvviiRo8ereXLlysqKkp33XWX\n",
              "HnjgAc81ISEhWrlypR544AFVVlaqdevWWr16tefW+YUaO3as/vM///OsyzMeO3bMa3nGN954Q7Nn\n",
              "z9bDDz+skydPKjQ0VK+//rqR/gDAxYzyOAAuKyzPCAB1x4gigMsGyzMCQP0woggAAIAa8Z/UAAAA\n",
              "qBGJIgAAAGpEoggAAIAakSgCAACgRiSKAAAAqBGJIgAAAGpEoggAAIAakSgCAACgRiSKAAAAqNH/\n",
              "Awd6SCs4VrdoAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-682788ed-a25c-4838-b4c1-104abe946ca8\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-682788ed-a25c-4838-b4c1-104abe946ca8\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "import pandas as pd\n",
              "plt.subplots(figsize=(8, 8))\n",
              "df_2dhist = pd.DataFrame({\n",
              "    x_label: grp['Person Involved'].value_counts()\n",
              "    for x_label, grp in _df_19.groupby('Date of Transaction')\n",
              "})\n",
              "sns.heatmap(df_2dhist, cmap='viridis')\n",
              "plt.xlabel('Date of Transaction')\n",
              "_ = plt.ylabel('Person Involved')"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-7739103f-663a-4766-98e0-2392b00968ea\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAosAAAMKCAYAAADtcQ7eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABiPElEQVR4nO3de3xU1b3///ckQIKXgIBcFEIgXKLckiBgRQGFCghGIVhSxBpF\n",
              "DaBf6UmtNuIF9NRIj0KxloIKsVwOlpuaUxUrghiOBxQwCgIqgSQIxIAi90wyM/v3BzK/xmQkA7Oy\n",
              "N5PX8zz245F9mVkr+ZyNn37WXmu7LMuyBAAAAFQjwu4OAAAAwLlIFgEAABAQySIAAAACIlkEAABA\n",
              "QCSLAAAACIhkEQAAAAGRLAIAACAgkkUAAAAERLIIAACAgEgWAQAAbPTggw8qLi5OLpdL+fn51V5T\n",
              "WFioAQMGqFGjRkpMTKxyfu7cuerYsaPi4+N17733qqKiokbnaoJkEQAAwEajRo3SunXr1LZt24DX\n",
              "xMTE6D//8z/13//931XO7d69W48//rjy8vK0c+dOffvtt3rppZfOeK6mSBYBAABs1K9fP7Vu3fpn\n",
              "r2nSpImuvfZaXXjhhVXOLVu2TCkpKWrZsqVcLpfGjx+vxYsXn/FcTdUL6moAAACckdvtltvtrnQs\n",
              "KipKUVFRIW+ruLi4UlUyLi5OxcXFZzxXU+dNsth+5nS7u4Af7ZqUaXcXAAA4I19JJ9vazp49RlOn\n",
              "Tq107Mknn9SUKVPs6dA5OG+SRQAAgPNFVlaWMjMrF1dMVBUlKTY2VgUFBf79wsJCxcbGnvFcTfHM\n",
              "IgAACEs+G/8vKipKMTExlTZTyWJqaqpyc3NVUlIiy7I0e/ZspaWlnfFcTZEsAgAA2CgjI0OtW7fW\n",
              "N998o8GDB6tDhw6SpHvuuUe5ubmSpBMnTqh169a67bbbtG3bNrVu3VpZWVmSpPbt22vq1Knq27ev\n",
              "OnTooEsvvVQZGRlnPFdTLsuyrBD+vsbwzKJz8MwiAOB84CnpYFvb9VrutK3tUOOZRQAAEJa8ls+2\n",
              "tsMpwWIYGgAAAAGFU+ILAADg59N58aSd41FZBAAAQEAkiwAAAAiIYWgAABCWfLJvgks4obIIAACA\n",
              "gKgsAgCAsOQ9P5aSdjwqiwAAAAiIyiIAAAhLLJ0TGlQWAQAAEBDJIgAAAAJiGBoAAIQlL8PQIUFl\n",
              "EQAAAAFRWQQAAGGJCS6hQWURAAAAAZEsAgAAICCGoQEAQFjiDS6hQWURAAAAAVFZBAAAYclndwfC\n",
              "BJVFAAAABERlEQAAhCUW5Q4NKosAAAAIiGQRAAAAAdkyDP3dd9+padOmdjQNAADqCC+j0CFhS2Ux\n",
              "KSnJjmYBAAAQJGOVxdzc3IDnysrKTDULAAAgiaVzQsVYsjhixAj1799fVjWrpx89etRUswAAAAgh\n",
              "Y8lix44dNW/ePMXFxVU516ZNG1PNAgAAIISMJYt33nmnDh48WG2yOH78eFPNAgAASJK8ctndhbDg\n",
              "sqobJ3ag9jOn290F/GjXpEy7uwAAwBl9/c1ltrXdsfU+29oONaNL53i9Xq1du1bFxcWSpNjYWPXv\n",
              "31+RkZEmmwUAAJDvvCiHOZ+xZDEvL09jxozR5ZdfrrZt20qSCgsLtW/fPi1atEj9+vUz1TQAAABC\n",
              "xFiyeP/99+v111/XVVddVen4J598orvvvltbtmwx1TQAAADPLIaIsUW5y8rKqiSKktSrVy+53W5T\n",
              "zQIAACCEjCWL8fHxeuqpp1RaWuo/VlpaqqlTp6pdu3ammgUAAEAIGUsW58+fr6KiIsXHx6thw4Zq\n",
              "2LCh4uPjVVRUpAULFphqFgAAQNKpYWi7tnBi7JnFSy+9VHPnztXcuXP1/fffS5KaNGliqjkAAAAY\n",
              "YHTpnNN+miR+9913atq0aW00DQAA6iifFV4VPrsYG4b+OUlJSXY0CwAAgCAZqyzm5uYGPFdWVmaq\n",
              "WQAAAISQsWRxxIgR6t+/v6p7m+DRo0dNNQsAACCJdRZDxViy2LFjR82bN09xcXFVzrVp08ZUswAA\n",
              "AAghY8ninXfeqYMHD1abLI4fP95UswAAAJIkrz1TM8KOsWQxKysr4LnJkyebahYAAAAhVCtL5/wU\n",
              "S+cAAADTWDonNFg6BwAAAAGxdA4AAAACYukcAAAQllg6JzRYOgcAAAABsXQOAAAIS16LpXNCwfal\n",
              "c3bv3q127dqZ6gYAAADOge0pd2pqqt1dAAAAQAC2rLP476qbAAMAAHCufPbXxMKC7X9Fl4uZSgAA\n",
              "AE5le2WxOm63W263u9Ixy+ORq54juwsAAByIpXNCw/bKYnXD0NnZ2WrUqFGl7Yf33rehdwAAAHWb\n",
              "7cniyJEjqxzLysrS4cOHK22NfznQht4BAIDzldeKsG0LJ8bHdT0ej5YvX66CggJ5PB7/8SeeeEKS\n",
              "9Pjjj1f5TFRUlKKioiodYwgaAACg9hnPwNLS0lRSUqLevXsrMjLSdHMAAAAIIePJ4pYtW7Rjxw5m\n",
              "PQMAgFrlY4JLSBgfVG/Tpo3Ky8tNNwMAAAADjFcWO3TooAEDBmjEiBGKjo72H3/wwQdNNw0AAOow\n",
              "r/3zeMOC8WTR7XYrISFB27dv9x9jSBoAAOD8YDxZzMnJMd0EAAAADKmVpXNmzJih9957T5I0ePBg\n",
              "TZo0SfVYCgcAABgUbusd2sV4xpaZmamCggJNnDhRLpdLr7zyioqKivTCCy+YbhoAAADnyHiy+MEH\n",
              "Hyg/P18REaey+2HDhik5Odl0swAAoI7zMcElJIz/FS3Lks/nq7Rf3fugAQAA6qIHH3xQcXFxcrlc\n",
              "ys/PD3jd3Llz1bFjR8XHx+vee+9VRUWFpFPzQxITE/1bs2bN/K9TLiwsVGRkZKXzBQUFQfXPeGVx\n",
              "yJAhuvHGG5Weni5Jmj9/voYOHWq6WQAAUMd5rfNj9ZVRo0bp4Ycf1rXXXhvwmt27d+vxxx/X5s2b\n",
              "1aJFC91yyy166aWXdP/99+uuu+7SXXfd5b+2a9euuv322/37F1988c8moWdivLI4bdo03XbbbcrN\n",
              "zVVubq5GjRqlZ5991nSzAAAA54V+/fqpdevWP3vNsmXLlJKSopYtW8rlcmn8+PFavHhxles2bNig\n",
              "0tJSpaSkhKx/xiuLERERmjBhgiZMmCDLsnTs2DH/84sAAADhyO12y+12VzoWFRWlqKios/q+4uJi\n",
              "tW3b1r8fFxen4uLiKtfNnTtXd9xxh+rXr+8/dvz4cfXq1Uter1e33nqrJk+erMjIyBq3bTxrGzdu\n",
              "nH744QeVl5crMTFRLVq00KxZs0w3CwAA6jivImzbsrOz1ahRo0pbdna20d/3+PHjeu211zRu3Dj/\n",
              "sVatWmnv3r365JNPtGrVKuXl5en5558P6nuNJ4ubNm1S48aNtXLlSiUlJamkpESzZ8823SwAAIBt\n",
              "srKydPjw4UpbVlbWWX9fbGysioqK/PuFhYWKjY2tdM3SpUvVpUsXXXnllf5jUVFRat68uSSpSZMm\n",
              "uvvuu5WXlxdU27UyG1qS8vLyNHz4cMXExARV+gQAADgbPivCti0qKkoxMTGVtrMdgpak1NRU5ebm\n",
              "qqSkRJZlafbs2UpLS6t0zdy5cytVFSWptLTUP2va7XZrxYoVSkpKCqpt48liy5YtNWHCBC1dulSD\n",
              "Bg1SRUWFvF6v6WYBAADOCxkZGWrdurW++eYbDR48WB06dJAk3XPPPcrNzZUktW/fXlOnTlXfvn3V\n",
              "oUMHXXrppcrIyPB/x5dffqn8/HyNHj260nevW7dOSUlJ6tGjh5KTk9WyZUtNnjw5qP65LMOLHh48\n",
              "eFALFy7U1VdfrauvvlqFhYX64IMP/Evp1FT7mdPNdBBB2zUp0+4uAABwRv/Y2cu2tkd3+MS2tkPN\n",
              "6Gxor9ertLQ0rVq1yn8sLi4u6EQRAAAgWF7e4BISRv+KkZGROnHiRKU3uAAAAOD8YXydxV69emn4\n",
              "8OEaO3asLrroIv/xUC4WCQAA8FPnyxtcnM54svj5559Lkl5++WX/MZfLRbIIAABwHjCeLK5Zs8Z0\n",
              "EwAAADDE+JOfHo9Hzz//vCZOnChJKigo0OrVq003CwAA6jifImzbwonxyuIDDzwgr9erdevWSZKa\n",
              "Nm2q0aNHa+PGjaabBgAAwDkyniyuX79e+fn5/tXCGzdu7F9JHAAAwBSvFV4VPrsY/ytGR0dX2vd6\n",
              "vSylAwAAcJ4wXlns3r27Fi5cKJ/Pp507d2ratGkaMGCA6WYBAEAd5xNL54SC0cri1q1b1a9fP+Xk\n",
              "5KikpER9+/ZVRESEpk2bZrJZAAAAhIixyuKsWbP06KOPqnPnztqxY4dycnI0cuRIU80BAADAAKPJ\n",
              "4ueff67Y2Fht2bJFEyZMIFkEAAC1hgkuoWHsr1i/fn3FxsZKkrp166bjx4+bagoAAACGGKsslpWV\n",
              "acuWLbIsq9r97t27m2oaAABA3jBbHNsuxpLFkydPVnn/8+l9l8ulXbt2mWoaAAAAIWIsWSwsLDT1\n",
              "1QAAAKglxtdZBAAAsIPPYp3FUGAwHwAAAAFRWQQAAGGJCS6hwV8RAAAAAVFZBAAAYcnHotwhwV8R\n",
              "AAAAAZEsAgAAICCGoQEAQFjyiqVzQoHKIgAAAAKisggAAMISE1xCg78iAAAAAiJZBAAAQEAMQwMA\n",
              "gLDEBJfQoLIIAACAgKgsAgCAsMQEl9DgrwgAAICAqCwCAICw5KWyGBL8FQEAABAQySIAAAACYhga\n",
              "AACEJR9L54QElUUAAAAERGURAACEJSa4hAZ/RQAAAAREsggAAICAzpth6F2TMu3uAgD8rPYzp9vd\n",
              "BfyI/2ZAknwWE1xCgcoiAAAAAjpvKosAAADB8FITCwn+igAAAAiIyiIAAAhLPLMYGlQWAQAAEBDJ\n",
              "IgAAAAJiGBoAAIQlHzWxkOCvCAAAgICoLAIAgLDkZYJLSFBZBAAAQEAkiwAAAAiIYWgAABCWWGcx\n",
              "NKgsAgAAICAqiwAAICz5LGpiocBfEQAAAAFRWQQAAGHJK55ZDAUqiwAAAAiIZBEAAAABMQwNAADC\n",
              "EkvnhAaVRQAAAAREZREAAIQlls4JDf6KAAAACIhkEQAAAAExDA0AAMKSj3UWQ4LKIgAAAAKisggA\n",
              "AMKSl6VzQoLKIgAAgI0efPBBxcXFyeVyKT8/P+B1c+fOVceOHRUfH697771XFRUVkqQPPvhADRs2\n",
              "VGJion87efLkGT9XUySLAAAgLPmsCNu2YIwaNUrr1q1T27ZtA16ze/duPf7448rLy9POnTv17bff\n",
              "6qWXXvKf79y5s/Lz8/1bw4YNa/S5mjCWLBYXF6usrEySZFmWZs2apd/85jd6/vnn5fF4TDULAABw\n",
              "XunXr59at279s9csW7ZMKSkpatmypVwul8aPH6/Fixef8bvP9nP/zliyOHz4cHm9XknSk08+qRUr\n",
              "Vujqq69WXl6eJk2aZKpZAAAA27ndbh05cqTS5na7z/r7iouLK1Ue4+LiVFxc7N8vKChQcnKyevXq\n",
              "pVmzZtX4czVhbIKLZVm68MILJUlvvfWW8vLydMEFF+i+++5TcnKyqWYBAAAk2ftu6OzsbE2dOrXS\n",
              "sSeffFJTpkwJeVvJycn65ptv1KhRI33zzTe66aab1KxZM/3qV78Kyfcbqyy6XC59++23kqSLL75Y\n",
              "9eqdyksjIyMZhgYAAGEtKytLhw8frrRlZWWd9ffFxsaqqKjIv19YWKjY2FhJUkxMjBo1aiRJat26\n",
              "tX79618rLy/vjJ+rKWPJ4pNPPqnrr79er7zyiq677jqlpqbq1Vdf1e23365hw4aZahYAAEDSqUW5\n",
              "7dqioqIUExNTaYuKijrr3yU1NVW5ubkqKSmRZVmaPXu20tLSJEn79++Xz+eTJB09elT//Oc/lZSU\n",
              "dMbP1ZSxZDE1NVWLFi3S2rVr9fbbb6u4uFjLly/XzTffrD/96U+mmgUAADivZGRkqHXr1vrmm280\n",
              "ePBgdejQQZJ0zz33KDc3V5LUvn17TZ06VX379lWHDh106aWXKiMjQ5K0fPlydevWTT169NDVV1+t\n",
              "X/7yl7rrrrvO+LmaclmWZYXw9wWAOqv9zOl2dwE/2jUp0+4uwAFu33CvbW0v6vOybW2HmtF1Fk+e\n",
              "PFntjJsvvvjCZLMAAADyWS7btnBiLFn817/+pVatWqlbt25KTk7Wzp07/efuuOMOU80CAAAghIwl\n",
              "i4899pg+/PBD/fDDD7r//vs1aNAgbd26VdKpZXUAAABMOl/e4OJ0xtZZrKioUPfu3SVJ48aNU1xc\n",
              "nIYPH64333xTLld4lWcBAADClbFksaysTG632z9NfODAgfr73/+ulJQUlZeXm2oWAAAAIWSsTjpy\n",
              "5Eh98MEHlY71799fCxYs0MUXX2yqWQAAAElMcAkVY5XFP/7xj9Ue79evn7766itTzQIAACCEjCWL\n",
              "p/l8PkVEVC5gHjp0SJdcconppgEAQB3mU3hV+OxibBh648aNateunRo2bKgRI0bowIED/nMDBw40\n",
              "1SwAAABCyFiy+B//8R968cUXtW/fPnXt2lX9+vXT3r17JbF0DgAAMI9nFkPD2DD0sWPHNGzYMEnS\n",
              "008/rc6dO+uGG27QqlWrWDoHAADgPGEsWTxx4kSl5xXHjh2r+vXra+DAgXK73aaaBQAAQAgZSxb7\n",
              "9u2rt99+W8OHD/cfGz16tFwul8aOHWuqWQAAAEkKu+Fguxh7ZnHevHmVEsXTfvWrX7EoNwAAwHnC\n",
              "+NI5p3k8Hm3ZskXt27dXo0aNaqtZAABQR1FZDA1jlcXVq1eradOmatasmdauXatrrrlGY8aMUXx8\n",
              "vNauXWuqWQAAAISQscpiVlaW3n//ff3www9KTU3VkiVLdMMNN+jjjz/W7373O+Xl5ZlqGgAAACFi\n",
              "LFksLy9XYmKiJKlx48a64YYbJEm9e/fWsWPHTDULAAAgiWHoUDE2DO3z+fw/33bbbZXOeb1eU80C\n",
              "AAAghIxVFnv27KkjR44oJiZG2dnZ/uMFBQWKiYkx1SwAAIAk3g0dKkaXzqkuKWzbtq3ee+89//7u\n",
              "3btNdQEAAADnyFiyGEi9evXUsGFD/35qamptdwEAANQBvBs6NGo9Wfwpy7Ls7gIAAAACsD1ZdLnC\n",
              "K/sGAAAIJ7X2BhcAAIDaFG7DwXaxPVmsbhja7XbL7XZXOhYVFaWoqKja6hYAAADkgGHokSNHVjmW\n",
              "nZ2tRo0aVdr+ffkdAACAM2GCS2i4LMMzTDwej5YvX66CggJ5PB7/8SeeeCLgZ6gsAjgftZ853e4u\n",
              "4Ee7JmXa3QU4wMA19v3/wfvXh8+/B8aHodPS0lRSUqLevXsrMjKyRp8hMQQAAHAG48nili1btGPH\n",
              "DmY9AwCAWhVuw8F2Mf7MYps2bVReXm66GQAAABhgvLLYoUMHDRgwQCNGjFB0dLT/+IMPPmi6aQAA\n",
              "UIdZVBZDwniy6Ha7lZCQoO3bt/uPMSQNAABwfjCeLObk5JhuAgAAoAqfKE6FgvFk0ePxaMaMGXrv\n",
              "vfckSYMHD9akSZNUr57t64EDAADgDIxnbJmZmSooKNDEiRPlcrn0yiuvqKioSC+88ILppgEAAHCO\n",
              "jCeLH3zwgfLz8xURcWri9bBhw5ScnGy6WQAAUMexdE5oGF86x7Is+Xy+SvuGXxoDAACAEDFeWRwy\n",
              "ZIhuvPFGpaenS5Lmz5+voUOHmm4WAADUcSydExrGk8Vp06Zpzpw5ys3NlSSNGjVK9913n+lmAQAA\n",
              "EALGk8WIiAhNmDBBEyZMkGVZOnbsmP/5RQAAADib8axt3Lhx+uGHH1ReXq7ExES1aNFCs2bNMt0s\n",
              "AACo43yWy7YtnBhPFjdt2qTGjRtr5cqVSkpKUklJiWbPnm26WQAAAISA8WHo0zOf8/LyNHz4cMXE\n",
              "xCgyMtJ0swAAoI5jgktoGK8stmzZUhMmTNDSpUs1aNAgVVRUyOv1mm4WAAAAIWA8WVy0aJE6d+6s\n",
              "1157TY0bN9bevXuVmZlpulkAAFDH8cxiaBgdhvZ6vUpLS9OqVav8x+Li4vxrLgIAAMDZjFYWIyMj\n",
              "deLEiUpvcAEAAMD5w/gEl169emn48OEaO3asLrroIv/xlJQU000DAIA6jLcLh4bxZPHzzz+XJL38\n",
              "8sv+Yy6Xi2QRAADgPGA8WVyzZo3pJgAAAKrwKbwmmtjF+Gxoj8ej559/XhMnTpQkFRQUaPXq1aab\n",
              "BQAAQAgYryw+8MAD8nq9WrdunSSpadOmGj16tDZu3Gi6aQAAAJwj48ni+vXrlZ+fr6SkJElS48aN\n",
              "VVFRYbpZAABQx/EGl9AwPgwdHR1dad/r9bKUDgAAwHnCeGWxe/fuWrhwoXw+n3bu3Klp06ZpwIAB\n",
              "ppsFAAB1XLi9ScUuRiuLW7duVb9+/ZSTk6OSkhL17dtXERERmjZtmslmAQAAECLGKouzZs3So48+\n",
              "qs6dO2vHjh3KycnRyJEjTTUHAABQCYtyh4axyuKsWbP0+eefa8OGDVq3bp2mT59uqikAAAAYYixZ\n",
              "rF+/vmJjYyVJ3bp10/Hjx001BQAAAEOMDUOXlZVpy5Ytsn6sAf90v3v37qaaBgAAYOmcEDGWLJ48\n",
              "ebLK+59P77tcLu3atctU0wAAAAgRY8liYWGhqa8GAAA4IyqLoWF8UW4AAACcv0gWAQAAEJDxN7gA\n",
              "AADYgTe4hAaVRQAAAAREZREAAIQl3uASGlQWAQAAbPTggw8qLi5OLpdL+fn5Aa+bO3euOnbsqPj4\n",
              "eN17772qqKiQJK1evVq9e/fWlVdeqS5duujhhx+Wz+eTdGp1msjISCUmJvq3goKCoPpHsggAAMKS\n",
              "Zbls24IxatQorVu3Tm3btg14ze7du/X4448rLy9PO3fu1LfffquXXnpJknTJJZfotdde07Zt27Rp\n",
              "0yZ99NFHmj9/vv+zF198sfLz8/1bfHx8UP0jWQQAALBRv3791Lp165+9ZtmyZUpJSVHLli3lcrk0\n",
              "fvx4LV68WJKUlJSk9u3bS5Kio6OVmJgY0vWueWYRAAAgxNxut9xud6VjUVFRioqKOqvvKy4urlR5\n",
              "jIuLU3FxcZXrSkpKtGzZMv3zn//0Hzt+/Lh69eolr9erW2+9VZMnT1ZkZGSN26ayCAAAwpKdw9DZ\n",
              "2dlq1KhRpS07O9vo73vkyBHdfPPNevjhh3XVVVdJklq1aqW9e/fqk08+0apVq5SXl6fnn38+qO8l\n",
              "WQQAAAixrKwsHT58uNKWlZV11t8XGxuroqIi/35hYaFiY2P9+0ePHtWQIUN0yy23KDMz0388KipK\n",
              "zZs3lyQ1adJEd999t/Ly8oJqm2QRAACEJcvGLSoqSjExMZW2sx2ClqTU1FTl5uaqpKRElmVp9uzZ\n",
              "SktLkyQdO3ZMQ4YM0ZAhQ/TYY49V+lxpaal/1rTb7daKFSuUlJQUVNskiwAAADbKyMhQ69at9c03\n",
              "32jw4MHq0KGDJOmee+5Rbm6uJKl9+/aaOnWq+vbtqw4dOujSSy9VRkaGJGnmzJn6+OOPtWLFCv/y\n",
              "OH/84x8lSevWrVNSUpJ69Oih5ORktWzZUpMnTw6qfy7LYslKAAiF9jOn290F/GjXpMwzX4Sw13nF\n",
              "U7a1/eXIJ2xrO9SYDQ0AAMJSsOsdonoMQwMAACAgKosAACA88aBdSFBZBAAAQEAkiwAAAAiIYWgA\n",
              "ABCWmOASGlQWAQAAEBCVRQAAEJZYSTo0qCwCAAAgICqLAAAgLPHMYmiQLALnMV4v5yy8Yg5AOGIY\n",
              "GgAAAAFRWQQAAOGJYeiQoLIIAACAgKgsAgCAsMTSOaFBZREAAAABkSwCAAAgIIahAQBAeGIYOiSo\n",
              "LAIAACAgKosAACAs8QaX0KCyCAAAgICoLAIAgPDEM4shQWURAAAAAZEsAgAAICCGoQEAQFhigkto\n",
              "UFkEAABAQFQWAQBAeGKCS0hQWQQAAEBAJIsAAAAIqEbD0E899dTPnn/iiSdC0hkAAIDQYYJLKNQo\n",
              "WTx69Kgk6ZtvvtH777+vlJQUuVwu5ebmauDAgUY7CAAAAPvUKFn8r//6L0nSjTfeqPz8fF122WWS\n",
              "TlUc09PTjXUOAADgrDHBJSSCemZx3759/kRRklq1aqW9e/eGvFMAAABwhqCSxdatW+vJJ5/Unj17\n",
              "tGfPHk2ZMkWtW7c21TcAAICzZ9m4hZGgksVXX31V27dvV2JiopKSkrRjxw69+uqrhroGAAAAuwW1\n",
              "KHfLli21ZMkSU30BAACAwwRVWTx8+LAeeOAB3XzzzZKkbdu2afHixUY6BgAAcE4sl31bGAkqWczI\n",
              "yFDLli21e/duSVK7du00bdo0Ix0DAACA/YJKFr/66is99thjql+/viSpYcOGsqwwe4oTAACEBcuy\n",
              "bwsnQSWLDRo0qLR/8uRJkkUAAIAwFlSyeP311+uPf/yjysrKtGrVKo0aNUojR46s8ecPHToUdAcB\n",
              "AABgn6CSxaeffloRERGKiYnRo48+qr59++rxxx+v9tqZM2f6f969e7e6dOmiyy67TO3atdOWLVvO\n",
              "rdcAAABnwjqLIRFUsuhyuZSVlaUNGzbo448/1qOPPqrIyMhqr/373//u//nRRx/VxIkTdfLkST33\n",
              "3HPKzMw8t14DAACgVgT9BpeHH35Y27dvD6qRbdu26f7775ckpaam6sCBA0F9HgAAIGgsnRMSQSWL\n",
              "//d//6cLLrhAw4YNU58+fTRnzhwdOXKk2mt/+OEH/c///I9yc3NVUVFR6RyTYgAAAM4PQSWLcXFx\n",
              "mjJlinbt2qVp06ZpzZo1atWqVbXXxsbGavr06ZoxY4ZatGihvXv3SpJKS0urzKoGAAAINZdl3xZO\n",
              "gnrd32mbNm3S0qVLtXr1al1//fXVXvPBBx9Ue7xp06Zau3bt2TQLAACAWhZUsvj888/r1Vdfldfr\n",
              "1V133aXPPvssYGWxOocOHdIll1yiCy64IOiOAgAAoPYFNQz95Zdf6qWXXtK2bdv0+9///mcTRZbO\n",
              "AQAAtmLpnJAIKll86aWX9Itf/KJG17J0DgAAwPmvRsPQSUlJcrkCTwPfvHnzz35+27ZtWrx4saRT\n",
              "S+c8/fTTQXQRAADgLITZEjZ2qVGy+Oc//znoLz69dI5lWSydAwAAcJ6qUbLYv3//Svv79u2TJF12\n",
              "2WUBP3N66RxJ/qVzLr/8cpbOAQAAOI8ENRt6+/btGjVqlD9ZbN26tZYuXaqEhIQq17J0DgAAsBUD\n",
              "mSER1ASXiRMnavLkyTp06JAOHTqkyZMna8KECTX67JYtWzRv3jx9+umnLJ0DAABwnggqWTx06JDG\n",
              "jBnj309LS9OhQ4eqvXbgwIEqLS2VJC1ZskRDhgzRypUrNWrUKM2ZM+ccugwAAFADLJ0TEkEli5GR\n",
              "kdq2bZt/f9u2bYqMjKz22gMHDqh58+aSpBkzZuijjz7SkiVLtHnzZv31r389hy4DAACgtgT1zOIz\n",
              "zzyjfv36qXv37pJODS0vWrSo2mvdbre8Xq8iIyNlWZbatm0rSWrSpAmzoQEAgHmkGyERVLI4ePBg\n",
              "bd++XRs2bJAkXX311WrWrFm11/7617/W6NGj9eyzz2rUqFH64x//qNtvv13vvPOO2rdvf+49BwAA\n",
              "gHFBJYt/+ctfdMcdd2j48OFnvHbKlCmaOXOmrr/+en377bfyeDz605/+pF//+tfKyck56w4DAACg\n",
              "9gT1zOLmzZsVHx+vX/3qV3rnnXfOOJw8adIk7dmzR999952+++47HT58WLNnz1aTJk3OqdMAAABn\n",
              "ZLns28JIUMliTk6OiouLNXToUE2bNk1t27bV5MmTz/i5iy++WBdffLE+/fRTHT58+Kw7CwAAgNoV\n",
              "VLIoSRdeeKHuuusuvfbaa7rpppv07LPPVnvd6tWr1bRpUzVr1kxr167VNddcozFjxig+Pp5FuQEA\n",
              "gHEuy74tnASVLHo8Hq1YsULDhw9Xjx49VK9ePf9kl5/KysrS+++/r2XLlik1NVXPPvustm/frrff\n",
              "fluPPfZYSDoPAAAAs4Ka4HL55ZcrKSlJ6enpWr58uaKiogJeW15ersTERElS48aNdcMNN0iSevfu\n",
              "rWPHjp19jwEAAFBrgkoWN23apNatW9foWp/P5//5tttuq3TO6/UG0ywAAEDwwmw42C5BJYutW7fW\n",
              "hg0bVFBQII/H4z/+m9/8psq1PXv21JEjRxQTE6Ps7Gz/8YKCAsXExJxDlwEAAFBbgkoWJ0yYoHff\n",
              "fVeJiYn+1/y5XK5qk8V58+ZV+x1t27bVe++959/fvXu32rVrF0w3AAAAUEuCmuCyatUqbdu2TStW\n",
              "rNDSpUu1dOlSLVmyJKgG69Wrp4YNG/r3U1NTg/o8AABAOHnwwQcVFxcnl8ul/Pz8gNfNnTtXHTt2\n",
              "VHx8vO69915VVFSc87maCCpZbNWq1c9OajkbvCcaAACYcL4snTNq1CitW7dObdu2DXjN7t279fjj\n",
              "jysvL087d+7Ut99+q5deeumcztVUUMlinz59NGrUKC1ZskS5ubn+7Vy4XOG1yjkAAEAw+vXrd8YJ\n",
              "xMuWLVNKSopatmwpl8ul8ePHa/Hixed0rqaCemZx48aNkqS//e1v/mMul0spKSlBNXombrdbbre7\n",
              "0rGoqKiQVzUBAABMCHUuU1xcXKnyGBcXp+Li4nM6V1NBVRbXrFlTZVu9enVQDf5UdcPQ2dnZatSo\n",
              "UaXt32dUAwAAnJGN74YOp1ymRpXFzz///GfPd+/e/aw7MHLkyCrHsrKylJmZWekYVUUAAHC+CHUu\n",
              "Exsbq4KCAv9+YWGhYmNjz+lcTdUoWbzlllsCnnO5XNq1a1fA8x6PR8uXL6+yNuMTTzwhSXr88cer\n",
              "fIYhZwAAcM5snEMb6lwmNTVV1157raZMmaIWLVpo9uzZSktLO6dzNVWjZHH37t1B/kr/v7S0NJWU\n",
              "lKh3797+tRkBAABwSkZGht566y2VlJRo8ODBuvjii7Vz507dc889SklJUUpKitq3b6+pU6eqb9++\n",
              "kqQBAwYoIyNDks76XE25LMNr13Tu3Fk7duxg1jNgQPuZ0+3uAv7NrkmZZ74IQK1p/2f7/o3c9dvw\n",
              "+fcgqNnQZ6NNmzYqLy9nWBkAANQulnIOCePJYocOHTRgwACNGDFC0dHR/uMPPvig6aYBAABwjown\n",
              "i263WwkJCdq+fbv/GEPSAADAtGDfpILqBZUsnmlmc3VycnLOvncAAACwVVDJ4tnMbPZ4PJoxY4be\n",
              "e+89SdLgwYM1adIk1atnvKgJAACAcxRUxrZly5agZzZnZmaqoKBAEydOlMvl0iuvvKKioiK98MIL\n",
              "QXcWAACgxhiGDomgksWzmdn8wQcfKD8/XxERp94sOGzYMCUnJwfXSwAAANgiqGTxbGY2W5Yln8/n\n",
              "TxYty6r2fdAAAAAhRboREkEli2czs3nIkCG68cYblZ6eLkmaP3++hg4dGnxPAQAAUOuCShbPZmbz\n",
              "tGnTNGfOHOXm5kqSRo0apfvuuy/o7wEAAAgGS+eERtBL5wQ7szkiIkITJkzQhAkTZFmWjh075h+S\n",
              "BgAAgLMFlbVlZmbqgw8+0MSJE3X//ffrgw8+UGbmz7/7cNy4cfrhhx9UXl6uxMREtWjRQrNmzTqn\n",
              "TgMAAKB2BFVZPJuZzZs2bVLjxo2Vm5urpKQk5eXl6dprr9XEiRPPvtcAAABnYvHGuFAIqrJ4embz\n",
              "v++faWbz6fN5eXkaPny4YmJiarygNwAAAOwVVGXxbGY2t2zZUhMmTNA777yjyZMnq6KiQl6v96w7\n",
              "DAAAUCNMcAmJoJLFs5nZvGjRIi1cuFB33nmnGjdurMLCwjM+5wgAAABnCCpZDHZms9frVVpamlat\n",
              "WuU/FhcX569MAgAAwNmCemYx2JnNkZGROnHiRKXnHAEAAGqDy7JvCydBVRbPZmZzr169NHz4cI0d\n",
              "O1YXXXSR/3hKSsrZ9xoAAAC1Iqhk8WxmNn/++eeSpJdfftl/zOVykSwCAACzwqzCZ5egksWzmdm8\n",
              "Zs2ac+ogAAAA7BPUM4uLFi1S586d9dprr6lx48bau3fvGWc2ezwePf/88/6h6oKCAq1evfrsewwA\n",
              "AFADPLMYGjWuLJ7tzOYHHnhAXq9X69atkyQ1bdpUo0eP1saNG8+uxwAAAKg1NU4W/31m888tl/NT\n",
              "69evV35+vpKSkiRJjRs3VkVFRfA9BQAAQK0L6pnFs5nZHB0dXWnf6/WylA4AADAvzIaD7RJUsng2\n",
              "M5u7d++uhQsXyufzaefOnZo2bZoGDBhwdr0FAABArQoqWQx2ZvPWrVvVr18/5eTkqKSkRH379tWt\n",
              "t96qadOmBfU9AAAAQaOyGBJBJYsej0czZ85UQUGBZs2apYKCAhUVFemGG26ocu2sWbP06KOPqnPn\n",
              "ztqxY4dycnI0cuTIkHUcAAAA5gW1dM4DDzygHTt2+CuMTZs21cMPP1zttbNmzdLnn3+uDRs2aN26\n",
              "dZo+ffq59xYAAAC1KqjKYjAzm+vXr6/Y2FhJUrdu3XT8+PFz7CoAAEDNhdt6h3YJKlkMZmZzWVmZ\n",
              "tmzZ4n9F4E/3u3fvfjb9BQAAQC0KKlkMZmbzyZMnq8ySPr3vcrm0a9eus+sxAAAAak1QyeL06dP1\n",
              "u9/9rkYzmwsLC0PRPwAAANioxsni1q1b9dVXX+nhhx/WnDlzTPYJAADg3PHMYkjUaDb0rFmzdO21\n",
              "12ratGnq2bOnXn/9ddP9AgAAgAPUOFk8vQxOXl6enn/+edP9AgAAgAPUaBj6p8vgnDhxwminAAAA\n",
              "zhVL54RGjZLFny57c/LkSZbBAQAAqANqlCyyDA4AADjvUFkMiRoliyyDAwAAUDcF9W5oAAAA1C1B\n",
              "LcoNAABw3mAYOiSoLAIAACAgKosAACAssXROaFBZBAAAQEBUFgEAQHiishgSVBYBAAAQEMkiAAAA\n",
              "AmIYGgAAhCUmuIQGlUUAAAAERGURAACEJyqLIUFlEQAAAAGRLAIAACAghqEBAEB4Yhg6JKgsAgAA\n",
              "ICAqiwAAICyxdE5okCwiaO1nTre7C/jRrkmZdncBABDmSBYBAEB4orIYEjyzCAAAgIBIFgEAABAQ\n",
              "w9AAACA8MQwdElQWAQAAEBCVRQAAEJZYOic0qCwCAAAgIJJFAAAABMQwNAAACE8MQ4cElUUAAAAE\n",
              "RGURAACEJSa4hAaVRQAAABt9/fXXuuaaa9SpUyf16tVLX3zxRZVrfD6fHnroIXXt2lUJCQkaN26c\n",
              "ysvLJUnvvvuuEhMT/dtll12m5ORk/2ddLpe6devmP5+XlxdU/0gWAQBAeLJs3IKQkZGh++67T199\n",
              "9ZUeeeQRpaenV7lm7ty52rx5szZv3qzt27crIiJCM2fOlCQNHjxY+fn5/i05OVm33357pc/n5eX5\n",
              "z1933XVB9Y9kEQAAwCalpaXauHGjxo4dK0lKTU3Vnj17tHPnzkrXffbZZxo0aJAaNGggl8uloUOH\n",
              "asGCBVW+b9++fXr//fd1xx13hKyPJIsAAAAh5na7deTIkUqb2+2uct2ePXvUqlUr1at3ahqJy+VS\n",
              "bGysiouLK13Xs2dP5ebm6siRI6qoqNCSJUtUWFhY5fteffVV3XTTTWrevHml4wMHDlSPHj2UmZmp\n",
              "48ePB/W7kCwCAIDwZOMwdHZ2tho1alRpy87OPutfJT09XUOGDFH//v3Vv39/derUyZ9g+n9dy9K8\n",
              "efM0bty4SseLioq0adMmffTRRzpw4IB+//vfB9U2ySIAAECIZWVl6fDhw5W2rKysKte1adNG+/fv\n",
              "l8fjkXQq4SsuLlZsbGyl61wul6ZMmaJPP/1UH330ka688kp16dKl0jVr165VWVmZBg8eXOn46e+6\n",
              "8MILNXHiRCa4AAAASJLLxi0qKkoxMTGVtqioqCp9bN68uZKTk7Vw4UJJ0vLly9W6dWt16NCh0nVl\n",
              "ZWU6dOiQJOngwYN69tln9fDDD1e6Zu7cuUpPT1dkZKT/2KFDh3TixAlJp2ZU/+Mf/1BSUlJQf0fW\n",
              "WQQAALDRnDlzlJ6ermeeeUYxMTHKycmRJN1zzz1KSUlRSkqKDh8+rAEDBigiIkI+n0+TJk3SzTff\n",
              "7P+Ow4cPa8WKFdqyZUul796xY4cyMjLkcrnk8XiUnJzsn0VdUy7LsliyEkFpP3O63V3Aj3ZNyrS7\n",
              "CwDgWN0zZ9jW9ufT/8O2tkONyiIAAAhPlMNCgmcWAQAAEBCVRQAAEJZ4N3RoUFkEAABAQCSLAAAA\n",
              "CIhhaAAAEJ4Yhg4JKosAAAAIiMoiAAAIT1QWQ4LKIgAAAAKq1WTxiiuuqM3mAABAHeay7NvCibFh\n",
              "6OTk5CrHdu3a5T++efNmU00DAAAgRIwli2VlZbruuus0ZswYSZJlWfr1r3+tGTPse08jAAAAgmNs\n",
              "GHrz5s2Kjo7Wiy++qK5du2rAgAFq2LCh+vfvr/79+5tqFgAA4BTLxi2MGEsWo6OjNXPmTGVkZGjw\n",
              "4MFasWKFqaYAAABgiPEJLoMGDdL777+v119/XZYVZqk2AABwLCa4hEatrLPYqFEjLViwoDaaAgAA\n",
              "QAgZqyy63W7Nnj1b//znPyVJ8+bN0x133KFp06apvLzcVLMAAAAIIWOVxfHjx6ukpEQnTpzQW2+9\n",
              "pS+//FKjRo3Su+++q507d+rll1821TQAAEDYTTSxi7Fk8eOPP9bWrVtVVlamli1bat++fbrwwgt1\n",
              "7733KikpyVSzAAAACCFjyWK9evXkcrkUHR2t6OhoXXjhhZKk+vXrKzIy0lSzAAAAksJvooldjCWL\n",
              "bdu21UMPPaSjR4+qS5cu+n//7//p9ttv18qVK9WqVStTzQIAACCEjE1wmTdvnrxery688EItX75c\n",
              "V1xxhe69917l5+dr9uzZppoFAAA4hUW5Q8JYZbFZs2aVXu03ceJETZw40VRzAAAAMMD4otz/7q67\n",
              "7qrN5gAAAHCOjFUWMzMzqxx7/fXXdckll0iSpk+fbqppAACAsBsOtouxyuKcOXN04MABNWrUyL+5\n",
              "XC7/zwAAAHA+Y5XFjRs3KiMjQ3369NEDDzwgSXr11Vf15JNPmmoSAADAj6VzQsNYZfGKK67Q6tWr\n",
              "VVpaqhtvvFFFRUVyuVymmgMAAIABxiqL0qmFuZ966ilt2LBBqampOn78uMnmAAAAEGJGk8XT+vTp\n",
              "o7y8PBUUFNRGcwAAAExwCRFjw9Bbt26ttN+wYUN17drVVHMAAAAwwFiy2L17d/Xo0UMvvPCCvv/+\n",
              "e1PNAAAAVMtlWbZt4cRYstilSxc98cQTWrlypWJjY5WWlqZVq1aZag4AAAAGGEsW69evr9TUVL39\n",
              "9tvavn27unbtqoyMDMXFxempp54y1SwAAMApvBs6JGrldX9t2rTRY489poKCAs2dO1dffvllbTQL\n",
              "AACAc2RsNnSDBg2qPT5w4EANHDjQVLMAAAAIIWOVxfXr19fout27d5vqAgAAqMNcln1bOKmVYeif\n",
              "k5qaancXAAAAEECtLMr9c6wwm14OAAAcghQjJGyvLPK+aAAAAOeyvbJYHbfbLbfbXelYVFSUoqKi\n",
              "bOoRAABA3WR7ZbG6Yejs7Gw1atSo0padnW1D7wAAwPmKCS6hYXtlceTIkVWOZWVlKTMzs9IxqooA\n",
              "AAC1z3iy6PF4tHz5chUUFMjj8fiPP/HEE5Kkxx9/vMpnGHIGAADnLMwqfHYxniympaWppKREvXv3\n",
              "VmRkpOnmAAAAEELGk8UtW7Zox44dzHoGAAC1KtyeHbSL8Qkubdq0UXl5uelmAAAAYIDxymKHDh00\n",
              "YMAAjRgxQtHR0f7jDz74oOmmAQAAcI6MJ4tut1sJCQnavn27/xhD0gAAwDiGoUPCeLKYk5NjugkA\n",
              "AAAYUitL58yYMUPvvfeeJGnw4MGaNGmS6tWzfYlHAAAQxpjgEhrGM7bMzEwVFBRo4sSJcrlceuWV\n",
              "V1RUVKQXXnjBdNMAAAA4R8aTxQ8++ED5+fmKiDg18XrYsGFKTk423SwAAABCwHiyaFmWfD6fP1m0\n",
              "LKva90EDAACEFPlGSBhPFocMGaIbb7xR6enpkqT58+dr6NChppsFAABACBhPFqdNm6Y5c+YoNzdX\n",
              "kjRq1Cjdd999ppsFAAB1HBNcQsN4shgREaEJEyZowoQJsixLx44d8w9JAwAAwNmMZ23jxo3TDz/8\n",
              "oPLyciUmJqpFixaaNWuW6WYBAEBdZ9m4hRHjyeKmTZvUuHFjrVy5UklJSSopKdHs2bNNNwsAAIAQ\n",
              "MJ4snp75nJeXp+HDhysmJkaRkZGmmwUAAEAIGE8WW7ZsqQkTJmjp0qUaNGiQKioq5PV6TTcLAADq\n",
              "OJfPvi2cGE8WFy1apM6dO+u1115T48aNtXfvXmVmZppuFgAAACFgdDa01+tVWlqaVq1a5T8WFxfn\n",
              "X3MRAADAmDCbaGIXo5XFyMhInThxQj5fmNVjAQAA6gjj6yz26tVLw4cP19ixY3XRRRf5j6ekpJhu\n",
              "GgAAAOfIeLL4+eefS5Jefvll/zGXy0WyCAAAjOINLqFhPFlcs2aN6SYAAABgiPHZ0B6PR88//7wm\n",
              "TpwoSSooKNDq1atNNwsAAOo6y7JvCyPGk8UHHnhAO3bs8FcYmzZtqocffth0swAAAOeFr7/+Wtdc\n",
              "c406deqkXr166Ysvvqhyjc/n00MPPaSuXbsqISFB48aNU3l5uSSpsLBQkZGRSkxM9G8FBQX+z/7z\n",
              "n/9UQkKCOnbsqJEjR+rIkSNB9c94srh+/Xq9/PLLio6OliQ1btxYFRUVppsFAAB1nMuybwtGRkaG\n",
              "7rvvPn311Vd65JFHql1icO7cudq8ebM2b96s7du3KyIiQjNnzvSfv/jii5Wfn+/f4uPjJUnHjh3T\n",
              "uHHj9MYbb+jrr7/WZZddpqeffjqo/hlPFk8niad5vV6W0gEAAJBUWlqqjRs3auzYsZKk1NRU7dmz\n",
              "Rzt37qx03WeffaZBgwapQYMGcrlcGjp0qBYsWHDG73/nnXeUlJSkhIQESdLEiRO1ePHioPpoPFns\n",
              "3r27Fi5cKJ/Pp507d2r8+PEaMGCA6WYBAABs43a7deTIkUqb2+2uct2ePXvUqlUr1at3as6xy+VS\n",
              "bGysiouLK13Xs2dP5ebm6siRI6qoqNCSJUtUWFjoP3/8+HH16tVLycnJeuqpp/yvVi4uLlbbtm39\n",
              "18XFxWn//v3yeDw1/l2MJotbt25Vv379lJOTo5KSEvXt21cRERGaNm2ayWYBAABOvcHFpi07O1uN\n",
              "GjWqtGVnZ5/1r5Kenq4hQ4aof//+6t+/vzp16uRPMFu1aqW9e/fqk08+0apVq5SXl6fnn3/+rNv6\n",
              "KWPJ4qxZs3TttdfqL3/5izZu3Ki//e1v+vbbbzVnzhxdcMEFppoFAACwXVZWlg4fPlxpy8rKqnJd\n",
              "mzZtKlX6LMtScXGxYmNjK13ncrk0ZcoUffrpp/roo4905ZVXqkuXLpKkqKgoNW/eXJLUpEkT3X33\n",
              "3crLy5MkxcbGqqioyP89hYWFlSqZNWE0Wfz888+1YcMGrVu3TtOnTzfVFAAAQBV2TnCJiopSTExM\n",
              "pS0qKqpKH5s3b67k5GQtXLhQkrR8+XK1bt1aHTp0qHRdWVmZDh06JEk6ePCgnn32Wf/qMqWlpf7J\n",
              "w263WytWrFBSUpIkaciQIdq8ebN27Ngh6VR+lpaWFtTf0dii3PXr1/dnxd26ddPx48dNNQUAAHDe\n",
              "mjNnjtLT0/XMM88oJiZGOTk5kqR77rlHKSkpSklJ0eHDhzVgwABFRETI5/Np0qRJuvnmmyVJ69at\n",
              "0xNPPKHIyEh5PB7dcMMNmjx5sqRTs6RfeeUV3XrrrfJ4POratav+/ve/B9U/l2WZWTnyiiuu0JIl\n",
              "S3T660ePHl1pv3v37iaaRS1oP5MqsVPsmpRpdxcAwLGuG/GcbW3nvf6QbW2HmrHK4smTJ6u8//n0\n",
              "vsvl0q5du0w1DQAAEHZvUrGLsWTx36dzAwAA4PxkLFkEAACwU7BvUkH1jC/KDQAAgPMXySIAAAAC\n",
              "YhgaAACEJ4ahQ4LKIgAAAAKisggAAMISE1xCg8oiAAAAAqKyCAAAwpOP0mIoUFkEAABAQCSLAAAA\n",
              "CIhhaAAAEJ4YhQ4JKosAAAAIiMoiAAAISyydExpUFgEAABAQySIAAAACYhgaAACEJ4tx6FCgsggA\n",
              "AICAqCwCAICwxASX0KCyCAAAgICoLAIAgPBEZTEkqCwCAAAgIJJFAAAABMQwNAAACEsuls4JCSqL\n",
              "AAAACIjKIoK2a1Km3V0AgJ/VfuZ0u7uAH9n63wyffU2HEyqLAAAACIhkEQAAAAExDA0AAMISE1xC\n",
              "g8oiAAAAAqKyCAAAwhOFxZCgsggAAICAqCwCAIDwxDOLIUFlEQAAAAGRLAIAACAghqEBAEBYcjEK\n",
              "HRJUFgEAABAQlUUAABCemOASElQWAQAAEBDJIgAAAAJiGBoAAIQll8/uHoQHKosAAAAIiMoiAAAI\n",
              "T0xwCQkqiwAAAAiIyiIAAAhPFBZDgsoiAAAAAiJZBAAAQEAMQwMAgLDkYoJLSFBZBAAAQEBUFgEA\n",
              "QHiishgSVBYBAAAQEMkiAAAAAmIYGgAAhCfeDR0SVBYBAAAQUK1VFj/++GP97//+r5KSkjRgwIDa\n",
              "ahYAANRRLJ0TGsYqiwMHDvT/vGzZMo0aNUpffvml7rvvPs2ZM8dUswAAAAghY5XF77//3v/zn//8\n",
              "Z/3rX/9SQkKCvv32Ww0ePFgZGRmmmgYAAGDpnBAxVll0uVz+n0+cOKGEhARJUosWLRQRwaOSAAAA\n",
              "5wNjlcVdu3Zp5MiRsixL33zzjcrKyhQdHS1JKi8vN9UsAAAAQshYsjhz5kz/z7feeqtOnDih6Oho\n",
              "7du3T7fccoupZgEAAE5hGDokjCWLd955Z7XHL7vsMv3xj3801SwAAABCyNjDgy+++KIOHDhg6usB\n",
              "AAB+ns/GLYwYSxZ///vfKy4uTiNHjtTbb78ti1IwAADAecdYspiQkKBdu3apT58+yszMVJs2bTR5\n",
              "8mQVFBSYahIAAAAhZnTpnBYtWuiRRx7Rjh07tHjxYu3bt0+JiYm6/vrrTTULAAAg6dQbXOzawomx\n",
              "ZPGnw87XXXedcnJytHfvXo0ZM8ZUswAAAAghY7Oh+/btW+3xmJgY3XvvvaaaBQAAOCXMKnx2MTob\n",
              "GgAAAD/v66+/1jXXXKNOnTqpV69e+uKLL6pc4/P59NBDD6lr165KSEjQuHHj/C852bJli/r166eE\n",
              "hAR17dpVd999t06ePOn/rMvlUrdu3ZSYmKjExETl5eUF1T+j793bv3+//vznP+u3v/2tHnroIeXk\n",
              "5MjtdptsEgAA4BTLsm8LQkZGhu677z599dVXeuSRR5Senl7lmrlz52rz5s3avHmztm/froiICP8L\n",
              "UKKjo/Xiiy9qx44d+uyzz3T8+HFNmzat0ufz8vKUn5+v/Px8XXfddUH1z1iyuGTJEv3iF7/QmjVr\n",
              "tGjRIu3du1dLlixR586dtX37dlPNAgAAnDdKS0u1ceNGjR07VpKUmpqqPXv2aOfOnZWu++yzzzRo\n",
              "0CA1aNBALpdLQ4cO1YIFCyRJHTt2VPfu3SVJkZGR6tWrlwoLC0PWR2PJ4tNPP62NGzfqzTff1IYN\n",
              "G1RWVqZ33nlHc+bM0QMPPGCqWQAAANu53W4dOXKk0lbd6OqePXvUqlUr1at3ahqJy+VSbGysiouL\n",
              "K13Xs2dP5ebm6siRI6qoqNCSJUuqTQiPHz+uV155pcqrlQcOHKgePXooMzNTx48fD+p3MZYsRkZG\n",
              "qlmzZpKk9u3bq6ioSJI0ePBg7du3z1SzAAAAp9g4DJ2dna1GjRpV2rKzs8/6V0lPT9eQIUPUv39/\n",
              "9e/fX506dfInmKeVl5dr9OjRuvHGGzVixAj/8aKiIm3atEkfffSRDhw4oN///vdBtW0sWWzevLly\n",
              "cnJUUlKi5557Tu3bt5d0akkdj8djqlkAAADbZWVl6fDhw5W2rKysKte1adNG+/fv9+dGlmWpuLhY\n",
              "sbGxla5zuVyaMmWKPv30U3300Ue68sor1aVLF//5iooKjR49Wq1atfI/y3ja6e+68MILNXHiROdM\n",
              "cJk1a5bmzp2rjh076q233tL06dMlSQcOHNAf/vAHU80CAACcYuO7oaOiohQTE1Npi4qKqtLF5s2b\n",
              "Kzk5WQsXLpQkLV++XK1bt1aHDh0qXVdWVqZDhw5Jkg4ePKhnn31WDz/8sCTJ4/EoLS1NTZo00Usv\n",
              "vSSXy+X/3KFDh3TixIlTfw6fT//4xz+UlJQU1J/R2DqLHTp00Lp166ocb968ucaNG2eqWQAAgPPK\n",
              "nDlzlJ6ermeeeUYxMTHKycmRJN1zzz1KSUlRSkqKDh8+rAEDBigiIkI+n0+TJk3SzTffLEn6xz/+\n",
              "oRUrVqh79+7+RLBv377661//qh07digjI0Mul0sej0fJyclVKo9n4rJ++qqVENm6dau6du1q4qsB\n",
              "APhZ7WdOt7sL+NGuSZm2tT2ky2Tb2l75xR9tazvUjA1Dd+/eXT169NALL7yg77//3lQzAAAA1eLd\n",
              "0KFhLFns0qWLnnjiCa1cuVKxsbFKS0vTqlWrTDUHAAAAA4wli/Xr11dqaqrefvttbd++XV27dlVG\n",
              "Robi4uL01FNPmWoWAADglPPkDS5OZ/R1f6e1adNGjz32mAoKCjR37lx9+eWXtdEsAAAAzpGx2dAN\n",
              "GjSo9vjAgQM1cOBAU80CAAAghIxVFtevX1+j63bv3m2qCwAAoC7zWfZtYaRWhqF/Tmpqqt1dAAAA\n",
              "QADGhqFrytAyjwAAoK4jxwgJ2yuL//5KGgAAADiL7ZXF6rjdbrnd7krHoqKiqn2nIgAAQLWoLIaE\n",
              "7ZXF6oahs7Oz1ahRo0pbdna2Db0DAACo22yvLI4cObLKsaysLGVmVn6XJFVFAACA2mc8WfR4PFq+\n",
              "fLkKCgrk8Xj8x5944glJ0uOPP17lMww5AwCAc8YwdEgYTxbT0tJUUlKi3r17KzIy0nRzAAAACCHj\n",
              "yeKWLVu0Y8cOZj0DAIDaFWaLY9vF+ASXNm3aqLy83HQzAAAAMMB4ZbFDhw4aMGCARowYoejoaP/x\n",
              "Bx980HTTAAAAOEfGk0W3262EhARt377df4whaQAAYJzls7sHYcF4spiTk2O6CQAAABhSK0vnzJgx\n",
              "Q++9954kafDgwZo0aZLq1bN9iUcAABDOWDonJIxnbJmZmSooKNDEiRPlcrn0yiuvqKioSC+88ILp\n",
              "pgEAAHCOjCeLH3zwgfLz8xURcWri9bBhw5ScnGy6WQAAUNexdE5IGF86x7Is+Xy+SvvVvQ8aAAAA\n",
              "zmO8sjhkyBDdeOONSk9PlyTNnz9fQ4cONd0sAAAAQsB4sjht2jTNmTNHubm5kqRRo0bpvvvuM90s\n",
              "AACo6xjJDAnjyWJERIQmTJigCRMmyLIsHTt2zP/8IgAAAJzNeNY2btw4/fDDDyovL1diYqJatGih\n",
              "WbNmmW4WAADUdZZl3xZGjCeLmzZtUuPGjbVy5UolJSWppKREs2fPNt0sAAAAQqBWZkNLUl5enoYP\n",
              "H66YmBhFRkaabhYAAAAhYDxZbNmypSZMmKClS5dq0KBBqqiokNfrNd0sAACo6xiGDgnjyeKiRYvU\n",
              "uXNnvfbaa2rcuLH27t2rzMxM080CAAAgBIzOhvZ6vUpLS9OqVav8x+Li4vxrLgIAABjzby8Fwdkz\n",
              "WlmMjIzUiRMnKr3BBQAAAOcP4+ss9urVS8OHD9fYsWN10UUX+Y+npKSYbhoAANRlYfbsoF2MJ4uf\n",
              "f/65JOnll1/2H3O5XCSLAAAA5wHjyeKaNWtMNwEAAABDjM+G9ng8ev755zVx4kRJUkFBgVavXm26\n",
              "WQAAUNexdE5IGK8sPvDAA/J6vVq3bp0kqWnTpho9erQ2btxoumkAAACcI+PJ4vr165Wfn6+kpCRJ\n",
              "UuPGjVVRUWG6WQAAUNf5wqvCZxfjw9DR0dGV9r1eL0vpAAAAnCeMJ4vdu3fXwoUL5fP5tHPnTo0f\n",
              "P14DBgww3SwAAABCwGiyuHXrVvXr1085OTkqKSlR3759FRERoWnTpplsFgAAQJbls20LJ8aeWZw1\n",
              "a5YeffRRde7cWTt27FBOTo5GjhxpqjkAAAAYYKyyOGvWLH3++efasGGD1q1bp+nTp5tqCgAAoCqf\n",
              "Zd8WRowli/Xr11dsbKwkqVu3bjp+/LippgAAAGCIsWHosrIybdmyRdaPC1P+dL979+6mmgYAAAi7\n",
              "xbHtYixZPHnyZJX3P5/ed7lc2rVrl6mmAQAAECLGksXCwkJTXw0AAIBaYvwNLgAAALbgJSAhYXxR\n",
              "bgAAAJy/qCwCAIDwxASXkKCyCAAAgIBIFgEAABAQw9AAACAsWUxwCQkqiwAAAAiIyiIAAAhPTHAJ\n",
              "CSqLAAAACIjKIgAACE8+KouhQGURAAAAAZEsAgAAICCGoQEAQHiyWDonFKgsAgAAICAqiwAAICxZ\n",
              "THAJCSqLAAAACIhkEQAAAAExDA0AAMITE1xCgsoiAAAAAqKyCAAAwhITXEKDyiIAAICNvv76a11z\n",
              "zTXq1KmTevXqpS+++KLKNT6fTw899JC6du2qhIQEjRs3TuXl5f7z//znP5WQkKCOHTtq5MiROnLk\n",
              "SI3O1QTJIgAACE+Wz74tCBkZGbrvvvv01Vdf6ZFHHlF6enqVa+bOnavNmzdr8+bN2r59uyIiIjRz\n",
              "5kxJ0rFjxzRu3Di98cYb+vrrr3XZZZfp6aefPuO5miJZBAAAsElpaak2btyosWPHSpJSU1O1Z88e\n",
              "7dy5s9J1n332mQYNGqQGDRrI5XJp6NChWrBggSTpnXfeUVJSkhISEiRJEydO1OLFi894rqZIFgEA\n",
              "AELM7XbryJEjlTa3213luj179qhVq1aqV+/UNBKXy6XY2FgVFxdXuq5nz57Kzc3VkSNHVFFRoSVL\n",
              "lqiwsFCSVFxcrLZt2/qvjYuL0/79++XxeH72XE0xwaUWud1uZWdnKysrS1FRUXZ3p04jFs5BLJwj\n",
              "nGKxa1Km3V04J+EUCzu951tqW9tTpkzR1KlTKx178sknNWXKlLP6vvT0dBUVFal///5q2LChBg0a\n",
              "pH/9618h6OmZuSzLYqpQLTly5IgaNWqkw4cPKyYmxu7u1GnEwjmIhXMQC+cgFuc/t9tdpZIYFRVV\n",
              "JfkvLS1Vhw4d9P3336tevXqyLEutWrXSunXr1KFDh4Df/9prr+mvf/2r8vLytHTpUs2dO1crV66U\n",
              "JG3btk033nijvvnmm589V1MMQwMAAIRYVFSUYmJiKm3VVYmbN2+u5ORkLVy4UJK0fPlytW7dukqi\n",
              "WFZWpkOHDkmSDh48qGeffVYPP/ywJGnIkCHavHmzduzYIUmaNWuW0tLSzniuphiGBgAAsNGcOXOU\n",
              "np6uZ555RjExMcrJyZEk3XPPPUpJSVFKSooOHz6sAQMGKCIiQj6fT5MmTdLNN98sSbr44ov1yiuv\n",
              "6NZbb5XH41HXrl3197///Yznaoph6FrEsIJzEAvnIBbOQSycg1jASRiGrkVRUVF68skneVjZAYiF\n",
              "cxAL5yAWzkEs4CRUFgEAABAQlUUAAAAERLIIAACAgEgWAQAAEBDJIgAAAAJinUXDysvLtW/fPknS\n",
              "ZZddpgYNGtjco7qLWDjHrl27/O89jY2NVfv27W3uUd3FfeEc3BdwKpJFQ/bv369Jkybpf/7nf9So\n",
              "USNZlqUjR47o5ptv1owZM3T55Zfb3cU6g1g4x/bt23XnnXdqz549io2NlSQVFxerTZs2ysnJUZcu\n",
              "XWzuYd3BfeEc3BdwOoahDbnjjjvUp08fHThwQCUlJfr222914MAB9e7dW3fccYfd3atTiIVzpKen\n",
              "65FHHtH+/fu1YcMGbdiwQfv379fDDz+su+66y+7u1SncF87BfQGnY51FQxISEvzvYQzmHEKPWDhH\n",
              "586d9eWXXwZ9DqHHfeEc3BdwOiqLhkRHR+vDDz+scnzt2rWsyF/LiIVzNGvWTAsWLJDP5/Mf8/l8\n",
              "+vvf/66mTZva2LO6h/vCObgv4HRUFg3ZsGGDxo4dq/r166tt27aSpMLCQnk8Hi1cuFB9+vSxuYd1\n",
              "B7Fwjp07dyojI0ObNm1Sq1atJJ16di45OVmzZ89Wp06dbO5h3cF94RzcF3A6kkWDLMvSpk2bKs1u\n",
              "69mzp1wul809q3uIhbMcOHBAe/bskSS1adNGl156qc09qpu4L5yF+wJOxWxog1wul7p3765mzZpJ\n",
              "OrUsBf8I24NYOMvRo0d15MgR/8/8R9Ee3BfOwn0BpyJZNIRlKZyDWDjHtm3blJ6ezhIhDsB94Rzc\n",
              "F3A6JrgYUt2yFKWlpSxLYQNi4Rx33XUXS4Q4BPeFc3BfwOl4ZtEQlqVwDmLhHCwR4hzcF87BfQGn\n",
              "o7JoCMtSOAexcA6WCHEO7gvn4L6A01FZNIRlKZyDWDgHS4Q4B/eFc3BfwOlIFg1iWQrnIBbOwhIh\n",
              "zsB94SzcF3AqkkUAAAAExDOLtWDKlCk/u4/aQyyc47777vvZfdQe7gvn4L6AE5Es1oLTz6AE2kft\n",
              "IRbOcfPNN//sPmoP94VzcF/AiRiGBgAAQEC8wcWgQ4cO6fXXX6/08Pitt96qJk2a2NyzuodYOMf2\n",
              "7du1ePHiSrEYPXo0b6mwAfeFc3BfwMkYhjZk+fLlSkhI0L/+9S+dPHlSJ0+e1Lvvvqsrr7xSy5cv\n",
              "t7t7dQqxcI6//vWvGjp0qNxut/r06aM+ffrI7XZr2LBhevHFF+3uXp3CfeEc3BdwOoahDUlISNDK\n",
              "lSsVFxdX6fju3bs1dOhQ3o5Qi4iFc3Tq1EkbNmzQJZdcUun4999/rz59+ujrr7+2qWd1D/eFc3Bf\n",
              "wOmoLBri9Xqr/CMsSe3atZPH46n9DtVhxMI5fD5flf8gSlLjxo0rvb0C5nFfOAf3BZyOZxYN6dWr\n",
              "l+6++26NHz/e/3aEoqIizZ49W1dddZXNvatbiIVzDB06VL/85S917733VorFyy+/rJtuusnm3tUt\n",
              "3BfOwX0Bp2MY2pCTJ0/queee0z/+8Q//A8tt27bVqFGj9Pvf/14XXHCBzT2sO4iFc1iWpQULFmjJ\n",
              "kiWVHuS/7bbbdMcddygigsGO2sJ94RzcF3A6kkUAAAAExDC0QSxL4RzEwjmqWyIkLS1NV155pc09\n",
              "q3u4L5yD+wJORm3bkNPLUrz77rssS2EzYuEcgZYIuemmm1gipJZxXzgH9wWcjmFoQ1iWwjmIhXOw\n",
              "RIhzcF84B/cFnI7KoiEsS+EcxMI5WCLEObgvnIP7Ak7HM4uGsCyFcxAL52CJEOfgvnAO7gs4HcPQ\n",
              "hrAshXMQC+dgiRDn4L5wDu4LOB3JIgAAAALif64AsEVJScnP7gN1EfcFnIhksRYkJyf/7D5qD7Fw\n",
              "jqFDh/7sPmoP94VzcF/AiRiGrgX79+9Xq1atAu6j9hALoCruCwA/h2QRAACH+e6779S0aVO7uwFI\n",
              "YhjaGK/Xq7/97W/q16+f4uLiFBcXp379+mnWrFnyer12dw8/mjx5st1dqHPmz5+vp556Sps3b650\n",
              "PDs726Ye1U0VFRWaOXOmXnjhBXk8Hi1ZskS33HKLHn/8cZWXl9vdvTqvV69edncB8KOyaEhGRoZK\n",
              "Sko0fvx4/8K3hYWFmj17tlq0aKGXXnrJ3g5C0qnlKU4vVQHz/vCHP+h///d/lZiYqGXLlukPf/iD\n",
              "Jk2aJOnUc3I/TSBhzoQJE/Ttt9/q5MmTaty4sdxut0aPHq3XX39dLVu21J///Ge7u1hnVPcu7sOH\n",
              "D6tRo0aSTr3JBbATyaIhHTt2rPYVTZZlqVOnTry+qRYFeljfsixt375dZWVltdyjuqtbt27atGmT\n",
              "GjRooJKSEqWkpGjEiBHKyspSUlKSPv30U7u7WGd069ZNW7ZsUVlZmZo3b66SkhJdcMEFKi8vV8+e\n",
              "PbVlyxa7u1hn3HDDDerYsaP+8Ic/KCIiQpZl6brrrtO6deskyb9QN2AX3uBiiMvl0oEDB3TppZdW\n",
              "On7gwAGRn9euXbt2afHixVUWGbYsS6NHj7apV3WTZVlq0KCBJKlly5ZatWqVhg4dKq/XK5fLZXPv\n",
              "6pb69etLkqKjo9W+fXv//dGgQQPVq8d/GmrT6tWrNWPGDN1111166aWX1KlTJ9WvX58kEY7BvwiG\n",
              "PPLII0pMTNQtt9xS6fVNubm5mjp1qs29q1uSkpLUqFEjXXPNNVXOnU5cUDsuvPBCFRYW+h/NiImJ\n",
              "0bvvvqvBgwfriy++sLdzdYxlWfJ6vYqMjNSbb77pP+7xeHg3tA3+4z/+QzfeeKPuuusupaamUlSA\n",
              "ozAMbdDu3bu1fPnySq9vSk1NVbt27WzuWd1SVFSkmJgYXXLJJVXOnTx5Ug0bNrShV3XTmjVr1KRJ\n",
              "E/Xo0aPS8ePHj2vmzJl69NFHbepZ3bN+/XolJiYqOjq60vGCggKtXr1a9957r009q9s8Ho+mTJmi\n",
              "Dz/8UB9++KHd3QEkkSwCAADgZ7B0jiEsneMcxMI5iIVzBIrF3/72N2JRy07H4rrrruO+gCNRWTSE\n",
              "pXOcg1g4B7FwDmLhHMQCTkeyaAhL5zgHsXAOYuEcxMI5iAWcjmFoQ04vnfNTLJ1T+4iFcxAL5yAW\n",
              "zkEs4HQsnWMIS+c4B7FwDmLhHMTCOYgFnI5haIMKCwu1bNkyls5xAGLhHMTCOYiFcxALOBnJIgAA\n",
              "AALimUUAAAAERLIIAACAgEgWAQAAEBDJog2+++47u7uAHxEL5yAWzkEsnINYwAlIFm2QlJRkdxfw\n",
              "I2LhHMTCOYiFcxALOAHrLBqSm5sb8FxZWVkt9gTEwjmIhXMQC+cgFnA6ls4xJDIyUv3796929f31\n",
              "69fr5MmTNvSqbiIWzkEsnINYOAexgNNRWTSkY8eOmjdvnv+l8P+uTZs2td+hOoxYOAexcA5i4RzE\n",
              "Ak7HM4uG3HnnnTp48GC158aPH1/LvanbiIVzEAvnIBbOQSzgdAxDAwAAICCGoQ3yer1au3ZtpXd9\n",
              "9u/fX5GRkTb3rO4hFs5BLJyDWDgHsYCTkSwakpeXpzFjxujyyy9X27ZtJZ16Ufy+ffu0aNEi9evX\n",
              "z+Ye1h3EwjmIhXMQC+cgFnA8C0Z069bN+uSTT6oc//jjj62uXbva0KO6i1g4B7FwDmLhHMQCTscE\n",
              "F0PKysp01VVXVTneq1cvud1uG3pUdxEL5yAWzkEsnINYwOlIFg2Jj4/XU089pdLSUv+x0tJSTZ06\n",
              "Ve3atbOxZ3UPsXAOYuEcxMI5iAWcjmTRkPnz56uoqEjx8fFq2LChGjZsqPj4eBUVFWnBggV2d69O\n",
              "IRbOQSycg1g4B7GA07F0Ti34/vvvJUlNmjSxuScgFs5BLJyDWDgHsYATUVmsBU2aNFGTJk2UnZ1t\n",
              "d1fqPGLhHMTCOYiFcxALOBHJYi1aunSp3V3Aj4iFcxAL5yAWzkEs4CQki7WIEX/nIBbOQSycg1g4\n",
              "B7GAk5As1qLnnnvO7i7gR8TCOYiFcxAL5yAWcBImuNjgu+++U9OmTe3uBkQsnIRYOAexcA5iASeg\n",
              "smiDpKQku7uAHxEL5yAWzkEsnINYwAl4N7Qhubm5Ac+VlZXVYk9ALJyDWDgHsXAOYgGnYxjakMjI\n",
              "SPXv37/ah5TXr1+vkydP2tCruolYOAexcA5i4RzEAk5HZdGQjh07at68eYqLi6tyrk2bNrXfoTqM\n",
              "WDgHsXAOYuEcxAJOxzOLhtx55506ePBgtefGjx9fy72p24iFcxAL5yAWzkEs4HQMQwMAACAghqEN\n",
              "8nq9Wrt2rYqLiyVJsbGx6t+/vyIjI23uWd1DLJyDWDgHsXAOYgEnI1k0JC8vT2PGjNHll1+utm3b\n",
              "SpIKCwu1b98+LVq0SP369bO5h3UHsXAOYuEcxMI5iAUcz4IR3bp1sz755JMqxz/++GOra9euNvSo\n",
              "7iIWzkEsnINYOAexgNMxwcWQsrIyXXXVVVWO9+rVS26324Ye1V3EwjmIhXMQC+cgFnA6kkVD4uPj\n",
              "9dRTT6m0tNR/rLS0VFOnTlW7du1s7FndQyycg1g4B7FwDmIBpyNZNGT+/PkqKipSfHy8GjZsqIYN\n",
              "Gyo+Pl5FRUVasGCB3d2rU4iFcxAL5yAWzkEs4HQsnVMLvv/+e0lSkyZNbO4JiIVzEAvnIBbOQSzg\n",
              "RFQWa0GTJk3UpEkTZWdn292VOo9YOAexcA5i4RzEAk5EsliLli5dancX8CNi4RzEwjmIhXMQCzgJ\n",
              "yWItYsTfOYiFcxAL5yAWzkEs4CQki7Xoueees7sL+BGxcA5i4RzEwjmIBZyEZLGWeDweNWnSRIcP\n",
              "H7a7K3UesXAOYuEcxMI5iAWchmTRkNWrV6tp06Zq1qyZ1q5dq2uuuUZjxoxRfHy81q5da3f36hRi\n",
              "4RzEwjmIhXMQCziejW+PCWu9e/e2Pv30U2vNmjVW06ZNrffff9+yLMvasGGDde2119rcu7qFWDgH\n",
              "sXAOYuEcxAJOV8/uZDVclZeXKzExUZLUuHFj3XDDDZKk3r1769ixYzb2rO4hFs5BLJyDWDgHsYDT\n",
              "MQxtiM/n8/982223VTrn9Xpruzt1GrFwDmLhHMTCOYgFnI5k0ZCePXvqyJEjklRpcdWCggLFxMTY\n",
              "1a06iVg4B7FwDmLhHMQCTsfr/mqZx+NRRUWFGjZsaHdX6jxi4RzEwjmIhXMQCzgFlcVaVq9ePZ04\n",
              "ccLubkDEwkmIhXMQC+cgFnAKkkUbJCUl2d0F/IhYOAexcA5i4RzEAk7AbGhDcnNzA54rKyurxZ6A\n",
              "WDgHsXAOYuEcxAJOxzOLhkRGRqp///7Vvt9z/fr1OnnypA29qpuIhXMQC+cgFs5BLOB0VBYN6dix\n",
              "o+bNm6e4uLgq59q0aVP7HarDiIVzEAvnIBbOQSzgdDyzaMidd96pgwcPVntu/Pjxtdybuo1YOAex\n",
              "cA5i4RzEAk7HMDQAAAACYhjaIK/Xq7Vr16q4uFiSFBsbq/79+ysyMtLmntU9xMI5iIVzEAvnIBZw\n",
              "MpJFQ/Ly8jRmzBhdfvnlatu2rSSpsLBQ+/bt06JFi9SvXz+be1h3EAvnIBbOQSycg1jA8SwY0a1b\n",
              "N+uTTz6pcvzjjz+2unbtakOP6i5i4RzEwjmIhXMQCzgdE1wMKSsr01VXXVXleK9eveR2u23oUd1F\n",
              "LJyDWDgHsXAOYgGnI1k0JD4+Xk899ZRKS0v9x0pLSzV16lS1a9fOxp7VPcTCOYiFcxAL5yAWcDqS\n",
              "RUPmz5+voqIixcfHq2HDhmrYsKHi4+NVVFSkBQsW2N29OoVYOAexcA5i4RzEAk7H0jm14Pvvv5ck\n",
              "NWnSxOaegFg4B7FwDmLhHMQCTkSyCAAAgIAYhjakoKBA119/vdq3b6/MzMxKL4P/xS9+YWPP6h5i\n",
              "4RzEwjmIhXMQCzgdyaIhEydO1KhRo7R06VIdPHhQAwcO1NGjRyWp0j8EMI9YOAexcA5i4RzEAk5H\n",
              "smhIaWmp7r//fvXs2VPz58/XsGHDNHDgQB0+fFgul8vu7tUpxMI5iIVzEAvnIBZwOt7gYsjJkycr\n",
              "7T/66KNq0KBBpf/FiNpBLJyDWDgHsXAOYgGno7JoyBVXXKGVK1dWOvbQQw9pzJgxKigosKlXdROx\n",
              "cA5i4RzEwjmIBZyO2dCGnF51Pyoqqsq5vXv36vLLL6/tLtVZxMI5iIVzEAvnIBZwOiqLhkRFRVV7\n",
              "40vixq9l4RaLuLg4de7cWT169FCHDh10yy236KOPPqrRZ9944w2tX78+5H2aM2eOEhISlJiYqO++\n",
              "+85//IknnlBiYqISExN10UUXKSEhQX369FFiYqK+/PLLSt9hdyx++OEHPfvss5WO3XPPPVqzZo1N\n",
              "PTIr3O6L8xmxgNNRWQTOM3FxcXrjjTeUmJgoSVqxYoXuvvtuvfvuu+rTp8/PfjY9PV2JiYn67W9/\n",
              "G9I+XXHFFZo3b97PLvMxYMAA/fa3v9Wtt95a6bjP55MkRUTY+79dCwsLlZiYqB9++MHWfgCA01BZ\n",
              "BM5zI0eO1Pjx4/Xcc89Jkt5//3394he/UFJSkrp06aK5c+dKkt5++23l5ubqv/7rv5SYmKhXXnlF\n",
              "krRgwQL16dNHycnJ6tevnz777LNq29m4caOuueYade/eXb1799b//u//SpJGjRqlgoICpaena9So\n",
              "UTXq85QpU5SamqrBgwera9eu2r9/vx566CH16tVLiYmJ6tevX6XKo8vl0jPPPKPevXurXbt2ysnJ\n",
              "kXQq0XzggQd0xRVXqEePHurZs6fKysrk8Xg0ePBgXXXVVerSpYvGjBmj48eP+78vJydHiYmJ6tGj\n",
              "h6666ioVFhZq/PjxOnr0qBITE3XVVVdJOpXgvvHGG5JOzVgdOXKkunXrpq5du2rOnDn+74uLi9MT\n",
              "TzyhX/ziF2rXrp3+8z//s0Z/BwA4L1gAzitt27a1Pv3000rHVqxYYV1xxRWWZVnW999/b3k8Hsuy\n",
              "LOu7776zYmNjrT179liWZVl33nmnNWPGDP/n1q1bZw0dOtQqKyuzLMuyPvzwQ+vKK6+s0qbb7bba\n",
              "tGljrVy50rIsy8rLy7NatGhhHT16NGCffqp///7W66+/blmWZT355JNWq1atrJKSEv/50tJS/8+L\n",
              "Fy+2Bg8e7N+XZD333HOWZVnW9u3brYsuusiqqKiwNm/ebCUkJFher9eyLMv64YcfLK/Xa/l8Puvg\n",
              "wYOWZVmWz+ezxo8fb2VnZ1uWZVlr1qyx4uLirH379lmWZVnHjx+3jh8/bu3evdtq1KhRwD7/6le/\n",
              "sv7whz9YlmVZ3377rdW6dWvr//7v//y////7f//PsizLOnDggBUTE2N98803P/v3AIDzBUvnAGHA\n",
              "+renSb777juNGzdOX331lerVq6fvvvtOW7duVevWrat87s0339Rnn31Wafj6+++/18mTJ9WwYUP/\n",
              "sS+//FIREREaPHiwJOnaa69VixYtlJ+fr2uvvfas+nzTTTepRYsW/v333ntPf/nLX3T06FH5fD7/\n",
              "O3JPu/322yVJCQkJqlevnkpKStS+fXt5PB7dfffduv766zVs2DBFRETI5/NpxowZeuutt+TxeHT4\n",
              "8GFdc801kqS33npLd9xxh1q1aiVJuuCCC2rU31WrVmnTpk2SpObNm2vkyJFatWqVrr76aknSmDFj\n",
              "JEnNmjVT+/bttXv3bp43AxAWGIYGwsAnn3yirl27SpLGjx+va6+9Vlu2bFF+fr46deoU8C0QlmXp\n",
              "zjvvVH5+vn/bv39/pUQxkHNdLPiiiy7y/1xcXKwHHnhACxcu1NatW/Xaa69V6XN0dLT/58jISHk8\n",
              "HjVq1Ehbt27VmDFjtGPHDnXv3l07d+7Uf//3f2v16tVau3attmzZooceeijkb8L46e9fXf8AIByQ\n",
              "LALnuTfffFN/+9vf9Lvf/U6SdOjQIbVt21Yul0sffvhhpWcQY2JidPjwYf9+SkqKFi5cqOLiYkmn\n",
              "ngHcuHFjlTY6d+4sn8+n9957T5L00UcfqaSkxD/J5lwdPnxY9evXV6tWrWRZll588cUafe7AgQM6\n",
              "fvy4brzxRj3zzDOKi4vTtm3bdOjQITVr1kwxMTE6evSoXn31Vf9nbr75Zi1cuFD79++XJJ04cUIn\n",
              "TpxQTEyMTp48qfLy8mrbGjRokF5++WV/uytWrNAvf/nLc/vFAeA8wDA0cB4aPXq0oqOjdfz4cV15\n",
              "5ZV6++23/UPJzz77rCZOnKinn35aiYmJlYaY77jjDqWnp+uNN97Q/fffr3vuuUd/+tOfNGLECHk8\n",
              "HpWXl2vYsGH+CR6nNWjQQCtWrNCDDz6o3/3ud4qOjtayZcsqVQfPRbdu3ZSWlqYuXbqoadOmVWZM\n",
              "B7Jnzx7de++9qqiokNfrVd++fTV06FCdOHFCb775pjp37qxLL71U1113nYqKiiRJ/fr105NPPqnB\n",
              "gwfL5XKpQYMGWrZsmdq2bavf/OY36t69uy666KIqSfMLL7ygCRMmqFu3brIsS5MnTz7j7HMACAcs\n",
              "nQMAAICAGIYGAABAQCSLAAAACIhkEQAAAAGRLAIAACAgkkUAAAAERLIIAACAgEgWAQAAEBDJIgAA\n",
              "AAIiWQQAAEBAJIsAAAAIiGQRAAAAAf1/9gXsxRRn7MEAAAAASUVORK5CYII=\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-7739103f-663a-4766-98e0-2392b00968ea\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-7739103f-663a-4766-98e0-2392b00968ea\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<google.colab._quickchart_helpers.SectionTitle at 0x78367ba9ebc0>"
            ],
            "text/html": [
              "<h4 class=\"colab-quickchart-section-title\">Faceted distributions</h4>\n",
              "<style>\n",
              "  .colab-quickchart-section-title {\n",
              "      clear: both;\n",
              "  }\n",
              "</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:5: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "figsize = (12, 1.2 * len(_df_20['Transaction ID'].unique()))\n",
              "plt.figure(figsize=figsize)\n",
              "sns.violinplot(_df_20, x='Amount (USD)', y='Transaction ID', inner='stick', palette='Dark2')\n",
              "sns.despine(top=True, right=True, bottom=True, left=True)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-b313d64e-59ef-4af0-bfb0-93a54e9f76eb\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABB8AAAIDCAYAAABM/LZyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAAr1ElEQVR4nO3df5TVdZ3H8dc4wJAmmGLYRgMrP9z4OWSmZmloycl1M0ma8idb\n",
              "KY4hR8GjS27R0g+yLfsBtf5YEwOkMUXZyiy1LKNjgjJYQhLqiL+wlF8rJSrc/aPjbKy/Zlw+XBge\n",
              "j3PuOX2/n+/c+/bee0757Pv93ppKpVIJAAAAQCG7VXsAAAAAoHMTHwAAAICixAcAAACgKPEBAAAA\n",
              "KEp8AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8\n",
              "AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8AABgl9bU1JSmpqZqjwHQqXWp9gAAAFBNTzzxRLVH\n",
              "AOj0nPkAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQ\n",
              "lPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgA\n",
              "AAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAA\n",
              "FCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFFUs\n",
              "PjQ0NKShoSGDBw9ObW1t23ZjY2PuuOOO1NfXZ926dW3Hjx07NlOnTk2SbNmyJWeffXb69++fAQMG\n",
              "ZObMmW3Hdfa1H/3oRznwwANTV1eXc8455zW++wAAALDj6FLqiVtaWpIkra2taWhoaNt+wYknnpgJ\n",
              "EyZkzpw5mTt3blauXJmrr746STJnzpwsW7YsK1asyPr16zNy5MiMGjUqQ4YM6fRrAwcOzHe+8518\n",
              "//vfz9NPP13q4wEAAIDtpmqXXUybNi1Lly7NjBkzMnny5Fx11VXp2rVrkqS5uTmnn356amtrs/fe\n",
              "e6exsTHz5s3bJdYGDRqUESNGpEuXV+9CmzZtyoYNG7Z6bNq0aZt8PgAAALCtVC0+dOvWLZdddlkm\n",
              "TpyYpqamDB8+vG1t1apV6du3b9t2v379smrVql1irSOmT5+enj17bvWYPn16h58HAAAASip22UV7\n",
              "XH/99enTp8+LLsmgfaZMmZJJkyZtta+urq5K0wAAAMBLq9qZDwsXLsw111yTJUuWpLW1NXPnzm1b\n",
              "q6+vz0MPPdS23dramvr6+l1irSPq6urSo0ePrR7iAwAAADuaqsSHjRs3Zty4cbn00kvTq1evzJo1\n",
              "K5MnT87q1auT/PWXLy6//PJs3rw5a9asSXNzcxobG3eJNQAAAOhsqnLZxfnnn59Ro0Zl9OjRSZIR\n",
              "I0bkrLPOyvjx47NgwYKccsopWbRoUQYOHJiamppMmjQpw4YNS5JOv3brrbfmtNNOy4YNG1KpVHLt\n",
              "tdfm29/+dj7wgQ9sj48GAAAAtrmaSqVSqfYQAABQLWPGjEmSzJ8/v8qTAHReVbvnAwAAALBrEB8A\n",
              "AACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACA\n",
              "osQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQH\n",
              "AAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAA\n",
              "oCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAAoKgu1R4AAACqqXfv3tUeAaDTq6lUKpVqDwEAAAB0\n",
              "Xi67AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8\n",
              "AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8AAAA\n",
              "gB1IU1NTmpqaqj3GNtWl2gMAAAAA/+uJJ56o9gjbnDMfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAA\n",
              "ihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIf\n",
              "AAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAA\n",
              "gKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKLE\n",
              "BwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKKKxIeGhoY0NDRk8ODBqa2tbdtubGzMHXfckfr6+qxb\n",
              "t67t+LFjx2bq1KlJki1btuTss89O//79M2DAgMycObPtuM6+9s1vfjNDhw7NsGHDMnz48MyZM+e1\n",
              "fgQAAACww+hS4klbWlqSJK2trWloaGjbfsGJJ56YCRMmZM6cOZk7d25WrlyZq6++OkkyZ86cLFu2\n",
              "LCtWrMj69eszcuTIjBo1KkOGDOn0a0OGDMnChQvTs2fPPPzwwxk5cmQOPfTQ9O/fv8THBAAAANtF\n",
              "VS67mDZtWpYuXZoZM2Zk8uTJueqqq9K1a9ckSXNzc04//fTU1tZm7733TmNjY+bNm7dLrB111FHp\n",
              "2bNnkuQtb3lL9ttvvzz88MMv+z5u2rQpGzZs2OqxadOmbfdBAQAAwDZQlfjQrVu3XHbZZZk4cWKa\n",
              "mpoyfPjwtrVVq1alb9++bdv9+vXLqlWrdom1v3XLLbdk7dq1Oeigg17yPUyS6dOnp2fPnls9pk+f\n",
              "/rLHAwAAQDUUueyiPa6//vr06dPnRZdkkPz2t7/NP//zP6e5uTl77LHHyx43ZcqUTJo0aat9dXV1\n",
              "pccDAACADqnKmQ8LFy7MNddckyVLlqS1tTVz585tW6uvr89DDz3Utt3a2pr6+vpdYi1Jli1blmOP\n",
              "PTbf+c538q53vesV38e6urr06NFjq4f4AAAAwI5mu8eHjRs3Zty4cbn00kvTq1evzJo1K5MnT87q\n",
              "1auT/PWXLy6//PJs3rw5a9asSXNzcxobG3eJteXLl+eYY47JZZddlve9733b4+MAAACA4rb7ZRfn\n",
              "n39+Ro0aldGjRydJRowYkbPOOivjx4/PggULcsopp2TRokUZOHBgampqMmnSpAwbNixJOv3axIkT\n",
              "s379+lxwwQW54IILkiQXXXRR23sFAAAAO6OaSqVSqfYQAAAAwF+NGTMmSTJ//vwqT7LtVOWeDwAA\n",
              "AMCuQ3wAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAo\n",
              "SnwAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwA\n",
              "AAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAA\n",
              "ihIfAAAAgKLEBwAAAKAo8QEAAAAoSnwAAAAAihIfAAAAgKK6VHsAAAAA4H/17t272iNsczWVSqVS\n",
              "7SEAAACAzstlFwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+\n",
              "AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+sN01\n",
              "NTWlqamp2mMAAACwnXRp74GVSiU33XRT7r333iTJ0KFDM3r06NTU1BQbjs7piSeeqPYIAAAAbEft\n",
              "ig/r1q3LUUcdlSeffDIjR45MpVLJjBkzsu++++bWW29Nz549S88JAAAA7KTaddnF5z73uRx44IG5\n",
              "//77c8MNN2TBggVZuXJl3v72t2fatGmlZwQAAAB2Yu2KDz/96U/zta99LV26/O+JEl27ds1Xv/rV\n",
              "/OQnPyk2HAAAALDza1d8qFQq2WOPPV60/6X2AQAAAPytdsWHurq6l13r1q3bNhsGAAAA6HzadcPJ\n",
              "5cuX521ve9uL9lcqlaxYsWKbDwUAAAB0Hu2KDz/+8Y9LzwEAAAB0Uu2KD0cccUTpOQAAAIBOql3x\n",
              "YdKkSa+4fvHFF2+TYQAAAIDOp13xoWfPnqXnAAAAADqpdsWHqVOnlp4DAAAA6KTa9VObAAAAAK+V\n",
              "+AAAAAAUJT4AAAAARYkPAAAAQFHtuuHkC+6+++586lOfygMPPJDnn3++bf8DDzywzQcDAAAAOocO\n",
              "xYfTTjstEyZMyKGHHpra2tpSMwEAAACdSIfiQ21tbcaPH19qFgAAAKAT6tA9Hw477LAsXry41CwA\n",
              "AABAJ9ShMx9++ctf5vLLL8+AAQPSvXv3tv133333Nh8MAAAA6Bw6FB9mzpxZag4AAACgk+pQfDji\n",
              "iCOSJI899liS5O/+7u+2/UQAAABAp9Khez4sX748Q4YMaXsMGzYsv//970vNBgAAAHQCHYoPZ511\n",
              "Vi688MKsXbs2a9euzYUXXpimpqZSswEAAACdQIfiw9q1a3PiiSe2bX/kIx/J2rVrt/lQAAAAQOfR\n",
              "ofhQW1ubZcuWtW0vW7YstbW123woAAAAoPPo0A0nv/jFL+bwww/P8OHDkyS//e1vM3fu3CKDAQAA\n",
              "AJ1Dh+LD6NGjs2zZstx5551JkkMOOSS9evUqMhgAAADQOXQoPiTJG9/4xhx77LElZgEAAAA6oXbF\n",
              "hyOOOCK/+MUv8oY3vCE1NTVt+yuVSmpqarJmzZpiAwIAAAA7t3bFh+9973tJkpaWlpKzAAAAAJ1Q\n",
              "u37t4k1velOSZPbs2enbt+9Wj9mzZxcdEAAAANi5deinNufPn9+ufQAAAAAvaFd8+MlPfpJzzz03\n",
              "jz76aCZNmtT2+PjHP/6yf9PQ0JCGhoYMHjw4tbW1bduNjY254447Ul9fn3Xr1rUdP3bs2EydOjVJ\n",
              "smXLlpx99tnp379/BgwYkJkzZ7Yd19nXvvWtb2XYsGFpaGjI0KFD881vfvNVPx8AAADYkbXrng/d\n",
              "u3fPXnvtld122y09e/Zs2/+Wt7wln/70p1/yb164P0Rra2saGhpedL+IE088MRMmTMicOXMyd+7c\n",
              "rFy5MldffXWSZM6cOVm2bFlWrFiR9evXZ+TIkRk1alSGDBnS6ddOPvnkfPKTn0ySbNiwIUOHDs27\n",
              "3/3ujBw5sqOfLQAAAOwQ2nXmwxFHHJGpU6fmpptuytSpU9se5557bvr16/eaXnjatGlZunRpZsyY\n",
              "kcmTJ+eqq65K165dkyTNzc05/fTTU1tbm7333juNjY2ZN2/eLrH2t3Fn48aNee655172Pdy0aVM2\n",
              "bNiw1WPTpk2v6fMAAACAUjp0z4cZM2bkqaeeatt+8sknM378+Nf0wt26dctll12WiRMnpqmpKcOH\n",
              "D29bW7VqVfr27du23a9fv6xatWqXWEuSa6+9NkOGDEm/fv1y3nnnvexZD9OnT0/Pnj23ekyfPv0l\n",
              "jwUAAIBq6VB8uOuuu7LPPvu0bffq1SuLFi16zS9+/fXXp0+fPn7C8/844YQTcu+99+a+++7LnDlz\n",
              "ct99973kcVOmTMn69eu3ekyZMmU7TwsAAACvrEPx4fnnn99qu1Kp5Nlnn31NL7xw4cJcc801WbJk\n",
              "SVpbWzN37ty2tfr6+jz00ENt262tramvr98l1v5Wv379cvDBB+eHP/zhS76HdXV16dGjx1aPurq6\n",
              "lzwWAAAAqqVD8eGQQw7JhAkT8tBDD6W1tTVnn312DjnkkA6/6MaNGzNu3Lhceuml6dWrV2bNmpXJ\n",
              "kydn9erVSf76yxeXX355Nm/enDVr1qS5uTmNjY27xNqyZcva3qc//elP+dnPfrbVJSkAAACws2nX\n",
              "r1284Ktf/WrOOeecHHTQQampqckHPvCBfO1rX+vwi55//vkZNWpURo8enSQZMWJEzjrrrIwfPz4L\n",
              "FizIKaeckkWLFmXgwIGpqanJpEmTMmzYsCTp9Gvf+MY3cvvtt6dbt26pVCo555xz8r73va/D7zEA\n",
              "AADsKGoqlUql2kOwaxkzZkySZP78+VWeBAAAgO2hQ2c+JMljjz2W3/3ud3nmmWfa9n3gAx/YpkMB\n",
              "AAAAnUeH4sN3vvOdTJs2LWvWrMnAgQOzdOnSHHLIIeIDAAAA8LI6dMPJr33ta1myZEn69++fu+66\n",
              "Kz/72c8yaNCgUrMBAAAAnUCH4kO3bt3yhje8oe0nNw8//PC0tLSUmAsAAADoJDp02UVdXV0qlUoG\n",
              "DRqUr3/96+nbt2+efvrpUrMBAAAAnUCH4sPnP//5bNiwIV/+8pdz5plnZu3atfn2t79dajYAAACg\n",
              "E+hQfDjyyCOTJD179szNN99cZCAAAACgc+nQPR8+85nPZN26dalUKvnHf/zH9OrVK9ddd12p2QAA\n",
              "AIBOoEPxYcGCBdlrr71yyy23pEuXLlm4cGE+//nPl5oNAAAA6AQ6FB922+2vh//iF7/I2LFjc8AB\n",
              "B6SmpqbIYAAAAEDn0KF7Puyxxx656KKL8r3vfS8LFy5MpVLJs88+W2o2AAAAoBPo0JkPs2bNyuOP\n",
              "P54vf/nL6d27d+6///6cfPLJpWYDAAAAOoGaSqVSqfYQ7FrGjBmTJJk/f36VJwEAAGB76NBlF62t\n",
              "rbnoooty//335/nnn2/b/7Of/WybDwYAAAB0Dh2KDx/+8Idz1FFHZcKECamtrS01EwAAANCJdCg+\n",
              "PPPMM5k+fXqpWQAAAIBOqEM3nBw6dGhWrVpVahYAAACgE+rQmQ9/+tOfMmLEiBx66KHp3r172343\n",
              "DgQAAABeTofiw8knn+ynNQEAAIAO6VB8OO2000rNAQAAAHRSHYoPSXLNNdekpaUlzzzzTNu+iy++\n",
              "eJsOBQAAAHQeHbrh5MSJEzN79uzMmjUrNTU1ufbaa7N+/fpSswEAAACdQIfiw89//vMsWLAg++67\n",
              "b7761a/mzjvvzCOPPFJqNgAAAKAT6FB86N69e3bbbbfU1NTkueeey3777ZfHHnus1GwAAABAJ9Ch\n",
              "ez7sueee+fOf/5x3vetdOfnkk7Pffvtl9913LzUbAAAA0Al06MyHefPmpUuXLvn3f//3DB8+PF27\n",
              "ds21115bajYAAACgE2j3mQ+bN2/Oeeedl9mzZydJLrzwwmJDAQAAAJ1Hu898qK2tzYoVK0rOAgAA\n",
              "AHRCHbrnw6hRo3LGGWdk3Lhxef3rX9+2f/jw4dt8MAAAAKBzaFd8+OhHP5p58+alubk5SXLzzTe3\n",
              "rdXU1OSBBx4oMx0AAACw02tXfPj973+fJHnwwQeLDgMAAAB0Pu2KDzU1NaXnYBfSu3fvao8AAADA\n",
              "dlRTqVQqr3ZQly5d0qNHjxftr1QqqampyZo1a4oMBwAAAOz82nXmwwEHHJAbb7yx9CwAAABAJ9Su\n",
              "+FBXV5e+ffuWngUAAADohHZrz0HtuDIDAAAA4CW1654PAAAAAK9Vu858AAAAAHitxAcAAACgKPEB\n",
              "AAAAKEp8AAAAAIoSHwAAAICixAcAAACgKPEBAAAAKEp8AAAAAIoSHwAAAICixAcAAACgKPEBAAAA\n",
              "KEp8AAAAAIoSH+D/oampKU1NTdUeAwAAYIfWpdoDwM7siSeeqPYIAAAAOzxnPgAAAABFiQ8AAABA\n",
              "UeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeID\n",
              "AAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAA\n",
              "UJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4\n",
              "AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABRVJD40NDSkoaEhgwcPTm1tbdt2Y2Nj7rjj\n",
              "jtTX12fdunVtx48dOzZTp05NkmzZsiVnn312+vfvnwEDBmTmzJltx3X2tRf88Y9/TO/evfPBD36w\n",
              "g+88AAAA7Hi6lHjSlpaWJElra2saGhratl9w4oknZsKECZkzZ07mzp2blStX5uqrr06SzJkzJ8uW\n",
              "LcuKFSuyfv36jBw5MqNGjcqQIUM6/doLxo8fn2OPPTZPPfVUiY8HAAAAtquqXHYxbdq0LF26NDNm\n",
              "zMjkyZNz1VVXpWvXrkmS5ubmnH766amtrc3ee++dxsbGzJs3b5dYS5Irrrgif//3f593v/vdpT8G\n",
              "AAAA2C6qEh+6deuWyy67LBMnTkxTU1OGDx/etrZq1ar07du3bbtfv35ZtWrVLrH24IMP5pJLLskX\n",
              "vvCFV30Pk2TTpk3ZsGHDVo9Nmza1628BAABge6naDSevv/769OnT50WXZOyqKpVKPvaxj2XmzJl5\n",
              "3ete166/mT59enr27LnVY/r06YUnBQAAgI6pSnxYuHBhrrnmmixZsiStra2ZO3du21p9fX0eeuih\n",
              "tu3W1tbU19d3+rUNGzbknnvuSWNjY/r165fzzjsvP/3pT3PUUUe97Ps4ZcqUrF+/fqvHlClTXvZ4\n",
              "AAAAqIpKQQ8++GClZ8+eW+17+umnKwMGDKjcdNNNlUqlUmlpaan07t278vjjj1cqlUrlyiuvrBx5\n",
              "5JGV559/vvLUU09V6uvrK/fcc88usfa3rrzyyspxxx23rT4KCjn++OMrxx9/fLXHAAAA2KEV+bWL\n",
              "V3L++edn1KhRGT16dJJkxIgROeusszJ+/PgsWLAgp5xyShYtWpSBAwempqYmkyZNyrBhw5Kk068B\n",
              "AABAZ1RTqVQq1R4CdlZjxoxJksyfP7/KkwAAAOy4qnbDSQAAAGDXID4AAAAARYkPAAAAQFHiAwAA\n",
              "AFCU+AAAAAAUJT4AAAAARYkPAAAAQFHiAwAAAFCU+AAAAAAUJT4AAAAARYkPAAAAQFHiAwAAAFCU\n",
              "+AAAAAAUJT4AAAAARYkPAAAAQFHiAwAAAFCU+AAAAAAUJT4AAAAARYkPAAAAQFHiAwAAAFCU+AAA\n",
              "AAAUJT4AAAAARYkPAAAAQFHiAwAAAFCU+AAAAAAUJT4AAAAARYkPAAAAQFHiAwAAAFCU+AAAAAAU\n",
              "JT4AAAAARYkPAAAAQFFdqj0A7Mx69+5d7REAAAB2eDWVSqVS7SEAAACAzstlFwAAAEBR4gMAAABQ\n",
              "lPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgA\n",
              "AAAAFCU+AAAAAEWJDwAAAEBR4gMAAABQlPgAAAAAFCU+AAAAAEWJD7ALaWpqSlNTU7XHAAAAdjFd\n",
              "qj0AsP088cQT1R4BAADYBTnzAQAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACK\n",
              "Eh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8A\n",
              "AACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACA\n",
              "osQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAAAACKEh8AAACAosQH\n",
              "AAAAoCjxAQAAACiqWHxoaGhIQ0NDBg8enNra2rbtxsbG3HHHHamvr8+6devajh87dmymTp2aJNmy\n",
              "ZUvOPvvs9O/fPwMGDMjMmTPbjuvsa5/97Gez7777tr1fJ5100mv9CAAAAGCH0KXUE7e0tCRJWltb\n",
              "09DQ0Lb9ghNPPDETJkzInDlzMnfu3KxcuTJXX311kmTOnDlZtmxZVqxYkfXr12fkyJEZNWpUhgwZ\n",
              "0unXkuSkk07K17/+9VIfDQAAAGxXVbvsYtq0aVm6dGlmzJiRyZMn56qrrkrXrl2TJM3NzTn99NNT\n",
              "W1ubvffeO42NjZk3b94usdYRmzZtyoYNG7Z6bNq06bV+JAAAAFBE1eJDt27dctlll2XixIlpamrK\n",
              "8OHD29ZWrVqVvn37tm3369cvq1at2iXWkuT73/9+RowYkSOPPDI///nPX/Y9nD59enr27LnVY/r0\n",
              "6S97PAAAAFRDscsu2uP6669Pnz59XnRJxq7szDPPzIUXXpiuXbtm4cKFOf7447No0aKtYsULpkyZ\n",
              "kkmTJm21r66ubnuNCgAAAO1StTMfFi5cmGuuuSZLlixJa2tr5s6d27ZWX1+fhx56qG27tbU19fX1\n",
              "u8Tafvvt13b5yWGHHZaRI0dm8eLFL/ke1tXVpUePHls9xAcAAAB2NFWJDxs3bsy4ceNy6aWXplev\n",
              "Xpk1a1YmT56c1atXJ/nrL19cfvnl2bx5c9asWZPm5uY0NjbuEmuPPPJI2/v0hz/8IS0tLRk2bFjR\n",
              "zwMAAABKqsplF+eff35GjRqV0aNHJ0lGjBiRs846K+PHj8+CBQtyyimnZNGiRRk4cGBqamoyadKk\n",
              "tn8B7+xrF154Ye6666506dIltbW1+da3vpVBgwZtj48FAAAAiqipVCqVag8BbB9jxoxJksyfP7/K\n",
              "kwAAALuSqt3zAQAAANg1iA8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4\n",
              "AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAA\n",
              "ABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQl\n",
              "PgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUFSXag8AbD+9e/eu\n",
              "9ggAAMAuqKZSqVSqPQQAAADQebnsAgAAAChKfAAAAACKEh8AAACAosQHAAAAoCjxAQAAAChKfAAA\n",
              "AACKEh8AAACAosQHOmzTpk357Gc/m02bNlV7FGgX31l2Rr637Gx8Z9kZ+d6ys9mZv7M1lUqlUu0h\n",
              "2Lls2LAhPXv2zPr169OjR49qjwOvyneWnZHvLTsb31l2Rr637Gx25u+sMx8AAACAosQHAAAAoCjx\n",
              "AQAAAChKfKDD6urqMnXq1NTV1VV7FGgX31l2Rr637Gx8Z9kZ+d6ys9mZv7NuOAkAAAAU5cwHAAAA\n",
              "oCjxAQAAAChKfAAAAACKEh8AAACAosQHOuQPf/hD3vnOd2bQoEE56KCDcu+991Z7JHhFEydOTL9+\n",
              "/VJTU5OWlpZqjwOv6plnnskHP/jBDBo0KCNGjMj73ve+rFy5stpjwSs6+uijM3z48DQ0NOTd7353\n",
              "lixZUu2RoN2uvPLK1NTU5IYbbqj2KPCq+vXrlwMOOCANDQ1paGhIc3NztUdqN/GBDhk/fnzOOOOM\n",
              "rFixIhdccEHGjRtX7ZHgFZ1wwgn51a9+lb59+1Z7FGi3M844I/fdd1+WLl2a4447Lp/4xCeqPRK8\n",
              "omuuuSb33HNPWlpaMmnSJP/7gJ1Ga2trLr/88hxyyCHVHgXarbm5OS0tLWlpaUljY2O1x2k38YF2\n",
              "++Mf/5jFixfn5JNPTpJ86EMfysMPP+z/kWOHdvjhh6dPnz7VHgParXv37jnmmGNSU1OTJDnkkEPS\n",
              "2tpa3aHgVey1115t/3n9+vVt31/YkW3ZsiWf+MQnMmPGjNTV1VV7HOj0ulR7AHYeDz/8cN70pjel\n",
              "S5e/fm1qampSX1+fVatWZcCAAVWeDqBz+sY3vpHjjjuu2mPAqzr11FPz85//PEly4403VnkaeHUX\n",
              "X3xxDjvssBx44IHVHgU65NRTT02lUsk73vGOfOlLX8q+++5b7ZHaxZkPALCD+uIXv5iVK1dm+vTp\n",
              "1R4FXtV3v/vdPPzww/n85z+fCy64oNrjwCv63e9+l+uuuy7/+q//Wu1RoEN++ctf5p577sndd9+d\n",
              "Xr165bTTTqv2SO3mzAfa7S1veUsef/zxPP/88+nSpUsqlUpWrVqV+vr6ao8G0Ol85Stfyfz583PL\n",
              "Lbdk9913r/Y40G6nnXZazjzzzDz11FPZZ599qj0OvKTbb789ra2tGThwYJJk9erVOeOMM/L444+n\n",
              "qampytPBy3vh3726du2ac845J4MGDaryRO3nzAfa7Y1vfGPe9ra3Zc6cOUmS6667Ln369HHJBcA2\n",
              "dvHFF2fevHm5+eabt7qWHnZE69aty2OPPda2fcMNN2SfffbJ3nvvXcWp4JU1NTXl8ccfT2tra1pb\n",
              "W3PIIYfksssuEx7YoW3cuDHr1q1r2543b15GjhxZvYE6yJkPdMill16acePG5Ytf/GJ69OiRK6+8\n",
              "stojwSsaP358fvSjH2X16tUZPXp09txzTzdJZYf2yCOPZPLkydl///0zatSoJEldXV1+85vfVHky\n",
              "eGnr16/P2LFj85e//CW77bZb9t133/zwhz9000mAbeyJJ57Ihz70oWzevDmVSiX7779/vvvd71Z7\n",
              "rHarqVQqlWoPAQAAAHReLrsAAAAAihIfAAAAgKLEBwAAAKAo8QEA2K7++7//O69//evz8Y9/vNqj\n",
              "bOW2227LTTfd9IrH/PCHP8yZZ57ZdnxDQ8NW662trVv9Qskll1yS4cOHp6GhIf/wD/+Qk046qW2t\n",
              "X79+OeCAAzJixIgMGDAgxx13XH79619v9VpnnHHG//8fDAB2AOIDALBdNTc358ADD8z8+fPz9NNP\n",
              "V3ucNu2JD1OmTMmUKVPa9XyLFy/Ol7/85dx2221paWnJ8uXLM3ny5K2OaW5uztKlS7Ny5cqcdtpp\n",
              "OeaYY9p+2eTYY4/NXXfdlT/84Q+v7R8IAHYg4gMAsF1dccUVueCCC3L44Yenubm5bf+sWbPy3ve+\n",
              "Nx/96EczePDgvPOd78yyZcty/PHH561vfWuOPvrotljx9NNP52Mf+1iGDh2aoUOH5t/+7d/anuc9\n",
              "73lPbrjhhrbtE044IbNmzUqSjBs3LuPHj89RRx2VQYMGZcyYMXn22WfT0tKSSy65JHPnzk1DQ0Om\n",
              "TZv2orlvv/327LXXXunbt2+7/jkfeeSR7Lnnntlzzz2TJDU1NXnb2972ssePGTMmZ555Zr7yla+0\n",
              "7fvwhz+c//zP/2zX6wFAe02cODH9+vVLTU1NWlpa2vU3a9euzUknnZRBgwZlyJAh+Zd/+ZcOvab4\n",
              "AABsN8uWLcvDDz+c0aNH5+Mf/3iuuOKKrdYXLVqUiy66KMuWLUv//v3zT//0T7nkkkuyfPnydOvW\n",
              "LVdddVWS5HOf+1w2bdqUe+65J7/5zW9yww03bBUyXklLS0t+8IMfZPny5XniiSdy3XXXpaGhIWee\n",
              "eWZOOumktLS05DOf+cyL/u62227LwQcf3O5/1qOPPjp77rln6uvr09jYmJkzZ2bt2rWv+DcHH3xw\n",
              "7r333rbtQw89NLfeemu7XxMA2uOEE07Ir371q3YH9ST52Mc+lpEjR2bFihW59957c84553ToNcUH\n",
              "AGC7ueKKK3LqqaemtrY2xxxzTB588MEsX768bf3QQw9NfX19kuTtb397DjrooPTu3TtJctBBB7Vd\n",
              "gnDLLbfk9NNPz2677ZY99tgjp556am6++eZ2zXD88cdn9913T21tbd7xjnfk/vvvb9ffPfLII22z\n",
              "JH89k+GlvLB/9913z+23354bb7wxhx12WObPn5/hw4dnzZo1L/salUplq+399tsvjzzySLvmA4D2\n",
              "Ovzww9OnT58X7V+0aFGOPPLIvP3tb8/IkSPz/e9/P0mycuXKLF68OJMmTWo7dr/99uvQa4oPAMB2\n",
              "8dxzz2X27Nm56qqr0q9fvwwYMCB//vOftzr7oXv37m3/uba29kXbzz///Es+99+GgC5dumTz5s1t\n",
              "288888xWx7b3Of+v3Xfffavn2nffffPUU09tdcyTTz6ZN77xjVvNNXLkyEycODG33nprXv/61+e2\n",
              "22572ddYtGhRhg4dutXsr3vd69o1HwD8f6xbty5nnHFG5s6dm8WLF+fmm2/O5MmT8+ijj2bZsmXp\n",
              "06dPmpqacuCBB+boo4/OkiVLOvT84gMAsF3813/9V/bff/88+uijaW1tTWtra+64447Mnj07zz33\n",
              "XIee673vfW+uuOKKVCqVbNy4MbNnz87RRx+dJBkwYEDbTRsffPDB/OpXv2rXc/bo0SPr169/2fXh\n",
              "w4fnvvvua9seOHBgunbtmhtvvDFJsmXLllx66aVtc/z+97/PPffc03b8ww8/nD/96U/Zf//9X/L5\n",
              "FyxYkP/4j//Y6qaUy5cvz4gRI9o1PwD8f/z617/OAw88kPe///1paGjIe9/73iTJfffdl+effz53\n",
              "3nlnPvKRj+Suu+7Kueeem2OPPbZD//3dpdTgAAB/64orrtjqpyaT5K1vfWve/OY35wc/+EGHnuvT\n",
              "n/50Jk6cmGHDhiVJxo4dmw9/+MNJkvPPPz+NjY0ZNmxYhgwZ0u77NBx//PGZPXt2GhoaMmbMmBfd\n",
              "9+HYY4/NtGnTsnnz5tTW1qZr1665/vrrM2nSpHzqU5/Kli1bcvDBB+cLX/hCkuTPf/5zzj333Kxe\n",
              "vTqve93rUqlU8qUvfWmrn+dsbGxM9+7ds3HjxgwePDg33njjVvPedNNNOeGEEzr03gDAa1GpVDJk\n",
              "yJCtfvb5BYsXL86b3/zmjBo1Kkny/ve/P88++2weeuihDBgwoF3PX1P5vxcXAgDwkj75yU/mPe95\n",
              "T8aOHVv8tZ588skceeSRWbx4cbp161b89QDY9fTr1y833HBDGhoasnbt2gwePDizZ89uO+uhpaUl\n",
              "gwcPTteuXTNs2LBcffXVGT58eO68884cc8wxefTRR1NXV9eu1xIfAADa6amnnsqPf/zjnHzyycVf\n",
              "6ze/+U02b96cd77zncVfC4Bdy/jx4/OjH/0oq1evzj777JM999wzK1euzN13353zzjsvTz31VJ57\n",
              "7rnU19fnhhtuSPfu3XPXXXflrLPOyl/+8pfU1dXlK1/5So444oh2v6b4AAAAABTlhpMAAABAUeID\n",
              "AAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAAUJT4AAAAABQlPgAAAABFiQ8AAABAUeIDAAAA\n",
              "UNT/AH1nJCm6xm/VAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-b313d64e-59ef-4af0-bfb0-93a54e9f76eb\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-b313d64e-59ef-4af0-bfb0-93a54e9f76eb\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:5: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "figsize = (12, 1.2 * len(_df_21['Country'].unique()))\n",
              "plt.figure(figsize=figsize)\n",
              "sns.violinplot(_df_21, x='Amount (USD)', y='Country', inner='stick', palette='Dark2')\n",
              "sns.despine(top=True, right=True, bottom=True, left=True)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-5d16b6f6-41c5-4329-b462-1765a3b20fe9\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABA8AAAIDCAYAAABrUjeaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAApXklEQVR4nO3de7iVdZ3//9eGTRsZOZgpmoqEhAdOGwhPJeMpTLNR8ThJoFng\n",
              "NiPDinGmvqWVNMVlB51JZ4Y0yRo0ibLUIsuJyhTULSV4zD0qCilnTxzX7w8v9292yqe9lc3ayONx\n",
              "Xeu69rr3ve77vWhdl/Hkc9+rplKpVAIAAACwGZ2qPQAAAADQsYkHAAAAQJF4AAAAABSJBwAAAECR\n",
              "eAAAAAAUiQcAAABAkXgAAAAAFIkHAAAAQJF4AAAAABSJBwAAAECReAAAAAAUiQcAAABAkXgAAAAA\n",
              "FIkHAAAAQJF4AAAAABSJBwAAAECReAAAwDaroaEhDQ0N1R4D4E2vttoDAADA67V06dJqjwCwXbDy\n",
              "AAAAACgSDwAAAIAi8QAAAAAoEg8AAACAIvEAAAAAKBIPAAAAgCLxAAAAACgSDwAAAIAi8QAAAAAo\n",
              "Eg8AAACAIvEAAAAAKBIPAAAAgCLxAAAAACgSDwAAAIAi8QAAAAAoEg8AAACAIvEAAAAAKBIPAAAA\n",
              "gCLxAAAAACgSDwAAAIAi8QAAAAAoEg8AAACAIvEAAAAAKBIPAAAAgCLxAAAAACgSDwAAAIAi8QAA\n",
              "AAAoEg8AAACAIvEAAAAAKBIPAAAAgCLxAAAAACgSDwAAAIAi8QAAAAAo2q7iQd++fbPvvvumvr4+\n",
              "+++/fz74wQ/m+eef32LHr6+vz5o1a5rP1djYuMWODQAAANWyXcWDJJk5c2YaGxtz//33Z9WqVbnm\n",
              "mmtetc/GjRtf17EbGxvTvXv3NzghAAAAdCzbXTx4xbp16/LCCy9kp512yjXXXJMjjjgiJ598cgYP\n",
              "Hpy77rorl112WUaOHJn6+vqMHDkyd9xxR5KXA0F9fX3zo0ePHrn44ouTJDU1NVm5cmWrzr927dqs\n",
              "Xr26xWPt2rXt9XYBAADgddvu4sHpp5+e+vr67LbbbunUqVNOO+20JMmdd96ZSy+9NH/84x9zyCGH\n",
              "5EMf+lDmzZuXxsbGXH755Tn77LOTvHxpQmNjYxobG/P1r389u+66az7ykY+0eY6pU6emZ8+eLR5T\n",
              "p07dou8VAAAAtoTaag+wtc2cOTP19fXZsGFDJk6cmClTpmTw4ME59NBDs++++zbvd++99+bLX/5y\n",
              "li1bltra2jz44IN58cUXs8MOOyRJ/vSnP+Xss8/OT3/60+yxxx5tnuOiiy7K5MmTW2yrq6t7Y28O\n",
              "AAAA2sF2Fw9eUVtbm5NPPjmf/vSnM3jw4Oy4447Nv1u3bl3GjBmTX//61xk5cmRWr16dnj17Zu3a\n",
              "tdlhhx3y1FNP5cQTT8zVV1+dQYMGva7z19XViQUAAABsE7a7yxb+r1/96lctVhu84qWXXsq6devS\n",
              "p0+fJMnll1/e/Ls1a9bk/e9/fy6++OIcccQRW21WAAAAqJbtLh68cs+DQYMGZdGiRfnmN7/5qn16\n",
              "9OiRL33pSznwwAMzYsSIvOUtb2n+3axZs/LAAw/ka1/7WvNNE6+88sqt+RYAAABgq6qpVCqVag8B\n",
              "AACvx5gxY5K8/A88ALSf7W7lAQAAANA24gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAk\n",
              "HgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAA\n",
              "ReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAA\n",
              "AFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABFtdUeAAAAXq/e\n",
              "vXtXewSA7UJNpVKpVHsIAAAAoONy2QIAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAA\n",
              "AABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIB\n",
              "AAAAUCQeAAAAAEXiAQAAAGxBDQ0NaWhoqPYYW1RttQcAAACAN5OlS5dWe4QtzsoDAAAAoEg8AAAA\n",
              "AIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMA\n",
              "AACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8\n",
              "AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACK\n",
              "xAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBou4oHGzZsyMUXX5z99tsvgwYNSn19fSZM\n",
              "mJDZs2envr7+NV/z1FNP5bDDDtu6gwIAAEAHUlvtAbamc845J8uXL88dd9yRnXbaKZVKJT/84Q+z\n",
              "fPnyzb7m7W9/e+bOnbsVpwQAAICOZbtZefDII4/khhtuyNVXX52ddtopSVJTU5NTTz01/fr1y4YN\n",
              "G3Leeedl6NChGThwYObPn58kaWpqSq9evZqPU1NTk0svvTQHHnhg3vGOd+Tqq69u/t2nPvWpjBw5\n",
              "MvX19Rk1alQefPDBzc6zdu3arF69usVj7dq17fPmAQAA4A3YbuLBPffck3e+851529ve9pq/f+CB\n",
              "BzJ+/Pjcd999+fjHP55/+Zd/2eyx6urqctddd+WWW27JpEmTsmHDhiTJlClTMm/evDQ2Nua8887L\n",
              "Jz7xic0eY+rUqenZs2eLx9SpU9/YmwQAAIB2sF1dtlDSv3//HHTQQUmSQw45JNOmTdvsvmeeeWaS\n",
              "ZL/99kttbW2WLFmSPffcM3PmzMnll1+eNWvWZNOmTcXLIS666KJMnjy5xba6urot8E4AAABgy9pu\n",
              "4sHw4cPz8MMPZ9myZdl5551f9fuuXbs2/9y5c+fm1QSv5bX2ffzxx3P++edn3rx52WeffbJgwYKM\n",
              "GjVqs8eoq6sTCwAAANgmbDeXLfTv3z8nn3xyzjnnnKxcuTJJUqlUcuONN+bPf/7zGz7+qlWr0qVL\n",
              "l+y+++6pVCq54oor3vAxAQAAoCPYbuJBknznO9/J0KFDc9BBB2XgwIE54IAD8otf/CJvfetb3/Cx\n",
              "Bw8enDPOOCMDBw7MyJEj06dPny0wMQAAAFRfTaVSqVR7CAAAAHizGDNmTJJk1qxZVZ5ky9muVh4A\n",
              "AAAAbSceAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAk\n",
              "HgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAA\n",
              "ReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAA\n",
              "AFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUFRb7QEAAADgzaR3797VHmGLq6lUKpVqDwEAAAB0\n",
              "XC5bAAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAA\n",
              "AACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAPapKGhIQ0NDdUeAwAA\n",
              "gK2ottoDsG1ZunRptUcAAABgK7PyAAAAACgSDwAAAIAi8QAAAAAoEg8AAACAIvEAAAAAKBIPAAAA\n",
              "gCLxAAAAACgSDwAAAIAi8QAAAAAoEg8AAACAIvEAAAAAKBIPAAAAgCLxAAAAACgSDwAAAIAi8QAA\n",
              "AAAoEg8AAACAIvEAAAAAKGpzPDj44IPz/e9/P+vXr2+PeQAAAIAOps3x4JJLLsn111+fvn375nOf\n",
              "+1wWL17cHnMBAAAAHUSb48Ho0aMze/bs3HHHHdm4cWNGjhyZU089Nb/73e/aYz4AAACgyl73PQ9W\n",
              "rFiRpUuXplOnTtl9991z/vnn5/zzz9+SswEAAAAdQJvjwQ9+8IO8+93vztixY3PwwQfn4Ycfzre+\n",
              "9a3Mnz8/P/vZz9pjRgAAAKCKatv6gu9///u5+OKLc/TRR7fY3rlz53zrW9/aYoMBAAAAHUObVh5s\n",
              "3LgxvXr1elU4eMUHPvCBLTIUAAAA0HG0KR507tw5Dz30UHvNAgAAAHRAbb5s4YgjjsiECRNy1lln\n",
              "Zccdd2zePmTIkC06GAAAANAxtDkezJw5M0kyZ86c5m01NTX585//vOWmAgAAADqMNseDxx57rD3m\n",
              "AAAAADqoNn9V44knntiqbQAAAMCbQ5vjweOPP/6qbY8++ugWGQYAAADoeFp92cJVV12VK6+8Mg89\n",
              "9FCGDx/evH3VqlUZOHBguwwHAAAAVF+r48H73ve+7LvvvmloaMjXv/715u09evTwTQsAAADwJtbq\n",
              "eLD33ntn7733zqJFi9pzHgAAAKCDafM9D5qamtLQ0JDRo0fnyCOPbH5sD2pqarJy5coW2/r27ZvG\n",
              "xsYkyeGHH57Zs2cnSTZt2pSGhoaMGjUqq1at2rqDAgAAwBbU5q9qPO2003LUUUfl/PPPT+fOndtj\n",
              "pm3e+vXrM27cuDz33HP5+c9/nh122KHaIwEAAMDr1uZ48NJLL2Xq1KntMcubwosvvpgTTzwxO+20\n",
              "U370ox+ltva1/4jXrl2btWvXtthWV1eXurq6rTEmAAAAtFqbL1sYNGjQa35dIy/7+Mc/nl69emXG\n",
              "jBmbDQdJMnXq1PTs2bPFQ5QBAACgI2rzyoNnnnkmQ4cOzSGHHJKuXbs2b581a9YWHWxbUlNT0/zz\n",
              "Mccck1/96lf54x//WPwWiosuuiiTJ09usc2qAwAAADqiNseDsWPHZuzYse0xS4e3yy67ZNmyZenV\n",
              "q1fztmeffTa77rpr8/NTTz01J5xwQkaPHp1bb7019fX1r3kslygAAACwrWhzPBg/fnx7zLFNOOaY\n",
              "Y3LVVVflq1/9apLk2muvTb9+/bL77ru32O+0005Lp06d8r73vS+33HJLhg0bVo1xAQAAYItoczz4\n",
              "8Ic//Jrbv/Od77zhYTq6b3zjG7ngggsyZMiQdOrUKbvttltuuOGG19z3lFNOaQ4IN998c0aMGLGV\n",
              "pwUAAIAto6ZSqVTa8oJ/+7d/a/75pZdeyo033pjhw4fniiuu2OLD0fGMGTMmyfZ9jwsAAIDtTZtX\n",
              "HnzsYx9r8byhoSH/8A//sMUGAgAAADqWNn9V41/r2rVrnnzyyS0xCwAAANABtXnlwf/9esGNGzdm\n",
              "/vz5GTRo0BYdCgAAAOg42hwPevbs+f+/uLY2kyZNar4OHgAAAHjzaXM8+PznP98ecwAAAAAdVJvv\n",
              "ebBmzZp87GMfy4ABAzJgwICcf/75WbNmTXvMBgAAAHQAbY4H5513XjZs2JDrr78+N9xwQzZt2pTz\n",
              "zjuvPWYDAAAAOoA2X7awYMGC3Hfffc3P//3f/z1Dhw7dokMBAAAAHUebVx5s3LixxWUKa9asycaN\n",
              "G7foUAAAAEDH0eaVB+PHj8/BBx+c008/PUly/fXX5+yzz97igwEAAAAdQ6vjwerVq7N8+fJ8+tOf\n",
              "zqBBg3LbbbclefkeCGPHjm23AQEAAIDqavVlC5/5zGdy9913J0mOPfbYTJs2LdOmTctuu+2WKVOm\n",
              "tNuAAAAAQHW1Oh7cddddOfnkk1+1fcyYMfnNb36zRYcCAAAAOo5Wx4MNGzZs/iCd2nzfRQAAAGAb\n",
              "0eq/9a9fvz6rV69+1fZVq1Zl/fr1W3QoAAAAoONodTw444wz8qEPfSgrVqxo3rZixYqcffbZOeOM\n",
              "M9plOAAAAKD6Wh0PPvvZz6ZXr17Za6+9MmzYsAwbNix77bVXunfvns997nPtOSMAAABQRTWVSqXS\n",
              "lhc8+uijueeee5Ikw4cPzz777NMug9ExjRkzJkkya9asKk8CAADA1lLb1hfss88+ggEAAABsR3xN\n",
              "AgAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQ\n",
              "JB4AAAAAReIBAAAAUCQeAAAAAEW11R6AbUvv3r2rPQIAAABbWU2lUqlUewgAAACg43LZAgAAAFAk\n",
              "HgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAA\n",
              "ReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIB262GhoY0NDRUewwAAIAO\n",
              "r7baA0C1LF26tNojAAAAbBOsPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBI\n",
              "PAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAA\n",
              "isQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAA\n",
              "AKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDz4\n",
              "KzU1NVm5cmWLbX379k1jY2Pz80qlkne84x056qijWuzX1NSUzp07p76+vvlx0EEHbYWpAQAAoP3U\n",
              "VnuAbdFtt92WXr16ZcGCBXnsscfyjne8o/l33bt3bxEaAAAAYFtn5cHrMH369Hz0ox/NBz/4wXzn\n",
              "O9+p9jgAAADQrqw8aKPly5fn1ltvzbe//e08/vjjef/735+LL744nTq93GHWrFmT+vr65v0HDhyY\n",
              "66677lXHWbt2bdauXdtiW11dXerq6tp1fgAAAGgrKw9aqaamJkly3XXX5dhjj02vXr0yZMiQ9O7d\n",
              "Oz//+c+b93vlsoVXHq8VDpJk6tSp6dmzZ4vH1KlTt8p7AQAAgLaw8uCv7LLLLlm2bFl69erVvO3Z\n",
              "Z5/NrrvumuTlSxaWLFmSvn37Jnl5pcH06dNz7LHHtuk8F110USZPntxim1UHAAAAdETiwV855phj\n",
              "ctVVV+WrX/1qkuTaa69Nv379svvuu+fuu+/OM888k6eeeqr5MoWVK1dmr732yjPPPNOm87hEAQAA\n",
              "gG2FePBXvvGNb+SCCy7IkCFD0qlTp+y222654YYbkry86uCMM85oDgdJ0qtXr7z3ve/NjBkzMmbM\n",
              "mFfd8yBJ5s6dm+7du2/NtwEAAABbTE2lUqlUewiohjFjxiRJZs2aVeVJAAAAOjY3TAQAAACKxAMA\n",
              "AACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8\n",
              "AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACK\n",
              "xAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAA\n",
              "oEg8AAAAAIrEAwAAAKBIPAAAAACKaqs9AFRL7969qz0CAADANqGmUqlUqj0EAAAA0HG5bAEAAAAo\n",
              "Eg8AAACAIvEAAAAAKBIPAAAAgCLxAAAAACgSDwAAAIAi8QAAAAAoEg8AAACAIvEAAAAAKBIPAAAA\n",
              "gCLxAAAAACgSDwAAAIAi8QAAAAAoEg8AAACAIvEAAAAAKBIPAAAAgCLxALYhDQ0NaWhoqPYYAADA\n",
              "dqa22gMArbd06dJqjwAAAGyHrDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACg\n",
              "SDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAA\n",
              "AIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMA\n",
              "AACgSDwAAAAAisQDAAAAoEg8AAAAAIrEAwAAAKBIPAAAAACKxAMAAACgSDwAAAAAisQDAAAAoEg8\n",
              "AAAAAIraLR7MmjUrI0aMSH19ffbbb78ceeSR2bRp0xs65he+8IW89NJLzc/POuusfOMb32j169es\n",
              "WZMdd9wx55xzTovtjz76aIYPH55hw4bl6quvftXrnnrqqRx22GGve24AAADYlrVLPHj66aczYcKE\n",
              "zJo1K42NjXnggQcybdq01NTUvKHjXnzxxS3iQVvNnDkzI0aMyKxZs/Lcc881b//hD3+YkSNH5t57\n",
              "783ZZ5/d4jUbNmzI29/+9sydO/d1nxcAAAC2Ze0SD5YuXZrOnTvnrW99a/O24cOHN8eD+fPn59BD\n",
              "D82QIUNy4IEH5ne/+12SpKmpKb169Wp+zXPPPdf8mnPPPTdJcthhh6W+vj5/+ctfkiSLFi3KUUcd\n",
              "lQEDBmTMmDFZt27dZueaPn16pkyZklGjRmXmzJlJkmuvvTZf//rXM2vWrNTX12fhwoU5/PDDM2nS\n",
              "pBxyyCEZPXr0q+a644478p73vCdDhw7NkCFD8uMf/zhJ8qlPfSojR45MfX19Ro0alQcffHCzs6xd\n",
              "uzarV69u8Vi7dm1r/4gBAABgq2mXeDBkyJC85z3vyd57752TTjopX/va17J48eIkybp16zJmzJh8\n",
              "/vOfz4IFC3LZZZfl5JNPbrES4LVceeWVSZK5c+emsbExu+66a5KksbExN910UxYtWpSlS5fmxhtv\n",
              "fM3XL1y4ME888USOOeaYnHPOOZk+fXqSZNy4cTn33HNz5plnprGxMQcccECS5KGHHspvfvOb/OpX\n",
              "v2pxnOXLl+fEE0/M1KlTc99996WxsbH5koYpU6Zk3rx5aWxszHnnnZdPfOITm30/U6dOTc+ePVs8\n",
              "pk6d+rf+aAEAAGCrq22Pg3bq1Ck33nhjHnjggfzP//xPbrnllnz5y1/O/Pnz8+KLL6ZTp0455phj\n",
              "kiTvec970rt37zQ2NmbPPfds87lOOumkdOvWLUly4IEH5tFHH33N/aZPn55x48alc+fOOe644zJx\n",
              "4sQsWrQo+++//2vuP3bs2HTp0uVV2++4447su+++zcGgU6dOzSss5syZk8svvzxr1qzJpk2bsnz5\n",
              "8s3OfdFFF2Xy5MktttXV1f3tNwwAAABbWbt+28J+++2XiRMnZvbs2Tn44IPzk5/85DX3e+XShNra\n",
              "2mzcuLF5e2vub9C1a9fmnzt37pwNGza8ap/169dnxowZ+e53v5u+ffumf//+eeGFF5pXH7yWHXfc\n",
              "8W+e+/96/PHHc/755+d73/te/vSnP+W///u/i/PX1dWlR48eLR7iAQAAAB1Ru8SDxYsXN9/HIElW\n",
              "rFiRxx57LPvss0/23XffbNq0KXPmzEmS/P73v8+SJUtSX1+f3XbbLZVKJQsXLkzy8v0I/q/u3btn\n",
              "1apVbZ7nJz/5Sfr165fFixenqakpTU1N+cMf/pAZM2Zk/fr1bTrWoYcemocffrj5BoqvrDBYtWpV\n",
              "unTpkt133z2VSiVXXHFFm+cEAACAjqhdLlvYsGFDLrnkkjz22GPp1q1bNmzYkPHjx+eEE05I8vLX\n",
              "OE6aNCkXXnhhunbtmh/+8IfN/9J/+eWX5/jjj8/OO++cU045pcVxL7zwwrz3ve9Nt27d8otf/KLV\n",
              "80yfPj1nnnlmi237779/9thjj9x0001tem877bRTfvSjH+XCCy/MmjVr0qlTp3zxi1/MBz7wgZxx\n",
              "xhkZOHBgdt5555x44oltOi4AAAB0VDWVSqVS7SGA1hkzZkySlwMcAADA1tKu9zwAAAAAtn3iAQAA\n",
              "AFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4A\n",
              "AAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXi\n",
              "AQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQ\n",
              "JB4AAAAAReIBAAAAUCQeAAAAAEW11R4AaL3evXtXewQAAGA7VFOpVCrVHgIAAADouFy2AAAAABSJ\n",
              "BwAAAECReAAAAAAUiQcAAABAkXgAAAAAFIkHAAAAQJF4AAAAABSJB9uZtWvX5gtf+ELWrl1b7VGg\n",
              "1Xxu2db4zLIt8rllW+Mzy7ZoW/7c1lQqlUq1h2DrWb16dXr27JlVq1alR48e1R4HWsXnlm2Nzyzb\n",
              "Ip9btjU+s2yLtuXPrZUHAAAAQJF4AAAAABSJBwAAAECReLCdqaury+c///nU1dVVexRoNZ9btjU+\n",
              "s2yLfG7Z1vjMsi3alj+3bpgIAAAAFFl5AAAAABSJBwAAAECReAAAAAAUiQcAAABAkXiwHXn44Ydz\n",
              "6KGHZsCAARk5cmTuv//+ao8ERZMmTUrfvn1TU1OTxsbGao8Df9NLL72UE088MQMGDMjQoUPz3ve+\n",
              "N4888ki1x4K/afTo0RkyZEjq6+tz2GGH5d577632SNAqV199dWpqajJ79uxqjwJ/U9++fbPvvvum\n",
              "vr4+9fX1mTlzZrVHahPxYDsyceLETJgwIQ899FCmTJmSs846q9ojQdEpp5yS3/72t9l7772rPQq0\n",
              "2oQJE/Lggw/mvvvuywknnJCPfOQj1R4J/qbrr78+CxYsSGNjYyZPnuz/I7BNaGpqyn/+53/m4IMP\n",
              "rvYo0GozZ85MY2NjGhsbc/rpp1d7nDYRD7YTf/nLXzJ//vyMHTs2SXLyySfniSee8C9idGijRo3K\n",
              "nnvuWe0xoNW6du2a4447LjU1NUmSgw8+OE1NTdUdClqhV69ezT+vWrWq+TMMHdWmTZvykY98JJdf\n",
              "fnnq6uqqPQ5sF2qrPQBbxxNPPJHdd989tbUv/09eU1OTPn365PHHH0///v2rPB3Am9M3v/nNnHDC\n",
              "CdUeA1pl3Lhx+fWvf50kufnmm6s8DZRddtllefe7350RI0ZUexRok3HjxqVSqeTAAw/MV77yleyy\n",
              "yy7VHqnVrDwAgHZw6aWX5pFHHsnUqVOrPQq0yrXXXpsnnngiX/rSlzJlypRqjwOb9ac//Sk33nhj\n",
              "PvvZz1Z7FGiT3/zmN1mwYEHuueeevO1tb8v48eOrPVKbWHmwndhrr73y9NNPZ8OGDamtrU2lUsnj\n",
              "jz+ePn36VHs0gDedadOmZdasWfnlL3+Zbt26VXscaJPx48fn3HPPzbJly7LzzjtXexx4lblz56ap\n",
              "qSnvfOc7kyRLlizJhAkT8vTTT6ehoaHK08HmvfJ3ry5duuSCCy7IgAEDqjxR21h5sJ3YddddM3z4\n",
              "8Hzve99Lktx4443Zc889XbIAsIVddtll+cEPfpA5c+a0uI4cOqqVK1fmqaeean4+e/bs7Lzzznnr\n",
              "W99axalg8xoaGvL000+nqakpTU1NOfjgg/Mf//EfwgEd2vPPP5+VK1c2P//BD36QYcOGVW+g18HK\n",
              "g+3IVVddlbPOOiuXXnppevTokauvvrraI0HRxIkT87Of/SxLlizJMccck+7du7vJJx3ak08+mQsv\n",
              "vDD9+vXLEUcckSSpq6vLnXfeWeXJYPNWrVqVU089NS+++GI6deqUXXbZJT/96U/dNBFgC1q6dGlO\n",
              "PvnkbNy4MZVKJf369cu1115b7bHapKZSqVSqPQQAAADQcblsAQAAACgSDwAAAIAi8QAAAAAoEg8A\n",
              "gFZbs2ZNdtxxx5xzzjnVHqWF22+/Pbfeemtxn5/+9Kc599xzm/evr69v8fumpqYW35Bx5ZVXZsiQ\n",
              "Iamvr89+++2XM888s/l3ffv2zb777puhQ4emf//+OeGEE/L73/++xbkmTJjwxt8YAHQQ4gEA0Goz\n",
              "Z87MiBEjMmvWrDz33HPVHqdZa+LBRRddlIsuuqhVx5s/f36++tWv5vbbb09jY2MWLVqUCy+8sMU+\n",
              "M2fOzH333ZdHHnkk48ePz3HHHdf8zRrHH3987r777jz88MOv7w0BQAcjHgAArTZ9+vRMmTIlo0aN\n",
              "ysyZM5u3X3PNNTn66KPzj//4jznggANy6KGHZuHChTnppJOy//77Z/To0c2x4bnnnsuHP/zhDBo0\n",
              "KIMGDcrFF1/cfJzDDz88s2fPbn5+yimn5JprrkmSnHXWWZk4cWKOOuqoDBgwIGPGjMm6devS2NiY\n",
              "K6+8Mtddd13q6+tzySWXvGruuXPnplevXtl7771b9T6ffPLJdO/ePd27d0+S1NTUZPjw4Zvdf8yY\n",
              "MTn33HMzbdq05m2nnXZa/uu//qtV5wOA1po0aVL69u2bmpqaNDY2tuo1K1asyJlnnpkBAwZk4MCB\n",
              "+ad/+qc2n1c8AABaZeHChXniiSdyzDHH5Jxzzsn06dNb/H7evHn513/91yxcuDD77LNPPvCBD+TK\n",
              "K6/MokWL8pa3vCXf/e53kyRf/OIXs3bt2ixYsCB33nlnZs+e3SJElDQ2Nuamm27KokWLsnTp0tx4\n",
              "442pr6/PueeemzPPPDONjY35f//v/73qdbfffnsOOuigVr/X0aNHp3v37unTp09OP/30XHHFFVmx\n",
              "YkXxNQcddFDuv//+5ueHHHJIbrvttlafEwBa45RTTslvf/vbVgfxJPnwhz+cYcOG5aGHHsr999+f\n",
              "Cy64oM3nFQ8AgFaZPn16xo0bl86dO+e4447LY489lkWLFjX//pBDDkmfPn2SJO9617sycuTI9O7d\n",
              "O0kycuTI5iX8v/zlL/PRj340nTp1yt/93d9l3LhxmTNnTqtmOOmkk9KtW7d07tw5Bx54YB599NFW\n",
              "ve7JJ59sniV5eSXBa3lle7du3TJ37tzcfPPNefe7351Zs2ZlyJAhWb58+WbPUalUWjzfbbfd8uST\n",
              "T7ZqPgBorVGjRmXPPfd81fZ58+blyCOPzLve9a4MGzYsN9xwQ5LkkUceyfz58zN58uTmfXfbbbc2\n",
              "n1c8AAD+pvXr12fGjBn57ne/m759+6Z///554YUXWqw+6Nq1a/PPnTt3ftXzDRs2vOax/+9f5Gtr\n",
              "a7Nx48bm5y+99FKLfVt7zL/WrVu3FsfaZZddsmzZshb7PPvss9l1111bzDVs2LBMmjQpt912W3bc\n",
              "ccfcfvvtmz3HvHnzMmjQoBaz77DDDq2aDwDeiJUrV2bChAm57rrrMn/+/MyZMycXXnhhFi9enIUL\n",
              "F2bPPfdMQ0NDRowYkdGjR+fee+9t8znEAwDgb/rJT36Sfv36ZfHixWlqakpTU1P+8Ic/ZMaMGVm/\n",
              "fn2bjnX00Udn+vTpqVQqef755zNjxoyMHj06SdK/f//mmw4+9thj+e1vf9uqY/bo0SOrVq3a7O+H\n",
              "DBmSBx98sPn5O9/5znTp0iU333xzkmTTpk256qqrmud44IEHsmDBgub9n3jiiTzzzDPp16/fax7/\n",
              "xz/+cb797W+3uKniokWLMnTo0FbNDwBvxO9///v8+c9/zrHHHpv6+vocffTRSZIHH3wwGzZsyF13\n",
              "3ZUzzjgjd999dz75yU/m+OOPb/N/v2vbY3AA4M1l+vTpLb6qMEn233//7LHHHrnpppvadKzPfe5z\n",
              "mTRpUgYPHpwkOfXUU3PaaaclST7zmc/k9NNPz+DBgzNw4MBW36fgpJNOyowZM1JfX58xY8a86r4H\n",
              "xx9/fC655JJs3LgxnTt3TpcuXfKjH/0okydPzj//8z9n06ZNOeigg/LlL385SfLCCy/kk5/8ZJYs\n",
              "WZIddtghlUolX/nKV1p8vePpp5+erl275vnnn88BBxyQm2++ucW8t956a0455ZQ2/dkAwOtRqVQy\n",
              "cODAFl8b/Ir58+dnjz32yBFHHJEkOfbYY7Nu3br87//+b/r379/qc9RU/voCPQCAN6GPfexjOfzw\n",
              "w3Pqqae2+7meffbZHHnkkZk/f37e8pa3tPv5ANj+9O3bN7Nnz059fX1WrFiRAw44IDNmzGheddDY\n",
              "2JgDDjggXbp0yeDBg/P9738/Q4YMyV133ZXjjjsuixcvTl1dXavPJx4AANuFZcuW5ZZbbsnYsWPb\n",
              "/Vx33nlnNm7cmEMPPbTdzwXA9mXixIn52c9+liVLlmTnnXdO9+7d88gjj+See+7Jpz71qSxbtizr\n",
              "169Pnz59Mnv27HTt2jV33313zjvvvLz44oupq6vLtGnT8vd///dtOq94AAAAABS5YSIAAABQJB4A\n",
              "AAAAReIBAAAAUCQeAAAAAEXiAQAAAFAkHgAAAABF4gEAAABQJB4AAAAAReIBAAAAUCQeAAAAAEX/\n",
              "H1hHUQZQeT+zAAAAAElFTkSuQmCC\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-5d16b6f6-41c5-4329-b462-1765a3b20fe9\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-5d16b6f6-41c5-4329-b462-1765a3b20fe9\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:5: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "figsize = (12, 1.2 * len(_df_22['Transaction Type'].unique()))\n",
              "plt.figure(figsize=figsize)\n",
              "sns.violinplot(_df_22, x='Amount (USD)', y='Transaction Type', inner='stick', palette='Dark2')\n",
              "sns.despine(top=True, right=True, bottom=True, left=True)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-ca95ce29-2156-48cf-b894-22524f6bf0c1\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABCsAAAGnCAYAAACegd1VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAABauElEQVR4nO3deXxU5aH/8e+ZJftM9gRc2EShECAgCFhFUURLXSouqPVSr1Sq\n",
              "XsX+1KuXVqty29Jarb1Xa1GL2uJ6FdyqVkXBpUUrStxARRYRDdn3ZJKZOef3x8wcMiSBCSbkQD7v\n",
              "1+t5nees8xwmQ5Jvnuc5hmVZlgAAAAAAABzC1dcNAAAAAAAAaI+wAgAAAAAAOAphBQAAAAAAcBTC\n",
              "CgAAAAAA4CiEFQAAAAAAwFEIKwAAAAAAgKMQVgAAAAAAAEchrAAAAAAAAI5CWAEAAAAAAByFsAIA\n",
              "AAAAADgKYQUAAAAAAHAUwgoAAAAAAOAohBUAAAAAAMBRCCsAAAAAAICjEFYAAAAAAABHIawAAAAA\n",
              "AACOQlgBAAAAAAAchbACAABgP3fZZZfpsssu6+tmAADQYzx93QAAAAB8O2VlZX3dBAAAehQ9KwAA\n",
              "AAAAgKMQVgAAAAAAAEchrAAAAAAAAI5CWAEAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUwgoAAAAA\n",
              "AOAohBUAAAAAAMBRCCsAAAAAAICjEFYAAAAAAABHIawAAAAAAACOQlgBAAAAAAAchbACAAAAAAA4\n",
              "CmEFAAAAAABwFMIKAAAAAADgKIQVAAAAAADAUQgrAAAAAACAoxBWAAAAAAAARyGsAAAAAAAAjkJY\n",
              "AQAAAAAAHIWwAgAAAAAAOAphBQAAAAAAcBTCCgAAAAAA4CiEFQAAAAAAwFEIKwAAAAAAgKMQVgAA\n",
              "AAAAAEchrAAAAAAAAI5CWAEAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUwgoAAAAAAOAohBUAAAAA\n",
              "AMBRei2saGtr0/XXX6/hw4frO9/5jsaMGaO//OUvccfcdNNNGjlypCZPntzpemduvvlm/fSnP+2t\n",
              "Zncwa9YsFRcXq7i4WIZhaMyYMSouLtaxxx7b6689b948jRo1SmeeeWavvxYAAAAAAE7h6a0LX3TR\n",
              "RWptbdUHH3yg9PR0bd26Vd/73vcUCoU0b948SdKtt96qzZs3a+DAgZ2u95ZQKCSPJ7Fbf+GFF+y6\n",
              "YRh68803lZWVtdfXS1RZWZkee+wx1dfXy+12J3yeaZqSJJeLTjMAAAAAgP1Tr/xGu3HjRj399NO6\n",
              "9957lZ6eLkkaMmSIbr/9dt1yyy2SpKOPPlqBQEAzZ87UggULOqxv3LhR3/3udzVu3DiNGTNGN9xw\n",
              "g3390tJSnXbaaRo1apROOOEEVVdXS5LC4bD+8z//U0VFRSoqKtKVV16ptrY2SZHw5OKLL9a0adNU\n",
              "VFQkSVq2bJkmT56sCRMmaNq0afrggw8Svsfjjz9eCxYs0NSpUzVz5kyFQiGdfPLJmjhxokaPHq0L\n",
              "LrhATU1NkqTVq1erqKhIl19+ucaNG6fRo0dr7dq1kqSKigrNnDlTY8aM0dixY/Xv//7vqq2t1fTp\n",
              "0xUIBHTkkUfqN7/5jSTptttu01FHHaUJEybolFNO0Zdffikp0tvkrLPO0sknn6yioiKVlpbu9XsH\n",
              "AAAAAEBf65WeFevWrdPhhx+u3NzcuO1Tp07VV199pYqKCv3zn//s0FOh/fpVV12lU089VQsXLpQk\n",
              "O5CQpHfeeUfvvfeecnNzdd555+mee+7RwoULde+99+rdd9/Ve++9J7fbrdNPP1133HGHrr/+eknS\n",
              "e++9p7feeks+n0//+Mc/9Oijj+qNN95QcnKy3nzzTV1wwQX65JNPEr7Pzz//XG+88Ya8Xq8sy9Ij\n",
              "jzyi3NxcWZalyy+/XHfeeaf+67/+S5L06aefaunSpbr77ru1ZMkS/fznP9dLL72khx56SEOHDtXL\n",
              "L79s32dWVpZeeOEFFRcXq6SkRJL0yCOP6LPPPtOaNWvkdru1bNkyXX755Xr++eclSWvWrNG6detU\n",
              "WFjYaVtbW1vV2toaty05OVnJyckJ3y8AAAAAAPuCY8cKTJs2Tffdd59+/vOf6+WXX44benHKKafY\n",
              "QcjUqVO1adMmSdLKlSt10UUXKTk5WR6PR5dccoleeeUV+7xzzjlHPp9PkvTMM8/ogw8+0OTJk1Vc\n",
              "XKwrr7xS1dXVamlpSbiNF154obxeryTJsizdcccdGj9+vMaOHavnn3/eDhokafjw4fZcHO3bPGXK\n",
              "FL344ou65ppr9Mwzz9g9UXb19NNPa+XKlTryyCNVXFysW2+9Vdu2bbP3z5o1q8ugQpIWL16szMzM\n",
              "uLJ48eKE7xUAAAAAgH2lV8KK8ePHa+PGjaqqqorbvmbNGh166KHKz8/f4zXOOuss/eMf/9CIESN0\n",
              "11136dRTT7X3paSk2HW3261QKNTpNQzDiFvPyMiw65Zl6Uc/+pFKSkrsUlpaqtTU1ITucdfrPfLI\n",
              "I3rttdf0+uuv66OPPtK1116rQCCwxzZPnTpVJSUlmjx5slasWKFJkyYpHA53eC3LsrRw4UK7rR99\n",
              "9JE++uijTtvSmYULF6quri6uxHqtAAAAAADgJL0SVhx++OE67bTTNH/+fDU3N0uStm7dqmuuuUY3\n",
              "3nhjQtfYuHGjCgsLNXfuXN166616++2393jOjBkz9Ne//lVtbW0KhUL685//rJkzZ3Z67Omnn66H\n",
              "HnrI7p1gmqY9j8TeqKmpUV5envx+vxoaGvTggw8mdN6WLVuUkZGhc889V3feeac+//xzNTY2djju\n",
              "Bz/4gZYsWWIPhwkGg1q3bl3C7UtOTpbf748rDAEBAAAAADhRrz0N5K9//atuuOEGjRkzRklJSXK7\n",
              "3frP//xPXXzxxQmd/+STT+qhhx5SUlKSTNPUkiVL9njO/PnztWnTJk2YMEFSZBLMrh5zeuyxx+rW\n",
              "W2/VmWeeqVAopLa2Nn3/+9/XxIkTE77H9ubOnatnnnlGI0aMUH5+vo499lh7AszdWb16tX7/+9/b\n",
              "vS1+97vfKTMzUzU1NXHH/fCHP1RVVZWmT58uKfIEkosvvljjx4/fq/YCAAAAAOBUhmVZVl83AgAA\n",
              "AHtv9uzZkqQVK1b0cUsAAOgZjp1gEwAAAAAA9E+EFQAAAAAAwFEIKwAAAAAAgKMQVgAAAAAAAEch\n",
              "rAAAAAAAAI5CWAEAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUwgoAAAAAAOAohBUAAAAAAMBRCCsA\n",
              "AAAAAICjEFYAAAAAAABHIawAAAAAAACOQlgBAAAAAAAchbACAAAAAAA4CmEFAAAAAABwFMIKAAAA\n",
              "AADgKIQVAAAAAADAUQgrAAAAAACAoxBWAAAAAAAARyGsAAAAAAAAjkJYAQAAAAAAHIWwAgAAAAAA\n",
              "OAphBQAAAAAAcBTCCgAAAAAA4CiEFQAAAAAAwFEIKwAAAAAAgKMQVgAAAAAAAEchrAAAAAAAAI5C\n",
              "WAEAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUT183AAAAAN9OYWFhXzcBAIAeZViWZfV1IwAAAAAA\n",
              "AGIYBgIAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUwgoAAAAAAOAohBUAAAAAAMBRCCsAAAAAAICj\n",
              "EFYAAAAAAABHIawAAAAAAACOQlgBAAAAAAAchbACAAAAAAA4CmEFAAAAAABwFMIKAAAAAADgKIQV\n",
              "AAAAAADAUQgrAAAAAACAoxBWAAAAAAAAR/H0dQMAAAC+LcuyZJqmwuGwwuGwTNNUKBSy610Vy7IU\n",
              "DodlWdZuS3cYhiGXyyXDMCQpru52u2UYhr10uVwditvttovL5ZLH47H3AQDQXxBWAACAbgmHwwoE\n",
              "Amptbd1taWtr61CCwaBaW1sVDAbt9fYlFArZ+2L1UCjUoYTDIYVCYYVDIYWigcSBLhZyuN0uedwe\n",
              "eTzR4vXI4/Ha6263W0lJSfJ6I9u8Xm9c8Xg8SkpKsovX641bj5Xk5OQOpf32lJQUJSUl2UEMAAA9\n",
              "ybC6++cCAACwX2hra1Nzc7Oam5vV1NRk15ubm9XS0rLH0traqkAg0K60KBAIKBgM9Xrb3YYljxFZ\n",
              "umJLWXK32+YyLLm1c91Q7NjIPkORpUuSYSi6jG2XDEXOMex6pKjdPrXbtkeWZO2s2lewdtluWvHb\n",
              "LUsyo8eaVmS/Gd0elmFvsySFo3V7KSlsRY4JW4ZMGQpH94fNyHV7k8vlsoOLzkpqauoeS3p6utLS\n",
              "0uJKamoqPUkAoJ8jrAAAwGFM01RLS4saGhrU0NCgxsZGuzQ0NKipqUmNjY12CBFbj9WbmhrV3Nyi\n",
              "UOjbhQoel6Ukw5LXZclrWEpymXbd64ptl7wuS552x3narXsMydNum8dQu/rOQCJSIvti4QK+vUiI\n",
              "EQkyQrEQwzIUMg2FLCkU3R63bka2BaP1oGkoGN0XjK7bdctQW3Rbm2koaLki28KGvu0PmKmpqUpP\n",
              "S1N6RobS09PjSsYu23w+nzIyMuxlrO71envk3xEAsO8RVgAA0Assy1IgEFB9fb3q6ursZUNDg+rr\n",
              "63e7bG5u7vawBpcspbgtJbtMJceWLitaTCW5rfh1l6VkdyyEiGzzRvcnRYMIF4EB9pJlSSFL0SDD\n",
              "pdZomBFfIttb29fDRnRb9JywoYDpUqvpUpvZ/S/I5OQk+TJ88vn98vl88vl88kfr7Zd+v1+ZmZn2\n",
              "MiMjQ263uxf+ZQAAiSKsAABgDyzLUnNzs+rq6lRbW6va2tpO6+1Difq6OrUFgwm/RorbUoorrFS3\n",
              "pRSXqVS3GdnmNpXqiixT3KZSXJZS3TsDiZToPo9BbwQc2MLR8CMQjgQZgbBLgdgyGmq0hGPbd9Zb\n",
              "2q2HrcQ+JIZhKCMjIy7AyMrKilvG6u3XPR6mgwOAnkJYAQDolwKBgKqrq1VTU2Mva2trVV1dbYcQ\n",
              "sXpNTXVC8zQYkrwuU36PqTS3qTRPJHRIi5ZUexkJHFKj9RS3KTdBA9CrrGjY0RI21GJGQoxIMdQS\n",
              "dqk57FJzKLKMrde0uRVWZJhLIvx+n7KyspWdHV+ysrKUnZ2tnJwce+n3+5mXAwB2g7ACAHDAiAUQ\n",
              "VVVVdqmuru60tLS07PF6KW5L6e6w0t2m0j1hpXtMpbt3BhHp7ZfRnhC3fV6g60aU74O7BdDbbv0s\n",
              "8nkOmrLDjKZokNEUq0eXTSF3u6Uhcw+9OFwul7KzspSTm2sHGDk5OcrNzbWXsUKwAaA/oq8aAMDR\n",
              "LMtSQ0ODqqqqVFlZqcrKyg71qqoqVVVWqqm5ebfXchuWMjymctxhpWeY8nkiIUSGx1RGNHTIaBdK\n",
              "ePjdAIAkr0vKdJnK9CY2l4xpSYGwYYcXjdFAozHkUmN0vTHkUlNjq7bXVWljePfBhtvtjgsw8vLy\n",
              "lJeXZ9dzc3OVn5+vnJwchqIAOGDwvxkAoM80NzersrJSFRUVdvhQUVGxc72iQhWVlWpra+vyGi5J\n",
              "6Z6wMj1hHRwNIHyesDK8kaXPEwkgMjyR+R2Y1wFAb3MZUprHUponrPzk8B6PD5lSU9ilhmiQ0RCM\n",
              "LqPhRkPIpYa6Vm2pKtenuxmSYhiGMjMzVVBQYAca+fn5HZZZWVlMIArA8QgrAAA9zjRN1dTUqKKi\n",
              "QuXl5XYAESuRbeVqauq6J4TLsOT3mCr0hOX3h+0Qwucx5fOG5feE5fNGekDw1AoA+zNPN3putIaN\n",
              "SHgRDTHqg+2XbjW0tOmrTTX6/PPPu7yG2+1WXm6u8gsKlJ+fH1cK2m1LSUnpydsEgG4hrAAAdEss\n",
              "iCgrK1N5eXmnpaK8XMFQ1xNSpntM+T1hHeKL9IiIhA+m/NEQwu+NzAFBCAEA8ZLdlpLdYeXtocdG\n",
              "m2moPhgJNeqDLtWH3KoPuqNLl+prW/VpRZk+3s3cGlmZmSooLLRDjIKCAhUWFtr1goICAg0AvYaw\n",
              "AgBgsyxL9fX1Ki8v144dO+xAYtdlqIsgwiXJ7w3roKSw/Glh+b1hZUZL+zCCuSAAoHcluSzlJe8+\n",
              "1LCsyMSh9UGX6qJhRl3QrfqQK7IMBLVtU+1ue2lk+v0qHDDADjJiYcaA6LaCggLm0QCwV/ifAwD6\n",
              "kdbWVjt0iIURsbJjxw6Vl5WpJRDo9FyXJJ83rIOTwspMj/SIyPTGlwwPvSEAYH9hGJGebukeUwPV\n",
              "dW+41rChuqBbdSG36oKRIMMurUFt21TXZaBhGIby8vLsICNWBgwYYC8zMzNlMKEQgF0QVgDAAcKy\n",
              "LNXV1WnHjh12EBFfL1V1dU2X52d4TOV4Q8ryR4KHrHYhRFY0iHDzsyQA9DvJbksF7pAKdhNoBKKB\n",
              "Rm00xKhtF2jU1pXq08pyffzxx51fPznZDi5ipf16QUGBvF5vb90eAIcirACA/UQoFFJFRYXKyspU\n",
              "WloaF0iUlpaqrGyHAoHWTs/1uixleUM6PGNnCJG1SyDhZWgGAGAvpbgtpbhDKkzpPNCwrMgTT2Ih\n",
              "Rk3bzlCjNtim6m9atG3btk7PNQxDubm5GjhwYIdQY8CAARo4cKDS09N78/YA9AHCCgBwiEAgYPeE\n",
              "2BlA7AwjKioqZJqdzxSf4TGV6w0p278zhMhK2llPc5s8shMA0GcMI/K9KsNj6pDUYKfHBE3t7I0R\n",
              "dKsm6FZtmycSaNTv0PqqCn30UeffzDIyMjoEGLEeGgMHDlROTg5DTYD9DGEFAOwDlmWpoaEhLojY\n",
              "NZiora3t9FyXLPm9pganhOwAItsOI0LKolcEAOAA4HVpt5OCmpbUFHJFQoygW7VtsZ4ZHtUEg9q+\n",
              "pUFffPFFp+cmeb0q7CTMaD/UhIlAAWfhEwkAPcA0TVVXV3cZROzYsUPNzc2dntt+iEZ2u14RkXpI\n",
              "Pi9zRQAA4DIkn9eUz2tqkDrvnREIG9EeGdGeGUFPtN6mmtIv9dVXX3V+bZeh/PyCDiFG+x4aPKYV\n",
              "2LcIKwAgAcFgUBUVFV0GEeVlZWoLdv6DU6o7EkYM9kV6QWQnte8ZEVY6QzQAAOgRKW5LA90hDexi\n",
              "7oygKbs3xs5AIxpuVH+jD8vKVNLFtbOzszvMm9E+3PD5fAw1AXoQYQUASGpqarLniGg/V0SsVFZW\n",
              "yrKsTs/1ecIq9IaVlRoJImJDM7KjYUSKu/PzAADAvuV1SfnJYeV3MdQkbEkNwV1CjFi9qVwbP63W\n",
              "+vXrOz03LS0t7mkm7YONgQMHKjc3V263uzdvDzigEFYAOOCFw2FVVVXFhQ9xj/XcsUONTU2dnus2\n",
              "LGV6wxqW1n7iypCy2z1Rw8N8EQAAHBDchiI9H5M6DzPaP9Vk154ZtcE27fiyUZs3b+782m638vPz\n",
              "7RAjFmS0DzbS0tJ68/aA/QphBYD9mmVZqq+vV3l5uR1AlJWV2WXHjh27fYpGbIjGob6dPSLaP0Uj\n",
              "w2PKRY9OAACgxJ5q0ho2Io9njT2mtf1koFVf64MdO9T5TyU7n2qya5gRK/n5+UwEin6Dr3QAjtbU\n",
              "1GQHEWVlZXH1srIyVZSXqyUQ6PRcl2Ep0xPW4JRoL4h2IUSsJDNEAwAA9KBkt6UCd0gFXcybYVpS\n",
              "QyjSO6OmzRP3qNa6YFBf7+apJoZhKDc31w4vCgoKVFBQELfOcBMcKAgrAPQJy7LU2Nio8vLy3ZQy\n",
              "NTV1/gQNKfKXjRxvSFn+nUMystoNz6BXBAAAcBqXIWV6TWV6TQ1O67p3Rm20Z0aHZf0OfVZVoU8+\n",
              "+aTz67tcys/LU0G7MGPXkpubSw8NOB5foQB6XCgUUnV1tSoqKlRRUaHy8nK7bq+XlyvQ2trlNdI9\n",
              "pvyesAb5wvK3CyFiQYTfw1wRAADgwJTstlToDqmwi94Zsbkz6uwgY2e9PuhWbc03Wl9Rpo+szv9q\n",
              "YxiGcrKzVRAdWpKfn6+CggK7Hivp6em9eZvAbhFWAEiYaZqqra1VZWWlKisrVVVVFRdCVFZWqqK8\n",
              "XNU1NV0+OcOQlOEJK9cbVqYvLL/XtEOITG9YmZ5IOOEliAAAAOhU+7kzDu5i7gzTkprtQGNnmFEX\n",
              "dKs+5FZ9Y7m+qK3Shg1dd0NNTU21g4u8vLy4em5urvLz85Wbm6uUlJTeulX0Y4QVABQMBlVdXW0H\n",
              "EO3rsWCisrJS1dXVCoc7nx1bkpJdlvyekIamRXpF+L1h+T2RMMIf7Q3h85pyMzQDAACgV7niAo3O\n",
              "j7EsKWAaqrdDjEio0RCK9NCoD7ap5psmfbVtm3Y3y1dGerry2oUYsWWsnpOTo7y8PGVkZMgw+EEQ\n",
              "iSGsAA5QoVBINTU1qq6ujiuxMCJWKisrVF/fsNtreV2WfJ6wDk0Oy+cx5fdGlj5PpDeEz2sq08Nk\n",
              "lQAAAPsTw4g8GS11N0NOJClsSY0hVyTACLlVH3SpIeRWQ9Cl+pBbDaGgKrc3aOvWrbt9vSSvVznt\n",
              "Aozc3Fzl5OQoOztbOTk5cdvS0tIINvo5wgpgP2GaphoaGlRbW6uamprdl+pq1dXXdzkUIybdY8rn\n",
              "DqsgPRI4+DyRECIjOl9EhjeyTHZZ4nsFAABA/+RuNymo1PmwEyk+1GgMRQON6LIx5FJDsE0NVS36\n",
              "tLxUIXP3P1wmJycpOzvHDjN2LTk5OcrKylJ2draysrKUlJTUw3eNvkZYAfQBy7IUCARUW1ururo6\n",
              "u8TWa2tr7VAitqyrq5NpdvVU7ghDkQAi3R3WsDRTGZ7IEzEiAUSsHlmmexiOAQAAgJ4TH2p0zbKk\n",
              "VtNQQ8ilxliQ0a4eKW1qrGrWF+WlattDsCFJaWmpysrKjgswYiUzM9Nexuo+n49HvDocYQXwLcRC\n",
              "h4aGBtXX19vLurq6uOWu2+pqa9UW7DqVjjEkpUXDh8EpZrQeCRpi4UOGvW4q1c2jOgEAAOBshiGl\n",
              "uC2luMPKT+56PrSYNtNQkx1iuNQU3hlsNEXXm0NtaqpoVMWOrxVMINwwDEM+X4YyMyMhht/vl9/v\n",
              "t+vtt/n9fvl8Pvn9fmVkZBBy7COEFej32tra1NjYqMbGRjU1NamhoUGNjY2dLmMlEkzUq6G+QcFQ\n",
              "1+P72jMkpbojgUKhx1R6SiR4SHNHQoi0aOiQ5o5uj9YJHwAAANCfJbksJSWFlZ2052BDig83msPR\n",
              "Egs1ovXmsEtNbW2q21Gr0q9dexyW0l5Gerp87QIMn89nl4yMDLvE1ttvT01NlcvFY+8SQViB/ZJl\n",
              "WWppaVFLS4uamprU3Nys5uZmNTU12euxemw9FkbELRsbE+rh0J7HsKITEYV1UJKllFRTqS4zGkRY\n",
              "dsiQFg0mYvUUt0XwAAAAAPSy7oYbkhQ0FRdstIR3Bh0tYZdawkZ06VJLuE0tlXWqKXcrEO7eD/iG\n",
              "YSg9LU3p0fAiPT3dDjLS09OVlpam9PR0u7RfT0tLU1pamlJTU5Weni6P58D+df7AvjscEB5++GG9\n",
              "/PLLamlpUXNTk5qamxUIBPY4eWRXkl2Wkt2mUlymBngspSRHgoTkdoFDiiuyLdVtKsVtKtVlRZZu\n",
              "U16CUAAAAOCA4nVJma49z7exK9OSAmFDATMSZASioUbAjC6jQUfAdKk1ugyEWxWorlNpZeQ409q7\n",
              "v2gmeb1KTU1VWjTIyMvL0w033KD8/Py9up7TOCKsWLFihX71q18pHA4rEAjooIMO0sqVK+VyufSH\n",
              "P/xB5513ngYMGLDX17/oootUXFysn/70p90+d9asWfrmm28kSR988IGKiorkdrvl8/n05ptv7nWb\n",
              "EjFv3jytWbNGI0aM0FNPPdWrr+VkTz31lL7e/pV8XlPJLlM5LkvJaZaSXZaSXGYkfIgGELF6Sqwe\n",
              "Xaa0q9O7AQAAAEBPcBlSmsdSmsKSEu/JEWNZUtAyFAgbajUNtZmuaN2lVtPYWW+3v9WM1dvU2tas\n",
              "QEu1qsNubdq0SR9++KFOPPHEnr/RPtDnYUVpaanmz5+v9957T4MHD5Ykvf/++/Yzdf/whz/o+OOP\n",
              "/1Zhxbfxwgsv2HXDMPTmm28qKysr7phQKNTjXXDKysr02GOPqb6+vlsTuMSeFnGgjYPK9IZ1/Yjy\n",
              "vm4GAAAAAPQYw5CSDEtJrliv8e4HHpL0TnWanv4mq8fa5QR9/httWVmZ3G63cnJy7G0TJkyQYRha\n",
              "tGiRvvnmG82ZM0fFxcUqKSlRY2OjLr74YhUVFamoqEi33HKLfd7XX3+ts88+W2PGjNHYsWN14403\n",
              "dni9N998U6NGjdLatWtVUVGhmTNn2sf/+7//e8LtPv7447VgwQJNnTpVM2fOVCgU0sknn6yJEydq\n",
              "9OjRuuCCC9TU1CRJWr16tYqKinT55Zdr3LhxGj16tNauXStJnbahtrZW06dPVyAQ0JFHHqnf/OY3\n",
              "kqTbbrtNRx11lCZMmKBTTjlFX375pSTp5ptv1llnnaWTTz5ZRUVFKi0t7dDe1tZW+6kUsdLa2prw\n",
              "/QIAAAAAsK/0ec+KsWPH6phjjtHgwYN13HHH6eijj9YFF1yggw8+WL/4xS90//336/HHH1dxcbEk\n",
              "6frrr1dra6s+/PBDtbS06JhjjtHIkSM1Z84cXXjhhZo5c6aefPJJSZEgoL3HH39cixcv1vPPP6+h\n",
              "Q4fqjjvu0NChQ/Xyyy9Lkqqrq7vV9s8//1xvvPGGvF6vLMvSI488otzcXFmWpcsvv1x33nmn/uu/\n",
              "/kuS9Omnn2rp0qW6++67tWTJEv385z/XSy+9pIceeqhDG7KysvTCCy/YAY0kPfLII/rss8+0Zs0a\n",
              "ud1uLVu2TJdffrmef/55SdKaNWu0bt06FRYWdtrWxYsXxwU7knTTTTfp5ptv7tY9AwAAAADQ27od\n",
              "VpSWluqzzz7T8ccfr1AoJNM0lZSUtNcNcLlcWr58uT799FO9/vrrevHFF/WrX/1Ka9eu1fDhwzsc\n",
              "v3LlSt1+++1yuVxKT0/X3Llz9corr+j73/++3nrrLb300kv2se0nFlm2bJncbrdWrVql7OxsSdKU\n",
              "KVN0xx136JprrtG0adN0yimndKvtF154obxer6TI0ynuuOMOPf/88wqFQqqrq9PRRx9tHzt8+HBN\n",
              "njxZkjR16lTddttt3WrD008/rXfffVdHHnmkJCkcju8eNGvWrC6DCklauHChrr766rhtycnJ3bpf\n",
              "AAAAAAD2hW4NA3nyySc1ZcoUXXTRRZKkTz75RD/4wQ96pCEjR47UT37yEz399NOaMmWKnn322YTO\n",
              "i81tsSdjx45VdXW1PvroI3vb1KlTVVJSosmTJ2vFihWaNGlShxBgdzIyMuz6I488otdee02vv/66\n",
              "PvroI1177bUKBAL2/pSUFLvudrsVCoW61QbLsrRw4UKVlJSopKREH330Udy9tG9LZ5KTk+X3++MK\n",
              "YQUAAAAAwIm6FVYsXrxY77//vt0zYdy4cfa8CXvr66+/1j/+8Q97vaamRlu2bNFhhx0mSfL7/aqr\n",
              "q7P3z5gxQ0uXLpVlWWpqatKyZcs0c+ZMZWRkaNq0abr99tvtY9sPAxk3bpyee+45XXzxxfr73/8u\n",
              "SdqyZYsyMjJ07rnn6s4779Tnn3+uxsbGvbqPmpoa5eXlye/3q6GhQQ8++GBC5yXahh/84AdasmSJ\n",
              "PVQlGAxq3bp1e9XW/VFjyK2Ht2Xrye1ZevYbv17a4dOq8gz9sypda2tS9VFdij5vSNaXzV6VBTyq\n",
              "bYvMomvu3dNNAQAAAGCfsywpaEoNIZcqW936usWrzY1J2lCfrHW1qXqnOk1vVKZrZZlPz5f69dTX\n",
              "mXrsqyy9XZ3e103vcd0aBuJ2u5Wbmxu37dsMAZEiT9JYtGiRtmzZorS0NIVCIf3oRz/SGWecIUla\n",
              "sGCBLrnkEqWlpenBBx/UjTfeqAULFmjMmDGSpHPOOUfnnnuupMhQjyuvvFKjR4+W1+vVGWecETdP\n",
              "w3e+8x299NJLmjVrln7961+rvr5ev//97+2eDr/73e+UmZm5V/cxd+5cPfPMMxoxYoTy8/N17LHH\n",
              "JhTkrF69utM21NTUxB33wx/+UFVVVZo+fbr973bxxRdr/Pjxe9Xe/cno0aO1fft2fVyfulfnJ7ks\n",
              "JbtMpbhMJbsjjzFNcbdbd5v2tlS3pRSXqRS3pdToMSluHncKAAAAYPfaP4a0JexSwIwuw5E/pAbM\n",
              "jsvWsBHZbxpqNd1qNSXT2rtfPtLT0zRkyJCevak+ZFiWlfDfnk888UQ98sgj+t73vqf3339fr776\n",
              "qn7961/r1Vdf7c02AgqFQmpubu5QWlpa1NzcrKamJnvZZb2xUU1NTWppNzwnUcmxMMNlKtUdK1a7\n",
              "+s71tOh6usdUsougA+hvbv2sQNfxqGXggMDnGeifgqbUHHapOexSSyi6DLvUEg0fdpad67EAItzN\n",
              "oMHlcikjI13p6RnKyMhQenq60tLSlJ6ebpe0tDR7W6yelpam1NTUuPWUlJSEp0nYH3SrZ8Vvf/tb\n",
              "fe9739PmzZt1zDHHaMuWLfbTKIDe5PF47Lk2vq1QKGQHGU1NTWpsbLRLU1OTGhoa1NjYqIaGBrvs\n",
              "XK9XeWOTTNNM6LUMyQ4z0tot0z3xy13rnj5/qDAAAACwf7MsKWAaaooGDs1h1856yKWm8M4gojnk\n",
              "UrMZqQfNxH/hT0lOli/TrwF+vzIyIoGDz+eLW8bq6enpdiAR256cnHxABQw9qVthxcSJE7Vq1Sr9\n",
              "85//lGVZOvroo5WVldVLTQN6h8fjUWZm5l4P+TFNU83Nzaqvr1dDQ0PcMlavq6tTfX29vayvr1dZ\n",
              "XZ3aWtoSeo0Ut6U0V1jpnkiIke42le4JR5eRkuExlRHd7iXcAAAAwAHOtKSWsEuNoUjo0BgNH5pC\n",
              "bjXF6vbSreawkdCQCsMw5PNlyO/P1KGZmfL7/cqMLmPF5/N1uow9HRI9r9uPLq2rq1NVVZUMw1BD\n",
              "QwNhBfqdSFetjD0+gaUzgUDADjFqa2tVW1ururo6u8TWa2trVVtTo7La2oQCjmS3pXR3WBluUxme\n",
              "8M4wwxNZ90WXGZ7I/ByEtwAAAHCCkCk1hV1qCLnVGHJFi1sNsUAi5I4GE5HlnuYwMAxDfp9PudnZ\n",
              "OiwrS1lZWcrMzLSXu9YzMzPl8/nkcvHXP6fpVljxyCOP6Morr9Rxxx0ny7L005/+VHfeeafOO++8\n",
              "3mofcEBJSUlRSkqKCgoKEjresiy1tLTYwUZtba1qamq6LDuqq9XWEtztNT2GpXSPKV+7EMMXW/fG\n",
              "r9NjAwAAAN1lWpE5HxpCLjUEIyFEQzSAaGgXRjSGPGoJ7/mvaJl+v/JycnR4drZycnKUnZ2t7Oxs\n",
              "ZWVlxS1j4YPb7d4Hd4ne1q2wYtGiRVq7dq2GDh0qSdq6datOOeUUwgqglxiGYU+Yc9BBB+3xeMuy\n",
              "1NzcrOrqatXU1KiqqkrV1dUdS1WVKquqtL2hdbfXS3Fb8ntCdnjh98aW4bhtSS6eEQsAAHCgMy2p\n",
              "KeRSfTRsqA/GLxtCbtVHe0jsbviFy2UoKytLh+TmKScnZ7clMzNTHk+3BwTgANCtdz0tLc0OKiRp\n",
              "yJAhSktL6/FGAdg7hmHYswYfeuihezy+ublZlZWVqq6uVlVVVYd6bLlpl0fp7ioWavg9pvzesPye\n",
              "sHzesDK9pvztwg2ejAIAAOBMgbChumAkbKgPRgKIWChRF3RHekYE3drdNPMpKcnKLcjT0Lw85eVF\n",
              "goi8vDzl5ubGlaysLHo/YI+6FVZ8//vf180336wf//jHsixLDzzwgE477TTV19dLUo88qQHAvpOW\n",
              "lqZBgwZp0KBBuz0uFAqpurpalZWVdqmqqlJFRYW9Xl5eri9qa7u8hiHZvTIyo4FGpjcSbmS2287Q\n",
              "EwAAgJ5jWpE5IeqDbtUF3aqLBhF1IXckkAi6VB/yqHU3T8BISkpSfkG+hhUUKC8aRLQvubm5ys/P\n",
              "V1paGk+2QI8xLMtKuP/27iYdMQxD4XC4RxoFYP8UDAbt8KKiosIu5eXl7dbL1dra9aSh6R4zGmRE\n",
              "S/s6gQb2A7d+VqDrRpT3dTMA9AA+z3A6y5Iaw65oCLEzjGi/Xh9yK9zFkAzDMJSdnaX8/AIVFBQo\n",
              "Pz+/Q8nLy5PP5yOEwD7XrZ4Vprm7Tj8A+juv16uBAwdq4MCBXR5jWZbq6+vjgoxYia2Xle1QaUNz\n",
              "l9dI95jK9ISUZQcYprK8IXvp85py8/0UAADs5wJhQ7VBt2qjwYO9bIv0jKgLdh1EuFwu5eflaVRh\n",
              "oQoKCuJKfn6+CqK9JJgPAk7Vra/Me+65R3PnzlVqampvtQfAAc4wDHum5uHDh3d5XFNTU1yQUVZW\n",
              "prKysrj6Nw0tnZ7rkuTzhqMBRlhZu5akMI9wBQAAfSpkSvWhSABR27YzjNgZTHQ9NMPlMpSXl6dR\n",
              "hQPsAKIwGkrEljk5OcwLgf1at8KKN954QzfddJPOP/98/cd//Mduf9EAgG8jPT1dQ4cOjZvUtz3L\n",
              "stTY2NghwCgrK9OOHTtUXlamr8vL9WVzqNPzk12WMr2hDiFGrO73humdAQAA9oplSQHTUG2bWzV2\n",
              "AOFRTdvOMKIh5FZX4/GzMjM1eHChBgwYoMLCwg4lNzeXHhE44HXrK/zhhx9WWVmZ7rvvPp144oka\n",
              "NWqUrrzySs2aNau32gcAnTIMQz6fTz6fr8vg1DRNVVdX2wFGh+WOHfq8rq7z60vye8PKbh9oRMOM\n",
              "7GidR7YCANA/mZbUEHLZvSJqgx47lIitd9Urwuv1qHDAAB0RDSIGDBigAQMiPSRi4URKSso+viPA\n",
              "ebo1wWZ7q1ev1ty5c9XY2KiCggL98Y9/1IknntjT7QOAXhUIBLRjxw67xIKMWCkvL+9y8uA0d2SO\n",
              "jOyk+GEmsfU0t8lQk36ICfmAAwef5/4rZMoeklEbdKumzRO3vru5Ivx+nwYMGGgHD7EwIlays7N3\n",
              "++ACABEJ9azYuHGjDj/8cAUCAT300EP64x//qLS0NP3ud7/T2WefrXXr1unss8/W1q1be7m5ANCz\n",
              "UlJSNGTIEA0ZMqTT/eFwWFVVVXEBRmlp6c710lJ9Ux/o9Nwkl6Wsdj0zYiFGdlJkm89jykWYAQDA\n",
              "PtcaNuyeELGhGbXthmk0djFEwzCic0UMGKCBAwfGhRADBw5UQUGB0tPT9/n9AAeihMKKOXPm6P33\n",
              "39eQIUN00kkn6d5779WkSZPs/RMnTtRJJ53Ua40EgL7idrvtiavGjh3bYX/s6SZxAUb7Ulra5VAT\n",
              "t2HZE4Bm20NNQnY90xuWhz+8AADQLZYlNYV3DtFoPzyjJhjpIdES7vwbbGyIxoguwoj8/Hx5vd59\n",
              "fEdA/5RQWBEbKbJu3bouH0l433339VyrAGA/0f7pJiNHjuz0mJaWlg4hxs5wo1RbKyu1uanj328M\n",
              "ST5PJLSI9MoIxfXQyPKGleJm3gwAQP8StqSGWK8Ie46IdqFE0KNgF/NFpKWlaeDgzoOIAQMGKCcn\n",
              "hyEagEMkFFbU1dXpueeeU1fTW5x++uk92igAOJCkpqbu9skmoVBI5eXlHXpntF//qqXzp5qkutsP\n",
              "NQl1eKpJBkNNAAD7mdawobpo+FC3SyBRG/SoPuiW2cW52dnZOnzgwLi5ItoHEz6fTwYTSgH7hYTC\n",
              "ioqKCt1xxx2dhhWGYRBWAMC34PF4dNBBB+mggw7qdH/sqSad9c4oKyvTjtJSlTY0dXpu+6Emme2G\n",
              "m2S2CzV4qgkAYF8xLakx9hSNuKdnRIKIuqBbzV0M0XC5XCooKNDQgfGTVw5st85TNIADR0JhxfDh\n",
              "w/Xaa6/1dlsAAJ1wuVzKy8tTXl6eioqKOj2msbGxQ5jR/skmWyoru+wdl+Y2o4FGKBpkRJ5yElmG\n",
              "5fOG5eaPUACAPbAsKWBGekXEnpgRCyNi9fpQ10/RSEtL04DBAzo8RaOwsFADBw5UXl6e3G73Pr4r\n",
              "AH0lobACAOBsGRkZGj58uIYPH97p/thQk/YBRnl5eVyoUdrQ3Om5hiSfN6xMz85eGbHeGn6GmwBA\n",
              "vxEbnhELIuqCbtWFXDvrQY9au5grwuVyKT8/X6OjIURhYaEdSMSWGRkZDNEAYEsorGCYBwDs3/Y0\n",
              "1ESK9M4oKyuzw4uysjI74CgrK1NZebm+agl2eq5LOwONTG988XvD8ntM+Tw83QQAnMiypJawofpQ\n",
              "JHSo7ySIqA95FAh3HSRkZWVpyJBIAFFQUBAXRBQWFio3N1ceD38nBZC4hP7HuOWWW3q7HQCAPpaR\n",
              "kaGMjAwddthhne63LEs1NTVxAUZ5ebnKy8tVUVGh8rIylVaUa1sXk4FKUobHlN8TGWISCTF2hhmx\n",
              "9VS3Jf6wBgA9I2RKDdEQoiHkUn10KEZdMFKvC0XCiVAXQzOkSBAxKPoY74KCAuXn59shRGxbcnLy\n",
              "PrwrAP0B8SYAICGGYSgnJ0c5OTldPqbVsizV1tbagUYsyIiVyHq5vmlo6fJ1PIYlX7sQw9euZ4bf\n",
              "G5YvWifUANCfxUKIhpBLDSG36oORZUPQpfpoAFEf6nqySikyNCM3N1cjogFEfn6+HUa0DyYIIgD0\n",
              "BcIKAECPMQxD2dnZys7O1ogRI7o8rqmpyQ4yKisrVVlZaQcasfrXVVX6srnrXhqxUMMXDTQyoiGG\n",
              "z2MqI7Y9Wmf4CYD9gWVJzWFDje1CiMaQa+d6LJDYQwghSX6/TwMOjgQQeXl59jIvL88OIXJycpiw\n",
              "EoBjEVYAAPa59PR0DR06VEOHDu3yGNM0VVdXZwcYVVVVHZYVFRUqrarc7dATSUp1W8pwh5TuMZVh\n",
              "l3BcPd0dqSe76LEBoOeETKk57FJTNHxoCrvsACKyjJawW00hV5dPyojJ9Ps18JBI8JCbmxu3jJXc\n",
              "3Fwe4Qlgv9etsOL999/Xz372M23evFmh0M4fDDdv3tzjDQMA9G8ul8vupXHEEUd0eZxlWWpoaFB1\n",
              "dbWqqqrsZay0376tpkamae72dd2GpXSPGQ0vIiFGbD3dEylp7sh6WrTOk1CA/sGypDbTiIQP0QAi\n",
              "FkTE1iP1SBDRHHarZTeTUsakp6UpJz9XQ7KzlZeXp5ycHOXm5io3Nzeunp2dLa/Xuw/uFAD6nmFZ\n",
              "lpXowWPGjNEVV1yhqVOnxnUZGz16dK80DgCAnmSapurr61VTU6Pq6uq4UlNTo9raWtXU1ETqNTVq\n",
              "bGra4zUNSanuSGiR5jZVGvCoKDNgBxqp7UKNNLepVLelVDc9OIC+FjKllrBLLeFI4NASjoQQdgm5\n",
              "tK42VYPSgnY40Rxy7XYiyhiPx6OsrCzl5OQoKytLWVlZys7Otuf92bXOnBAA0FG3wori4mKVlJT0\n",
              "YnMAAHCOtrY21dbW2iFGXV2damtr7WX7ek1NjRrq69UW7PzxrruKhRwpdoixM8hIcZlK6bRuKcUd\n",
              "WfcahB3o38KW1Bo2FDBdCoQNtYRd8fWwSy2mEVlGt9nFdCloJvYBMgxDGRnpysrKVlZWljIzM+OW\n",
              "u9ZzcnKUnp4ugw8oAHwr3RoG8t3vfldr167VxIkTe6s9AAA4RlJSkj0jfiIsy1IgEFB9fb3q6upU\n",
              "V1dn12PLhoYGNTQ0qL6+XvX19WpoaFBVfb1aGgPdaptLlpLdUrIrrBS3pRRXpLdGSrTXRrLLUlK7\n",
              "erLLVLJ7Zz3JZcUVhrKgt1mWFLKkNtOlNtNQm2mo1TTUarrUGo7VDbXFrbvUGg0cAqah1rBLrWak\n",
              "nmjYEONyueT3+ZTp9+sQn09+v19+v1++aD0zM7PDMjMzUxkZGUxCCQB9oNvDQD777DMNHz48btKe\n",
              "999/v1caBwBAfxEMBlVfX6/Gxka7NDQ0xC1j9aampvjS2KjGpiaFw+G9fn2Py1KSYSlplyDDa1jy\n",
              "uiIlKXpMbD22z7PLMlKXvIYlT3S7x7DkcUWe4kIw4hyWFemhELKMSDENhaLrQdNQMLoM2UvFbQ9G\n",
              "Q4eO6zsDibZoANFmGkr4h85OpKamKj09Xenp6crIyIirZ2RkyOfz2cv09HR7PSMjQ36/X2lpafR2\n",
              "AID9SLfCitdff73T7ccdd1yPNQgAAHSfZVlqbW21A4zm5uYOpf32QCCw+2VLiwKtrerGjwkJc2ln\n",
              "cOE2JLdhym3E1q12dUXXIwFH3FLqsN0lyRXd5zJ21g17KXvdUGQojsuQDEWG1MR+jbWX9jZLif6K\n",
              "a2nnL+SxfzorWiTJtCL7LSuyzV6XZFqR800rUjftuiFTO7eFo9vaL8PRY8Lt1mPLUKxuGgortm4o\n",
              "ZCqh+Rf2RpLXq+SUZKWmpik1NbXTkpKSovT0dKWlpXUo7bfH6vRuAID+pVthRcw333wjSTrooIN6\n",
              "vEEAAMAZLMtSW1ubAoGAWltbFQgE1NLSokAgYG/bXWlra7NLMBhUa2urgsFgh+3tSygYVDAUVDAY\n",
              "2uOTW7B7Xq9HHo9XXq9HXo9X3qQkeb1eeTweJSUl2cXr9So5OVlerzdue3JycocS256SkqLk5GQ7\n",
              "dIitx5YeT7dGGgMA0EG3wooNGzbo7LPPtsOKQw45RE888YRGjhzZaw0EAAD9UzgcVjAYVDgcVigU\n",
              "iivtt4XD4Q7FNE37GNM07WJZlsLhcKfL9kWKhDWxwCTRH5cMw9htcblcdtl1PVbcbnenpf0+j8ez\n",
              "x8KQBwDA/qxbYcX06dN1ySWX6IILLpAkPfbYY7rnnnu0atWqXmsgAAAAAADoX771o0t5nCkAAAAA\n",
              "AOhJru4c7Ha7tX79ent9/fr1THYEAAAAAAB6VLdmP/r1r3+tadOmaezYsZKkjz76SA8//HCvNAwA\n",
              "AAAAAPRP3X4aSHl5uf71r39JkqZMmaK8vLxeaRgAAAAAAOif9urRpQAAAAAAAL0loWEgxx13nF5/\n",
              "/XVlZ2fHPQbLsiwZhqHq6upeayAAAAAAAOhfEupZUVpaqoEDB+rLL7/sdP/gwYN7vGEAAAAAAKB/\n",
              "SuhpIAMHDpQkLVu2TIMHD44ry5Yt69UGAgAAAACA/qVbjy5dsWJFQtsAAAAAAAD2VkJzVrz00kv6\n",
              "+9//rq+//lpXX321vb2urq7XGgYAAAAAAPqnhMKKlJQUZWVlyeVyKTMz095+6KGH6sYbb+y1xgEA\n",
              "AAAAgP6nW48u/eCDDzRu3LjebA8AAAAAAOjnujVnxZ133qmqqip7vbKyUj/5yU96vFEAAAAAAKD/\n",
              "6lZY8d577yk3N9dez8vL07vvvtvjjQIAAAAAAP1Xt8KKUCgUt25Zltra2nq0QQAAAAAAoH/rVlgx\n",
              "ZcoUXXHFFfryyy+1detWXXnllZoyZUpvtQ0AgH7jsssu02WXXdbXzQAAAHCEboUVt99+u5qbmzVp\n",
              "0iRNnjxZra2tuuOOO3qrbQAA9BtlZWUqKyvr62YAAAA4QkKPLo3x+/26//77e6stAAAAAAAA3Qsr\n",
              "JOmbb77Rxx9/rEAgYG87/fTTe7RRAAAAAACg/+pWWHH//fdr0aJFqq6u1uGHH64PPvhAU6ZMIawA\n",
              "AAAAAAA9pltzVtxxxx1at26dDjvsML333nt67bXXdMQRR/RW2wAAAAAAQD/UrbAiKSlJ2dnZ9iNM\n",
              "p02bppKSkt5oFwAAAAAA6Ke6NQwkOTlZlmXpiCOO0B/+8AcNHjxYjY2NvdU2AAAAAADQD3UrrPjl\n",
              "L3+p+vp63Xrrrbr00ktVU1Oju+++u7faBgAAAAAA+qFuhRUnnHCCJCkzM1OvvPJKrzQIAAAAAAD0\n",
              "b92as+IXv/iFamtrZVmWvv/97ysvL0/Lly/vrbYBAAAAAIB+qFthxTPPPKOsrCytXLlSHo9H//jH\n",
              "P/TLX/6yt9oGAAAAAAD6oW6FFS5X5PDXX39d55xzjkaMGCHDMHqlYQAAAAAAoH/q1pwV6enp+u1v\n",
              "f6vHHntM//jHP2RZltra2nqrbQAAAAAAoB/qVs+KBx98UKWlpbr11ltVWFioTZs26cILL+yttgEA\n",
              "AAAAgH7IsCzL6utGAADQ382ePVuStGLFij5uCQAAQN/r1jCQrVu36re//a02bdqkUChkb3/ttdd6\n",
              "vGEAAAAAAKB/6lZYce655+rEE0/UFVdcIbfb3VttAgAAAAAA/Vi3wopAIKDFixf3VlsAAAAAAAC6\n",
              "N8FmUVGRtm3b1lttAQAAAAAA6F7PioqKCo0bN05Tp05VSkqKvZ3JwAAAAAAAQE/pVlhx4YUX8qhS\n",
              "AAAAAADQq7oVVvzoRz/qrXYAAAAAAABI6mZYIUn/93//p5KSEgUCAXvb73//+x5tFAAAAAAA6L+6\n",
              "NcHmggULtGzZMj344IMyDENPPvmk6urqeqttAAAAAACgH+pWWLFq1So988wzys/P1+23365//etf\n",
              "2r59e2+1DQAAAAAA9EPdCitSUlLkcrlkGIaCwaAGDBigb775prfaBgAAAAAA+qFuzVnh8/nU3Nys\n",
              "Y445RhdeeKEGDBigtLS03mobAAAAAADoh7rVs+LRRx+Vx+PR7373O40dO1Zer1dPPvlkb7UNAAAA\n",
              "AAD0Qwn3rAiHw7r22mu1bNkySdLPf/7zXmsUAAAAAADovxLuWeF2u/X555/3ZlsAAAAAAAC6N2fF\n",
              "9OnTNX/+fF100UXKyMiwt48dO7bHGwYAAAAAAPqnhMKK888/X48++qgef/xxSdIrr7xi7zMMQ5s3\n",
              "b+6d1gEAAAAAgH4nobDi008/lSRt2bKlVxsDAAAAAACQ0JwVhmHs9QuEQiHdcsstGjlypIqKilRc\n",
              "XKz58+ertrZ2r6530UUX6Q9/+MMej7vlllv04x//2F5/6623ZBiGVq9ebW+79NJLdeONN2rt2rWa\n",
              "M2eOJKm2tla/+c1v4q51/PHH6+mnn06ofddee61uvvnmhI7taYZh7PW/KwAAAAAATpFQWPHhhx8q\n",
              "JyenQ8nOzlZOTs5uz503b57Wrl2rNWvW6OOPP9a6det00kknqbq6ukduoCvTp0+PCyZWrVqlyZMn\n",
              "d9h2wgknaOLEifYQl87Cip5gmqZM0+zx6wIAAAAAcKBJKKwYMWKE1q1b16GUlJRo3bp1XZ73xRdf\n",
              "6IknntADDzyg7OxsSZG//p9zzjkaNmyYduzYoenTp+vII4/U6NGjdcUVV9i/0L/99ts68sgjVVxc\n",
              "rKKiIv3pT3+yr7thwwadeOKJOuKIIzR79my1tbV1eO0pU6bom2++0fbt2yVJq1ev1i9+8Qs7rCgt\n",
              "LdW2bds0depUrV69WsXFxZIivS0aGhpUXFysiRMn2td76623dOyxx+qwww7TpZdeam8vLS3VySef\n",
              "rFGjRmnGjBn260nSzTffrLPOOksnn3yyioqKVFpaqmuvvVaTJk1ScXGxpk2bps8++0ySdO+992r+\n",
              "/PmSpPXr18swDL388suSpEWLFmnRokWS1OX5AAAAAAAcKBIKK5KTkzV48OAuS1fef/99HX744crL\n",
              "y+t0f1ZWlp577jm99957+vDDD7V161b93//9nyRp8eLFuvbaa1VSUqKPP/5Y5513nn1eSUmJnnvu\n",
              "OW3YsEFlZWVavnx5h2snJSXp6KOP1qpVq9Ta2qotW7Zo1qxZ2r59uwKBgFatWqWpU6cqJSUl7rwl\n",
              "S5bI5/OppKREa9eutbdv2rRJq1at0scff6yXXnpJa9askSQtWLBARx11lNavX6+//OUvevXVV+Ou\n",
              "t2bNGv31r3/V+vXrdfDBB+v666/Xu+++q5KSEl1++eW66qqrJEkzZszQypUrJUUmMJ06dWrc+owZ\n",
              "MySpy/P3pLW1VfX19XGltbU1oXMBAAAAANiXEgorLMvqlRc3TVPXX3+9xo0bp/Hjx2vt2rUqKSmR\n",
              "FBnG8d///d9atGiR3nrrLbtnhiSdeeaZSktLk9vt1lFHHaVNmzZ1ev3YUJB33nlHRx11lKRIj4s1\n",
              "a9Zo9erVmj59esJtnTNnjjwej1JTU1VcXGy/5quvvmrPjXHwwQfr9NNPjztv1qxZKiwstNdjQURR\n",
              "UZEWLVpk3++wYcMkSZs3b9bKlSu1ePFivfbaa2psbNT69evt9nd1/p4sXrxYmZmZcWXx4sUJ3z8A\n",
              "AAAAAPtKQk8D2d1Qj92ZMGGCNm7cqKqqKuXm5nbY//vf/17l5eV65513lJKSoquvvlqBQECS9NOf\n",
              "/lRnnHGGVq5cqZ/97GcqKirS3XffLUlxvSHcbrdCoVCnrz99+nQtXbpUgwYN0vHHHy9JOu6447Rq\n",
              "1SqtWrVKDz74YML3kuhr7joZaUZGhl3ftm2brrjiCr377rs67LDD9OGHH2ratGn2/hkzZujFF1/U\n",
              "xo0bddxxx8myLC1fvlxTp06Vx+PZ4/m7s3DhQl199dVx25KTkxM6FwAAAACAfSmhnhV7a/jw4Trr\n",
              "rLM0b948+ykVsV/AN2/erJqaGg0YMEApKSnasWOHnnjiCfvczz77TEOHDtUll1yin/3sZ3r77be7\n",
              "/fqTJk1SeXm5Hn744biw4rHHHlNpaandW6E9v9+vlpaWTufB6MyMGTN0//33S4rMX/Hss892eWxd\n",
              "XZ28Xq8GDhwoy7J01113dbjW7373O7tdJ5xwgm666SZ7CMiezt+d5ORk+f3+uEJYAQAAAABwooR6\n",
              "Vnwb999/v375y19q8uTJ8ng8Mk1T06ZN04knnqirrrpKZ599tkaPHq2DDjrI/qVcku666y699tpr\n",
              "SkpKktvt1u23397t1/Z6vTrmmGP0wQcfaOTIkZKkI444Qg0NDTrmmGPk9Xo7nJOTk6O5c+dq7Nix\n",
              "ysjIiJu3ojP/8z//o4suukijRo3SwQcfrBNOOKHLY8eMGaPzzjtPo0ePVm5urn7wgx/E7T/xxBO1\n",
              "bds2+9/hpJNO0m233aYTTzwxofMBAAAAADgQGFZvTUgBAAASNnv2bEnSihUr+rglAAAAfa9Xh4EA\n",
              "AAAAAAB0F2EFAAAAAABwFMIKAAAAAADgKIQVAAAAAADAUQgrAAAAAACAoxBWAAAAAAAARyGsAAAA\n",
              "AAAAjkJYAQAAAAAAHIWwAgAAAAAAOAphBQAAAAAAcBTCCgAAAAAA4CiEFQAAAAAAwFEIKwAAAAAA\n",
              "gKMQVgAAAAAAAEchrAAAAAAAAI5CWAEAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUwgoAAAAAAOAo\n",
              "hBUAAAAAAMBRCCsAAAAAAICjEFYAAAAAAABHIawAAAAAAACOQlgBAAAAAAAchbACAAAAAAA4CmEF\n",
              "AAAAAABwFMIKAAAAAADgKIQVAAAAAADAUQgrAAAAAACAoxBWAAAAAAAARyGsAAAAAAAAjuLp6wYA\n",
              "AACpsLCwr5sAAADgGIZlWVZfNwIAAAAAACCGYSAAAAAAAMBRCCsAAAAAAICjEFYAAAAAAABHIawA\n",
              "AAAAAACOQlgBAAAAAAAchbACAAAAAAA4CmEFAAAAAABwFMIKAAAAAADgKIQVAAAAAADAUQgrAAAA\n",
              "AACAoxBWAAAAAAAARyGsAAAAAAAAjkJYAQAAAAAAHIWwAgAAAAAAOAphBQAAAAAAcBTCCgAAAAAA\n",
              "4CiEFQAA7Kcuu+wyXXbZZX3dDAAAgB7n6esGAACAvVNWVtbXTQAAAOgV9KwAAAAAAACOQlgBAAAA\n",
              "AAAchbACAAAAAAA4CmEFAAAAAABwFMIKAAAAAADgKIQVAAAAAADAUQgrAAAAAACAoxBWAAAAAAAA\n",
              "RyGsAAAAAAAAjkJYAQAAAAAAHIWwAgAAAAAAOAphBQAAAAAAcBTCCgAAAAAA4CiEFQAAAAAAwFEI\n",
              "KwAAAAAAgKMQVgAAAAAAAEchrAAAAAAAAI5CWAEAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUwgoA\n",
              "AAAAAOAohBUAAAAAAMBRCCsAAAAAAICjEFYAAAAAAABHIawAAAAAAACOQlgBAAAAAAAchbACAAAA\n",
              "AAA4CmEFAAAAAABwFMIKAAAAAADgKIQVAAAAAADAUQgrAAAAAACAoxBWAAAAAAAAR3FEWBEKhXTL\n",
              "Lbdo5MiRKioqUnFxsebPn6/a2toee42bb75ZgUCgx64HAAAAAAB6hyPCinnz5mnt2rVas2aNPv74\n",
              "Y61bt04nnXSSqqur444LhUJ7/Rq33HLLPgsrOmvnt2k7AAAAAAD9SZ+HFV988YWeeOIJPfDAA8rO\n",
              "zpYkGYahc845R9u2bdPo0aM1b948FRcX6+GHH1ZhYaGam5vt8y+44AL96U9/ss+74YYbNH78eB1x\n",
              "xBF6+OGHJUmXXnqpJOnYY49VcXGxysvLVV5ertmzZ2vMmDEqKirSPffcY19zw4YNOvnkkzV27FiN\n",
              "HTtWS5YskSQdf/zxevrpp+3jzj77bD344IOSpIsuukgXX3yxpk2bpqKiIq1evTqu7U899ZQ2btyo\n",
              "73//+5o0aZLGjh2ru+66y76WYRj69a9/raOOOkpDhw7VAw88sNv2rF27ViNHjpRlWfZxRx99tF58\n",
              "8cWeeFsAAAAAAOgznr5uwPvvv6/DDz9ceXl5ne7fsGGD7r77bi1dulSS9PLLL+uhhx7S/PnzVVZW\n",
              "ppUrV+ree++1jzcMQ+vWrdPmzZs1ceJEffe739WSJUt0zz336M0331RWVpYkac6cORoxYoRWrFih\n",
              "8vJyHXnkkRo3bpwmTpyoM844Q7fccovOP/98SVJlZWVC9/Lee+/prbfeks/n0+rVq+PaHg6HNXny\n",
              "ZD300EMaOXKkmpubNWXKFE2ePFmTJk2SJCUnJ+tf//qXPv30U02aNEn/9m//JkmdticvL0+5ubl6\n",
              "5ZVXNHPmTK1bt04VFRU65ZRTOm1ba2urWltb47YlJycrOTk5oXsDAAAAAGBf6fOeFXsybNgwHXfc\n",
              "cfb6VVddpT/+8Y+SpPvuu0/nn3++MjIy7P0//vGP7fOmTZumN954o9Prrly5Uj/5yU8kSQUFBZo9\n",
              "e7ZWrlypzz77TIFAwA4GJHUZpOzqnHPOkc/n67Ttn332mT755BOdd955Ki4u1tFHH62GhgatX7/e\n",
              "Pv6HP/yhJGnkyJHyeDzasWPHbttz1VVX2b0z/vjHP+ryyy+XYRidtm3x4sXKzMyMK4sXL07ovgAA\n",
              "AAAA2Jf6vGfFhAkTtHHjRlVVVSk3N7fD/vZBhCQdddRRSktL06pVq3Tvvfdq5cqVu71+V7+8781x\n",
              "Ho9H4XDYXt91Doxd29p+3bIs5eTkqKSkpMvrp6Sk2HW3273HeS5mz56t6667TuvWrdOzzz6r2267\n",
              "rctjFy5cqKuvvjpuG70qAAAAAABO1Oc9K4YPH66zzjpL8+bNs5/+YVmWli9frs2bN3d6zlVXXaW5\n",
              "c+fqO9/5jo444oi4fbG5HrZu3ao333xTxx57rCTJ5/Oprq7OPm7GjBm67777JEkVFRVasWKFTjrp\n",
              "JI0YMUJpaWl69NFH7WNjw0CGDx+ud955R5K0ZcsWvfXWWwnf54gRI+T3++Pmovjiiy86TCLa2Xld\n",
              "tcfj8ejSSy/V6aefrjPPPNMe4tKZ5ORk+f3+uEJYAQAAAABwoj4PKyTp/vvv17hx4zR58mSNHj1a\n",
              "o0aN0ssvv6ycnJxOjz/77LPV2NioK664osO+cDis8ePHa+bMmfrf//1fDRkyRJJ0zTXX6KSTTrIn\n",
              "2Pzf//1fbdiwQWPGjNH06dP185//XJMnT5bH49EzzzyjBx54QGPGjNG4ceO0fPlySdJ1112nVatW\n",
              "acyYMVq4cKEmT56c8D16PB797W9/04oVKzR27Fh78s2WlpY9ntdVe6TIk1S+/vrrTv8tAAAAAADY\n",
              "HxlW+8dJ7CfWrl2rCy64QJ9++qlcrp15i2EYqqmp2W0PgwPNk08+qT/96U969dVX+7opAIB9bPbs\n",
              "2ZKkFStW9HFLAAAAelafz1nRXT/+8Y/18ssv689//nNcUNEfnXLKKfr888/11FNP9XVTAAAAAADo\n",
              "MftlzwoAAEDPCgAAcODq310TAAAAAACA4xBWAAAAAAAARyGsAAAAAAAAjkJYAQAAAAAAHIWwAgAA\n",
              "AAAAOAphBQAAAAAAcBTCCgAAAAAA4CiEFQAAAAAAwFEIKwAAAAAAgKMQVgAAAAAAAEchrAAAAAAA\n",
              "AI5CWAEAAAAAAByFsAIAAAAAADgKYQUAAAAAAHAUwgoAAAAAAOAohBUAAAAAAMBRCCsAAAAAAICj\n",
              "EFYAAAAAAABHIawAAAAAAACOQlgBAAAAAAAchbACAAAAAAA4CmEFAAAAAABwFMIKAAAAAADgKIQV\n",
              "AAAAAADAUQgrAAAAAACAoxBWAAAAAAAARyGsAAAAAAAAjkJYAQAAAAAAHIWwAgAAAAAAOAphBQAA\n",
              "AAAAcBRPXzcAAADsncLCwr5uAgAAQK8wLMuy+roRAAAAAAAAMQwDAQAAAAAAjkJYAQAAAAAAHIWw\n",
              "AgAAAAAAOAphBQAAAAAAcBTCCgAAAAAA4CiEFQAAAAAAwFEIKwAAAAAAgKMQVgC9oLW1VTfffLNa\n",
              "W1v7uinYR3jP+x/e8/6J973/4T3vf3jP+yfed+cxLMuy+roRwIGmvr5emZmZqqurk9/v7+vmYB/g\n",
              "Pe9/eM/7J973/of3vP/hPe+feN+dh54VAAAAAADAUQgrAAAAAACAoxBWAAAAAAAARyGsAHpBcnKy\n",
              "brrpJiUnJ/d1U7CP8J73P7zn/RPve//De97/8J73T7zvzsMEmwAAAAAAwFHoWQEAAAAAAByFsAIA\n",
              "AAAAADgKYQUAAAAAAHAUwgpgL5mmqSuvvFKHHXaYhg8frrvuuqvLY4cMGaIRI0aouLhYxcXFevzx\n",
              "x+19Gzdu1NFHH60jjjhCkyZN0ieffLIvmo+9kOh7HggE9IMf/EBHHHGExo0bp5NOOklffPGFvf/4\n",
              "44/X0KFD7a+HO+64Y1/dAhKU6Ody6dKlOvzww3XYYYfpkksuUTAYTGgfnCeR9/y1117TUUcdpVGj\n",
              "Rmn06NG67rrrZJqmJGnr1q1yu93257q4uFibNm3a17eBbkjkPV+9erVSU1Pj3teWlhZ7P5/z/U8i\n",
              "7/sDDzwQ957n5eVp9uzZkvis748WLFigIUOGyDAMlZSUdHkc39MdyAKwV/7yl79YJ5xwghUKhayq\n",
              "qipr0KBB1scff9zpsYMHD7bWrVvX6b7p06dbDzzwgGVZlvXEE09YEydO7KUW49tK9D1vaWmxnn/+\n",
              "ecs0TcuyLOvOO++0jjvuOHv/cccdZz311FP7qNXYG4l8Ljdv3mwNHDjQKi0ttUzTtE477TTrrrvu\n",
              "2uM+OFMi7/n7779vbdq0ybKsyOf8u9/9rn3Oli1brMzMzH3UWvSERN7zVatWWePGjev0fD7n+6e9\n",
              "+blr9OjR1pNPPmlZFp/1/dHrr79uffXVV7v9eZzv6c5EzwpgLz3++OO65JJL5Ha7lZOTozlz5ujR\n",
              "Rx/t1jXKy8u1du1aXXjhhZKks846S1999VXcX+HhHIm+5ykpKZo1a5YMw5AkTZkyRVu3bt3HrcXe\n",
              "SvRz+eSTT+r000/XgAEDZBiGLr30UvvrYXf74DyJvufjx4/XsGHDJEU+58XFxXy291M98f2Xz/n+\n",
              "Z2/e93feeUfl5eU6/fTT91Uz0cOmTZumQw45ZLfH8D3dmQgrgL20bds2DR482F4fMmSItm3b1uXx\n",
              "c+fO1ZgxYzRv3jxVVFRIkr766isNHDhQHo9HkmQYhgYNGrTb66DvdPc9j/mf//kfnXHGGXHb/uu/\n",
              "/ktjxozRnDlztHnz5h5vK/Zeop/L3X097O3XCvrG3vxfvGPHDj355JM69dRT7W1NTU2aNGmSJkyY\n",
              "oEWLFikcDvd627F3uvOeb9q0SRMmTNCkSZN0991329v5nO9/9uazvnTpUv3bv/2bvF6vvY3P+oGH\n",
              "7+nO5OnrBgBONXXqVG3cuLHTfevWrevWtd544w0NGjRIwWBQN9xwg370ox/phRde6Ilmogf15Hse\n",
              "8+tf/1pffPGFXn31VXvbsmXLdOihh8qyLP3xj3/UqaeeqvXr1+/V9QHse/X19TrttNN03XXXaeLE\n",
              "iZKkgQMH6uuvv1ZBQYGqq6s1Z84c3X777bruuuv6uLX4NiZMmKDt27crMzNT27dv16xZs5SXl6dz\n",
              "zz23r5uGfaCpqUmPPfaY3n77bXsbn3Vg36FnBdCFNWvWqLKystNy6KGHatCgQfryyy/t47du3apB\n",
              "gwZ1eq3Ydq/Xq5/+9Kd68803JUmHHnqoSktLFQqFJEmWZWnbtm1dXge9qyffc0m67bbbtGLFCr34\n",
              "4otKS0uztx966KGSIn/RueKKK7R582ZVVVX13o2hWxL9XO7u66G7XyvoW935v7ihoUGnnHKKzjjj\n",
              "DF199dX29uTkZBUUFEiScnJydPHFF9v/18N5En3P/X6/MjMzJUmHHHKIzj//fPt95XO+/+nuz11P\n",
              "PPGERo8erVGjRtnb+KwfmPie7kyEFcBeOuecc3TfffcpHA6rurpajz/+uObMmdPhuKamJtXW1trr\n",
              "jz76qMaPHy9JKigo0IQJE/TQQw9JkpYvX65DDjlEw4cP3yf3gO5J9D2XpN///vd69NFH9corrygr\n",
              "K8veHgqFVFZWZq8vX75chYWFys3N7e3mI0GJfi7POussPfvss9qxY4csy9KSJUt03nnn7XEfnCfR\n",
              "97yxsVGnnHKKTjnlFN1www1x+8rLy+3Z4VtbW7VixQr7/3o4T6LveWlpqf3El4aGBv3tb3+z31c+\n",
              "5/uf7v7ctXTpUs2bNy9uG5/1AxPf0x2qjyb2BPZ7oVDIuvzyy62hQ4daw4YNs/7whz/Y+5555hlr\n",
              "3rx5lmVZ1qZNm6zi4mJrzJgxVlFRkXX66adbW7ZssY/99NNPrSlTpliHH364deSRR1offvjhvr4V\n",
              "JCjR9/yrr76yJFnDhg2zxo0bZ40bN8466qijLMuyrMbGRuvII4+0ioqKrLFjx1onnHCCVVJS0if3\n",
              "g6519bmcN2+e9cwzz9jH3XvvvdawYcOsYcOGWRdffLHV1taW0D44TyLv+S9/+UvL4/HYn+tx48ZZ\n",
              "v/zlLy3Lsqzly5dbo0ePtsaOHWuNGjXKuuKKK6xAINBn94M9S+Q9v/POO61Ro0bZ7+tNN91kP+nJ\n",
              "svic748S/f/9008/tTIyMqz6+vq48/ms73/mz59vHXzwwZbb7bYKCgqsww47zLIsvqfvDwzLsqy+\n",
              "DkwAAAAAAABiGAYCAAAAAAAchbACAAAAAAA4CmEFAAAAAABwFMIKAADgWA0NDcrIyOgwI39fW716\n",
              "tf7+97/v9pi//e1vuvTSS+3ji4uL4/Zv3bo17mlBS5Ys0dixY1VcXKyRI0fqhz/8ob1vyJAhGjFi\n",
              "hMaNG6fhw4frjDPO0D//+c+415o/f/63vzEAAByCsAIAADjW448/riOPPFIrVqxQY2NjXzfHlkhY\n",
              "sXDhQi1cuDCh661du1a33nqrVq9erZKSEm3YsEHXXHNN3DGPP/64PvjgA33xxRf60Y9+pFmzZumd\n",
              "d96RJJ166ql67733tHHjxr27IQAAHIawAgAAONbSpUt1/fXXa9q0aXr88cft7Q8++KBmzJih888/\n",
              "X6NGjdLRRx+t9evX68wzz9R3vvMdzZw50w43GhsbdfHFF6uoqEhFRUW65ZZb7Oscf/zxevrpp+31\n",
              "s88+Ww8++KAk6aKLLtJPfvITnXjiiTriiCM0e/ZstbW1qaSkREuWLNHDDz+s4uJiLVq0qEO733zz\n",
              "TWVlZWnw4MEJ3ef27dvl8/nk8/kkSYZhaMKECV0eP3v2bF166aW67bbb7G3nnnuu/vznPyf0egAA\n",
              "JGrBggUaMmSIDMNQSUnJHo//6KOPVFxcbJchQ4YoJyen269LWAEAABxp/fr1+uqrr3TyySdr3rx5\n",
              "Wrp0adz+d999V7/97W+1fv16HXbYYTrttNO0ZMkSbdiwQUlJSfrLX/4iSfrv//5vtba26sMPP9Q7\n",
              "77yjp59+Oi742J2SkhI999xz2rBhg8rKyrR8+XIVFxfr0ksv1Q9/+EOVlJToF7/4RYfzVq9ercmT\n",
              "Jyd8rzNnzpTP59OgQYM0Z84c3XXXXaqpqdntOZMnT9Ynn3xir0+dOlWvvvpqwq8JAEAizj77bL31\n",
              "1lsJB/BjxoxRSUmJXU499dS4oY2JIqwAAACOtHTpUs2dO1dut1uzZs3Sli1btGHDBnv/1KlTNWjQ\n",
              "IEnSxIkTNWnSJBUWFkqSJk2aZA+JWLlypS655BK5XC6lp6dr7ty5euWVVxJqw5lnnqm0tDS53W4d\n",
              "ddRR2rRpU0Lnbd++3W6LFOkp0ZnY9rS0NL355pt64YUX9N3vflcrVqzQ2LFjVV1d3eVrWJYVtz5g\n",
              "wABt3749ofYBAJCoadOm6ZBDDumw/d1339UJJ5ygiRMnavz48XriiSc6HBMIBPTwww/v1dxThBUA\n",
              "AMBxgsGgli1bpr/85S8aMmSIhg8frubm5rjeFSkpKXbd7XZ3WA+FQp1eu31w4PF4FA6H7fVAIBB3\n",
              "bKLX3FVaWlrctfLz81VVVRV3TGVlpQoKCuLaNX78eC1YsECvvvqqMjIytHr16i5f491331VRUVFc\n",
              "21NTUxNqHwAA30Ztba3mz5+vhx9+WGvXrtUrr7yia665Rl9//XXccStWrNCwYcM6TDKdCMIKAADg\n",
              "OM8++6yGDRumr7/+Wlu3btXWrVv19ttva9myZQoGg9261owZM7R06VJZlqWmpiYtW7ZMM2fOlCQN\n",
              "Hz7cnqRyy5YteuuttxK6pt/vV11dXZf7x44dq88++8xeP/zww+X1evXCCy9IkkzT1D333GO349NP\n",
              "P9WHH35oH//VV1+poqJCw4YN6/T6zzzzjP70pz/FTcK5YcMGjRs3LqH2AwDwbfzzn//U5s2b9b3v\n",
              "fU/FxcWaMWOGJMV975MivST39olenm/dSgAAgB62dOnSDuNbv/Od7+jggw/Wc889161r3XjjjVqw\n",
              "YIHGjBkjSTrnnHN07rnnSpKuu+46zZkzR2PGjNHo0aMTnmfizDPP1LJly1RcXKzZs2d3mLfi1FNP\n",
              "1aJFixQOh+V2u+X1evXUU0/p6quv1s9+9jOZpqnJkyfrV7/6lSSpublZ/+///T/t2LFDqampsixL\n",
              "v/nNb+L+EjVnzhylpKSoqalJo0aN0gsvvBDX3r///e86++yzu/VvAwDA3rAsS6NHj457jPautmzZ\n",
              "orffflvLly/fq9cwrF0HPAIAAOBb+4//+A8df/zxOuecc3r9tSorK3XCCSdo7dq1SkpK6vXXAwD0\n",
              "P0OGDNHTTz+t4uJi1dTUaNSoUVq2bJndq6KkpESjRo2yvw/deOON2rJlix566KG9ej3CCgAAgF5Q\n",
              "VVWlF198URdeeGGvv9Y777yjcDiso48+utdfCwDQv/zkJz/R888/rx07dig3N1c+n09ffPGF3n//\n",
              "fV177bWqqqpSMBjUoEGD9PTTTyslJUWmaWrw4MH661//qunTp+/V6xJWAAAAAAAAR2GCTQAAAAAA\n",
              "4CiEFQAAAAAAwFEIKwAAAAAAgKMQVgAAAAAAAEchrAAAAAAAAI5CWAEAAAAAAByFsAIAAAAAADgK\n",
              "YQUAAAAAAHAUwgoAAAAAAOAohBUAAAAAAMBRCCsAAAAAAICj/H+UMYSpeomNkQAAAABJRU5ErkJg\n",
              "gg==\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-ca95ce29-2156-48cf-b894-22524f6bf0c1\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-ca95ce29-2156-48cf-b894-22524f6bf0c1\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:5: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "from matplotlib import pyplot as plt\n",
              "import seaborn as sns\n",
              "figsize = (12, 1.2 * len(_df_23['Date of Transaction'].unique()))\n",
              "plt.figure(figsize=figsize)\n",
              "sns.violinplot(_df_23, x='Amount (USD)', y='Date of Transaction', inner='stick', palette='Dark2')\n",
              "sns.despine(top=True, right=True, bottom=True, left=True)"
            ],
            "text/html": [
              "      <div class=\"colab-quickchart-chart-with-code\" id=\"chart-a05e7cf6-bb4c-479c-bec1-5fc04b9ff97c\">\n",
              "        <img style=\"width: 180px;\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABEAAAAIDCAYAAAAE42u3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\n",
              "bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9h\n",
              "AAAPYQGoP6dpAAA8jElEQVR4nO3dfZSXdZ038PfI8BAEmqQSCeLw4J0SjA8Y2YZIYMZJQBA1TVNX\n",
              "BNqNNdtjepu1JQK7y9HcvLcR86ZWi4MJEm6yHbF8KuPBHDXrVqcYQZCHSEdSwQF+9x+d5jiIMIMz\n",
              "jly+Xuf8zpnf9bnm8/38pt85xbvr+l5lpVKpFAAAAIACO6CtBwAAAABobQIQAAAAoPAEIAAAAEDh\n",
              "CUAAAACAwhOAAAAAAIUnAAEAAAAKTwACAAAAFJ4ABAAAACg8AQgAAABQeAIQAAAAoPAEIAAAAEDh\n",
              "CUAAAACAwhOAAAAAAIUnAAEAAAAKTwACAAAAFJ4ABAAAACg8AQgAALShqVOnZurUqW09BkDhlbf1\n",
              "AAAA8F62YcOGth4B4D3BFSAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAo\n",
              "PAEIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAo\n",
              "PAEIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAo\n",
              "PAEIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAo\n",
              "PAEIAAAAUHgCEAAAAKDwBCAAAABA4bVaALJ169aMGzcuAwYMyODBgzNq1KjU1NQ01Ddu3JjTTjst\n",
              "/fv3z8CBA/Pggw821GbMmJGjjjoqBxxwQBYtWtSo70UXXZRBgwalsrIyQ4YMyX333feWM+zrGq3V\n",
              "59Zbb03//v3Tt2/fTJo0KfX19U2qvdGrr76az33uc+nXr18GDBiQO++8s0m1XS1btiyDBw/OgAED\n",
              "MmLEiKxdu7ZJNQAAANgfteoVIJdeemmefvrpPP744xk7dmwuueSShtqVV16ZoUOH5tlnn83cuXNz\n",
              "7rnnNvyjf+TIkVmyZEmGDRv2pp433HBDnnjiiVRXV2fOnDmZOHFidu7cudv193WN1uizatWqXHPN\n",
              "NXnooYdSU1OTDRs2ZM6cOXut7Wr27Nnp2LFjampq8rOf/Sxf/OIXs3nz5r3W3mjnzp0577zz8u1v\n",
              "fzvPPPNMRo8encsuu2yvNQAAANhftVoA0qlTp4wePTplZWVJkqFDh6a2trahfscdd2TKlClJkiFD\n",
              "hqRnz5554IEHkiQnnnhiKioqdtv3oIMOavi5rq5ujzPs6xqt0efOO+/MmDFj0qNHj5SVlWXKlCmZ\n",
              "N2/eXmu7mj9/fsMsRx55ZIYPH5677rprr7U3evTRR1NeXp5TTjklSTJ58uTcfffd2bp16x5ru9q2\n",
              "bVtefvnlRq9t27bt9W8BAAAA77R3bA+QG2+8MWPHjk2SbN68OfX19enRo0dDvU+fPlm9enWTel15\n",
              "5ZXp27dvxo8fnwULFuSAA978Md7uGi3dZ/Xq1TniiCN222NPtXXr1qWysvJt96mqqsrXv/713Z7X\n",
              "tWvXdOvWLevWrdtjbVczZ87MgQce2Og1c+bMZv1dAAAA4J1Q/k4sMmPGjNTU1Oxxv47mmDVrVmbN\n",
              "mpWlS5fmiiuuyC9/+ct06NChRXq/2/Ts2TPV1dVvu8/frgxpSVdddVUuv/zyRsc6duzY4usAAADA\n",
              "29XqV4DMnj07CxcuzJIlS9K5c+ckSffu3VNeXp7169c3nFdbW5vevXs3q/fIkSOzZcuWPPnkk1m6\n",
              "dGkqKytTWVmZ6667bp/XaKk+u+rdu3eee+653fbYU601+ux63pYtW1JXV5eePXvusbarjh07plu3\n",
              "bo1eAhAAAADejVo1ALn++uszb9683HvvvY327kiSiRMnpqqqKkmyYsWKrF27NieffPIe+9XX1zd6\n",
              "kszy5cuzcePGVFRUZOTIkamurk51dXWuvvrqfV6jpfrsasKECVm8eHHWr1+fUqmUqqqqnHPOOXut\n",
              "7eqNs6xatSr3339/xo0bt9faGx1//PGpr6/PL37xiyTJzTffnNNPPz2dOnXaYw0AAAD2V2WlUqnU\n",
              "Go2ff/759OrVKxUVFenatWuSv14xsGzZsiTJhg0bcv7552fVqlXp0KFDbrrppoaNN6dPn56qqqps\n",
              "2rQpXbt2TadOnfLYY4+lS5cuGTVqVOrq6lJeXp4uXbrk2muvzYgRI3Y7w76sccghh7Ran1tuuSWz\n",
              "Zs1KkgwfPjxVVVVp3779Hmvr1q3L6NGjG26DeeWVV3LxxRdn5cqVadeuXaZPn56zzjprr7Wqqqqs\n",
              "W7cu3/rWt5IkjzzySCZPnpytW7emZ8+eue2229KrV6+91gAAaFnjx49PkixcuLCNJwEotlYLQAAA\n",
              "gL0TgAC8M96xp8AAAAAAtBUBCAAAAFB4AhAAAACg8AQgAAAAQOEJQAAAAIDCE4AAAAAAhScAAQAA\n",
              "AApPAAIAAAAUngAEAAAAKDwBCAAAAFB4AhAAAACg8AQgAAAAQOEJQAAAAIDCE4AAAAAAhScAAQAA\n",
              "AApPAAIAAAAUngAEAAAAKDwBCAAAAFB4AhAAAACg8AQgAAAAQOEJQAAAAIDCE4AAAAAAhScAAQAA\n",
              "AApPAAIAAAAUngAEAAAAKDwBCAAAAFB4AhAAAACg8AQgAAAAQOEJQAAAAIDCE4AAAAAAhScAAQAA\n",
              "AAqvvK0HAACA97LDDjusrUcAeE8oK5VKpbYeAgAAAKA1uQUGAAAAKDwBCAAAAFB4AhAAAACg8AQg\n",
              "AAAAQOEJQAAAAIDCE4AAAAAAhScAAQAAAApPAAIAAAAUngAEAAAAKDwBCAAAAFB4AhAAAACg8AQg\n",
              "AAAAQOEJQAAAAIDCE4AAAAAAhScAAQAAAApPAAIAAAAUngAEAAAAaGTq1KmZOnVqW4/RosrbegAA\n",
              "AADg3WXDhg1tPUKLcwUIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8A\n",
              "AgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8A\n",
              "AgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8A\n",
              "AgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8A\n",
              "AgAAABSeAAQAAAAoPAEIAAAAUHitFoBs3bo148aNy4ABAzJ48OCMGjUqNTU1DfWNGzfmtNNOS//+\n",
              "/TNw4MA8+OCDDbUZM2bkqKOOygEHHJBFixY16nvRRRdl0KBBqayszJAhQ3Lfffe95Qz7ukZr9bn1\n",
              "1lvTv3//9O3bN5MmTUp9fX2SpLa2NsOHD8+BBx6YysrKPfZ49dVX87nPfS79+vXLgAEDcueddzap\n",
              "tqtly5Zl8ODBGTBgQEaMGJG1a9c2qQYAAAD7o1a9AuTSSy/N008/nccffzxjx47NJZdc0lC78sor\n",
              "M3To0Dz77LOZO3duzj333IZAYOTIkVmyZEmGDRv2pp433HBDnnjiiVRXV2fOnDmZOHFidu7cudv1\n",
              "93WN1uizatWqXHPNNXnooYdSU1OTDRs2ZM6cOUmSbt26Zfr06fnRj36011lmz56djh07pqamJj/7\n",
              "2c/yxS9+MZs3b95r7Y127tyZ8847L9/+9rfzzDPPZPTo0bnsssv2WgMAAID9VasFIJ06dcro0aNT\n",
              "VlaWJBk6dGhqa2sb6nfccUemTJmSJBkyZEh69uyZBx54IEly4oknpqKiYrd9DzrooIaf6+rq9jjD\n",
              "vq7RGn3uvPPOjBkzJj169EhZWVmmTJmSefPmJUkOPvjg/N3f/V26dOmy1z7z589vmOXII4/M8OHD\n",
              "c9ddd+219kaPPvpoysvLc8oppyRJJk+enLvvvjtbt27dY21X27Zty8svv9zotW3btr1+BgAAAHin\n",
              "vWN7gNx4440ZO3ZskmTz5s2pr69Pjx49Gup9+vTJ6tWrm9TryiuvTN++fTN+/PgsWLAgBxzw5o/x\n",
              "dtdo6T6rV6/OEUcc0ewe69ata3RbzJ767KlWVVWVr3/967s9r2vXrunWrVvWrVu3x9quZs6cmQMP\n",
              "PLDRa+bMmXv9TAAAAPBOK38nFpkxY0Zqamr2uF9Hc8yaNSuzZs3K0qVLc8UVV+SXv/xlOnTo0CK9\n",
              "32169uyZ6urqt93nb1eGtKSrrroql19+eaNjHTt2bPF1AAAA4O1q9StAZs+enYULF2bJkiXp3Llz\n",
              "kqR79+4pLy/P+vXrG86rra1N7969m9V75MiR2bJlS5588sksXbo0lZWVqayszHXXXbfPa7RUn131\n",
              "7t07zz333Nvqsbc+TV1j1/O2bNmSurq69OzZc4+1XXXs2DHdunVr9BKAAAAA8G7UqgHI9ddfn3nz\n",
              "5uXee+9ttHdHkkycODFVVVVJkhUrVmTt2rU5+eST99ivvr6+0ZNkli9fno0bN6aioiIjR45MdXV1\n",
              "qqurc/XVV+/zGi3VZ1cTJkzI4sWLs379+pRKpVRVVeWcc85pVo9dZ1m1alXuv//+jBs3bq+1Nzr+\n",
              "+ONTX1+fX/ziF0mSm2++Oaeffno6deq0xxoAAADsr8pKpVKpNRo///zz6dWrVyoqKtK1a9ckf71i\n",
              "YNmyZUmSDRs25Pzzz8+qVavSoUOH3HTTTQ0bb06fPj1VVVXZtGlTunbtmk6dOuWxxx5Lly5dMmrU\n",
              "qNTV1aW8vDxdunTJtddemxEjRux2hn1Z45BDDmm1PrfccktmzZqVJBk+fHiqqqrSvn37vPrqqxkw\n",
              "YEC2bduWurq6HHrooTn//PMzc+bMrFu3LqNHj264DeaVV17JxRdfnJUrV6Zdu3aZPn16zjrrrL3W\n",
              "qqqqsm7dunzrW99KkjzyyCOZPHlytm7dmp49e+a2225Lr1699loDAACg+MaPH58kWbhwYRtP0nJa\n",
              "LQABAAAA9k9FDEDesafAAAAAALQVAQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUn\n",
              "AAEAAAAKTwACAAAAFJ4ABAAAACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUn\n",
              "AAEAAAAKTwACAAAAFJ4ABAAAACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUn\n",
              "AAEAAAAKTwACAAAAFJ4ABAAAACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUn\n",
              "AAEAAAAKr7ytBwAAAADeXQ477LC2HqHFlZVKpVJbDwEAAADQmtwCAwAAABRes2+BeeGFF7Jq1aps\n",
              "37694diwYcNadCgAAACAltSsAOS6667Lv//7v6eioiLt2rVLkpSVlWX58uWtMhwAAABAS2jWHiB9\n",
              "+/bN8uXL071799acCQAAAKBFNWsPkMMOO0z4AQAAAOx3mnUFyDe+8Y3U1dXl3HPPTadOnRqODxo0\n",
              "qFWGAwAAAGgJzQpAjjzyyDc3KCvLH//4xxYdCgAAAKAlNSsAAQAAANgfNfsxuMuXL8/SpUuTJKee\n",
              "empOOOGEFh8KAAAAoCU1axPUOXPm5Mwzz8zGjRuzadOmTJgwId/73vdaazYAAACAFtGsW2AGDRqU\n",
              "++67L4ccckiSZNOmTfnUpz6VJ554otUGBAAAAHi7mnUFSJKG8GPXnwEAAADerZoVgPTv3z9XX311\n",
              "Vq9endWrV+eaa65J//79W2s2AAAAgBbRrACkqqoqf/jDH3LcccfluOOOS01NTb773e+21mwAAAAA\n",
              "LcJjcAEAAIDCa9JjcB944IGcfPLJWbx48W7rY8aMadGhoDVNnTo1SVy9BAAA8B7SpADk9ttvz8kn\n",
              "n5wbbrjhTbWysjIBCPuVDRs2tPUIAAAAvMPcAsN7zvjx45MkCxcubONJAAAAeKc0axPUE088sUnH\n",
              "AAAAAN5NmhWAbN++vdH7+vr6bNmypUUHAgAAAGhpTQpA/vVf/zUf+MAH8uSTT+bggw9ueHXr1i3D\n",
              "hg1r7RkBAAAA3pYmbYI6ZcqUnH322Zk6dWqqqqoajnfr1i0f+MAHWm04AAAAgJbQpADkwAMPzIEH\n",
              "Hpibb745hx56aDp16pQk2bp1a9asWZNevXq16pAAAAAAb0ez9gA588wzG70vlUpvOgYAAADwbtOs\n",
              "AOT1119vuPojSd73vvdl27ZtLT4UAAAAQEtqVgBSVlaWjRs3Nrxfv359SqVSiw8FAAAA0JKatAfI\n",
              "30ybNi0f//jHc/755ydJbr/99nzjG99olcEAAAAAWkqzApCLLrooRx55ZO65554kydy5c/PJT36y\n",
              "VQYDAAAAaCnNCkCSZPjw4Rk+fHgrjAIAAADQOpoVgLz22mv5zne+k+rq6mzdurXh+MKFC1t8MAAA\n",
              "AICW0qxNUCdNmpTa2tr86le/yimnnJLnnnsuRxxxRGvNBgAAANAimhWAPP744/nP//zPdOvWLV/6\n",
              "0pdy//3359FHH22t2QAAAABaRLMCkPe9731JkvLy8rzyyivp2rVrNm3a1CqDAQAAALSUZu0BcvDB\n",
              "B+fFF1/M6NGj8+lPfzof/OAHc/jhh7fWbAAAAAAtoqxUKpWaevKOHTvSrl27lEql/OhHP8qLL76Y\n",
              "Cy64IN26dWvNGaFFjR8/PonNewEAAN5LmnULzI4dO5IkZWVl+fjHP54+ffqkS5curTIYAAAAQEtp\n",
              "VgDyiU98Ilu2bMnmzZvzyU9+MrNmzco//MM/tNZsAAAAAC2iWQHI9u3b07Vr1/z0pz/NF77whTz8\n",
              "8MP55S9/2VqzAQAAALSIZgUgr7/+epLk/vvvz4gRI5Ik7dq1a/mpAAAAAFpQs54Cc8opp+Too4/O\n",
              "jh07cvPNN+fFF19MeXmzWgAAAAC845qVXnznO9/J448/noqKirRv3z47duzILbfc0lqzAQAAALSI\n",
              "ZgUgZWVlGTRoUNavX5+XXnopSdK9e/fWmAsAAACgxTQrAPn+97+fadOmpX379jnggL9uH1JWVpaN\n",
              "Gze2ynAAAAAALaFZAci1116bFStW5KijjmqteQAAAABaXLOeAvPBD35Q+AEAAADsd5oVgIwbNy7f\n",
              "/va3s3Hjxrz88ssNLwAAAIB3s7JSqVRq6sl/2/cj+eveH6VSKWVlZdmxY0erDAetYfz48UmShQsX\n",
              "tvEkAAAAvFOatQfIzp07W2sOAAAAgFbTrFtgAAAAAPZHzQpAnn322XzmM59Jz549c/DBBze8AAAA\n",
              "AN7NmhWATJo0KRdeeGE+8IEP5IEHHsiZZ56Zf/7nf37TeVu3bs24ceMyYMCADB48OKNGjUpNTU1D\n",
              "fePGjTnttNPSv3//DBw4MA8++GBDbcaMGTnqqKNywAEHZNGiRY36XnTRRRk0aFAqKyszZMiQ3Hff\n",
              "fW85676u0Vp9br311vTv3z99+/bNpEmTUl9fnyT5+c9/nhNPPDFHH310jjnmmFxxxRVveavRzp07\n",
              "86UvfSl9+/ZNv379ctNNNzWptqtnn302J510UgYMGJAhQ4bkqaeealINAAAA9lulZjj22GNLpVKp\n",
              "NHDgwFKpVCrt3LmzNGTIkDed99prr5V++tOflnbu3FkqlUql73znO6WTTz65oX7RRReVvvGNb5RK\n",
              "pVJp+fLlpQ9/+MOl119/vVQqlUrLli0r/eEPfyidfPLJpbvuuqtR3xdffLHh59/85jelD3zgA6Ud\n",
              "O3bsdtZ9XaM1+vzxj38sfehDHyq98MILpZ07d5ZOP/300k033dTwOf7whz+USqW//t0+8YlPlObO\n",
              "nbvbPj/4wQ9KI0aMKG3fvr20efPmUu/evUu//e1v91rb1SmnnNKwxo9//OPSCSec0KRaUZxxxhml\n",
              "M844o63HAAAA4B3UrCtA2rdvnyTp2rVramtrs23btvzpT39603mdOnXK6NGjU1ZWliQZOnRoamtr\n",
              "G+p33HFHpkyZkiQZMmRIevbsmQceeCBJcuKJJ6aiomK36x900EENP9fV1e1x1n1dozX63HnnnRkz\n",
              "Zkx69OiRsrKyTJkyJfPmzUuSHHvssQ09OnXqlMrKykZ/qzeaP39+Jk2alHbt2uXggw/O2Wef3dBn\n",
              "T7U32rhxY1auXJnPf/7zSZIJEyZkzZo1qamp2WNtd7Zt29boccgvv/xytm3btte/BwAAALzTmhWA\n",
              "DBs2LJs3b84//uM/5vjjj8+RRx6ZsWPH7vX3brzxxobzNm/enPr6+vTo0aOh3qdPn6xevbpJM1x5\n",
              "5ZXp27dvxo8fnwULFjR6NO/fvN01WrrP6tWrc8QRR+y1x/r163PnnXfms5/9bMOxysrKrFu3bq99\n",
              "9lRbvHhxLrnkkiTJmjVr8qEPfSjl5X99AFBZWVl69+6d1atX77G2OzNnzsyBBx7Y6DVz5sxm/W0A\n",
              "AADgndDkx+CWSqVcfvnl6d69e84999x88pOfTF1dXQYOHLjH35sxY0Zqamr2uF9Hc8yaNSuzZs3K\n",
              "0qVLc8UVV+SXv/xlOnTo0CK929LLL7+c008/PVdccUVOOOGEhuPV1dVvu/eYMWMyZsyYt91nV1dd\n",
              "dVUuv/zyRsc6duzY4usAAADA29WsK0BGjRrV8HOvXr32Gn7Mnj07CxcuzJIlS9K5c+ckSffu3VNe\n",
              "Xp7169c3nFdbW5vevXs3Z5SMHDkyW7ZsyZNPPpmlS5emsrIylZWVue666/Z5jZbqs6vevXvnueee\n",
              "e8seW7ZsyWmnnZaxY8e+KVBoap+9rfE3vXr1ygsvvJDt27cn+WuwtXr16vTu3XuPtd3p2LFjunXr\n",
              "1uglAAEAAODdqMkBSFlZWQ4//PDd7vmxO9dff33mzZuXe++9t9HeHUkyceLEVFVVJUlWrFiRtWvX\n",
              "5uSTT95jv/r6+kZ7USxfvjwbN25MRUVFRo4cmerq6lRXV+fqq6/e5zVaqs+uJkyYkMWLF2f9+vUp\n",
              "lUqpqqrKOeeckyT5y1/+ktNOOy2nnXZavva1r+2xz8SJE3PLLbdkx44d+fOf/5z58+fn7LPP3mvt\n",
              "jQ499NAcd9xxuf3225MkCxYsyOGHH55+/frtsQYAAAD7s7JSqVTa20nPPvts+vfvnzPPPDO//vWv\n",
              "M3r06Lz//e9vqF9//fWNzn/++efTq1evVFRUpGvXrkn+erXAsmXLkiQbNmzI+eefn1WrVqVDhw65\n",
              "6aabcsoppyRJpk+fnqqqqmzatCldu3ZNp06d8thjj6VLly4ZNWpU6urqUl5eni5duuTaa6/NiBEj\n",
              "djvzvqxxyCGHtFqfW265JbNmzUqSDB8+PFVVVWnfvn2uu+66/Mu//EuOOeaYhnMnTpzYEMBUVlbm\n",
              "nnvuSc+ePbNjx45MmzYtS5YsSVlZWaZNm5Z/+qd/SpI91hYvXpzFixfne9/7XpLk6aefzoUXXpjN\n",
              "mzenW7dumTt3bj760Y/utVYU48ePT5IsXLiwjScBAADgndKkAOS4447Lb37zm3zzm9/cbf0b3/hG\n",
              "iw8GrUUAAgAA8N7TpE1Q/5aRCDoAAACA/VGTApC6urrcfffdeauLRVrjCSMAAAAALaVJAcimTZty\n",
              "ww037DYAKSsrE4AAAAAA72pNCkD69euXn//85609CwAAAECraPJjcAEAAAD2V00KQNziAgAAAOzP\n",
              "mhSAvNXjbwEAAAD2B26BAQAAAAqvSQHIiy++2NpzAAAAALSaJgUgn/rUp5IkZ511VqsOAwAAANAa\n",
              "mvQY3Ndeey3Lli3Lk08+mSeffDKlUqlRfdCgQa0yHAAAAEBLaFIActlll+Wiiy7KqlWr3vREmLKy\n",
              "svzxj39sleEAAAAAWkJZadfLOfZg4sSJ+fGPf9ya80CrGz9+fJJk4cKFbTwJAAAA75QmXQHyNz/+\n",
              "8Y/z6quvprq6OklSWVmZzp07t8ZcAAAAAC2mWQHII488kvHjx+ewww5LWVlZNmzYkAULFuTjH/94\n",
              "a80HAAAA8LY1KwD58pe/nDvvvDOf+MQnkiS/+tWv8uUvfzm//vWvW2U4AAAAgJbQpMfg/s1rr73W\n",
              "EH4kyUknnZStW7e2+FAAAAAALalZAcj73//+LF26tOH9fffdly5durT4UAAAAAAtqVm3wNx4442Z\n",
              "MGFC2rVrlyTZuXOnJ2kAAAAA73rNCkBOOOGE1NTU5Omnn06SHHXUUWnfvn2rDAYAAADQUpoVgCRJ\n",
              "+/btM3DgwNaYBQAAAKBVNGsPEAAAAID9kQAEAAAAKLx9CkC2bdvW0nMAAAAAtJpmBSBPPPFEBg4c\n",
              "mL59+yZJHn300VxxxRWtMhgAAABAS2lWADJt2rRUVVXlkEMOSZIcd9xx+elPf9oqgwEAAAC0lGYF\n",
              "IH/5y1/yd3/3dw3vy8rK0qFDhxYfCgAAAKAlNSsAKS8vT319fcrKypIka9asSbt27VplMAAAAICW\n",
              "0qwA5B//8R8zbty4bNq0KV/72tfyyU9+0h4gAAAAwLteeXNO/vznP5+Kior85Cc/yeuvv57bb7+9\n",
              "0S0xAAAAAO9GzQpA/u///b+5+OKLc9JJJ73pGAAAAMC7VbNugbnpppvedOz//J//02LDwDvhsMMO\n",
              "y2GHHdbWYwAAAPAOatIVIMuXL88jjzySTZs25T/+4z8ajtfV1WXbtm2tNhy0hu9+97ttPQIAAADv\n",
              "sCYFIC+88EKqq6vz6quv5rHHHms43q1bt3z/+99vrdkAAAAAWkRZqVQqNfXkJUuW5DOf+UxrzgMA\n",
              "AADQ4poVgCTJunXr8tvf/jZbt25tODZmzJgWHwwAAACgpTQrAJk7d26++c1v5s9//nP69++fxx9/\n",
              "PEOHDs3DDz/cmjMCAAAAvC3NegrM9ddfn8ceeyx9+/bNo48+mp///OcZMGBAa80GAAAA0CKaFYB0\n",
              "6NAhH/jAB7J9+/YkybBhw1JdXd0acwEAAAC0mCY9BeZvOnbsmFKplAEDBuTb3/52jjjiiPzlL39p\n",
              "rdkAAAAAWkSz9gD5+c9/nuOPPz5/+tOfMmXKlLz00kuZOXNmRo4c2ZozAgAAALwtzX4KDAAAAMD+\n",
              "psl7gKxYsSJnn312Bg4cmIEDB+acc87JypUrW3M2AAAAgBbRpADkkUceyamnnpqKiopMnz491157\n",
              "bY488siceuqpWbZsWWvPCAAAAPC2NOkWmDPOOCMXXHBBzjjjjEbHf/KTn2Tu3LlZtGhRa80HAAAA\n",
              "8LY1KQAZMGBAnnnmmWbXAAAAAN4NmnQLTOfOnd+y1qVLlxYbBgAAAKA1lDflpG3btuXJJ5/M7i4W\n",
              "2bp1a4sPBQAAANCSmnQLTJ8+fVJWVrb7BmVl+eMf/9jigwEAAAC0lCYFIMC709SpU5Mk3/3ud9t4\n",
              "EgAAgHe3Jt0CA7w7bdiwoa1HAAAA2C80aRNUAAAAgP2ZAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAA\n",
              "AABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAA\n",
              "AABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAA\n",
              "AABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAA\n",
              "AABA4QlAAAAAgMITgAAAAACFJwABAAAACq/VApCtW7dm3LhxGTBgQAYPHpxRo0alpqamob5x48ac\n",
              "dtpp6d+/fwYOHJgHH3ywoTZjxowcddRROeCAA7Jo0aJGfS+66KIMGjQolZWVGTJkSO677763nGFf\n",
              "12itPrfeemv69++fvn37ZtKkSamvr0+SPPLII6msrExlZWWOOeaYTJ48Odu2bdttj1dffTWf+9zn\n",
              "0q9fvwwYMCB33nlnk2q7WrZsWQYPHpwBAwZkxIgRWbt2bZNqAAAAsD9q1StALr300jz99NN5/PHH\n",
              "M3bs2FxyySUNtSuvvDJDhw7Ns88+m7lz5+bcc89tCARGjhyZJUuWZNiwYW/qecMNN+SJJ55IdXV1\n",
              "5syZk4kTJ2bnzp27XX9f12iNPqtWrco111yThx56KDU1NdmwYUPmzJmTJBk8eHBWrFiR6urqPPnk\n",
              "k9m4cWP+8z//c7d9Zs+enY4dO6ampiY/+9nP8sUvfjGbN2/ea+2Ndu7cmfPOOy/f/va388wzz2T0\n",
              "6NG57LLL9loDAACA/VWrBSCdOnXK6NGjU1ZWliQZOnRoamtrG+p33HFHpkyZkiQZMmRIevbsmQce\n",
              "eCBJcuKJJ6aiomK3fQ866KCGn+vq6vY4w76u0Rp97rzzzowZMyY9evRIWVlZpkyZknnz5iVJOnfu\n",
              "nPbt2ydJXn/99bz22msNf7ddzZ8/v2GWI488MsOHD89dd92119obPfrooykvL88pp5ySJJk8eXLu\n",
              "vvvubN26dY81AAAA2F+9Y3uA3HjjjRk7dmySZPPmzamvr0+PHj0a6n369Mnq1aub1OvKK69M3759\n",
              "M378+CxYsCAHHPDmj/F212jpPqtXr84RRxzxlj1qa2szePDgfPCDH8yBBx6YL37xi0mSdevWpbKy\n",
              "skl99lSrqqrK17/+9d2e17Vr13Tr1i3r1q3bY21X27Zty8svv9zo9Va37gAAAEBbekcCkBkzZqSm\n",
              "piYzZ85skX6zZs3KH/7wh9xxxx254oor8vrrr7dI37bUp0+fPP7441m/fn22bduWhQsXJkl69uyZ\n",
              "6urqt91/ypQp+da3vvW2+7zRzJkzc+CBBzZ6tdR/xgAAANCSWj0AmT17dhYuXJglS5akc+fOSZLu\n",
              "3bunvLw869evbzivtrY2vXv3blbvkSNHZsuWLXnyySezdOnSho1Er7vuun1eo6X67Kp379557rnn\n",
              "9trj/e9/f84555z88Ic/bHafpq6x63lbtmxJXV1devbsucfarq666qrU1dU1el111VV7+jMAAABA\n",
              "m2jVAOT666/PvHnzcu+99zbauyNJJk6cmKqqqiTJihUrsnbt2px88sl77FdfX9/oSTLLly/Pxo0b\n",
              "U1FRkZEjR6a6ujrV1dW5+uqr93mNluqzqwkTJmTx4sVZv359SqVSqqqqcs455yRJampqGjZVff31\n",
              "13PXXXdl0KBBu+3zxllWrVqV+++/P+PGjdtr7Y2OP/741NfX5xe/+EWS5Oabb87pp5+eTp067bG2\n",
              "q44dO6Zbt26NXh07dmzW3wUAAADeCWWlUqnUGo2ff/759OrVKxUVFenatWuSv/6DedmyZUmSDRs2\n",
              "5Pzzz8+qVavSoUOH3HTTTQ0bb06fPj1VVVXZtGlTunbtmk6dOuWxxx5Lly5dMmrUqNTV1aW8vDxd\n",
              "unTJtddemxEjRux2hn1Z45BDDmm1PrfccktmzZqVJBk+fHiqqqrSvn37zJkzJ//xH/+Rdu3aZfv2\n",
              "7fnUpz6Vf/u3f0unTp2ybt26jB49uuE2mFdeeSUXX3xxVq5cmXbt2mX69Ok566yz9lqrqqrKunXr\n",
              "Gm6DeeSRRzJ58uRs3bo1PXv2zG233ZZevXrttca7y/jx45Ok4ZYpAAAAdq/VAhCg9QlAAAAAmuYd\n",
              "ewoMAAAAQFsRgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAAAABA\n",
              "4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAAAABA\n",
              "4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAAAABA\n",
              "4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEIAAAAUHgCEAAAAKDwytt6AGDf\n",
              "HXbYYW09AgAAwH6hrFQqldp6CAAAAIDW5BYYAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUnAAEA\n",
              "AAAKTwACAAAAFJ4ABAAAACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUnAAEA\n",
              "AAAKTwACAAAAFJ4ABAAAACg8AQgAAABQeAIQ4B01derUTJ06ta3HAAAA3mPK23oA4L1lw4YNbT0C\n",
              "AADwHuQKEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEI\n",
              "AAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEI\n",
              "AAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEI\n",
              "AAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSeAAQAAAAoPAEI\n",
              "AAAAUHgCEAAAAKDwWiUA2bp1a8aNG5cBAwZk8ODBGTVqVGpqahrqGzduzGmnnZb+/ftn4MCBefDB\n",
              "BxtqM2bMyFFHHZUDDjggixYtatT3oosuyqBBg1JZWZkhQ4bkvvvue8sZ9nWN1upz6623pn///unb\n",
              "t28mTZqU+vr6RvVSqZQRI0bkoIMOesseO3fuzJe+9KX07ds3/fr1y0033dSk2q6effbZnHTSSRkw\n",
              "YECGDBmSp556qkk1AAAA2F+12hUgl156aZ5++uk8/vjjGTt2bC655JKG2pVXXpmhQ4fm2Wefzdy5\n",
              "c3Puuec2BAIjR47MkiVLMmzYsDf1vOGGG/LEE0+kuro6c+bMycSJE7Nz587drr+va7RGn1WrVuWa\n",
              "a67JQw89lJqammzYsCFz5sx502fr27fvHvvcfvvt+d3vfpdnnnkmy5cvz7//+783BBR7qu1q8uTJ\n",
              "ufTSS/PMM8/kq1/9ai688MIm1QAAAGB/1SoBSKdOnTJ69OiUlZUlSYYOHZra2tqG+h133JEpU6Yk\n",
              "SYYMGZKePXvmgQceSJKceOKJqaio2G3fN14dUVdXt8cZ9nWN1uhz5513ZsyYMenRo0fKysoyZcqU\n",
              "zJs3r6H+1FNPZdGiRbnyyiv32Gf+/PmZNGlS2rVrl4MPPjhnn312Q5891d5o48aNWblyZT7/+c8n\n",
              "SSZMmJA1a9akpqZmj7Xd2bZtW15++eVGr23btu317wEAAADvtHdkD5Abb7wxY8eOTZJs3rw59fX1\n",
              "6dGjR0O9T58+Wb16dZN6XXnllenbt2/Gjx+fBQsW5IAD3vwR3u4aLd1n9erVOeKII3bbo76+PpMm\n",
              "TcrNN9+cdu3avel3Kysrs27dur322VNt8eLFDVfgrFmzJh/60IdSXl6eJCkrK0vv3r2zevXqPdZ2\n",
              "Z+bMmTnwwAMbvWbOnNmsvw0AAAC8E8pbe4EZM2akpqZmj/t1NMesWbMya9asLF26NFdccUV++ctf\n",
              "pkOHDi3Suy1885vfzPjx4/ORj3yk0VUyf1NdXf221xgzZkzGjBnztvvs6qqrrsrll1/e6FjHjh1b\n",
              "fB0AAAB4u1r1CpDZs2dn4cKFWbJkSTp37pwk6d69e8rLy7N+/fqG82pra9O7d+9m9R45cmS2bNmS\n",
              "J598MkuXLk1lZWUqKytz3XXX7fMaLdVnV717985zzz232x4PPPBAvvOd76RPnz75u7/7u7z88svp\n",
              "06dPNm3a1Kw+e6q9Ua9evfLCCy9k+/btSf66+erq1avTu3fvPdZ2p2PHjunWrVujlwAEAACAd6NW\n",
              "C0Cuv/76zJs3L/fee++bnmwyceLEVFVVJUlWrFiRtWvX5uSTT95jv/r6+kZ7USxfvjwbN25MRUVF\n",
              "Ro4cmerq6lRXV+fqq6/e5zVaqs+uJkyYkMWLF2f9+vUplUqpqqrKOeeckyR56KGH8txzz6W2tjYP\n",
              "P/xwunXrltra2hxyyCFv6jNx4sTccsst2bFjR/785z9n/vz5Ofvss/dae6NDDz00xx13XG6//fYk\n",
              "yYIFC3L44YenX79+e6wBAADA/qysVCqVWrrp888/n169eqWioiJdu3ZN8terBZYtW5Yk2bBhQ84/\n",
              "//ysWrUqHTp0yE033ZRTTjklSTJ9+vRUVVVl06ZN6dq1azp16pTHHnssXbp0yahRo1JXV5fy8vJ0\n",
              "6dIl1157bUaMGLHbGfZljd2FDi3V55ZbbsmsWbOSJMOHD09VVVXat2/f6Jza2tpUVlbmpZdeajhW\n",
              "WVmZe+65Jz179syOHTsybdq0LFmyJGVlZZk2bVr+6Z/+KUn2WFu8eHEWL16c733ve0mSp59+Ohde\n",
              "eGE2b96cbt26Ze7cufnoRz+61xq0hPHjxydJFi5c2MaTAAAA7yWtEoAAvBUBCAAA0BbekafAAAAA\n",
              "ALQlAQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUnAAEAAAAKTwACAAAAFJ4ABAAA\n",
              "ACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUnAAEAAAAKTwACAAAAFJ4ABAAA\n",
              "ACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUnAAEAAAAKTwACAAAAFJ4ABAAA\n",
              "ACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOAAAAAAIUnAAEAAAAKr7ytBwDeWw477LC2\n",
              "HgEAAHgPKiuVSqW2HgIAAACgNbkFBgAAACg8AQgAAABQeAIQAAAAoPAEIAAAAEDhCUAAAACAwhOA\n",
              "AAAAAIUnAAEAAAAKTwDCfmfbtm35l3/5l2zbtq2tR4Em871lf+M7y/7I95b9je8s+6P9+XtbViqV\n",
              "Sm09BDTHyy+/nAMPPDB1dXXp1q1bW48DTeJ7y/7Gd5b9ke8t+xvfWfZH+/P31hUgAAAAQOEJQAAA\n",
              "AIDCE4AAAAAAhScAYb/TsWPHfOMb30jHjh3behRoMt9b9je+s+yPfG/Z3/jOsj/an7+3NkEFAAAA\n",
              "Cs8VIAAAAEDhCUAAAACAwhOAAAAAAIUnAAEAAAAKTwDCfuXZZ5/NSSedlAEDBmTIkCF56qmn2nok\n",
              "2KNp06alT58+KSsrS3V1dVuPA3u1devWjBs3LgMGDMjgwYMzatSo1NTUtPVYsFennnpqBg0alMrK\n",
              "ynzyk5/MY4891tYjQZPMnTs3ZWVlWbRoUVuPAnvVp0+fHHXUUamsrExlZWXmz5/f1iM1iwCE/crk\n",
              "yZNz6aWX5plnnslXv/rVXHjhhW09EuzRmWeemYcffjhHHHFEW48CTXbppZfm6aefzuOPP56xY8fm\n",
              "kksuaeuRYK/uuOOOPPHEE6murs7ll1/ufyOwX6itrc0tt9ySoUOHtvUo0GTz589PdXV1qqurc/bZ\n",
              "Z7f1OM0iAGG/sXHjxqxcuTKf//znkyQTJkzImjVr/D+TvKsNGzYshx9+eFuPAU3WqVOnjB49OmVl\n",
              "ZUmSoUOHpra2tm2HgiY46KCDGn6uq6tr+A7Du9XOnTtzySWX5Dvf+U46duzY1uPAe0J5Ww8ATbVm\n",
              "zZp86EMfSnn5X7+2ZWVl6d27d1avXp1+/fq18XQAxXTjjTdm7NixbT0GNMkFF1yQX/ziF0mSe+65\n",
              "p42ngT27/vrr84lPfCLHH398W48CzXLBBRekVCrlxBNPzKxZs3LIIYe09UhN5goQAGC3ZsyYkZqa\n",
              "msycObOtR4Em+a//+q+sWbMm06dPz1e/+tW2Hgfe0m9/+9ssWLAgX/va19p6FGiWBx98ME888UR+\n",
              "85vf5IMf/GC+8IUvtPVIzeIKEPYbvXr1ygsvvJDt27envLw8pVIpq1evTu/evdt6NIDCmT17dhYu\n",
              "XJilS5emc+fObT0ONMsXvvCFTJkyJZs3b0737t3behx4k4ceeii1tbXp379/kmT9+vW59NJL88IL\n",
              "L2Tq1KltPB28tb/926t9+/a57LLLMmDAgDaeqHlcAcJ+49BDD81xxx2X22+/PUmyYMGCHH744W5/\n",
              "AWhh119/febNm5d777230b4K8G710ksvZd26dQ3vFy1alO7du+fggw9uw6ngrU2dOjUvvPBCamtr\n",
              "U1tbm6FDh2bOnDnCD97VXnnllbz00ksN7+fNm5djjz227QbaB64AYb9y880358ILL8yMGTPSrVu3\n",
              "zJ07t61Hgj2aPHlyfvrTn2b9+vX59Kc/na5du9q4l3e1559/Pl/5yldSUVGRU045JUnSsWPHLFu2\n",
              "rI0ng7dWV1eXiRMn5rXXXssBBxyQQw45JP/93/9tI1SAFrRhw4ZMmDAhO3bsSKlUSkVFRf7rv/6r\n",
              "rcdqlrJSqVRq6yEAAAAAWpNbYAAAAIDCE4AAAAAAhScAAQAAAApPAAIAvKds2bIl73//+/P3f//3\n",
              "bT1KI/fff3/+53/+Z4/n/Pd//3emTJnScH5lZWWjem1tbaMn91RVVWXQoEGprKzM//pf/yvnnXde\n",
              "Q61Pnz456qijMnjw4PTr1y9jx47Nr371q0ZrXXrppW//gwHAu4QABAB4T5k/f36OP/74LFy4MH/5\n",
              "y1/aepwGTQlArrrqqlx11VVN6rdy5cr827/9W+6///5UV1fn97//fb7yla80Omf+/Pl5/PHHU1NT\n",
              "ky984QsZPXp0wxN/PvvZz+bRRx/Ns88+u28fCADeZQQgAMB7yq233pqvfvWrGTZsWObPn99w/Pvf\n",
              "/35GjhyZz33uczn66KNz0kkn5Xe/+13OOOOMfOQjH8mpp57aEJj85S9/ycUXX5yBAwdm4MCB+eY3\n",
              "v9nQZ/jw4Vm0aFHD+zPPPDPf//73kyQXXnhhJk+enE996lMZMGBAxo8fn9dffz3V1dWpqqrKD3/4\n",
              "w1RWVuZb3/rWm+Z+6KGHctBBB+WII45o0ud8/vnn07Vr13Tt2jVJUlZWluOOO+4tzx8/fnymTJmS\n",
              "2bNnNxw766yz8r3vfa9J6wFAU02bNi19+vRJWVlZqqurm/Q7L774Ys4777wMGDAgxxxzTK688spm\n",
              "rysAAQDeM373u99lzZo1+fSnP52///u/z6233tqovmLFivzrv/5rfve736Vv3745/fTTU1VVld//\n",
              "/vfp0KFDfvCDHyRJrr322mzbti1PPPFEli1blkWLFjUKU/akuro6d999d37/+99nw4YNWbBgQSor\n",
              "KzNlypScd955qa6uzte//vU3/d7999+fj33sY03+rKeeemq6du2a3r175+yzz85NN92UF198cY+/\n",
              "87GPfSxPPfVUw/uPf/zjue+++5q8JgA0xZlnnpmHH364yaF+klx88cU59thj88wzz+Spp57KZZdd\n",
              "1ux1BSAAwHvGrbfemgsuuCDt2rXL6NGjs2rVqvz+979vqH/84x9P7969kyQnnHBChgwZksMOOyxJ\n",
              "MmTIkIbbQZYuXZpJkyblgAMOSJcuXXLBBRfk3nvvbdIMZ5xxRjp37px27drlxBNPzB/+8Icm/d7z\n",
              "zz/fMEvy1ys6dudvxzt37pyHHnoo99xzTz7xiU9k4cKFGTRoUP785z+/5RqlUqnR+x49euT5559v\n",
              "0nwA0FTDhg3L4Ycf/qbjK1asyIgRI3LCCSfk2GOPzY9//OMkSU1NTVauXJnLL7+84dwePXo0e10B\n",
              "CADwnlBfX5/bbrstP/jBD9KnT5/069cvr776aqOrQDp16tTwc7t27d70fvv27bvt/cYwory8PDt2\n",
              "7Gh4v3Xr1kbnNrXnrjp37tyo1yGHHJLNmzc3OudPf/pTDj300EZzHXvssZk2bVruu+++vP/978/9\n",
              "99//lmusWLEiAwcObDT7+973vibNBwBvx0svvZRLL700P/zhD7Ny5crce++9+cpXvpK1a9fmd7/7\n",
              "XQ4//PBMnTo1xx9/fE499dQ89thjzV5DAAIAvCcsXrw4FRUVWbt2bWpra1NbW5tf//rXue2221Jf\n",
              "X9+sXiNHjsytt96aUqmUV155JbfddltOPfXUJEm/fv0aNhJdtWpVHn744Sb17NatW+rq6t6yPmjQ\n",
              "oDz99NMN7/v375/27dvnnnvuSZLs3LkzN998c8Mc/+///b888cQTDeevWbMmmzZtSkVFxW77/+Qn\n",
              "P8l3v/vdRhul/v73v8/gwYObND8AvB2/+tWv8sc//jGf+cxnUllZmZEjRyZJnn766Wzfvj3Lly/P\n",
              "Oeeck0cffTRf/vKX89nPfrbZ//1d3hqDAwC829x6662NHgObJB/5yEfy4Q9/OHfffXezel1zzTWZ\n",
              "Nm1aPvrRjyZJJk6cmLPOOitJcsUVV+Tss8/ORz/60RxzzDFN3rfjjDPOyG233ZbKysqMHz/+TfuA\n",
              "fPazn823vvWt7NixI+3atUv79u1z11135fLLL8///t//Ozt37szHPvaxXHfddUmSV199NV/+8pez\n",
              "fv36vO9970upVMqsWbMaPTr37LPPTqdOnfLKK6/k6KOPzj333NNo3v/5n//JmWee2ay/DQDsi1Kp\n",
              "lGOOOabRI9n/ZuXKlfnwhz+cU045JUnymc98Jq+//nqee+659OvXr8lrlJV2vdkTAIB3pX/4h3/I\n",
              "8OHDM3HixFZf609/+lNGjBiRlStXpkOHDq2+HgDvPX369MmiRYtSWVmZF198MUcffXRuu+22hqs/\n",
              "qqurc/TRR6d9+/b56Ec/mh/96EcZNGhQli9fntGjR2ft2rXp2LFjk9cTgAAA7Cc2b96cJUuW5POf\n",
              "/3yrr7Vs2bLs2LEjJ510UquvBcB7y+TJk/PTn/4069evT/fu3dO1a9fU1NTkN7/5Tf75n/85mzdv\n",
              "Tn19fXr37p1FixalU6dOefTRR/PFL34xr732Wjp27JjZs2fn5JNPbta6AhAAAACg8GyCCgAAABSe\n",
              "AAQAAAAoPAEIAAAAUHgCEAAAAKDwBCAAAABA4QlAAAAAgMITgAAAAACFJwABAAAACk8AAgAAABSe\n",
              "AAQAAAAovP8Pc1Sqps5jtEoAAAAASUVORK5CYII=\n",
              "\">\n",
              "      </div>\n",
              "      <script type=\"text/javascript\">\n",
              "        (() => {\n",
              "          const chartElement = document.getElementById(\"chart-a05e7cf6-bb4c-479c-bec1-5fc04b9ff97c\");\n",
              "          async function getCodeForChartHandler(event) {\n",
              "            const chartCodeResponse =  await google.colab.kernel.invokeFunction(\n",
              "                'getCodeForChart', [\"chart-a05e7cf6-bb4c-479c-bec1-5fc04b9ff97c\"], {});\n",
              "            const responseJson = chartCodeResponse.data['application/json'];\n",
              "            await google.colab.notebook.addCell(responseJson.code, 'code');\n",
              "          }\n",
              "          chartElement.onclick = getCodeForChartHandler;\n",
              "        })();\n",
              "      </script>\n",
              "      <style>\n",
              "        .colab-quickchart-chart-with-code  {\n",
              "            display: block;\n",
              "            float: left;\n",
              "            border: 1px solid transparent;\n",
              "        }\n",
              "\n",
              "        .colab-quickchart-chart-with-code:hover {\n",
              "            cursor: pointer;\n",
              "            border: 1px solid #aaa;\n",
              "        }\n",
              "      </style>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# View the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "gYV9SDr56wGl",
        "outputId": "9f15bd63-6e3d-40d5-e23c-748f0c8d15f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transaction ID                 0\n",
              "Country                        0\n",
              "Amount (USD)                   0\n",
              "Transaction Type               0\n",
              "Date of Transaction            0\n",
              "Person Involved                0\n",
              "Industry                       0\n",
              "Destination Country            0\n",
              "Reported by Authority          0\n",
              "Source of Money                0\n",
              "Money Laundering Risk Score    0\n",
              "Shell Companies Involved       0\n",
              "Financial Institution          0\n",
              "Tax Haven Country              0\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Transaction ID</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount (USD)</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Transaction Type</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date of Transaction</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Person Involved</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Destination Country</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Reported by Authority</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Source of Money</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Financial Institution</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tax Haven Country</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Check for missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LYEVd61j6wGm",
        "outputId": "260725a0-45d3-4172-b73b-20e5229554fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transaction ID                  object\n",
              "Country                         object\n",
              "Amount (USD)                   float64\n",
              "Transaction Type                object\n",
              "Date of Transaction             object\n",
              "Person Involved                 object\n",
              "Industry                        object\n",
              "Destination Country             object\n",
              "Reported by Authority             bool\n",
              "Source of Money                 object\n",
              "Money Laundering Risk Score      int64\n",
              "Shell Companies Involved         int64\n",
              "Financial Institution           object\n",
              "Tax Haven Country               object\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Transaction ID</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Country</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount (USD)</th>\n",
              "      <td>float64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Transaction Type</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date of Transaction</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Person Involved</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Industry</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Destination Country</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Reported by Authority</th>\n",
              "      <td>bool</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Source of Money</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <td>int64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Financial Institution</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tax Haven Country</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Get data types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "kANCm_zF6wGm",
        "outputId": "33e75b1b-38bc-4a78-9dc6-59fe2e9b2fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Amount (USD)  Money Laundering Risk Score  Shell Companies Involved\n",
              "count  1.000000e+04                 10000.000000              10000.000000\n",
              "mean   2.501818e+06                     5.526400                  4.469400\n",
              "std    1.424364e+06                     2.893603                  2.879773\n",
              "min    1.003180e+04                     1.000000                  0.000000\n",
              "25%    1.279005e+06                     3.000000                  2.000000\n",
              "50%    2.501310e+06                     6.000000                  4.000000\n",
              "75%    3.722416e+06                     8.000000                  7.000000\n",
              "max    4.999812e+06                    10.000000                  9.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b72a1bf-c2cd-440f-9e54-fef5123e614f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.000000e+04</td>\n",
              "      <td>10000.000000</td>\n",
              "      <td>10000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.501818e+06</td>\n",
              "      <td>5.526400</td>\n",
              "      <td>4.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.424364e+06</td>\n",
              "      <td>2.893603</td>\n",
              "      <td>2.879773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.003180e+04</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.279005e+06</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.501310e+06</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.722416e+06</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>7.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.999812e+06</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b72a1bf-c2cd-440f-9e54-fef5123e614f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2b72a1bf-c2cd-440f-9e54-fef5123e614f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2b72a1bf-c2cd-440f-9e54-fef5123e614f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f3b0a987-b2b2-49ca-b2fc-9260dd34766e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3b0a987-b2b2-49ca-b2fc-9260dd34766e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f3b0a987-b2b2-49ca-b2fc-9260dd34766e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Amount (USD)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1740887.1434555505,\n        \"min\": 10000.0,\n        \"max\": 4999812.408537276,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2501817.6645666594,\n          2501310.403604204,\n          10000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Money Laundering Risk Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3533.6956191901877,\n        \"min\": 1.0,\n        \"max\": 10000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.5264,\n          6.0,\n          10000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Shell Companies Involved\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3534.052664823103,\n        \"min\": 0.0,\n        \"max\": 10000.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          4.4694,\n          4.0,\n          10000.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znkDwy846wGm"
      },
      "source": [
        "## Processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7YWgMej6wGm"
      },
      "source": [
        "### Handle missing values if applicable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "R1AAOvjD6wGm"
      },
      "outputs": [],
      "source": [
        "# For numerical features\n",
        "numerical_features = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "df[numerical_features] = imputer.fit_transform(df[numerical_features])\n",
        "\n",
        "# For categorical features\n",
        "categorical_features = ['Country', 'Transaction Type', 'Person Involved', 'Industry',\n",
        "                        'Destination Country', 'Financial Institution', 'Tax Haven Country']\n",
        "imputer_cat = SimpleImputer(strategy='most_frequent')\n",
        "df[categorical_features] = imputer_cat.fit_transform(df[categorical_features])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5cXCduR6wGm"
      },
      "source": [
        "### Dropping Features and OHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4tgHlLPf6wGm"
      },
      "outputs": [],
      "source": [
        "# Drop Irrelevant Features\n",
        "df.drop('Transaction ID', axis=1, inplace=True) # Dropped because it is unique for each transaction\n",
        "# df.drop('Person Involved', axis=1, inplace=True) # Frequency Encoding will be implemented\n",
        "# df.drop('Financial Institution', axis=1, inplace=True) # Implement Frequency Encoding\n",
        "df.drop('Date of Transaction', axis=1, inplace=True) # Date of transaction is not relevant\n",
        "\n",
        "# Convert 'Reported by Authority' to integer\n",
        "df['Reported by Authority'] = df['Reported by Authority'].astype(int)\n",
        "\n",
        "# Frequency encoding for 'Financial Institution'\n",
        "df['Financial Institution'] = df.groupby('Financial Institution')['Financial Institution'].transform('count')\n",
        "\n",
        "# Frequency encoding for 'Person Involved'\n",
        "df['Person Involved'] = df.groupby('Person Involved')['Person Involved'].transform('count')\n",
        "\n",
        "# Encode target variable\n",
        "le = LabelEncoder()\n",
        "df['Source of Money'] = le.fit_transform(df['Source of Money'])\n",
        "\n",
        "# One-Hot Encode nominal categorical features\n",
        "nominal_features = ['Country', 'Transaction Type', 'Industry',\n",
        "                    'Destination Country', 'Tax Haven Country']\n",
        "df = pd.get_dummies(df, columns=nominal_features, drop_first=True)\n",
        "\n",
        "dummy_columns = df.filter(like='_').columns\n",
        "df[dummy_columns] = df[dummy_columns].astype(int)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "XRGGPdSZ6wGm",
        "outputId": "b50b172e-b0f2-40e8-86e3-31928874c553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n",
              "       'Source of Money', 'Money Laundering Risk Score',\n",
              "       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n",
              "       'Country_India', 'Country_Russia', 'Country_Singapore',\n",
              "       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n",
              "       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n",
              "       'Transaction Type_Offshore Transfer',\n",
              "       'Transaction Type_Property Purchase',\n",
              "       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n",
              "       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n",
              "       'Industry_Oil & Gas', 'Industry_Real Estate',\n",
              "       'Destination Country_China', 'Destination Country_India',\n",
              "       'Destination Country_Russia', 'Destination Country_Singapore',\n",
              "       'Destination Country_South Africa', 'Destination Country_Switzerland',\n",
              "       'Destination Country_UAE', 'Destination Country_UK',\n",
              "       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n",
              "       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n",
              "       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "W6CsiyNz6wGm",
        "outputId": "17179b88-34fe-42f4-f1f2-ad06c9bb390b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Amount (USD)  Person Involved  Reported by Authority  Source of Money  \\\n",
              "0  3.267530e+06                2                      1                0   \n",
              "1  4.965767e+06                1                      0                0   \n",
              "2  9.416750e+04                1                      1                0   \n",
              "3  3.864201e+05                5                      0                0   \n",
              "4  6.433784e+05                4                      1                0   \n",
              "\n",
              "   Money Laundering Risk Score  Shell Companies Involved  \\\n",
              "0                          6.0                       1.0   \n",
              "1                          9.0                       0.0   \n",
              "2                          1.0                       3.0   \n",
              "3                          7.0                       2.0   \n",
              "4                          1.0                       9.0   \n",
              "\n",
              "   Financial Institution  Country_China  Country_India  Country_Russia  ...  \\\n",
              "0                     17              0              0               0  ...   \n",
              "1                     24              1              0               0  ...   \n",
              "2                     12              0              0               0  ...   \n",
              "3                     18              0              0               0  ...   \n",
              "4                     19              0              0               0  ...   \n",
              "\n",
              "   Destination Country_South Africa  Destination Country_Switzerland  \\\n",
              "0                                 0                                0   \n",
              "1                                 1                                0   \n",
              "2                                 0                                1   \n",
              "3                                 0                                0   \n",
              "4                                 0                                0   \n",
              "\n",
              "   Destination Country_UAE  Destination Country_UK  Destination Country_USA  \\\n",
              "0                        0                       0                        1   \n",
              "1                        0                       0                        0   \n",
              "2                        0                       0                        0   \n",
              "3                        0                       0                        0   \n",
              "4                        0                       0                        1   \n",
              "\n",
              "   Tax Haven Country_Cayman Islands  Tax Haven Country_Luxembourg  \\\n",
              "0                                 0                             0   \n",
              "1                                 0                             0   \n",
              "2                                 0                             0   \n",
              "3                                 0                             0   \n",
              "4                                 0                             1   \n",
              "\n",
              "   Tax Haven Country_Panama  Tax Haven Country_Singapore  \\\n",
              "0                         0                            1   \n",
              "1                         0                            0   \n",
              "2                         0                            0   \n",
              "3                         1                            0   \n",
              "4                         0                            0   \n",
              "\n",
              "   Tax Haven Country_Switzerland  \n",
              "0                              0  \n",
              "1                              0  \n",
              "2                              1  \n",
              "3                              0  \n",
              "4                              0  \n",
              "\n",
              "[5 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa6e81fb-65ac-4d93-886f-acf16b74d0ce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Amount (USD)</th>\n",
              "      <th>Person Involved</th>\n",
              "      <th>Reported by Authority</th>\n",
              "      <th>Source of Money</th>\n",
              "      <th>Money Laundering Risk Score</th>\n",
              "      <th>Shell Companies Involved</th>\n",
              "      <th>Financial Institution</th>\n",
              "      <th>Country_China</th>\n",
              "      <th>Country_India</th>\n",
              "      <th>Country_Russia</th>\n",
              "      <th>...</th>\n",
              "      <th>Destination Country_South Africa</th>\n",
              "      <th>Destination Country_Switzerland</th>\n",
              "      <th>Destination Country_UAE</th>\n",
              "      <th>Destination Country_UK</th>\n",
              "      <th>Destination Country_USA</th>\n",
              "      <th>Tax Haven Country_Cayman Islands</th>\n",
              "      <th>Tax Haven Country_Luxembourg</th>\n",
              "      <th>Tax Haven Country_Panama</th>\n",
              "      <th>Tax Haven Country_Singapore</th>\n",
              "      <th>Tax Haven Country_Switzerland</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.267530e+06</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.965767e+06</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.416750e+04</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.864201e+05</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>18</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.433784e+05</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows  40 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa6e81fb-65ac-4d93-886f-acf16b74d0ce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fa6e81fb-65ac-4d93-886f-acf16b74d0ce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fa6e81fb-65ac-4d93-886f-acf16b74d0ce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4a3cbc61-c9e9-4dd8-8c9b-f55c2cec591b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4a3cbc61-c9e9-4dd8-8c9b-f55c2cec591b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4a3cbc61-c9e9-4dd8-8c9b-f55c2cec591b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "y3M1fT0C6wGn",
        "outputId": "48534882-bb57-41ed-ff12-bd34c0d4dc48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Source of Money\n",
              "0    7017\n",
              "1    2983\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Source of Money</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2983</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "df['Source of Money'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FVJFdDDr6wGn"
      },
      "outputs": [],
      "source": [
        "features_to_modify = ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved']\n",
        "\n",
        "def scale_features(df, features):\n",
        "    df_S = df.copy()\n",
        "    scaler = StandardScaler()\n",
        "    df_S[features] = scaler.fit_transform(df[features])\n",
        "    return df_S\n",
        "\n",
        "def normalize_features(df, features):\n",
        "    df_N = df.copy()\n",
        "    scaler = MinMaxScaler()\n",
        "    df_N[features] = scaler.fit_transform(df[features])\n",
        "    return df_N\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvwpmmQE6wGn"
      },
      "source": [
        "### Biased data correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "dKMUlEYs6wGn"
      },
      "outputs": [],
      "source": [
        "def Undersampling(X,Y, test_size):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = test_size, random_state=0)\n",
        "    rus = RandomUnderSampler(random_state=0)\n",
        "    X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
        "    return X_resampled, X_test, y_resampled, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLpCpmLS6wGn"
      },
      "source": [
        "## Feature Selectors (Optional):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR3MQUR16wGn"
      },
      "source": [
        "### Feature selector functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WYkFROWN6wGn"
      },
      "outputs": [],
      "source": [
        "def cor_selector(X, y,num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    cor_list = []\n",
        "    feature_name = X.columns.tolist()\n",
        "    for i in feature_name:\n",
        "        cor = np.corrcoef(X[i], y)[0, 1]\n",
        "        cor_list.append(cor)\n",
        "    #print(np.argsort(np.abs(cor_list)))\n",
        "    cor_list = [0 if np.isnan(i) else i for i in cor_list]\n",
        "    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-num_feats:]].columns.tolist()\n",
        "    #print(cor_feature)\n",
        "    cor_support = [True if i in cor_feature else False for i in feature_name]\n",
        "    # Your code ends here\n",
        "    return cor_support, cor_feature\n",
        "\n",
        "def chi_squared_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    X_norm = MinMaxScaler().fit_transform(X)\n",
        "    chi_selector = SelectKBest(chi2, k=num_feats)\n",
        "    chi_selector.fit(X_norm, y)\n",
        "    chi_support = chi_selector.get_support()\n",
        "    #print(chi_support)\n",
        "    chi_feature = X.loc[:,chi_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return chi_support, chi_feature\n",
        "\n",
        "def rfe_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    rfe_selector = RFE(estimator=LogisticRegression(random_state=42), n_features_to_select=num_feats, step=10, verbose=5)\n",
        "    rfe_selector.fit(X, y)\n",
        "    rfe_support = rfe_selector.support_\n",
        "    rfe_feature = X.loc[:,rfe_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return rfe_support, rfe_feature\n",
        "\n",
        "def embedded_log_reg_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    embedded_lr_selector = SelectFromModel(LogisticRegression(penalty=\"l2\", random_state = 42), max_features=num_feats)\n",
        "    embedded_lr_selector.fit(X, y)\n",
        "    embedded_lr_support = embedded_lr_selector.get_support()\n",
        "    embedded_lr_feature = X.loc[:,embedded_lr_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_lr_support, embedded_lr_feature\n",
        "\n",
        "def embedded_rf_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42), max_features=num_feats)\n",
        "    embeded_rf_selector.fit(X, y)\n",
        "    embedded_rf_support = embeded_rf_selector.get_support()\n",
        "    embedded_rf_feature = X.loc[:,embedded_rf_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_rf_support, embedded_rf_feature\n",
        "\n",
        "def embedded_lgbm_selector(X, y, num_feats):\n",
        "    # Your code goes here (Multiple lines)\n",
        "    lgbc = LGBMClassifier(n_estimators=500, learning_rate=0.05, num_leaves=32, colsample_bytree=0.2, reg_alpha=3, reg_lambda=1, min_split_gain=0.01, min_child_weight=40)\n",
        "    embeded_lgbm_selector = SelectFromModel(lgbc, max_features=num_feats)\n",
        "    embeded_lgbm_selector.fit(X, y)\n",
        "    embedded_lgbm_support = embeded_lgbm_selector.get_support()\n",
        "    embedded_lgbm_feature = X.loc[:,embedded_lgbm_support].columns.tolist()\n",
        "    # Your code ends here\n",
        "    return embedded_lgbm_support, embedded_lgbm_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdq2lh2f6wGn"
      },
      "source": [
        "### Feature Selectors Combined:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "uwkj0SqO6wGn"
      },
      "outputs": [],
      "source": [
        "def autoFeatureSelector(X, y, num_feats, methods=[]):\n",
        "\n",
        "    support_dict = {}\n",
        "\n",
        "    feature_name = list(X.columns)\n",
        "    support_dict['Feature'] = feature_name\n",
        "\n",
        "    if 'pearson' in methods:\n",
        "        cor_support, cor_feature = cor_selector(X, y, num_feats)\n",
        "        support_dict['Pearson'] = cor_support\n",
        "    if 'chi-square' in methods:\n",
        "        chi_support, chi_feature = chi_squared_selector(X, y, num_feats)\n",
        "        support_dict['Chi-2'] = chi_support\n",
        "    if 'rfe' in methods:\n",
        "        rfe_support, rfe_feature = rfe_selector(X, y, num_feats)\n",
        "        support_dict['RFE'] = rfe_support\n",
        "    if 'log-reg' in methods:\n",
        "        embedded_lr_support, embedded_lr_feature = embedded_log_reg_selector(X, y, num_feats)\n",
        "        support_dict['Logistics'] = embedded_lr_support\n",
        "    if 'rf' in methods:\n",
        "        embedded_rf_support, embedded_rf_feature = embedded_rf_selector(X, y, num_feats)\n",
        "        support_dict['Random Forest'] = embedded_rf_support\n",
        "    if 'lgbm' in methods:\n",
        "        embedded_lgbm_support, embedded_lgbm_feature = embedded_lgbm_selector(X, y, num_feats)\n",
        "        support_dict['LightGBM'] = embedded_lgbm_support\n",
        "\n",
        "    # Combine all the above feature list and count the maximum set of features that got selected by all methods\n",
        "\n",
        "    print(\"Combining all methods\")\n",
        "    feature_selection_df = pd.DataFrame(support_dict)\n",
        "    feature_selection_df['Total'] = feature_selection_df.apply(lambda row: np.sum(row[1:].astype(int)), axis=1)\n",
        "    print(\"Sorting features\")\n",
        "    feature_selection_df = feature_selection_df.sort_values(['Total','Feature'] , ascending=False)\n",
        "    feature_selection_df.index = range(1, len(feature_selection_df)+1)\n",
        "    print(\"Selecting best features\")\n",
        "    best_features = feature_selection_df['Feature'].tolist()[:num_feats]\n",
        "    return best_features, feature_selection_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S14DFsWA6wGn"
      },
      "source": [
        "# Models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB2eHh_Q6wGn"
      },
      "source": [
        "- Utilize GridSearchCV to tune the parameters of each of the models.\n",
        "- Check if better results can be obtained for any of the models.\n",
        "- Discuss your observations regarding model performance.\n",
        "- Randomly remove some features (or based on a certain hypothesis) and re-evaluate the models.\n",
        "- Document your observations concerning model performances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPCRye0hI5va"
      },
      "source": [
        "## Logistic Regression: Saif, Dwip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYUwZsu36wGn"
      },
      "source": [
        "### Data for LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "QXrNCYAEuQ1i",
        "outputId": "b317d58c-0b75-43a5-b073-bcca0db39f2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Amount (USD)', 'Person Involved', 'Reported by Authority',\n",
              "       'Source of Money', 'Money Laundering Risk Score',\n",
              "       'Shell Companies Involved', 'Financial Institution', 'Country_China',\n",
              "       'Country_India', 'Country_Russia', 'Country_Singapore',\n",
              "       'Country_South Africa', 'Country_Switzerland', 'Country_UAE',\n",
              "       'Country_UK', 'Country_USA', 'Transaction Type_Cryptocurrency',\n",
              "       'Transaction Type_Offshore Transfer',\n",
              "       'Transaction Type_Property Purchase',\n",
              "       'Transaction Type_Stocks Transfer', 'Industry_Casinos',\n",
              "       'Industry_Construction', 'Industry_Finance', 'Industry_Luxury Goods',\n",
              "       'Industry_Oil & Gas', 'Industry_Real Estate',\n",
              "       'Destination Country_China', 'Destination Country_India',\n",
              "       'Destination Country_Russia', 'Destination Country_Singapore',\n",
              "       'Destination Country_South Africa', 'Destination Country_Switzerland',\n",
              "       'Destination Country_UAE', 'Destination Country_UK',\n",
              "       'Destination Country_USA', 'Tax Haven Country_Cayman Islands',\n",
              "       'Tax Haven Country_Luxembourg', 'Tax Haven Country_Panama',\n",
              "       'Tax Haven Country_Singapore', 'Tax Haven Country_Switzerland'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "# Get the data for Logistic Regression\n",
        "df_LR = df.copy()\n",
        "\n",
        "# Normalize the variables that are greater than 1 as this will affect the models proformnce\n",
        "df_LR = normalize_features(df_LR, ['Amount (USD)', 'Money Laundering Risk Score', 'Shell Companies Involved', 'Financial Institution', 'Person Involved'])\n",
        "\n",
        "df_LR.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0xoujTX6wGn"
      },
      "source": [
        "### Simple LR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "BQVmIY1D6wGn",
        "outputId": "b29c2ba5-dc0c-4c4d-c1df-ade1dfee889c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "[[1279  110]\n",
            " [ 558   53]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.92      0.79      1389\n",
            "           1       0.33      0.09      0.14       611\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.51      0.50      0.46      2000\n",
            "weighted avg       0.58      0.67      0.59      2000\n",
            "\n",
            "Accuracy:  0.666\n"
          ]
        }
      ],
      "source": [
        "# Implement a logistic regression model\n",
        "X = df_LR.drop('Source of Money', axis=1)\n",
        "Y = df_LR['Source of Money']\n",
        "\n",
        "# Create a poly feature data\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "698W25HN6wGo"
      },
      "source": [
        "#### Using Undersampling to train the LR Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vpHJFNju6wGo",
        "outputId": "b748e910-60a3-43eb-be99-5f841674db9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "[[696 710]\n",
            " [298 296]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.50      0.58      1406\n",
            "           1       0.29      0.50      0.37       594\n",
            "\n",
            "    accuracy                           0.50      2000\n",
            "   macro avg       0.50      0.50      0.47      2000\n",
            "weighted avg       0.58      0.50      0.52      2000\n",
            "\n",
            "Accuracy:  0.496\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = Undersampling(X, Y, 0.2)\n",
        "\n",
        "# Fit the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hyOxe4S6wGp"
      },
      "source": [
        "### GridSearchCV LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "o31tkMpM6wGt",
        "outputId": "a77ba8be-033f-40ce-9052-e297d178ccfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "0.5\n",
            "LogisticRegression(C=0.01, penalty='l1', solver='liblinear')\n",
            "Logistic Regression\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "Accuracy:  0.703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameters\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thvgyoTA6wGt"
      },
      "source": [
        "#### Modifying features and testing proformance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "0xs-YANl6wGt",
        "outputId": "657819e1-40c2-47ea-f472-f2650acdbce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}\n",
            "0.5033495688049738\n",
            "LogisticRegression(C=0.1, max_iter=1000, penalty='l1', solver='saga')\n",
            "Logistic Regression\n",
            "[[ 960 1145]\n",
            " [ 389  506]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.46      0.56      2105\n",
            "           1       0.31      0.57      0.40       895\n",
            "\n",
            "    accuracy                           0.49      3000\n",
            "   macro avg       0.51      0.51      0.48      3000\n",
            "weighted avg       0.59      0.49      0.51      3000\n",
            "\n",
            "0.4886666666666667\n"
          ]
        }
      ],
      "source": [
        "df_LR_Mod = df_LR.copy()\n",
        "\n",
        "# Remove the features that are not important\n",
        "df_LR_Mod = df_LR_Mod.drop(['Person Involved', 'Financial Institution'], axis=1)\n",
        "\n",
        "X = df_LR_Mod.drop('Source of Money', axis=1)\n",
        "Y = df_LR_Mod['Source of Money']\n",
        "\n",
        "# Create a poly feature data\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = Undersampling(X_poly, Y, 0.3)\n",
        "\n",
        "# Define the hyperparameters\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'C': [0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=LogisticRegression(max_iter=2000), param_grid=param_grid, cv=5)\n",
        "\n",
        "# Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWV5cWNBI9xr"
      },
      "source": [
        "## Decision Tree: Nitish, Sehaj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qbcv8p46wGt"
      },
      "source": [
        "### Data for DT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_gz9k5t6wGt"
      },
      "outputs": [],
      "source": [
        "df_DT = df.copy()\n",
        "# Drop person involved\n",
        "df_DT.drop('Person Involved', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOj4PAeh6wGt"
      },
      "source": [
        "### DT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x69OpAlw6wGt",
        "outputId": "8f1c21c0-2111-49e9-f1e6-e39f3e1fc437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Accuracy: 0.6715\n"
          ]
        }
      ],
      "source": [
        "# Splitting the data into features (X) and target (y)\n",
        "X = df_DT.drop('Source of Money', axis=1)\n",
        "y = df_DT['Source of Money']\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model training using RandomForestClassifier\n",
        "clf = DecisionTreeClassifier(random_state=42, max_depth=10) # Changed\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy score\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzPVU_PO6wGt",
        "outputId": "a04609c8-94a7-4500-919b-3b4b19fda076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[1307   82]\n",
            " [ 575   36]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.94      0.80      1389\n",
            "           1       0.31      0.06      0.10       611\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.50      0.50      0.45      2000\n",
            "weighted avg       0.58      0.67      0.59      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-g7-N3w6wGu",
        "outputId": "bbe509e5-b1ea-4972-b05b-fb198e1fc51b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "feature_importances = clf.feature_importances_\n",
        "feature_names = X.columns\n",
        "importance_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
        "\n",
        "# Get top 10 features\n",
        "top_features = importance_df.sort_values(by='importance', ascending=False).head(10)\n",
        "\n",
        "#\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='importance', y='feature', data=top_features)\n",
        "plt.title('Top 10 Feature Importances - Optimized Decision Tree')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paUNpLRh6wGu",
        "outputId": "8c7c3bfc-17d3-48b6-a240-aceb5d337f12"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Fit a single decision tree to visualize\n",
        "tree_clf = DecisionTreeClassifier(max_depth=4)  # Limit depth for clarity\n",
        "tree_clf.fit(X_train, y_train)\n",
        "\n",
        "# Plot the decision tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(tree_clf, feature_names=X.columns, class_names=['Illegal', 'Legal'], filled=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMxwAy9DI-Gj"
      },
      "source": [
        "## Random Forest: Egor, Ash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyuPWJkk6wGu"
      },
      "source": [
        "### Data for RF Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHpdHftWuY9G"
      },
      "outputs": [],
      "source": [
        "# Get the data for RF model\n",
        "df_RF = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c1xZqar6wGu"
      },
      "source": [
        "### This needs to be reviewed RF X and Y???"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3_fcJDb6wGu",
        "outputId": "56ebc1c3-1468-4f64-dd98-444dfd98d5fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[ 183 1066]\n",
            " [ 253 1498]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.15      0.22      1249\n",
            "           1       0.58      0.86      0.69      1751\n",
            "\n",
            "    accuracy                           0.56      3000\n",
            "   macro avg       0.50      0.50      0.46      3000\n",
            "weighted avg       0.52      0.56      0.50      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define features (X) and target (y) - binary classification on 'Money Laundering Risk Score'\n",
        "X_new = df_RF.drop(columns=['Money Laundering Risk Score'])\n",
        "y_new = (df_RF['Money Laundering Risk Score'] >= 5).astype(int)  # Binary target: 1 if score >= 5, else 0\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the RandomForest classifier\n",
        "rf_clf_new = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf_new.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_new = rf_clf_new.predict(X_test_new)\n",
        "\n",
        "# Generate the confusion matrix and classification report\n",
        "conf_matrix_new = confusion_matrix(y_test_new, y_pred_new)\n",
        "class_report_new = classification_report(y_test_new, y_pred_new)\n",
        "\n",
        "# Display the results\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix_new)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_XHnycD6wGu",
        "outputId": "2f8fbfc2-18d6-4b09-c2df-78487b527697"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
            "Best Hyperparameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "Confusion Matrix:\n",
            " [[   5 1244]\n",
            " [   4 1747]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.56      0.00      0.01      1249\n",
            "           1       0.58      1.00      0.74      1751\n",
            "\n",
            "    accuracy                           0.58      3000\n",
            "   macro avg       0.57      0.50      0.37      3000\n",
            "weighted avg       0.57      0.58      0.43      3000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define a simplified parameter grid for GridSearchCV\n",
        "simple_param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Initialize the RandomForest model\n",
        "rf_clf_simplified = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Set up the GridSearchCV\n",
        "simple_grid_search = GridSearchCV(estimator=rf_clf_simplified,\n",
        "                                  param_grid=simple_param_grid,\n",
        "                                  cv=3,  # 3-fold cross-validation\n",
        "                                  verbose=1,\n",
        "                                  n_jobs=-1)\n",
        "\n",
        "# Fit the simplified grid search model\n",
        "simple_grid_search.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Best hyperparameters from the grid search\n",
        "best_params_simplified = simple_grid_search.best_params_\n",
        "\n",
        "# Train the best model on the training set\n",
        "best_rf_model_simplified = simple_grid_search.best_estimator_\n",
        "best_rf_model_simplified.fit(X_train_new, y_train_new)\n",
        "\n",
        "# Make predictions with the tuned model\n",
        "y_pred_tuned_simplified = best_rf_model_simplified.predict(X_test_new)\n",
        "\n",
        "# Generate confusion matrix and classification report for the tuned model\n",
        "conf_matrix_tuned_simplified = confusion_matrix(y_test_new, y_pred_tuned_simplified)\n",
        "class_report_tuned_simplified = classification_report(y_test_new, y_pred_tuned_simplified)\n",
        "\n",
        "# Output best parameters, confusion matrix, and classification report\n",
        "print(\"Best Hyperparameters:\", best_params_simplified)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix_tuned_simplified)\n",
        "print(\"Classification Report:\\n\", class_report_tuned_simplified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiehsO8sI-ch"
      },
      "source": [
        "## SGD: Devanshi, James, Abraham"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvQ8IYnXueAV"
      },
      "outputs": [],
      "source": [
        "# Stochastic Gradient Descent\n",
        "from sklearn.pipeline import make_pipeline\n",
        "#SGD\n",
        "df_SGD = df.copy()\n",
        "df_SGD_S = scale_features(df_SGD, features_to_modify)\n",
        "df_SGD_N = normalize_features(df_SGD, features_to_modify)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FSW-W7h6wGu",
        "outputId": "5a450080-df04-416c-9c20-3ba1e413ac48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       1.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.85      0.50      0.41      2000\n",
            "weighted avg       0.79      0.70      0.58      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1406    0]\n",
            " [ 593    1]]\n",
            "Classification Reports:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.32      0.44      1406\n",
            "           1       0.30      0.69      0.42       594\n",
            "\n",
            "    accuracy                           0.43      2000\n",
            "   macro avg       0.50      0.50      0.43      2000\n",
            "weighted avg       0.59      0.43      0.43      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[448 958]\n",
            " [184 410]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"\\nBased on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\\nit has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\\n\""
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_list = [df_SGD_S, df_SGD_N]\n",
        "cr_list = []\n",
        "cm_list = []\n",
        "\n",
        "# Search for the best df for standardization and normalization\n",
        "for i in range(len(df_list)):\n",
        "    df = df_list[i]\n",
        "    X = df.drop('Source of Money', axis=1)\n",
        "    y = df['Source of Money'].values\n",
        "    X_resampled, X_test, y_resampled, y_test = Undersampling(X, y, test_size=0.2)\n",
        "    SGD_classifier = SGDClassifier(random_state=0)\n",
        "    SGD_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred_lr_b = SGD_classifier.predict(X_test)\n",
        "    lr_cr = classification_report(y_test, y_pred_lr_b)\n",
        "    lr_cm = confusion_matrix(y_test, y_pred_lr_b)\n",
        "    cr_list.append(lr_cr)\n",
        "    cm_list.append(lr_cm)\n",
        "\n",
        "# Print the classification reports and confusion matrices\n",
        "for i in range(len(cr_list)):\n",
        "    print(\"Classification Reports:\")\n",
        "    print(cr_list[i])\n",
        "    print('Confusion Matrix:')\n",
        "    print(cm_list[i])\n",
        "\n",
        "'''\n",
        "Based on the results, the best df is df_SGD_N, although it doesn't have performance like the other models,\n",
        "it has a more balanced performance, and not identifying everything as illegal money, which is also more close to real-life application.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eMvTYXz6wGu",
        "outputId": "7c84c9ff-8f40-42d0-e638-9fdade26e588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.2s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.9s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.9s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   1.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   1.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.6s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   1.0s[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.8s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.8s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.4s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.6s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.7s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.2s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.5s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.3s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.3s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
            "540 fits failed out of a total of 2700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "35 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'squared_error', 'modified_huber', 'huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'hinge', 'log_loss', 'perceptron'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "95 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'hinge', 'epsilon_insensitive', 'perceptron', 'squared_error', 'modified_huber', 'log_loss', 'squared_epsilon_insensitive', 'huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "36 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_error', 'squared_hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'modified_huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "77 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_hinge', 'huber', 'epsilon_insensitive', 'perceptron', 'modified_huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "77 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'hinge', 'huber', 'epsilon_insensitive', 'log_loss', 'squared_hinge', 'perceptron', 'squared_epsilon_insensitive', 'squared_error'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "79 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'log_loss', 'huber', 'epsilon_insensitive', 'perceptron', 'squared_error', 'squared_epsilon_insensitive', 'squared_hinge', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "64 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_hinge', 'modified_huber', 'squared_error', 'epsilon_insensitive', 'perceptron', 'squared_epsilon_insensitive', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "77 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'squared_error', 'hinge', 'squared_epsilon_insensitive', 'huber', 'perceptron', 'squared_hinge', 'epsilon_insensitive', 'log_loss'}. Got 'log' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.49728055 0.49728055 0.50083704        nan        nan        nan\n",
            " 0.4962354  0.49937282 0.50083682 0.50041885 0.49979036 0.50000022\n",
            " 0.50104712 0.50146553 0.49790576 0.49937173 0.49979058 0.4991623\n",
            "        nan        nan        nan 0.48828167 0.47990624 0.48723455\n",
            " 0.50418169 0.50397249 0.50397249 0.4966525  0.49916296 0.50020942\n",
            " 0.48450963 0.48262591 0.48639423        nan        nan        nan\n",
            " 0.48995268 0.49330369 0.49204605 0.49204868 0.49100177 0.49037372\n",
            " 0.49497733 0.49016013 0.49874937 0.49728055 0.49728055 0.50083704\n",
            "        nan        nan        nan 0.4962354  0.49937282 0.50083682\n",
            " 0.50041885 0.49979036 0.50000022 0.50104712 0.50146553 0.49790576\n",
            " 0.49330478 0.49120682 0.49706982        nan        nan        nan\n",
            " 0.4939267  0.49560297 0.49727749 0.48325484 0.48598151 0.49393218\n",
            " 0.49958115 0.49937282 0.4981154  0.48702469 0.48576508 0.48200026\n",
            "        nan        nan        nan 0.49225766 0.48974392 0.48932704\n",
            " 0.49769787 0.49811628 0.49811672 0.50251309 0.49267476 0.50356393\n",
            " 0.49728055 0.49728055 0.50083704        nan        nan        nan\n",
            " 0.4962354  0.49937282 0.50083682 0.50041885 0.49979036 0.50000022\n",
            " 0.50104712 0.50146553 0.49790576 0.49979079 0.49204232 0.50020921\n",
            "        nan        nan        nan 0.49958115 0.49979079 0.49874346\n",
            " 0.49790488 0.4953942  0.49267279 0.50230104 0.50083638 0.50271923\n",
            " 0.49016364 0.4861848  0.4893231         nan        nan        nan\n",
            " 0.48974019 0.4918364  0.48430261 0.49246336 0.48828189 0.49623212\n",
            " 0.50104778 0.49016189 0.49790466 0.49393305 0.50313808 0.498536\n",
            "        nan        nan        nan 0.49958115 0.50083704 0.50062762\n",
            " 0.4964468  0.4905849  0.49539946 0.49644351 0.49916274 0.49393043\n",
            " 0.49937173 0.49979058 0.49937173        nan        nan        nan\n",
            " 0.48849088 0.48430152 0.4817915  0.50271616 0.50292558 0.5027166\n",
            " 0.49853096 0.50041841 0.50313917 0.48534689 0.48618568 0.48597516\n",
            "        nan        nan        nan 0.48953668 0.49141646 0.49204758\n",
            " 0.49456242 0.49162961 0.49456242 0.50084033 0.48932989 0.49309382\n",
            " 0.49393305 0.50313808 0.498536          nan        nan        nan\n",
            " 0.49958115 0.50083704 0.50062762 0.4964468  0.4905849  0.49539946\n",
            " 0.49644351 0.49916274 0.49393043 0.4983257  0.48932112 0.49623037\n",
            "        nan        nan        nan 0.4939267  0.49162501 0.49518215\n",
            " 0.49413985 0.4989531  0.48890929 0.5        0.49769634 0.49979101\n",
            " 0.49058227 0.48995159 0.48639444        nan        nan        nan\n",
            " 0.48911345 0.48890556 0.49037219 0.50376744 0.49853513 0.50460361\n",
            " 0.50523451 0.49037569 0.49099915 0.49393305 0.50313808 0.498536\n",
            "        nan        nan        nan 0.49958115 0.50083704 0.50062762\n",
            " 0.4964468  0.4905849  0.49539946 0.49644351 0.49916274 0.49393043\n",
            " 0.5        0.50020942 0.4976959         nan        nan        nan\n",
            " 0.49958115 0.50020833 0.49979123 0.50251046 0.49267279 0.49811715\n",
            " 0.49874521 0.49727793 0.50083726 0.48157988 0.49162567 0.48534908\n",
            "        nan        nan        nan 0.48848518 0.4895334  0.48953405\n",
            " 0.4951839  0.48870096 0.4912112  0.50041819 0.4966536  0.48702381\n",
            " 0.49958115 0.49979058 0.49246117        nan        nan        nan\n",
            " 0.49372078 0.49330215 0.49350676 0.49183816 0.50250936 0.49121032\n",
            " 0.49560341 0.50020942 0.49958137 0.4991623  0.5        0.50041885\n",
            "        nan        nan        nan 0.48995487 0.49811584 0.48430196\n",
            " 0.50334597 0.50208745 0.49811102 0.50188263 0.49832461 0.49916165\n",
            " 0.4945541  0.49832636 0.49204123        nan        nan        nan\n",
            " 0.49204473 0.48848628 0.48911674 0.5016743  0.49246621 0.50146553\n",
            " 0.50272339 0.50062827 0.4964433  0.49958115 0.49979058 0.49246117\n",
            "        nan        nan        nan 0.49372078 0.49330215 0.49350676\n",
            " 0.49183816 0.50250936 0.49121032 0.49560341 0.50020942 0.49958137\n",
            " 0.49853403 0.49518325 0.49015751        nan        nan        nan\n",
            " 0.4939267  0.49664746 0.49162413 0.48598042 0.49163005 0.48932726\n",
            " 0.5008377  0.5        0.49623431 0.48304497 0.50105084 0.48995203\n",
            "        nan        nan        nan 0.48848825 0.49204627 0.48702403\n",
            " 0.49183356 0.49916274 0.48639729 0.50230542 0.49769699 0.4991634\n",
            " 0.49958115 0.49979058 0.49246117        nan        nan        nan\n",
            " 0.49372078 0.49330215 0.49350676 0.49183816 0.50250936 0.49121032\n",
            " 0.49560341 0.50020942 0.49958137 0.50020942 0.5        0.50146444\n",
            "        nan        nan        nan 0.49329843 0.5        0.49811628\n",
            " 0.50209183 0.49644132 0.492673   0.49748932 0.5        0.4993726\n",
            " 0.48932638 0.49979233 0.49120463        nan        nan        nan\n",
            " 0.48807006 0.49037065 0.48513856 0.49036934 0.48744244 0.48807203\n",
            " 0.50481588 0.49832439 0.49979233 0.50104712 0.49979058 0.5\n",
            "        nan        nan        nan 0.494137   0.5        0.49350895\n",
            " 0.49267476 0.50020658 0.49288462 0.49916318 0.5        0.5\n",
            " 0.49979058 0.5        0.5               nan        nan        nan\n",
            " 0.49330347 0.5        0.49811737 0.50439111 0.49455585 0.50103551\n",
            " 0.49665053 0.5        0.5        0.49120266 0.5        0.50146444\n",
            "        nan        nan        nan 0.48932551 0.50377007 0.49016035\n",
            " 0.49267257 0.49142018 0.48995246 0.50021249 0.5        0.49874477\n",
            " 0.50104712 0.49979058 0.5               nan        nan        nan\n",
            " 0.494137   0.5        0.49350895 0.49267476 0.50020658 0.49288462\n",
            " 0.49916318 0.5        0.5        0.49644351 0.5        0.5\n",
            "        nan        nan        nan 0.4939267  0.49979058 0.49811518\n",
            " 0.51255734 0.48953778 0.5117203  0.49979058 0.5        0.50020942\n",
            " 0.49434949 0.5        0.49895397        nan        nan        nan\n",
            " 0.48493067 0.49685864 0.49351092 0.48639423 0.49330412 0.49518456\n",
            " 0.49812154 0.49979058 0.49497777 0.50104712 0.49979058 0.5\n",
            "        nan        nan        nan 0.494137   0.5        0.49350895\n",
            " 0.49267476 0.50020658 0.49288462 0.49916318 0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.49664921 0.5        0.49979058 0.50020942 0.49748954 0.49518325\n",
            " 0.5        0.5        0.5        0.49644176 0.5        0.49895397\n",
            "        nan        nan        nan 0.49057942 0.49686214 0.49372144\n",
            " 0.49016079 0.49225832 0.49162523 0.5008423  0.5        0.5       ]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'alpha': 0.1, 'eta0': 0.1, 'learning_rate': 'invscaling', 'loss': 'squared_hinge', 'penalty': 'l2'}\n",
            "0.5125573397007601\n",
            "SGDClassifier(alpha=0.1, eta0=0.1, learning_rate='invscaling',\n",
            "              loss='squared_hinge', random_state=0)\n",
            "[[ 395 1011]\n",
            " [ 159  435]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.28      0.40      1406\n",
            "           1       0.30      0.73      0.43       594\n",
            "\n",
            "    accuracy                           0.41      2000\n",
            "   macro avg       0.51      0.51      0.41      2000\n",
            "weighted avg       0.59      0.41      0.41      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#grip search for SGD\n",
        "X = df_SGD_N.drop('Source of Money', axis=1)\n",
        "y = df_SGD_N['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
        "    'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "    'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
        "    'eta0': [0.01, 0.1, 1.0]\n",
        "}\n",
        "sgd = SGDClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=sgd,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=2,\n",
        "                           #scoring='precision'\n",
        "                           )\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n",
        "print(grid_search.best_estimator_)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdxFiSw86wGu",
        "outputId": "99bc0ede-6ea6-4c93-d9b1-1af78a2a8cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000879 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000229 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000036 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000312 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000069 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "Fitting estimator with 9 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000203 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000034 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000051 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000223 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000207 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000038 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000035 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000173 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "Fitting estimator with 19 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000047 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000280 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n",
            "Fitting estimator with 39 features.\n",
            "Fitting estimator with 29 features.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 2389, number of negative: 2389\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000049 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 378\n",
            "[LightGBM] [Info] Number of data points in the train set: 4778, number of used features: 39\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
            "Combining all methods\n",
            "Sorting features\n",
            "Selecting best features\n"
          ]
        }
      ],
      "source": [
        "b_features_list = []\n",
        "feature_selection_df_list = []\n",
        "X = df_SGD_N.drop('Source of Money', axis=1)\n",
        "y = df_SGD_N['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "num_feats = 20\n",
        "for num in range(0, num_feats):\n",
        "    methods = ['pearson', 'chi-square', 'rfe', 'log-reg', 'rf', 'lgbm']\n",
        "    best_features, feature_selection_df = autoFeatureSelector(X_resampled, y_resampled, num+1, methods)\n",
        "    b_features_list.append(best_features)\n",
        "    feature_selection_df_list.append(feature_selection_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ2wvlf06wGv",
        "outputId": "e3c83522-df77-4bae-aff0-c366ab018db6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[  41 1365]\n",
            " [  22  572]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.03      0.06      1406\n",
            "           1       0.30      0.96      0.45       594\n",
            "\n",
            "    accuracy                           0.31      2000\n",
            "   macro avg       0.47      0.50      0.25      2000\n",
            "weighted avg       0.55      0.31      0.17      2000\n",
            "\n",
            "[[ 330 1076]\n",
            " [ 137  457]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.23      0.35      1406\n",
            "           1       0.30      0.77      0.43       594\n",
            "\n",
            "    accuracy                           0.39      2000\n",
            "   macro avg       0.50      0.50      0.39      2000\n",
            "weighted avg       0.59      0.39      0.38      2000\n",
            "\n",
            "[[ 244 1162]\n",
            " [ 127  467]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.17      0.27      1406\n",
            "           1       0.29      0.79      0.42       594\n",
            "\n",
            "    accuracy                           0.36      2000\n",
            "   macro avg       0.47      0.48      0.35      2000\n",
            "weighted avg       0.55      0.36      0.32      2000\n",
            "\n",
            "[[698 708]\n",
            " [316 278]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.50      0.58      1406\n",
            "           1       0.28      0.47      0.35       594\n",
            "\n",
            "    accuracy                           0.49      2000\n",
            "   macro avg       0.49      0.48      0.46      2000\n",
            "weighted avg       0.57      0.49      0.51      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "[[1030  376]\n",
            " [ 443  151]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.73      0.72      1406\n",
            "           1       0.29      0.25      0.27       594\n",
            "\n",
            "    accuracy                           0.59      2000\n",
            "   macro avg       0.49      0.49      0.49      2000\n",
            "weighted avg       0.58      0.59      0.58      2000\n",
            "\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "[[1296  110]\n",
            " [ 557   37]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.92      0.80      1406\n",
            "           1       0.25      0.06      0.10       594\n",
            "\n",
            "    accuracy                           0.67      2000\n",
            "   macro avg       0.48      0.49      0.45      2000\n",
            "weighted avg       0.57      0.67      0.59      2000\n",
            "\n",
            "[[1255  151]\n",
            " [ 551   43]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.89      0.78      1406\n",
            "           1       0.22      0.07      0.11       594\n",
            "\n",
            "    accuracy                           0.65      2000\n",
            "   macro avg       0.46      0.48      0.45      2000\n",
            "weighted avg       0.55      0.65      0.58      2000\n",
            "\n",
            "[[ 173 1233]\n",
            " [  80  514]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.12      0.21      1406\n",
            "           1       0.29      0.87      0.44       594\n",
            "\n",
            "    accuracy                           0.34      2000\n",
            "   macro avg       0.49      0.49      0.32      2000\n",
            "weighted avg       0.57      0.34      0.28      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 365 1041]\n",
            " [ 169  425]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.26      0.38      1406\n",
            "           1       0.29      0.72      0.41       594\n",
            "\n",
            "    accuracy                           0.40      2000\n",
            "   macro avg       0.49      0.49      0.39      2000\n",
            "weighted avg       0.57      0.40      0.39      2000\n",
            "\n",
            "[[1405    1]\n",
            " [ 593    1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.50      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.60      0.50      0.41      2000\n",
            "weighted avg       0.64      0.70      0.58      2000\n",
            "\n",
            "[[1132  274]\n",
            " [ 479  115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.81      0.75      1406\n",
            "           1       0.30      0.19      0.23       594\n",
            "\n",
            "    accuracy                           0.62      2000\n",
            "   macro avg       0.50      0.50      0.49      2000\n",
            "weighted avg       0.58      0.62      0.60      2000\n",
            "\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "cm_list = []\n",
        "cr_list = []\n",
        "accuracy_list = []\n",
        "f1_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "for i in range(0,len(b_features_list)):\n",
        "    X = df_SGD_N[b_features_list[i]]\n",
        "    y = df_SGD_N['Source of Money']\n",
        "    X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "    SGD_classifier = SGDClassifier(random_state=0)\n",
        "    SGD_classifier.fit(X_resampled, y_resampled)\n",
        "    y_pred = SGD_classifier.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cr = classification_report(y_test, y_pred)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "\n",
        "    print(cm)\n",
        "    print(cr)\n",
        "    cm_list.append(cm)\n",
        "    cr_list.append(cr)\n",
        "    accuracy_list.append(accuracy)\n",
        "    f1_list.append(f1)\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loV9YnmZ6wGv",
        "outputId": "1818de0f-6603-4c40-eff5-cbb95fb8394b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Model Accuracy\n",
            "2\n",
            "[[1406    0]\n",
            " [ 594    0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.00      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.35      0.50      0.41      2000\n",
            "weighted avg       0.49      0.70      0.58      2000\n",
            "\n",
            "Best Model F1\n",
            "0\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n",
            "Best Model Precision\n",
            "17\n",
            "[[1405    1]\n",
            " [ 593    1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      1.00      0.83      1406\n",
            "           1       0.50      0.00      0.00       594\n",
            "\n",
            "    accuracy                           0.70      2000\n",
            "   macro avg       0.60      0.50      0.41      2000\n",
            "weighted avg       0.64      0.70      0.58      2000\n",
            "\n",
            "Best Model Recall\n",
            "0\n",
            "[[   0 1406]\n",
            " [   0  594]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00      1406\n",
            "           1       0.30      1.00      0.46       594\n",
            "\n",
            "    accuracy                           0.30      2000\n",
            "   macro avg       0.15      0.50      0.23      2000\n",
            "weighted avg       0.09      0.30      0.14      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#select the best model\n",
        "best_model_acc = np.argmax(accuracy_list)\n",
        "best_model_f1 = np.argmax(f1_list)\n",
        "best_model_precision = np.argmax(precision_list)\n",
        "best_model_recall = np.argmax(recall_list)\n",
        "print(\"Best Model Accuracy\")\n",
        "print(best_model_acc)\n",
        "print(cm_list[best_model_acc])\n",
        "print(cr_list[best_model_acc])\n",
        "print(\"Best Model F1\")\n",
        "print(best_model_f1)\n",
        "print(cm_list[best_model_f1])\n",
        "print(cr_list[best_model_f1])\n",
        "print(\"Best Model Precision\")\n",
        "print(best_model_precision)\n",
        "print(cm_list[best_model_precision])\n",
        "print(cr_list[best_model_precision])\n",
        "print(\"Best Model Recall\")\n",
        "print(best_model_recall)\n",
        "print(cm_list[best_model_recall])\n",
        "print(cr_list[best_model_recall])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlHuXkq06wGv",
        "outputId": "19dfb275-0159-4f81-c890-2b124b877846"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#visualize the results\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Total', y='Feature', data=feature_selection_df_list[best_model])\n",
        "plt.title('Feature Selection')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHBoid3O6wGv",
        "outputId": "fdbac012-06a8-4192-88a6-81bca102f0a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 540 candidates, totalling 2700 fits\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.0001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.001, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.1s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.1s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.01, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.01, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=optimal, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=invscaling, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=log, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=0.1, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=optimal, loss=hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=modified_huber, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=squared_hinge, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l2; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=l1; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "[CV] END alpha=0.1, eta0=1.0, learning_rate=adaptive, loss=perceptron, penalty=elasticnet; total time=   0.0s\n",
            "{'alpha': 0.0001, 'eta0': 0.01, 'learning_rate': 'adaptive', 'loss': 'hinge', 'penalty': 'l2'}\n",
            "0.5161131678678613\n",
            "SGDClassifier(eta0=0.01, learning_rate='adaptive', random_state=0)\n",
            "[[ 302 1104]\n",
            " [ 132  462]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.21      0.33      1406\n",
            "           1       0.30      0.78      0.43       594\n",
            "\n",
            "    accuracy                           0.38      2000\n",
            "   macro avg       0.50      0.50      0.38      2000\n",
            "weighted avg       0.58      0.38      0.36      2000\n",
            "\n",
            "0.382\n",
            "0.42777777777777776\n",
            "0.2950191570881226\n",
            "0.7777777777777778\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
            "540 fits failed out of a total of 2700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "81 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'squared_error', 'modified_huber', 'huber', 'squared_epsilon_insensitive', 'epsilon_insensitive', 'hinge', 'log_loss', 'perceptron'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "92 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'squared_hinge', 'hinge', 'epsilon_insensitive', 'perceptron', 'squared_error', 'modified_huber', 'log_loss', 'squared_epsilon_insensitive', 'huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "85 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'squared_error', 'hinge', 'squared_epsilon_insensitive', 'huber', 'perceptron', 'squared_hinge', 'epsilon_insensitive', 'log_loss'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "94 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_hinge', 'modified_huber', 'squared_error', 'epsilon_insensitive', 'perceptron', 'squared_epsilon_insensitive', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "70 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'hinge', 'huber', 'epsilon_insensitive', 'log_loss', 'squared_hinge', 'perceptron', 'squared_epsilon_insensitive', 'squared_error'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "28 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'log_loss', 'huber', 'squared_error', 'squared_hinge', 'epsilon_insensitive', 'squared_epsilon_insensitive', 'perceptron', 'hinge', 'modified_huber'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "19 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'modified_huber', 'log_loss', 'huber', 'epsilon_insensitive', 'perceptron', 'squared_error', 'squared_epsilon_insensitive', 'squared_hinge', 'hinge'}. Got 'log' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "71 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 1467, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 666, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 95, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'loss' parameter of SGDClassifier must be a str among {'hinge', 'squared_hinge', 'huber', 'epsilon_insensitive', 'perceptron', 'modified_huber', 'squared_error', 'log_loss', 'squared_epsilon_insensitive'}. Got 'log' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.5        0.50502092 0.5               nan        nan        nan\n",
            " 0.49937173 0.50104712 0.50146597 0.49937173 0.50104712 0.50146597\n",
            " 0.49623453 0.50125523 0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51443953 0.51443953 0.51443953\n",
            " 0.51443953 0.51443953 0.51443953 0.50272251 0.49979058 0.5\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.50753029 0.49895288 0.49895288 0.5        0.50502092 0.5\n",
            "        nan        nan        nan 0.49937173 0.50104712 0.50146597\n",
            " 0.49937173 0.50104712 0.50146597 0.49623453 0.50125523 0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.49560209 0.49979058 0.49979058 0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.49895354 0.50125654 0.50020942\n",
            " 0.5        0.50502092 0.5               nan        nan        nan\n",
            " 0.49937173 0.50104712 0.50146597 0.49937173 0.50104712 0.50146597\n",
            " 0.49623453 0.50125523 0.5        0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.50899582 0.51192469 0.50878661\n",
            " 0.50899582 0.51192469 0.50878661 0.51192753 0.5        0.51276654\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.51046288 0.5        0.4991623  0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.4991634  0.51192469 0.51192469\n",
            " 0.4991634  0.51192469 0.51192469 0.49623321 0.50020942 0.50125654\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51443953 0.51443953 0.51443953 0.51443953 0.51443953 0.51443953\n",
            " 0.49560209 0.5        0.5        0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.49978926 0.5        0.49895288\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.4991634  0.51192469 0.51192469 0.4991634  0.51192469 0.51192469\n",
            " 0.49623321 0.50020942 0.50125654 0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.50209424 0.5        0.5\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317 0.51611317\n",
            " 0.49916428 0.5        0.50125654 0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.4991634  0.51192469 0.51192469\n",
            " 0.4991634  0.51192469 0.51192469 0.49623321 0.50020942 0.50125654\n",
            " 0.51611317 0.51611317 0.51611317        nan        nan        nan\n",
            " 0.50899582 0.5041841  0.50878661 0.50899582 0.5041841  0.50878661\n",
            " 0.50941664 0.4991623  0.50020942 0.51611317 0.51611317 0.51611317\n",
            "        nan        nan        nan 0.51611317 0.51611317 0.51611317\n",
            " 0.51611317 0.51611317 0.51611317 0.50230301 0.50020942 0.50272251\n",
            " 0.51611317 0.5        0.51213411        nan        nan        nan\n",
            " 0.5        0.5        0.50418848 0.5        0.5        0.50418848\n",
            " 0.4991623  0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51527635 0.50376635 0.51443953\n",
            " 0.51527635 0.50376635 0.51443953 0.50209424 0.5        0.5\n",
            " 0.51611317 0.49895288 0.51255296        nan        nan        nan\n",
            " 0.51611317 0.51255296 0.51611317 0.51611317 0.51255296 0.51611317\n",
            " 0.50418301 0.5        0.5        0.51611317 0.5        0.51213411\n",
            "        nan        nan        nan 0.5        0.5        0.50418848\n",
            " 0.5        0.5        0.50418848 0.4991623  0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.51255296 0.51611317 0.51611317 0.51255296 0.51611317\n",
            " 0.50272251 0.5        0.5        0.50376635 0.5        0.49895288\n",
            "        nan        nan        nan 0.51611317 0.50983329 0.51611317\n",
            " 0.51611317 0.50983329 0.51611317 0.49330193 0.5        0.5\n",
            " 0.51611317 0.5        0.51213411        nan        nan        nan\n",
            " 0.5        0.5        0.50418848 0.5        0.5        0.50418848\n",
            " 0.4991623  0.5        0.5        0.51611317 0.5        0.51611317\n",
            "        nan        nan        nan 0.508159   0.50648536 0.50732218\n",
            " 0.508159   0.50648536 0.50732218 0.50836864 0.5        0.4991623\n",
            " 0.51611317 0.5        0.50460251        nan        nan        nan\n",
            " 0.51611317 0.50292953 0.51611317 0.51611317 0.50669522 0.51611317\n",
            " 0.49937107 0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.5        0.4991623\n",
            " 0.51611317 0.5        0.5        0.4991623  0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51527635 0.5        0.49937173 0.51527635 0.5        0.49937173\n",
            " 0.50272251 0.5        0.5        0.50711275 0.49979058 0.49979058\n",
            "        nan        nan        nan 0.51611317 0.5        0.49937173\n",
            " 0.51611317 0.5        0.49937173 0.50334553 0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.5        0.4991623  0.51611317 0.5        0.5\n",
            " 0.4991623  0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.5        0.49937173\n",
            " 0.51611317 0.5        0.49937173 0.50314136 0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.51611317 0.5        0.50020942 0.51611317 0.5        0.50020942\n",
            " 0.49162501 0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.5        0.4991623\n",
            " 0.51611317 0.5        0.5        0.4991623  0.5        0.5\n",
            " 0.5        0.5        0.5               nan        nan        nan\n",
            " 0.50774059 0.5        0.5        0.50774059 0.5        0.5\n",
            " 0.50188241 0.5        0.5        0.5        0.5        0.5\n",
            "        nan        nan        nan 0.51611317 0.50020942 0.50062827\n",
            " 0.51611317 0.5        0.50020942 0.50041863 0.5        0.5       ]\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "X = df_SGD_N[best_features]\n",
        "y = df_SGD_N['Source of Money']\n",
        "X_resampled, X_test, y_resampled, y_test = Undersampling(X,y,test_size = 0.2)\n",
        "param_grid = {'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        "                'penalty': ['l2', 'l1', 'elasticnet'], 'alpha': [0.0001, 0.001, 0.01, 0.1],\n",
        "                'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
        "                'eta0': [0.01, 0.1, 1.0] }\n",
        "sgd = SGDClassifier(random_state=0)\n",
        "grid_search = GridSearchCV(estimator=sgd,\n",
        "                        param_grid=param_grid,\n",
        "                        cv=5,\n",
        "                        n_jobs=-1,\n",
        "                        verbose=2,\n",
        "                        #scoring='precision'\n",
        "                        )\n",
        "grid_search.fit(X_resampled, y_resampled)\n",
        "#print(grid_search.best_params_)\n",
        "#print(grid_search.best_score_)\n",
        "#print(grid_search.best_estimator_)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cr = classification_report(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "print(cm)\n",
        "print(cr)\n",
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-w0T1cN_6wGv",
        "outputId": "8ec688fb-0559-454a-cc7b-7fd24f451f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 302 1104]\n",
            " [ 132  462]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.21      0.33      1406\n",
            "           1       0.30      0.78      0.43       594\n",
            "\n",
            "    accuracy                           0.38      2000\n",
            "   macro avg       0.50      0.50      0.38      2000\n",
            "weighted avg       0.58      0.38      0.36      2000\n",
            "\n",
            "0.382\n",
            "0.42777777777777776\n",
            "0.2950191570881226\n",
            "0.7777777777777778\n"
          ]
        }
      ],
      "source": [
        "print(cm)\n",
        "print(cr)\n",
        "print(accuracy)\n",
        "print(f1)\n",
        "print(precision)\n",
        "print(recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfLZECBwI-xV"
      },
      "source": [
        "## SVM: Eric, Moosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdXWC5Pr6wGv"
      },
      "source": [
        "### Data for SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XowPRU4muiu-"
      },
      "outputs": [],
      "source": [
        "df_svm = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHBJiJen6wGv"
      },
      "outputs": [],
      "source": [
        "X = df_svm.drop(columns=['Source of Money'])\n",
        "y = df_svm['Source of Money']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz09tJyW6wGv"
      },
      "source": [
        "### Splitting the data for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG5GOJ9i6wGv"
      },
      "outputs": [],
      "source": [
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmlRjiEu6wGv"
      },
      "source": [
        "### SVM Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5UTDI2d6wGv"
      },
      "outputs": [],
      "source": [
        "svc_model = SVC()\n",
        "svc_model.fit(X_train, y_train)\n",
        "y_pred = svc_model.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(f\"SVC Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(classification_report(y_test, y_pred,  zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXveRmRB6wGv"
      },
      "source": [
        "### GridSearchCV (Hyper Parameter tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBFFQc7E6wGv"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter Tuning using GridSearchCV\n",
        "# SVM GridSearchCV params\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'kernel': ['rbf'],\n",
        "    'gamma': [1,0.1,0.01,0.001,0.0001]\n",
        "}\n",
        "grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)\n",
        "grid_svm.fit(X_train, y_train)\n",
        "print(f\"Best SVM Parameters: {grid_svm.best_params_}\")\n",
        "print(f\"Best SVM Accuracy: {grid_svm.best_score_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mTwjwTnqDiD"
      },
      "source": [
        "# Conclusion and comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQg4pZMmIX90"
      },
      "source": [
        "Present your work including approach and findings during the class on September 24th or 26th, 2024. Each group will have a maximum of 15 minutes to present their project. It is advised that your PowerPoint files to be no longer than 15 slides.\n",
        "\n",
        "Prepare a written technical report of no longer than 15 pages to discuss the problem statement, various steps conducted, summary of findings and conclusions. Submit the report and the notebook file (with proper headings, explanatory comments and code sections) by the midnight of September 29th, 2024."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}